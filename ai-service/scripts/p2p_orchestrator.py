#!/usr/bin/env python3
"""Distributed P2P Orchestrator - Self-healing compute cluster for RingRift AI training.

This orchestrator runs on each node in the cluster and:
1. Discovers other nodes via broadcast UDP or known peer list
2. Participates in leader election for coordination tasks
3. Monitors local resources and shares status with peers
4. Auto-starts selfplay/training jobs based on cluster needs
5. Self-heals when nodes go offline or IPs change

Architecture:
- Each node runs this script as a daemon
- Nodes communicate via HTTP REST API (port 8770)
- Leader election uses Bully algorithm (highest node_id wins)
- Heartbeats every 30 seconds detect failures
- Nodes maintain local SQLite state for crash recovery

Usage:
    # On each node:
    python scripts/p2p_orchestrator.py --node-id mac-studio
    python scripts/p2p_orchestrator.py --node-id vast-5090-quad --port 8770

    # With known peers (for cloud nodes without broadcast):
    python scripts/p2p_orchestrator.py --node-id vast-3090 --peers <peer-ip>:8770,<peer-ip>:8770
"""
from __future__ import annotations

# Load .env.local BEFORE app.p2p.constants imports (for SWIM/Raft feature flags)
# This must happen before any app.* imports that read environment variables
def _load_env_local():
    """Load .env.local from script directory or ai-service root."""
    import os as _os
    from pathlib import Path as _Path
    for base in [_Path(__file__).parent.parent, _Path.cwd()]:
        env_file = base / ".env.local"
        if env_file.exists():
            try:
                with open(env_file) as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith("#") and "=" in line:
                            key, _, value = line.partition("=")
                            key = key.strip()
                            value = value.strip().strip('"').strip("'")
                            if key not in _os.environ:  # Don't override existing
                                _os.environ[key] = value
                break
            except (OSError, IOError, UnicodeDecodeError):
                pass  # Skip if .env.local can't be read

_load_env_local()

import argparse
import asyncio
import contextlib

# Python 3.10 compatibility: asyncio.timeout was added in 3.11
# Use a compatibility shim that works with Python 3.10+
try:
    from asyncio import timeout as async_timeout
except ImportError:
    # Python 3.10 fallback using wait_for
    from contextlib import asynccontextmanager

    @asynccontextmanager
    async def async_timeout(delay):
        """Compatibility shim for asyncio.timeout (Python 3.11+)."""
        task = asyncio.current_task()
        loop = asyncio.get_running_loop()

        def cancel_task():
            if task is not None:
                task.cancel()

        handle = loop.call_later(delay, cancel_task)
        try:
            yield
        except asyncio.CancelledError:
            raise asyncio.TimeoutError()
        finally:
            handle.cancel()
import gzip
import importlib
import ipaddress
import json
import os
import secrets
import shutil
import signal
import socket
import sqlite3
import subprocess
import sys

# Safe database connection context manager (December 2025)
try:
    from app.distributed.db_utils import safe_db_connection
except ImportError:
    # Fallback for when db_utils isn't available
    from contextlib import contextmanager as _cm
    @_cm
    def safe_db_connection(db_path, timeout=30):
        conn = sqlite3.connect(str(db_path), timeout=timeout)
        try:
            yield conn
            conn.commit()
        except sqlite3.Error:
            conn.rollback()
            raise
        finally:
            conn.close()
import threading
import time
import uuid
from collections.abc import Generator
from dataclasses import asdict
from datetime import datetime
from pathlib import Path
from typing import TYPE_CHECKING, Any, Callable, Optional
from urllib.parse import urlparse

# P2P Managers - Phase 1 Consolidation (Jan 2026)
from scripts.p2p.managers.quorum_manager import QuorumManager, QuorumConfig

if TYPE_CHECKING:
    from app.coordination.unified_queue_populator import UnifiedQueuePopulator as QueuePopulator
    from app.coordination.p2p_auto_deployer import P2PAutoDeployer
    from scripts.p2p.loops import LoopManager

# =============================================================================
# PRE-FLIGHT DEPENDENCY VALIDATION (January 2026)
# =============================================================================
# Critical dependencies that must be present before P2P startup.
# Failing fast with clear errors prevents cryptic runtime failures.

_CRITICAL_DEPENDENCIES = {
    "aiohttp": "HTTP server and client functionality",
    "psutil": "Process and system monitoring",
    "yaml": "Configuration file parsing",
}

_OPTIONAL_DEPENDENCIES = {
    "prometheus_client": "Metrics export (optional)",
    "paramiko": "SSH connections for remote operations",
}


def _validate_preflight_dependencies() -> tuple[bool, list[str]]:
    """Validate critical dependencies are available before startup.

    Returns:
        Tuple of (all_ok, list of error messages)
    """
    errors = []
    warnings = []

    # Check critical dependencies
    for module_name, purpose in _CRITICAL_DEPENDENCIES.items():
        try:
            importlib.import_module(module_name)
        except ImportError:
            errors.append(f"CRITICAL: Missing '{module_name}' - required for {purpose}")
            errors.append(f"  Fix: pip install {module_name}")

    # Check optional dependencies (warn only)
    for module_name, purpose in _OPTIONAL_DEPENDENCIES.items():
        try:
            importlib.import_module(module_name)
        except ImportError:
            warnings.append(f"Optional: Missing '{module_name}' - {purpose}")

    # Log warnings
    for warn in warnings:
        print(f"[P2P] {warn}", file=sys.stderr)

    return len(errors) == 0, errors


# =============================================================================
# Work queue for centralized work distribution (lazy import to avoid circular deps)
_work_queue = None
def get_work_queue():
    """Get the work queue singleton (lazy load)."""
    global _work_queue
    if _work_queue is None:
        try:
            from app.coordination.work_queue import get_work_queue as _get_wq
            _work_queue = _get_wq()
        except ImportError:
            _work_queue = None
    return _work_queue

# Automation managers (lazy imports to avoid circular deps)
_health_manager = None  # December 2025: Consolidated from recovery_manager
_predictive_alerts = None
# Dec 2025: Removed unused _tier_calibrator global (never used)
# Dec 28, 2025: Removed unused get_auto_scaler() - never called

def get_health_manager():
    """Get the health manager singleton (lazy load).

    December 2025: Consolidated from get_recovery_manager().
    Uses UnifiedHealthManager which combines recovery + error coordination.
    """
    global _health_manager
    if _health_manager is None:
        try:
            from app.coordination.unified_health_manager import (
                get_health_manager as _get_uhm,
            )
            _health_manager = _get_uhm()
        except ImportError:
            _health_manager = None
    return _health_manager


# Job Reaper Daemon (leader-only, kills stuck jobs and reassigns work)
_job_reaper = None
def get_job_reaper(work_queue=None, ssh_config=None):
    """Get the job reaper singleton (lazy load).

    The JobReaperDaemon enforces job timeouts by:
    1. Detecting jobs past their timeout
    2. Killing stuck processes via SSH
    3. Marking jobs as TIMEOUT
    4. Reassigning failed work to other nodes
    5. Blacklisting nodes that repeatedly fail
    """
    global _job_reaper
    if _job_reaper is None and work_queue is not None:
        try:
            from app.coordination.job_reaper import JobReaperDaemon
            _job_reaper = JobReaperDaemon(
                work_queue=work_queue,
                ssh_config=ssh_config,
            )
        except ImportError as e:
            logger.warning(f"JobReaperDaemon not available: {e}")
            _job_reaper = None
    return _job_reaper

def get_predictive_alerts():
    """Get the predictive alerts manager (lazy load)."""
    global _predictive_alerts
    if _predictive_alerts is None:
        try:
            from app.monitoring.predictive_alerts import PredictiveAlertManager
            _predictive_alerts = PredictiveAlertManager()
        except ImportError:
            _predictive_alerts = None
    return _predictive_alerts

# Dec 2025: Removed unused get_tier_calibrator() function


# SWIM membership manager for leaderless gossip-based membership
_swim_manager = None
SWIM_AVAILABLE = False

# Jan 22, 2026: SWIM callback registration for state synchronization.
# Problem: SWIM adapter has callbacks but they were never wired to orchestrator.
# SWIM detects failures at 90s but never syncs state to gossip layer.
# Solution: Register callbacks BEFORE get_swim_manager() creates the manager.
_swim_on_member_alive: Callable[[str], None] | None = None
_swim_on_member_failed: Callable[[str], None] | None = None


def set_swim_callbacks(
    on_alive: Callable[[str], None] | None = None,
    on_failed: Callable[[str], None] | None = None,
) -> None:
    """Register SWIM membership callbacks before get_swim_manager().

    Jan 22, 2026: Wire SWIM failure detection to gossip layer.

    Must be called BEFORE get_swim_manager() to ensure callbacks are set
    during manager creation. If manager already exists, sets callbacks directly.

    Args:
        on_alive: Callback when a member becomes alive (member_id: str)
        on_failed: Callback when a member fails (member_id: str)
    """
    global _swim_on_member_alive, _swim_on_member_failed
    _swim_on_member_alive = on_alive
    _swim_on_member_failed = on_failed

    # If manager already exists, set callbacks directly
    if _swim_manager is not None:
        _swim_manager.on_member_alive = on_alive
        _swim_manager.on_member_failed = on_failed
        logger.info("SWIM callbacks registered on existing manager")


def get_swim_manager(node_id: str | None = None, bind_port: int = 7947):
    """Get the SWIM membership manager singleton (lazy load).

    SWIM (Scalable Weakly-consistent Infection-style Membership) provides:
    - O(1) message complexity per node (constant bandwidth)
    - Failure detection in <5 seconds (vs 60+ seconds with heartbeat-based)
    - No single leader required - truly distributed
    - Suspicion mechanism to reduce false positives

    Args:
        node_id: Node identifier (required for first initialization)
        bind_port: UDP port for SWIM protocol (default 7947)

    Returns:
        SwimMembershipManager instance or None if swim-p2p not installed
    """
    global _swim_manager, SWIM_AVAILABLE
    if _swim_manager is None and node_id is not None:
        try:
            from app.p2p.swim_adapter import SwimMembershipManager, SWIM_AVAILABLE as _swim_avail
            SWIM_AVAILABLE = _swim_avail
            if SWIM_AVAILABLE:
                _swim_manager = SwimMembershipManager.from_distributed_hosts(
                    node_id=node_id,
                    bind_port=bind_port,
                )
                # Jan 22, 2026: Wire SWIM callbacks registered via set_swim_callbacks()
                if _swim_on_member_alive is not None:
                    _swim_manager.on_member_alive = _swim_on_member_alive
                if _swim_on_member_failed is not None:
                    _swim_manager.on_member_failed = _swim_on_member_failed
                callback_status = "with callbacks" if (_swim_on_member_alive or _swim_on_member_failed) else "no callbacks"
                logger.info(f"SWIM membership manager initialized for {node_id} ({callback_status})")
            else:
                logger.warning("swim-p2p not installed - using HTTP heartbeats only")
        except ImportError as e:
            logger.warning(f"SWIM adapter not available: {e}")
            _swim_manager = None
    return _swim_manager


# Dead Peer Cooldown Manager (Jan 2026)
# Adaptive cooldown with probe-based early recovery
_dead_peer_cooldown_manager = None


def get_dead_peer_cooldown_manager():
    """Get the dead peer cooldown manager singleton (lazy load).

    The DeadPeerCooldownManager replaces the static 1-hour cooldown with:
    - Tiered cooldowns (30s -> 2min -> 10min -> 30min) based on failure frequency
    - Probe-based early recovery when gossip reports a dead node might be alive
    - Prevents 25-40% node loss from brief network blips
    """
    global _dead_peer_cooldown_manager
    if _dead_peer_cooldown_manager is None:
        try:
            from scripts.p2p.dead_peer_recovery import DeadPeerCooldownManager
            _dead_peer_cooldown_manager = DeadPeerCooldownManager()
            logger.info("DeadPeerCooldownManager initialized with adaptive cooldown")
        except ImportError as e:
            logger.warning(f"DeadPeerCooldownManager not available: {e}")
            _dead_peer_cooldown_manager = None
    return _dead_peer_cooldown_manager


# ============================================
# Phase 4: Extracted Background Loops (Dec 2025)
# ============================================
# These loops are extracted from the monolithic orchestrator for modularity.
# They use dependency injection via callbacks for testability.

# Feature flag for gradual rollout
EXTRACTED_LOOPS_ENABLED = os.environ.get("RINGRIFT_EXTRACTED_LOOPS", "true").lower() in ("true", "1", "yes")
JOB_REAPER_FALLBACK_ENABLED = os.environ.get("RINGRIFT_JOB_REAPER_FALLBACK_ENABLED", "true").lower() in ("true", "1", "yes")

# Lazy import to avoid circular dependencies
_loop_manager_instance = None
_loop_classes_loaded = False


def _load_loop_classes():
    """Lazy-load loop classes to avoid import-time dependencies."""
    global _loop_classes_loaded
    if _loop_classes_loaded:
        return True
    try:
        from scripts.p2p.loops import (
            LoopManager,
            QueuePopulatorLoop,
            EloSyncLoop,
            ModelSyncLoop,
            DataAggregationLoop,
            IpDiscoveryLoop,
            TailscaleRecoveryLoop,
            TailscalePeerDiscoveryLoop,
            FollowerDiscoveryLoop,
            AutoScalingLoop,
            HealthAggregationLoop,
            JobReaperLoop,
            IdleDetectionLoop,
            UdpDiscoveryLoop,
            SplitBrainDetectionLoop,
            QuorumCrisisDiscoveryLoop,
            QuorumCrisisConfig,
        )
        _loop_classes_loaded = True
        return True
    except ImportError as e:
        logger.error(f"[LoopManager] CRITICAL: Extracted loops import failed: {e}")
        logger.error("[LoopManager] WorkerPullLoop will NOT start - workers won't claim work!")
        return False


def get_loop_manager() -> "LoopManager | None":
    """Get or create the global LoopManager singleton.

    Returns None if extracted loops are disabled or unavailable.
    """
    global _loop_manager_instance
    if not EXTRACTED_LOOPS_ENABLED:
        return None
    if _loop_manager_instance is None:
        if not _load_loop_classes():
            return None
        try:
            from scripts.p2p.loops import LoopManager
            _loop_manager_instance = LoopManager(name="p2p_loops")
            logger.info("LoopManager: initialized for extracted background loops")
        except (ImportError, TypeError, ValueError, AttributeError) as e:
            # ImportError: loops module not available
            # TypeError: wrong constructor signature
            # ValueError: invalid argument
            # AttributeError: LoopManager not found in module
            logger.error(f"LoopManager: failed to initialize: {e}")
            return None
    return _loop_manager_instance


# Board priority overrides from unified_loop.yaml
# 0=CRITICAL, 1=HIGH, 2=MEDIUM, 3=LOW (lower value = higher priority)
_board_priority_cache: dict[str, int] | None = None
_board_priority_cache_time: float = 0


def get_board_priority_overrides() -> dict[str, int]:
    """Load board priority overrides from config, cached for 60 seconds.

    Returns dict mapping config keys (e.g., 'hexagonal_2p') to priority levels.
    Priority levels: 0=CRITICAL, 1=HIGH, 2=MEDIUM, 3=LOW
    """
    global _board_priority_cache, _board_priority_cache_time
    now = time.time()

    # Return cached value if fresh (60 second TTL)
    if _board_priority_cache is not None and now - _board_priority_cache_time < 60:
        return _board_priority_cache

    try:
        import yaml
        config_path = Path(__file__).parent.parent / "config" / "unified_loop.yaml"
        if config_path.exists():
            with open(config_path) as f:
                yaml_config = yaml.safe_load(f)
            selfplay_config = yaml_config.get("selfplay", {})
            overrides = selfplay_config.get("board_priority_overrides", {})
            # Convert config keys like "hexagonal_2p" -> priority int
            _board_priority_cache = {k: int(v) for k, v in overrides.items()}
            _board_priority_cache_time = now
            return _board_priority_cache
    except (OSError, ValueError, AttributeError, ImportError):
        pass

    # Default: empty (no overrides)
    return {}


# =============================================================================
# P2P Event Emission Helpers (December 2025 - CRITICAL gap fix)
# =============================================================================
# These helpers safely emit events for P2P lifecycle changes. Events enable:
# - LeadershipCoordinator to track leader changes
# - UnifiedHealthManager to respond to node failures
# - Cluster-wide coordination on membership changes

_p2p_event_emitters_available: bool | None = None
_p2p_event_emitters_last_check: float = 0.0
_P2P_EMITTER_CACHE_TTL: float = 30.0  # Retry every 30 seconds if failed


def _check_event_emitters() -> bool:
    """Check if event emitters are available (cached with TTL for retries).

    December 27, 2025: Fixed bug where negative result was cached permanently.
    Now retries every 30 seconds if event system becomes available later.
    """
    global _p2p_event_emitters_available, _p2p_event_emitters_last_check
    import time

    now = time.time()

    # Use cached positive result indefinitely
    if _p2p_event_emitters_available is True:
        return True

    # For negative results, retry after TTL expires
    if _p2p_event_emitters_available is False:
        if now - _p2p_event_emitters_last_check < _P2P_EMITTER_CACHE_TTL:
            return False
        # TTL expired, retry below

    try:
        from app.coordination.event_router import (
            emit_host_online,
            emit_host_offline,
            emit_leader_elected,
        )
        _p2p_event_emitters_available = True
        _p2p_event_emitters_last_check = now
        return True
    except ImportError:
        _p2p_event_emitters_available = False
        _p2p_event_emitters_last_check = now
        return False


# December 28, 2025: Module-level emit functions (27 methods, ~911 LOC) were moved to
# EventEmissionMixin in scripts/p2p/event_emission_mixin.py.
# P2POrchestrator now inherits from EventEmissionMixin and uses self._emit_* methods.
# See scripts/p2p/__init__.py for the mixin export.


# Add project root to path for scripts.lib imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scripts.lib.file_formats import open_jsonl_file
from scripts.lib.logging_config import setup_script_logging
from scripts.lib.process import (
    SingletonLock,
    find_processes_by_pattern,
    kill_process,
    is_process_running,
)

logger = setup_script_logging("p2p_orchestrator")

# Singleton lock for duplicate process prevention (December 2025)
_P2P_LOCK: SingletonLock | None = None


def _validate_p2p_dependencies() -> None:
    """Pre-flight check for required modules. Exits with code 2 if missing.

    This catches import errors early with a clear message, rather than
    failing deep in the call stack with confusing tracebacks.
    """
    required_modules = [
        ("aiohttp", "pip install aiohttp"),
        ("psutil", "pip install psutil"),
        ("yaml", "pip install pyyaml"),
    ]
    missing = []
    for module_name, install_hint in required_modules:
        try:
            __import__(module_name)
        except ImportError:
            missing.append(f"{module_name} ({install_hint})")

    if missing:
        # Use print since logger may not be fully initialized
        print(f"CRITICAL: Missing required dependencies: {', '.join(missing)}", file=sys.stderr)
        print("Run: pip install -r requirements.txt", file=sys.stderr)
        sys.exit(2)  # Exit code 2 = missing dependencies


# Validate dependencies before any heavy imports
_validate_p2p_dependencies()


@contextlib.contextmanager
def db_connection(db_path: str | Path, timeout: float = 30.0) -> Generator[sqlite3.Connection]:
    """Context manager for SQLite connections to prevent leaks.

    Usage:
        with db_connection(db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(...)
    """
    conn = None
    try:
        conn = sqlite3.connect(str(db_path), timeout=timeout)
        yield conn
    finally:
        if conn:
            with contextlib.suppress(Exception):
                conn.close()


# =============================================================================
# Async subprocess helper - Jan 19, 2026
# Prevents blocking the event loop during subprocess operations
# =============================================================================

async def async_subprocess_run(
    cmd: list[str],
    cwd: str | Path | None = None,
    timeout: float = 30.0,
    capture_output: bool = True,
    text: bool = True,
    env: dict | None = None,
) -> subprocess.CompletedProcess:
    """Run subprocess in thread pool to avoid blocking the event loop.

    This is a drop-in replacement for subprocess.run() in async contexts.
    Wraps the blocking subprocess.run() call in asyncio.to_thread().

    Args:
        cmd: Command and arguments to run
        cwd: Working directory for the command
        timeout: Timeout in seconds (default 30)
        capture_output: Capture stdout/stderr (default True)
        text: Return text instead of bytes (default True)
        env: Environment variables (default None = inherit)

    Returns:
        CompletedProcess with returncode, stdout, stderr

    Example:
        result = await async_subprocess_run(["git", "status"], cwd="/path")
        if result.returncode == 0:
            print(result.stdout)
    """
    def _run():
        return subprocess.run(
            cmd,
            cwd=cwd,
            timeout=timeout,
            capture_output=capture_output,
            text=text,
            env=env,
        )

    return await asyncio.to_thread(_run)


async def async_db_query(
    db_path: str | Path,
    query: str,
    params: tuple = (),
    timeout: float = 30.0,
    fetch_all: bool = True,
) -> list | None:
    """Run SQLite query in thread pool to avoid blocking the event loop.

    Args:
        db_path: Path to SQLite database
        query: SQL query to execute
        params: Query parameters (default empty)
        timeout: Connection timeout in seconds (default 30)
        fetch_all: If True, return all rows; if False, return one row

    Returns:
        Query results or None on error
    """
    def _query():
        with db_connection(db_path, timeout=timeout) as conn:
            cursor = conn.cursor()
            cursor.execute(query, params)
            if fetch_all:
                return cursor.fetchall()
            return cursor.fetchone()

    try:
        return await asyncio.to_thread(_query)
    except Exception as e:
        logger.warning(f"[async_db_query] Query failed on {db_path}: {e}")
        return None


async def async_db_execute(
    db_path: str | Path,
    query: str,
    params: tuple = (),
    timeout: float = 30.0,
) -> bool:
    """Execute SQLite write query in thread pool.

    Args:
        db_path: Path to SQLite database
        query: SQL query to execute (INSERT, UPDATE, DELETE)
        params: Query parameters (default empty)
        timeout: Connection timeout in seconds (default 30)

    Returns:
        True if successful, False on error
    """
    def _execute():
        with db_connection(db_path, timeout=timeout) as conn:
            cursor = conn.cursor()
            cursor.execute(query, params)
            conn.commit()
            return True

    try:
        return await asyncio.to_thread(_execute)
    except Exception as e:
        logger.warning(f"[async_db_execute] Execute failed on {db_path}: {e}")
        return False


# Centralized ramdrive utilities for auto-detection
# Shared database integrity utilities
from app.db.integrity import (
    check_and_repair_databases,
)

# Circuit breaker for fault-tolerant network operations
from app.distributed.circuit_breaker import (
    CircuitState,
    get_circuit_registry,
)
# Jan 2026: Adaptive budget selection based on config Elo
from app.coordination.budget_calculator import (
    get_adaptive_budget_for_elo,
)
from app.utils.ramdrive import (
    RamdriveSyncer,
    get_system_resources,
    log_storage_recommendation,
    should_use_ramdrive,
)
from scripts.p2p.cluster_config import (
    get_cluster_config,
    get_webhook_urls,
)
from scripts.p2p.handlers import (
    ABTestHandlersMixin,
    AdminHandlersMixin,
    CanonicalGateHandlersMixin,
    CMAESHandlersMixin,
    DeliveryHandlersMixin,
    ElectionHandlersMixin,
    EloSyncHandlersMixin,
    GauntletHandlersMixin,
    GossipHandlersMixin,
    ImprovementHandlersMixin,
    JobsApiHandlersMixin,
    MetricsHandlersMixin,  # January 2026 - P2P Modularization (Prometheus metrics)
    SelfplayHandlersMixin,  # January 2026 - P2P Modularization (Selfplay API)
    ClusterApiHandlersMixin,  # January 2026 - P2P Modularization (Cluster API)
    DashboardHandlersMixin,  # January 2026 - P2P Modularization (Dashboard)
    RecoveryHandlersMixin,  # January 2026 - P2P Modularization Phase 2b (Rollback)
    ConfigurationHandlersMixin,  # January 2026 - P2P Modularization Phase 2c (Config/Registration)
    TrainingControlHandlersMixin,  # January 2026 - P2P Modularization Phase 3a (Training Control)
    EloAnalyticsHandlersMixin,  # January 2026 - P2P Modularization Phase 4a (Elo Analytics)
    EvaluationPlayHandlersMixin,  # January 2026 - P2P Modularization Phase 5a (Elo Match Play)
    EventManagementHandlersMixin,  # January 2026 - P2P Modularization Phase 5b (Event Subscriptions)
    StatusHandlersMixin,  # January 2026 - P2P Modularization Phase 6a (Status/Health/Loops)
    ModelHandlersMixin,  # January 2026 - Comprehensive Model Evaluation Pipeline
    VoterConfigHandlersMixin,  # January 2026 - Consensus-safe voter config sync
    RegistryHandlersMixin,
    ManifestHandlersMixin,
    RelayHandlersMixin,
    SSHTournamentHandlersMixin,
    SyncHandlersMixin,
    TableHandlersMixin,
    TournamentHandlersMixin,
    WorkQueueHandlersMixin,
    setup_model_routes,  # January 2026 - Model inventory route setup
)
from scripts.p2p.network_utils import NetworkUtilsMixin
from scripts.p2p.peer_manager import PeerManagerMixin
from scripts.p2p.leader_election import LeaderElectionMixin
from scripts.p2p.gossip_protocol import GossipProtocolMixin  # Contains merged GossipMetricsMixin (Dec 28, 2025)

# Phase 5: SWIM + Raft integration mixins (Dec 26, 2025)
from scripts.p2p.membership_mixin import MembershipMixin
from scripts.p2p.consensus_mixin import ConsensusMixin
from scripts.p2p.handlers.swim import SwimHandlersMixin
from scripts.p2p.handlers.raft import RaftHandlersMixin
from scripts.p2p.handlers.network_health import NetworkHealthMixin, setup_network_health_routes

# Leadership mixins for voter/quorum monitoring and state transitions (Jan 2026)
from scripts.p2p.mixins import (
    AdvertiseValidationMixin,
    LeadershipHealthMixin,
    LeadershipTransitionsMixin,
)

# Import constants from the refactored module (Phase 2 refactoring - consolidated)
from scripts.p2p.constants import (
    ADVERTISE_HOST_ENV,
    ADVERTISE_PORT_ENV,
    AGENT_MODE_ENABLED,
    ARBITER_URL,
    # Auth and build info
    AUTH_TOKEN_ENV,
    AUTH_TOKEN_FILE_ENV,
    AUTO_ASSIGN_ENABLED,
    AUTO_TRAINING_THRESHOLD_MB,
    AUTO_UPDATE_ENABLED,
    AUTO_WORK_BATCH_SIZE,
    BUILD_VERSION_ENV,
    COORDINATOR_URL,
    DATA_MANAGEMENT_INTERVAL,
    DB_EXPORT_THRESHOLD_MB,
    # Network configuration
    DEFAULT_PORT,
    DISCOVERY_INTERVAL,
    DISCOVERY_PORT,
    DISK_CLEANUP_THRESHOLD,
    # Resource thresholds
    DISK_CRITICAL_THRESHOLD,
    DISK_WARNING_THRESHOLD,
    # Dynamic voter management
    DYNAMIC_VOTER_ENABLED,
    DYNAMIC_VOTER_MAX_QUORUM,
    DYNAMIC_VOTER_MIN,
    DYNAMIC_VOTER_TARGET,
    ELECTION_TIMEOUT,
    ELO_K_FACTOR,
    GH200_MAX_SELFPLAY,
    GH200_MIN_SELFPLAY,
    GIT_BRANCH_NAME,
    GIT_REMOTE_NAME,
    # Auto-update settings
    GIT_UPDATE_CHECK_INTERVAL,
    # Safeguards
    GPU_IDLE_RESTART_TIMEOUT,
    GPU_IDLE_THRESHOLD,
    GPU_POWER_RANKINGS,
    GRACEFUL_SHUTDOWN_BEFORE_UPDATE,
    HEARTBEAT_INTERVAL,
    # Connection robustness
    HTTP_CONNECT_TIMEOUT,
    HTTP_TOTAL_TIMEOUT,
    IDLE_CHECK_INTERVAL,
    IDLE_GPU_THRESHOLD,
    IDLE_GRACE_PERIOD,
    # Elo constants (from app.config.thresholds)
    BASELINE_ELO_RANDOM,  # Random AI pinned at 400 Elo
    INITIAL_ELO_RATING,
    JOB_CHECK_INTERVAL,
    LEADER_DEGRADED_STEPDOWN_DELAY,
    LEADER_HEALTH_CHECK_INTERVAL,
    LEADER_LEASE_DURATION,
    LEADER_LEASE_RENEW_INTERVAL,
    LEADER_MIN_RESPONSE_RATE,
    LEADERLESS_TRAINING_TIMEOUT,
    LEADER_WORK_DISPATCH_TIMEOUT,
    # Leader stickiness (Jan 2, 2026)
    INCUMBENT_LEADER_GRACE_PERIOD,
    RECENT_LEADER_WINDOW,
    # Probabilistic fallback leadership (Jan 1, 2026)
    PROVISIONAL_LEADER_MIN_LEADERLESS_TIME,
    PROVISIONAL_LEADER_INITIAL_PROBABILITY,
    PROVISIONAL_LEADER_MAX_PROBABILITY,
    PROVISIONAL_LEADER_PROBABILITY_GROWTH_RATE,
    PROVISIONAL_LEADER_QUORUM_TIMEOUT,
    PROVISIONAL_LEADER_CHECK_INTERVAL,
    # Jan 2026: ULSM tiered fallback
    ELECTION_RETRY_COUNT_BEFORE_PROVISIONAL,
    DETERMINISTIC_FALLBACK_TIME,
    LOAD_AVERAGE_MAX_MULTIPLIER,
    LOAD_MAX_FOR_NEW_JOBS,
    MANIFEST_JSONL_LINECOUNT_CHUNK_BYTES,
    # Data management
    MANIFEST_JSONL_LINECOUNT_MAX_BYTES,
    MANIFEST_JSONL_SAMPLE_BYTES,
    MAX_CONCURRENT_EXPORTS,
    MAX_CONSECUTIVE_FAILURES,
    MAX_DISK_USAGE_PERCENT,
    MAX_GAUNTLET_RUNTIME,
    # Stale process cleanup
    MAX_SELFPLAY_RUNTIME,
    MAX_TOURNAMENT_RUNTIME,
    MAX_TRAINING_RUNTIME,
    MEMORY_CRITICAL_THRESHOLD,
    MEMORY_WARNING_THRESHOLD,
    MIN_GAMES_FOR_SYNC,
    MIN_MEMORY_GB_FOR_TASKS,
    MODEL_SYNC_INTERVAL,
    NAT_BLOCKED_PROBE_INTERVAL,
    NAT_BLOCKED_PROBE_TIMEOUT,
    NAT_BLOCKED_RECOVERY_TIMEOUT,
    NAT_EXTERNAL_IP_CACHE_TTL,
    NAT_HOLE_PUNCH_RETRY_COUNT,
    # NAT/Relay settings
    NAT_INBOUND_HEARTBEAT_STALE_SECONDS,
    NAT_RELAY_PREFERENCE_THRESHOLD,
    NAT_STUN_LIKE_PROBE_INTERVAL,
    NAT_SYMMETRIC_DETECTION_ENABLED,
    P2P_DATA_SYNC_BASE,
    P2P_DATA_SYNC_MAX,
    P2P_DATA_SYNC_MIN,
    P2P_MODEL_SYNC_BASE,
    P2P_MODEL_SYNC_MAX,
    P2P_MODEL_SYNC_MIN,
    P2P_SYNC_BACKOFF_FACTOR,
    P2P_SYNC_SPEEDUP_FACTOR,
    P2P_TRAINING_DB_SYNC_BASE,
    P2P_TRAINING_DB_SYNC_MAX,
    P2P_TRAINING_DB_SYNC_MIN,
    PEER_BOOTSTRAP_INTERVAL,
    PEER_BOOTSTRAP_MIN_PEERS,
    PEER_DEATH_RATE_LIMIT,
    PEER_PURGE_AFTER_SECONDS,
    PEER_RECOVERY_RETRY_INTERVAL,
    PEER_RETIRE_AFTER_SECONDS,
    PEER_TIMEOUT,
    PEER_TIMEOUT_JITTER_FACTOR,
    get_jittered_peer_timeout,
    get_cpu_adaptive_timeout,
    CPU_LOAD_HIGH_THRESHOLD,
    RELAY_COMMAND_MAX_ATTEMPTS,
    RELAY_COMMAND_MAX_BATCH,
    RELAY_COMMAND_TTL_SECONDS,
    RELAY_HEARTBEAT_INTERVAL,
    RELAY_MAX_PENDING_START_JOBS,
    RETRY_DEAD_NODE_INTERVAL,
    RETRY_RETIRED_NODE_INTERVAL,
    RUNAWAY_SELFPLAY_PROCESS_THRESHOLD,
    SPAWN_RATE_LIMIT_PER_MINUTE,
    STALE_PROCESS_CHECK_INTERVAL,
    STARTUP_GRACE_PERIOD,
    ELECTION_PARTICIPATION_DELAY,
    STALE_PROCESS_PATTERNS,
    STARTUP_JSONL_GRACE_PERIOD_SECONDS,
    # State directory
    STATE_DIR,
    TAILSCALE_CGNAT_NETWORK,
    TARGET_GPU_UTIL_MAX,
    # GPU configuration
    TARGET_GPU_UTIL_MIN,
    TRAINING_DATA_SYNC_THRESHOLD_MB,
    # Training node sync
    TRAINING_NODE_COUNT,
    TRAINING_SYNC_INTERVAL,
    # Unified inventory / Idle detection
    UNIFIED_DISCOVERY_INTERVAL,
    VOTER_DEMOTION_FAILURES,
    VOTER_HEALTH_THRESHOLD,
    VOTER_HEARTBEAT_INTERVAL,
    VOTER_HEARTBEAT_TIMEOUT,
    VOTER_MESH_REFRESH_INTERVAL,
    VOTER_MIN_QUORUM,
    VOTER_NAT_RECOVERY_AGGRESSIVE,
    VOTER_PROMOTION_UPTIME,
    # Phase 26: Multi-seed bootstrap and mesh resilience
    BOOTSTRAP_SEEDS,
    MIN_BOOTSTRAP_ATTEMPTS,
    ISOLATED_BOOTSTRAP_INTERVAL,
    MIN_CONNECTED_PEERS,
    # Phase 28: Gossip protocol
    GOSSIP_FANOUT,
    GOSSIP_INTERVAL,
    GOSSIP_MAX_PEER_ENDPOINTS,
    # Phase 27: Peer cache
    PEER_CACHE_TTL_SECONDS,
    PEER_CACHE_MAX_ENTRIES,
    PEER_REPUTATION_ALPHA,
    # Phase 29: Cluster epochs
    INITIAL_CLUSTER_EPOCH,
)
from scripts.p2p.models import (
    ClusterDataManifest,
    ClusterJob,
    ClusterSyncPlan,
    DataFileInfo,
    DataSyncJob,
    DistributedCMAESState,
    DistributedTournamentState,
    ImprovementLoopState,
    NodeDataManifest,
    NodeInfo,
    PeerCircuitBreaker,  # Jan 3, 2026: Sprint 10+ P2P hardening
    PeerHealthScore,     # Jan 3, 2026: Sprint 10+ P2P hardening
    SSHTournamentRun,
    TrainingJob,
    TrainingThresholds,
)
from scripts.p2p.p2p_mixin_base import SubscriptionRetryConfig
from scripts.p2p.network import (
    JobSnapshot,  # Jan 12, 2026: Lock-free job reads
    NonBlockingAsyncLockWrapper,
    PeerSnapshot,  # Jan 12, 2026: Lock-free peer reads
    TimeoutAsyncLockWrapper,
    get_client_session,
)

# Import refactored utilities (Phase 2 refactoring)
from scripts.p2p.resource_utils import (
    check_disk_has_capacity,
)

# Import refactored P2P types and models
# These were extracted from this file for modularity (Phase 1 refactoring)
from scripts.p2p.types import JobType, NodeRole
from scripts.p2p.utils import (
    safe_json_response,
    systemd_notify_ready,
    systemd_notify_watchdog,
)
from scripts.p2p.managers import (
    AnalyticsCacheConfig,
    AnalyticsCacheManager,
    CMAESConfig,
    CMAESCoordinator,
    DataSyncCoordinator,
    DataSyncCoordinatorConfig,
    JobManager,
    JobOrchestrationConfig,
    JobOrchestrationManager,
    NodeSelector,
    SelfplayScheduler,
    StateManager,
    SyncPlanner,
    SyncPlannerConfig,
    TrainingCoordinator,
    create_analytics_cache_manager,
    create_cmaes_coordinator,
    create_data_sync_coordinator,
    create_job_orchestration_manager,
)
from scripts.p2p.managers.state_manager import PersistedLeaderState
from scripts.p2p.managers.voter_config_manager import (
    get_voter_config_manager,
    VoterConfigManager,
)
from scripts.p2p.managers.work_discovery_manager import (
    _is_selfplay_enabled_for_node,
    _is_training_enabled_for_node,
    set_selfplay_disabled_override,
)
from scripts.p2p.metrics_manager import MetricsManager
from scripts.p2p.query_builders import PeerQueryBuilder
from scripts.p2p.resource_detector import ResourceDetector, ResourceDetectorMixin
from scripts.p2p.config.selfplay_job_configs import (
    DIVERSE_PROFILES,
    SELFPLAY_CONFIGS,
    get_diverse_profile_weights,
    get_filtered_configs,
    get_unique_configs,
    get_weighted_configs,
    select_diverse_profiles,
)
from scripts.p2p.job_spawner import (
    GUMBEL_ENGINE_MODES,
    SELFPLAY_ENGINE_MODES,
)
from scripts.p2p.event_emission_mixin import EventEmissionMixin
from scripts.p2p.failover_integration import FailoverIntegrationMixin
from scripts.p2p.relay_leader_propagator import RelayLeaderPropagatorMixin  # Phase 1: NAT-blocked leader propagation (Jan 4, 2026)
from scripts.p2p.leadership_state_machine import (
    LeadershipStateMachine,
    LeaderState,
    TransitionReason,
)

# Unified resource checking utilities (80% max utilization)
# Includes graceful degradation for dynamic workload management
try:
    from app.utils.resource_guard import (
        LIMITS as RESOURCE_LIMITS,
        OperationPriority,
        check_cpu as unified_check_cpu,
        check_disk_space as unified_check_disk,
        check_memory as unified_check_memory,
        get_degradation_level,
        should_proceed_with_priority,
    )
    HAS_RESOURCE_GUARD = True
except ImportError:
    HAS_RESOURCE_GUARD = False
    unified_check_disk = None
    unified_check_memory = None
    unified_check_cpu = None
    RESOURCE_LIMITS = None
    should_proceed_with_priority = None
    OperationPriority = None
    get_degradation_level = None

# ELO database sync manager for cluster-wide consistency
try:
    from app.tournament.elo_sync_manager import (
        EloSyncManager,
        ensure_elo_synced,
        get_elo_sync_manager,
        sync_elo_after_games,
    )
    HAS_ELO_SYNC = True
except ImportError:
    HAS_ELO_SYNC = False
    EloSyncManager = None
    get_elo_sync_manager = None
    sync_elo_after_games = None
    ensure_elo_synced = None

# Distributed data sync manager for model/data distribution
# Prefer new sync_coordinator, fallback to deprecated data_sync
try:
    from app.distributed.sync_coordinator import SyncCoordinator, full_cluster_sync
    HAS_SYNC_COORDINATOR = True

    def get_sync_coordinator():
        return SyncCoordinator.get_instance()
except ImportError:
    HAS_SYNC_COORDINATOR = False
    SyncCoordinator = None
    full_cluster_sync = None

# SyncRouter: Intelligent data routing with quality-based priority (December 2025)
try:
    from app.coordination.sync_router import get_sync_router, SyncRouter
    HAS_SYNC_ROUTER = True
except ImportError:
    HAS_SYNC_ROUTER = False
    get_sync_router = None
    SyncRouter = None

# Phase 3.1: Curriculum weights integration for selfplay prioritization
try:
    from scripts.unified_loop.curriculum import load_curriculum_weights
    HAS_CURRICULUM_WEIGHTS = True
except ImportError:
    HAS_CURRICULUM_WEIGHTS = False
    load_curriculum_weights = None

# Unified node inventory for multi-CLI discovery (Vast, Tailscale, Lambda, Hetzner)
try:
    from app.coordination.unified_inventory import UnifiedInventory, get_inventory
    HAS_UNIFIED_INVENTORY = True
except ImportError:
    HAS_UNIFIED_INVENTORY = False
    UnifiedInventory = None
    get_inventory = None

# HTTP server imports
try:
    import aiohttp
    from aiohttp import ClientSession, ClientTimeout, web
    HAS_AIOHTTP = True
except ImportError:
    HAS_AIOHTTP = False
    aiohttp = None
    logger.warning("aiohttp not installed. Install with: pip install aiohttp")

# SOCKS proxy support for userspace Tailscale networking
try:
    from aiohttp_socks import ProxyConnector
    HAS_SOCKS = True
except ImportError:
    HAS_SOCKS = False
    ProxyConnector = None

# Get SOCKS proxy from environment (e.g., socks5://localhost:1055)
SOCKS_PROXY = os.environ.get("RINGRIFT_SOCKS_PROXY", "")


# =============================================================================
# HTTP Handler Timeout Decorator (December 30, 2025)
# =============================================================================
# Added to fix P2P cluster connectivity issues where HTTP handlers blocked
# indefinitely on slow operations (lock acquisition, daemon status collection).

def with_request_timeout(timeout_seconds: float = 20.0):
    """Decorator to add timeout protection to HTTP handlers.

    December 30, 2025: Added to prevent HTTP endpoints from blocking indefinitely.
    January 10, 2026: Increased default from 10s to 20s to exceed typical lock wait
    times (reduced from 5s to 2s for gossip locks, but other operations can take longer).

    Usage:
        @with_request_timeout(5.0)
        async def handle_health(self, request):
            ...

    Args:
        timeout_seconds: Maximum time in seconds for handler to complete.

    Returns:
        Decorated handler that returns 504 Gateway Timeout on timeout.
    """
    import functools

    def decorator(handler):
        @functools.wraps(handler)
        async def wrapper(self_or_request, *args, **kwargs):
            # Handle both bound methods (self, request) and plain functions (request)
            try:
                return await asyncio.wait_for(
                    handler(self_or_request, *args, **kwargs),
                    timeout=timeout_seconds
                )
            except asyncio.TimeoutError:
                # Return 504 Gateway Timeout with details
                return web.json_response(
                    {
                        "error": "Request timed out",
                        "timeout_seconds": timeout_seconds,
                        "timestamp": time.time(),
                    },
                    status=504
                )
        return wrapper
    return decorator


# Systemd watchdog support for service health monitoring
# When running under systemd with WatchdogSec set, we need to periodically
# notify systemd that the service is healthy. If we miss the deadline,
# systemd will restart the service.
try:
    import sdnotify
    SYSTEMD_NOTIFIER = sdnotify.SystemdNotifier()
    HAS_SYSTEMD = True
except ImportError:
    SYSTEMD_NOTIFIER = None
    HAS_SYSTEMD = False


# ============================================
# Utilities (Refactored - Phase 2)
# ============================================
# The following utilities have been moved to scripts/p2p/ for modularity:
# - systemd_notify_watchdog, systemd_notify_ready (scripts/p2p/utils.py)
# - AsyncLockWrapper, get_client_session (scripts/p2p/network.py)
# - check_peer_circuit, record_peer_success, record_peer_failure (scripts/p2p/network.py)
# - peer_request (scripts/p2p/network.py)
# - get_disk_usage_percent, check_disk_has_capacity, check_all_resources (scripts/p2p/resource.py)
#
# They are imported at the top of this file for backward compatibility.
# ============================================

# Dynamic host registry for IP auto-update
try:
    from app.distributed.dynamic_registry import (
        NodeState,
        get_registry,
    )
    HAS_DYNAMIC_REGISTRY = True
except ImportError:
    HAS_DYNAMIC_REGISTRY = False
    get_registry = None
    NodeState = None

# Hybrid transport layer for HTTP/SSH fallback (self-healing Vast connectivity)
try:
    from app.distributed.hybrid_transport import (
        HybridTransport,
        diagnose_node_connectivity,
        get_hybrid_transport,
    )
    from app.distributed.ssh_transport import (
        SSHTransport,
        get_ssh_transport,
        probe_vast_nodes_via_ssh,
    )
    HAS_HYBRID_TRANSPORT = True
except ImportError:
    HAS_HYBRID_TRANSPORT = False
    HybridTransport = None
    get_hybrid_transport = None
    diagnose_node_connectivity = None
    SSHTransport = None
    get_ssh_transport = None
    probe_vast_nodes_via_ssh = None

# Improvement cycle manager for automated training
# Note: ImprovementCycleManager is deprecated - unified_ai_loop.py is the new approach
# Kept for backwards compatibility with older scripts
try:
    from scripts.improvement_cycle_manager import ImprovementCycleManager
    HAS_IMPROVEMENT_MANAGER = True
except ImportError:
    # Fallback - deprecated archive location removed in 2025-12
    HAS_IMPROVEMENT_MANAGER = False
    ImprovementCycleManager = None

# Task coordination safeguards - prevents runaway spawning
try:
    from app.coordination.safeguards import Safeguards, check_before_spawn
    HAS_SAFEGUARDS = True
    _safeguards = Safeguards.get_instance()
except ImportError:
    HAS_SAFEGUARDS = False
    _safeguards = None
    def check_before_spawn(task_type, node_id):
        return True, ""

# New coordination features: OrchestratorRole, backpressure, sync_lock, bandwidth
try:
    from app.coordination import (
        NodeResources,
        # Orchestrator role management (SQLite-backed with heartbeat)
        OrchestratorRole,
        # Queue backpressure
        QueueType,
        # Resource optimizer for cluster-wide PID-controlled optimization
        ResourceOptimizer,
        TransferPriority,
        acquire_orchestrator_role,
        get_cluster_utilization,
        get_host_targets,
        get_optimal_concurrency,
        get_resource_optimizer,
        # Resource targets for unified utilization management
        get_resource_targets,
        get_target_job_count,
        get_throttle_factor,
        record_utilization,
        release_bandwidth,
        release_orchestrator_role,
        # Bandwidth management
        request_bandwidth,
        should_scale_down,
        should_scale_up,
        should_stop_production,
        should_throttle_production,
        # Sync mutex for data transfer coordination
        sync_lock,
    )

    # Import rate negotiation functions for cooperative utilization (60-80% target)
    from app.coordination.resource_optimizer import (
        apply_feedback_adjustment,
        get_config_weights,
        get_current_selfplay_rate,
        get_hybrid_selfplay_limits,
        get_max_cpu_only_selfplay,
        # Hardware-aware selfplay limits (single source of truth)
        get_max_selfplay_for_node,
        get_utilization_status,
        negotiate_selfplay_rate,
        update_config_weights,
    )
    HAS_RATE_NEGOTIATION = True
    HAS_NEW_COORDINATION = True
    HAS_HW_AWARE_LIMITS = True
    # Get targets from unified source
    _unified_targets = get_resource_targets()
except ImportError:
    HAS_NEW_COORDINATION = False
    HAS_RATE_NEGOTIATION = False
    HAS_HW_AWARE_LIMITS = False
    OrchestratorRole = None
    _unified_targets = None
    negotiate_selfplay_rate = None
    get_current_selfplay_rate = None
    apply_feedback_adjustment = None
    get_utilization_status = None
    update_config_weights = None
    get_config_weights = None
    get_max_selfplay_for_node = None
    get_hybrid_selfplay_limits = None
    get_max_cpu_only_selfplay = None

# P2P-integrated monitoring management
try:
    from app.monitoring.p2p_monitoring import MonitoringManager
    HAS_P2P_MONITORING = True
except ImportError:
    HAS_P2P_MONITORING = False
    MonitoringManager = None

# Model sync across cluster
try:
    from scripts.sync_models import (
        HOSTS_MODULE_AVAILABLE as HAS_HOSTS_FOR_SYNC,
        ClusterModelState,
        scan_cluster as scan_cluster_models,
        sync_missing_models,
    )
    # Also import load_remote_hosts for scanning
    if HAS_HOSTS_FOR_SYNC:
        from app.distributed.hosts import filter_ready_hosts, load_remote_hosts
    HAS_MODEL_SYNC = True
except ImportError:
    HAS_MODEL_SYNC = False
    scan_cluster_models = None
    sync_missing_models = None
    ClusterModelState = None
    HAS_HOSTS_FOR_SYNC = False
    load_remote_hosts = None
    filter_ready_hosts = None

# PFSP (Prioritized Fictitious Self-Play) opponent pool
try:
    from app.training.advanced_training import (
        CMAESAutoTuner,
        OpponentStats,
        PFSPOpponentPool,
        PlateauConfig,
    )
    HAS_PFSP = True
except ImportError:
    HAS_PFSP = False
    PFSPOpponentPool = None
    OpponentStats = None
    CMAESAutoTuner = None
    PlateauConfig = None

# Configuration: See scripts/p2p/constants.py
# Types: See scripts/p2p/types.py and scripts/p2p/models.py


class WebhookNotifier:
    """Sends alerts to Slack/Discord webhooks for important events.

    Configure via environment variables:
    - RINGRIFT_SLACK_WEBHOOK: Slack incoming webhook URL
    - RINGRIFT_DISCORD_WEBHOOK: Discord webhook URL
    - RINGRIFT_ALERT_LEVEL: Minimum level to alert (debug/info/warning/error) default: warning
    """

    LEVELS = {"debug": 0, "info": 1, "warning": 2, "error": 3}

    def __init__(self):
        # Try environment variables first, then fall back to cluster.yaml
        self.slack_webhook = os.environ.get("RINGRIFT_SLACK_WEBHOOK", "")
        self.discord_webhook = os.environ.get("RINGRIFT_DISCORD_WEBHOOK", "")

        # Fall back to cluster.yaml config if env vars not set
        if not self.slack_webhook or not self.discord_webhook:
            try:
                yaml_webhooks = get_webhook_urls()
                if not self.slack_webhook and "slack" in yaml_webhooks:
                    self.slack_webhook = yaml_webhooks["slack"]
                if not self.discord_webhook and "discord" in yaml_webhooks:
                    self.discord_webhook = yaml_webhooks["discord"]
            except (KeyError, IndexError, AttributeError):
                pass  # Ignore config loading errors

        self.min_level = self.LEVELS.get(
            os.environ.get("RINGRIFT_ALERT_LEVEL", "warning").lower(), 2
        )
        self._session: ClientSession | None = None
        self._last_alert: dict[str, float] = {}  # Throttle repeated alerts
        self._throttle_seconds = 300  # 5 minutes between duplicate alerts

    async def _get_session(self) -> ClientSession:
        if self._session is None or self._session.closed:
            self._session = ClientSession(timeout=ClientTimeout(total=10))
        return self._session

    async def close(self) -> None:
        """Close the HTTP session to prevent memory leaks.

        December 2025: Added to fix memory leak from unclosed sessions.
        """
        if self._session is not None and not self._session.closed:
            await self._session.close()
            self._session = None

    def close_sync(self) -> None:
        """Synchronously close the HTTP session (for finally blocks)."""
        if self._session is not None and not self._session.closed:
            try:
                loop = asyncio.new_event_loop()
                loop.run_until_complete(self._session.close())
                loop.close()
            except (RuntimeError, OSError, asyncio.CancelledError) as e:
                # Dec 2025: Narrowed from bare Exception; best effort cleanup
                logger.debug(f"HTTP session close failed (best effort): {e}")
            self._session = None

    def _should_throttle(self, alert_key: str) -> bool:
        """Check if this alert should be throttled (duplicate within window)."""
        now = time.time()
        if alert_key in self._last_alert and now - self._last_alert[alert_key] < self._throttle_seconds:
            return True
        self._last_alert[alert_key] = now
        return False

    async def send(
        self,
        title: str,
        message: str = "",
        level: str = "warning",
        fields: dict[str, str] | None = None,
        node_id: str = "",
        # Aliases for backward compatibility (December 28, 2025)
        severity: str | None = None,
        context: dict[str, str] | None = None,
    ):
        """Send an alert to configured webhooks.

        Args:
            title: Alert title/subject (or message if message not provided)
            message: Alert body text
            level: debug/info/warning/error
            fields: Additional key-value pairs to include
            node_id: Node ID for deduplication
            severity: Alias for level (backward compatibility)
            context: Alias for fields (backward compatibility)

        December 28, 2025: Added severity and context aliases to fix API mismatch
        with callers using the alternative parameter names.
        """
        # Handle aliases - severity takes precedence if provided
        if severity is not None:
            level = severity
        if context is not None:
            fields = context
        # If message is empty, use title as message (for single-arg callers)
        if not message:
            message = title
            title = "RingRift Alert"

        if self.LEVELS.get(level, 2) < self.min_level:
            return

        if not self.slack_webhook and not self.discord_webhook:
            return

        # Throttle duplicate alerts
        alert_key = f"{title}:{node_id}"
        if self._should_throttle(alert_key):
            return

        try:
            session = await self._get_session()

            # Color based on level
            colors = {"debug": "#808080", "info": "#36a64f", "warning": "#ff9800", "error": "#ff0000"}
            color = colors.get(level, "#808080")

            # Send to Slack
            if self.slack_webhook:
                slack_fields = []
                if fields:
                    for k, v in fields.items():
                        slack_fields.append({"title": k, "value": str(v), "short": True})

                slack_payload = {
                    "attachments": [{
                        "color": color,
                        "title": f"[{level.upper()}] {title}",
                        "text": message,
                        "fields": slack_fields,
                        "footer": f"RingRift AI | {node_id}" if node_id else "RingRift AI",
                        "ts": int(time.time()),
                    }]
                }
                try:
                    async with session.post(self.slack_webhook, json=slack_payload) as resp:
                        if resp.status != 200:
                            logger.warning(f"[Webhook] Slack alert failed: {resp.status}")
                except Exception as e:  # noqa: BLE001
                    logger.error(f"[Webhook] Slack error: {e}")

            # Send to Discord
            if self.discord_webhook:
                discord_fields = []
                if fields:
                    for k, v in fields.items():
                        discord_fields.append({"name": k, "value": str(v), "inline": True})

                discord_payload = {
                    "embeds": [{
                        "title": f"[{level.upper()}] {title}",
                        "description": message,
                        "color": int(color.lstrip("#"), 16),
                        "fields": discord_fields,
                        "footer": {"text": f"RingRift AI | {node_id}" if node_id else "RingRift AI"},
                        "timestamp": datetime.utcnow().isoformat(),
                    }]
                }
                try:
                    async with session.post(self.discord_webhook, json=discord_payload) as resp:
                        if resp.status not in (200, 204):
                            logger.warning(f"[Webhook] Discord alert failed: {resp.status}")
                except Exception as e:  # noqa: BLE001
                    logger.error(f"[Webhook] Discord error: {e}")

        except Exception as e:  # noqa: BLE001
            logger.error(f"[Webhook] Alert send error: {e}")

    # Dec 28, 2025: Removed duplicate close() method (was lines 2031-2033)
    # The proper close() is defined at line 1898 with docstring and session=None cleanup


class P2POrchestrator(
    WorkQueueHandlersMixin,
    ElectionHandlersMixin,
    RelayHandlersMixin,
    GauntletHandlersMixin,
    GossipHandlersMixin,
    AdminHandlersMixin,
    EloSyncHandlersMixin,
    TournamentHandlersMixin,
    CMAESHandlersMixin,
    SSHTournamentHandlersMixin,
    DeliveryHandlersMixin,  # Phase 3: Delivery verification (Dec 27, 2025)
    SyncHandlersMixin,      # Phase 8: Sync handlers extraction (Dec 28, 2025)
    TableHandlersMixin,     # Phase 8: Table/dashboard handlers extraction (Dec 28, 2025)
    RegistryHandlersMixin,  # Phase 8: Registry handlers extraction (Dec 28, 2025)
    ManifestHandlersMixin,  # Phase 8: Manifest handlers extraction (Dec 28, 2025)
    ABTestHandlersMixin,    # Phase 8: A/B test handlers extraction (Dec 28, 2025)
    ImprovementHandlersMixin,  # Phase 8: Improvement loop handlers extraction (Dec 28, 2025)
    CanonicalGateHandlersMixin,  # Phase 8: Canonical gate handlers extraction (Dec 28, 2025)
    JobsApiHandlersMixin,        # Phase 8: Jobs API handlers extraction (Dec 28, 2025)
    MetricsHandlersMixin,        # Prometheus metrics export (Jan 2026 - P2P Modularization)
    SelfplayHandlersMixin,       # Selfplay API endpoints (Jan 2026 - P2P Modularization)
    ClusterApiHandlersMixin,     # Cluster API endpoints (Jan 2026 - P2P Modularization)
    DashboardHandlersMixin,      # Dashboard endpoints (Jan 2026 - P2P Modularization)
    RecoveryHandlersMixin,       # Rollback endpoints (Jan 2026 - P2P Modularization Phase 2b)
    ConfigurationHandlersMixin,  # Config/Registration (Jan 2026 - P2P Modularization Phase 2c)
    TrainingControlHandlersMixin,  # Training Control (Jan 2026 - P2P Modularization Phase 3a)
    EloAnalyticsHandlersMixin,   # Elo Analytics (Jan 2026 - P2P Modularization Phase 4a)
    EvaluationPlayHandlersMixin,  # Elo Match Play (Jan 2026 - P2P Modularization Phase 5a)
    EventManagementHandlersMixin,  # Event Subscriptions (Jan 2026 - P2P Modularization Phase 5b)
    StatusHandlersMixin,         # Status/Health/Loops (Jan 2026 - P2P Modularization Phase 6a)
    ModelHandlersMixin,          # Model inventory endpoints (Jan 2026 - Comprehensive Eval Pipeline)
    NetworkHealthMixin,          # Network health endpoints (Dec 30, 2025)
    NetworkUtilsMixin,
    PeerManagerMixin,
    LeaderElectionMixin,
    GossipProtocolMixin,  # Provides gossip protocol + metrics (merged Dec 28, 2025)
    # Phase 5: SWIM + Raft integration (Dec 26, 2025)
    MembershipMixin,      # SWIM gossip-based membership
    ConsensusMixin,       # PySyncObj Raft consensus
    SwimHandlersMixin,    # /swim/* HTTP handlers
    RaftHandlersMixin,    # /raft/* HTTP handlers
    ResourceDetectorMixin,  # Resource detection delegation (Dec 28, 2025)
    RelayLeaderPropagatorMixin,  # NAT-blocked leader propagation via gossip (Jan 4, 2026 - Phase 1)
    EventEmissionMixin,     # Event emission consolidation (Dec 28, 2025 - Phase 8)
    FailoverIntegrationMixin,  # Multi-layer transport failover (Dec 30, 2025 - Phase 9)
    VoterConfigHandlersMixin,  # Voter config sync (Jan 20, 2026 - Consensus-safe config sync)
    LeadershipHealthMixin,    # Voter/quorum health monitoring (Jan 26, 2026)
    LeadershipTransitionsMixin,  # Step-down and state transitions (Jan 26, 2026)
    AdvertiseValidationMixin,    # IP validation and advertise host management (Jan 26, 2026)
):
    """Main P2P orchestrator class that runs on each node.

    Inherits from:
    - WorkQueueHandlersMixin: Work queue HTTP handlers (handle_work_*)
    - ElectionHandlersMixin: Leader election handlers (handle_election*, handle_lease*, handle_voter*)
    - RelayHandlersMixin: NAT relay handlers (handle_relay_*)
    - GauntletHandlersMixin: Gauntlet evaluation handlers (handle_gauntlet_*)
    - GossipHandlersMixin: Gossip protocol handlers (handle_gossip*)
    - AdminHandlersMixin: Admin and git handlers (handle_git_*, handle_admin_*)
    - EloSyncHandlersMixin: Elo sync handlers (handle_elo_sync_*)
    - TournamentHandlersMixin: Tournament handlers (handle_tournament_*)
    - CMAESHandlersMixin: CMA-ES optimization handlers (handle_cmaes_*)
    - SSHTournamentHandlersMixin: SSH tournament handlers (handle_ssh_tournament_*)
    - NetworkUtilsMixin: Peer address parsing, URL building, Tailscale detection
    - PeerManagerMixin: Peer discovery, reputation tracking, cache management
    - RelayLeaderPropagatorMixin: NAT-blocked leader propagation via gossip (Jan 4, 2026)
    """

    def __init__(
        self,
        node_id: str,
        host: str = "0.0.0.0",
        port: int = DEFAULT_PORT,
        known_peers: list[str] | None = None,
        relay_peers: list[str] | None = None,
        ringrift_path: str | None = None,
        advertise_host: str | None = None,
        advertise_port: int | None = None,
        auth_token: str | None = None,
        require_auth: bool = False,
        storage_type: str = "auto",  # "disk", "ramdrive", or "auto"
        sync_to_disk_interval: int = 300,  # Sync ramdrive to disk every N seconds
    ):
        self.node_id = node_id
        self.host = host
        self.port = port

        # Phase 26: Multi-seed bootstrap - merge CLI peers with hardcoded seeds
        # Priority: CLI peers first, then hardcoded seeds
        # Shuffle seeds to distribute load across bootstrap attempts
        import random
        cli_peers = known_peers or []
        merged_seeds = list(cli_peers)  # CLI peers have highest priority
        for seed in BOOTSTRAP_SEEDS:
            if seed not in merged_seeds:
                merged_seeds.append(seed)
        # Shuffle only the hardcoded portion to avoid overloading any single seed
        if len(merged_seeds) > len(cli_peers):
            hardcoded_portion = merged_seeds[len(cli_peers):]
            random.shuffle(hardcoded_portion)
            merged_seeds = merged_seeds[:len(cli_peers)] + hardcoded_portion
        self.known_peers = merged_seeds
        self.bootstrap_seeds = list(BOOTSTRAP_SEEDS)  # Store original for reference
        logger.info(f"Bootstrap seeds: {len(cli_peers)} CLI + {len(BOOTSTRAP_SEEDS)} hardcoded = {len(self.known_peers)} total")

        # Peers that should always receive relay heartbeats (for NAT-blocked nodes)
        self.relay_peers: set[str] = set(relay_peers or [])
        self.ringrift_path = ringrift_path or self._detect_ringrift_path()

        # Jan 5, 2026: Force relay mode for NAT-blocked nodes
        # NAT-blocked nodes should send ALL outbound heartbeats via relay to ensure
        # other nodes can discover them (since direct inbound connections fail).
        # This is loaded from distributed_hosts.yaml: `nat_blocked: true` or `force_relay_mode: true`
        self._force_relay_mode: bool = self._load_force_relay_mode()

        # Phase 29: Cluster epoch tracking for split-brain resolution
        self._cluster_epoch: int = INITIAL_CLUSTER_EPOCH
        # P2P Health state tracking (Dec 2025)
        self._cluster_health_degraded: bool = False
        # Gossip-learned peer endpoints (Phase 28)
        self._gossip_learned_endpoints: dict[str, dict[str, Any]] = {}

        # Phase 2.4 (Dec 29, 2025): Partition read-only mode
        # When in minority partition, pause job dispatch to prevent data divergence
        self._partition_readonly_mode: bool = False
        self._partition_readonly_since: float = 0.0
        self._last_partition_check: float = 0.0
        self._partition_check_interval: float = 30.0  # Check every 30 seconds

        # Storage configuration: "disk", "ramdrive", or "auto" (detected)
        self.sync_to_disk_interval = sync_to_disk_interval
        self.ramdrive_path = "/dev/shm/ringrift/data"  # Standard ramdrive location
        self.ramdrive_syncer: RamdriveSyncer | None = None

        # Resolve "auto" storage type based on system resources
        if storage_type == "auto":
            resources = get_system_resources()
            if should_use_ramdrive():
                self.storage_type = "ramdrive"
                logger.info(f"Auto-detected storage: RAMDRIVE "
                      f"(RAM: {resources.total_ram_gb:.0f}GB, "
                      f"Disk: {resources.free_disk_gb:.0f}GB free / {resources.disk_usage_percent:.0f}% used)")
            else:
                self.storage_type = "disk"
                logger.info(f"Auto-detected storage: DISK "
                      f"(RAM: {resources.total_ram_gb:.0f}GB, "
                      f"Disk: {resources.free_disk_gb:.0f}GB free / {resources.disk_usage_percent:.0f}% used)")
            log_storage_recommendation()
        else:
            self.storage_type = storage_type
        # Git 2.35+ enforces safe.directory for repos with different ownership.
        # Many nodes run the orchestrator as root against a checkout owned by
        # another user (e.g. ubuntu), so always provide a safe.directory override
        # for all git operations.
        self._git_safe_directory = os.path.abspath(self.ringrift_path)
        self.build_version = self._detect_build_version()
        self.start_time = time.time()
        self.last_peer_bootstrap = 0.0

        # Jan 26, 2026: Cache local IPs at startup to avoid DNS blocking in health endpoints
        # Root cause: _is_self_voter() calls socket.getaddrinfo() on every request,
        # which blocks when DNS is slow (common on Lambda nodes). Caching at startup
        # eliminates this per-request overhead.
        self._cached_local_ips: set[str] = self._cache_local_ips()
        logger.info(f"[P2P] Cached {len(self._cached_local_ips)} local IPs for voter recognition")

        # Resource detection delegation (Dec 28, 2025)
        self._resource_detector = ResourceDetector(
            ringrift_path=self.ringrift_path,
            start_time=self.start_time,
            startup_grace_period=STARTUP_JSONL_GRACE_PERIOD_SECONDS,
        )

        # Public endpoint peers should use to reach us. Peers learn our host from
        # the heartbeat socket address, but the port must be self-reported. This
        # matters for port-mapped environments like Vast.ai.
        self.advertise_host = (advertise_host or os.environ.get(ADVERTISE_HOST_ENV, "")).strip()

        # Jan 23, 2026: Check for public IP preference BEFORE setting Tailscale IP
        prefer_public = os.environ.get("RINGRIFT_PREFER_PUBLIC_IP", "").strip().lower() in ("1", "true", "yes")

        if not self.advertise_host:
            # Prefer a stable mesh address (Tailscale) when available so nodes
            # behind NAT remain reachable and the cluster converges on a single
            # view of peer endpoints.
            #
            # Jan 12, 2026: Multi-fallback IP resolution with YAML config priority.
            # Order: 1) YAML config tailscale_ip, 2) Tailscale CLI with retry,
            #        3) Local IP (last resort)
            # Jan 23, 2026: If RINGRIFT_PREFER_PUBLIC_IP=1, skip Tailscale and use
            #        public IP from YAML ssh_host or detected network interfaces.
            #
            # YAML fallback added because Tailscale CLI may not be ready at startup,
            # and the pre-configured tailscale_ip in distributed_hosts.yaml is a
            # reliable source for the correct IP.
            if not prefer_public:
                yaml_ip = self._get_yaml_tailscale_ip()
                if yaml_ip:
                    self.advertise_host = yaml_ip
                    logger.info(f"[P2P] Using YAML config tailscale_ip: {yaml_ip}")
            if not self.advertise_host and not prefer_public:
                # Try Tailscale detection with retry (up to 90s - increased Jan 12, 2026)
                # Root cause: 30s was insufficient when mac-studio boots and Tailscale
                # takes 45-60s to initialize. This caused persistent local IP (10.0.0.62)
                # advertisement, breaking voter quorum.
                ts_ip = _wait_for_tailscale_ip(timeout_seconds=90, interval_seconds=1.0)
                self.advertise_host = ts_ip or self._get_local_ip()
                if not ts_ip:
                    logger.warning(
                        f"[P2P] Tailscale unavailable, using local IP: {self.advertise_host}. "
                        "Set RINGRIFT_ADVERTISE_HOST or ensure Tailscale is running."
                    )
            if not self.advertise_host and prefer_public:
                # With RINGRIFT_PREFER_PUBLIC_IP=1, use ssh_host from YAML or detected public IP
                # _validate_and_fix_advertise_host() will select the best public IP
                logger.info("[P2P] RINGRIFT_PREFER_PUBLIC_IP=1: skipping Tailscale, will use public IP")

        # Dec 30, 2025: Validate advertise_host to prevent private IP issues
        # that cause P2P quorum loss when nodes can't reach each other
        self._validate_and_fix_advertise_host()

        self.advertise_port = advertise_port if advertise_port is not None else self._infer_advertise_port()

        # Optional auth token used to protect mutating endpoints and cluster control.
        # Default is allow-all unless a token is configured.
        env_token = (os.environ.get(AUTH_TOKEN_ENV, "")).strip()
        token_from_arg = (auth_token or "").strip()
        token = token_from_arg or env_token

        if not token:
            token_file = (os.environ.get(AUTH_TOKEN_FILE_ENV, "")).strip()
            if token_file:
                try:
                    token = Path(token_file).read_text().strip()
                except Exception as e:  # noqa: BLE001
                    logger.info(f"Auth: failed to read {AUTH_TOKEN_FILE_ENV}={token_file}: {e}")

        self.auth_token = token.strip()
        self.require_auth = bool(require_auth)
        if self.require_auth and not self.auth_token:
            raise ValueError(
                f"--require-auth set but {AUTH_TOKEN_ENV}/{AUTH_TOKEN_FILE_ENV}/--auth-token is empty"
            )

        # Optional split-brain mitigation: require a majority of "voter" nodes
        # to be visible before assuming or renewing leadership.
        #
        # Voters can be configured via:
        # - env: RINGRIFT_P2P_VOTERS="node-a,node-b,..."
        # - ai-service/config/distributed_hosts.yaml: per-host `p2p_voter: true`
        #
        # Jan 2026 (Phase 1 Consolidation): Voter logic delegated to QuorumManager.
        # The orchestrator maintains these attributes for backward compatibility.
        config_path = Path(self._get_ai_service_path()) / "config" / "distributed_hosts.yaml"
        self.quorum_manager = QuorumManager(
            config=QuorumConfig(
                node_id=self.node_id,
                config_path=config_path if config_path.exists() else None,
            ),
            get_peers=lambda: self.peers,
            get_peers_lock=lambda: self.peers_lock,
        )
        self.voter_node_ids: list[str] = self.quorum_manager.load_voter_node_ids()
        self.voter_config_source: str = self.quorum_manager.voter_config_source
        # SIMPLIFIED QUORUM: Fixed at 3 voters (or less if fewer voters exist)
        self.voter_quorum_size: int = min(VOTER_MIN_QUORUM, len(self.voter_node_ids)) if self.voter_node_ids else 0
        if self.voter_node_ids:
            print(
                f"[P2P] Voter quorum enabled: voters={len(self.voter_node_ids)}, "
                f"quorum={self.voter_quorum_size} ({', '.join(self.voter_node_ids)})"
            )

        # Jan 2, 2026: IP-to-node-name mapping for SWIM peer ID resolution
        # SWIM identifies peers by IP:port, but voters are configured by name.
        # This mapping translates between them.
        # Jan 2026: Delegated to QuorumManager.
        self._ip_to_node_map: dict[str, str] = self.quorum_manager.build_ip_to_node_map()

        # Jan 27, 2026: Cache cluster config from distributed_hosts.yaml
        # Used by loop_registry.py and autonomous_queue_loop.py for relay/selfplay config
        self._cluster_config: dict[str, Any] = self._load_cluster_config_raw()

        # Node state
        self.role = NodeRole.FOLLOWER
        self.leader_id: str | None = None

        # Unified Leadership State Machine (ULSM) - Jan 2026
        # Single source of truth for leadership state transitions.
        # Addresses silent step-down and split-brain issues.
        self._leadership_sm = LeadershipStateMachine(node_id=self.node_id)
        # Broadcast callback is set in _setup_routes() after app is created

        # Jan 23, 2026: HybridCoordinator for Raft-based leader election
        # Replaces buggy Bully algorithm when RINGRIFT_CONSENSUS_MODE=raft or hybrid.
        # HybridCoordinator routes is_leader() calls to PySyncObj's Raft implementation
        # which provides proven consensus with sub-second failover.
        self._hybrid_coordinator: Any = None  # Type: HybridCoordinator | None

        self.verbose = bool(os.environ.get("RINGRIFT_P2P_VERBOSE", "").strip())
        self.peers: dict[str, NodeInfo] = {}
        self._prepopulate_voter_peers()  # Jan 28, 2026: Bootstrap fix for gossip reachability
        # Jan 12, 2026: Lock-free peer snapshot for read-heavy operations like /status
        # PeerSnapshot maintains an immutable copy that can be read without acquiring peers_lock
        self._peer_snapshot: PeerSnapshot[NodeInfo] = PeerSnapshot()
        # Jan 20, 2026: Adaptive dead peer cooldown with probe-based recovery.
        # Replaces the static 1-hour cooldown that was causing 25-40% node loss.
        # The manager uses tiered cooldowns (30s -> 30min) based on failure frequency.
        self._cooldown_manager = get_dead_peer_cooldown_manager()
        # Fallback dict for compatibility if cooldown manager fails to load
        self._dead_peer_timestamps: dict[str, float] = {}

        # Jan 21, 2026: P2P Diagnostic Instrumentation (Phase 0)
        # Provides comprehensive visibility into peer state transitions, connection
        # failures, and probe effectiveness for diagnosing cluster instability.
        self._peer_state_tracker = None
        self._conn_failure_tracker = None
        self._probe_tracker = None
        try:
            from scripts.p2p.diagnostics import (
                PeerStateTracker,
                ConnectionFailureTracker,
                ProbeEffectivenessTracker,
            )
            self._peer_state_tracker = PeerStateTracker()
            self._conn_failure_tracker = ConnectionFailureTracker()
            self._probe_tracker = ProbeEffectivenessTracker()
            logger.info("[P2P] Diagnostic instrumentation enabled (Phase 0)")
        except ImportError as e:
            logger.warning(f"[P2P] Diagnostic instrumentation unavailable: {e}")

        # Jan 2026: P2P Stability Controller (Self-Healing Architecture)
        # Closes the feedback loop: Diagnostics -> Symptom Detection -> Recovery Action -> Effectiveness
        self._stability_controller = None
        self._adaptive_timeouts = None
        self._effectiveness_tracker = None
        try:
            from scripts.p2p.controllers import (
                StabilityController,
                RecoveryAction,
                AdaptiveTimeoutManager,
                EffectivenessTracker,
            )
            self._adaptive_timeouts = AdaptiveTimeoutManager()
            self._effectiveness_tracker = EffectivenessTracker()
            self._stability_controller = StabilityController(
                peer_state_tracker=self._peer_state_tracker,
                connection_failure_tracker=self._conn_failure_tracker,
                probe_tracker=self._probe_tracker,
                action_callbacks={
                    RecoveryAction.INCREASE_TIMEOUT: self._action_increase_timeout,
                    RecoveryAction.DECREASE_TIMEOUT: self._action_decrease_timeout,
                    RecoveryAction.SCALE_POOL_UP: self._action_scale_pool,
                    RecoveryAction.RESET_CIRCUIT: self._action_reset_circuits,
                    RecoveryAction.INCREASE_COOLDOWN: self._action_increase_cooldown,
                    RecoveryAction.REINJECT_PEER: self._action_reinject_peer,
                    RecoveryAction.EMIT_ALERT: self._action_emit_alert,
                },
            )
            # Wire effectiveness tracker to get metrics
            self._effectiveness_tracker.set_metrics_callback(self._get_stability_metrics)
            logger.info("[P2P] Stability controller enabled (Self-Healing Architecture)")
        except ImportError as e:
            logger.warning(f"[P2P] Stability controller unavailable: {e}")
        except Exception as e:
            logger.warning(f"[P2P] Stability controller init failed: {e}")

        self.local_jobs: dict[str, ClusterJob] = {}
        self.active_jobs: dict[str, dict[str, Any]] = {}  # Track running jobs by type (selfplay, training, etc.)

        # Network health tracking (December 30, 2025)
        # Reference to TailscalePeerDiscoveryLoop for stats reporting in /network/health
        self._tailscale_discovery_loop: Any = None

        # Distributed job state tracking (leader-only)
        self.distributed_cmaes_state: dict[str, DistributedCMAESState] = {}
        self.distributed_tournament_state: dict[str, DistributedTournamentState] = {}
        self.ssh_tournament_runs: dict[str, SSHTournamentRun] = {}
        self.improvement_loop_state: dict[str, ImprovementLoopState] = {}
        # Limit CPU-heavy CMA-ES local evaluations to avoid runaway process
        # explosions that can starve the orchestrator (especially on relay hubs).
        try:
            raw = (os.environ.get("RINGRIFT_P2P_MAX_CONCURRENT_CMAES_EVALS", "") or "").strip()
            self.max_concurrent_cmaes_evals = max(1, int(raw)) if raw else 2
        except (ValueError, AttributeError):
            self.max_concurrent_cmaes_evals = 2
        self._cmaes_eval_semaphore = asyncio.Semaphore(int(self.max_concurrent_cmaes_evals))

        # Tournament match semaphore - limit concurrent Elo calibration matches to prevent OOM
        # Each match can potentially load neural networks which use significant memory
        # NOTE: Set to None here, created lazily in async context to avoid event loop issues
        self._tournament_match_semaphore: asyncio.Semaphore | None = None

        # Phase 2: Distributed data sync state
        self.local_data_manifest: NodeDataManifest | None = None
        self.cluster_data_manifest: ClusterDataManifest | None = None  # Leader-only or received from broadcast
        self._cluster_manifest_received_at: float = 0.0  # When broadcast was received (followers)
        self.manifest_collection_interval = 300.0  # Collect manifests every 5 minutes
        self.last_manifest_collection = 0.0

        # Dashboard/selfplay stats history (leader-only). Stored in-memory to
        # enable lightweight throughput charts without adding DB migrations.
        self.selfplay_stats_history: list[dict[str, Any]] = []
        self.selfplay_stats_history_max_samples: int = 288  # ~24h @ 5-min cadence

        # Canonical gate jobs (leader-only): dashboard-triggered runs of
        # scripts/generate_canonical_selfplay.py.
        self.canonical_gate_jobs: dict[str, dict[str, Any]] = {}
        self.canonical_gate_jobs_lock = threading.RLock()

        # Phase 2: P2P rsync coordination state
        self.active_sync_jobs: dict[str, DataSyncJob] = {}
        self.current_sync_plan: ClusterSyncPlan | None = None  # Leader-only
        self.pending_sync_requests: list[dict[str, Any]] = []  # Requests from non-leader nodes
        self.sync_in_progress = False
        self.last_sync_time = 0.0
        self.auto_sync_interval = 600.0  # Auto-sync every 10 minutes when data is missing

        # Training node priority sync state (leader-only)
        self.training_sync_interval = TRAINING_SYNC_INTERVAL
        self.last_training_sync_time = 0.0
        self.training_nodes_cache: list[str] = []  # Cached list of top GPU nodes
        self.training_nodes_cache_time = 0.0
        self.games_synced_to_training: dict[str, int] = {}  # node_id -> last synced game count

        # Circuit breaker for fault-tolerant peer communication
        self._circuit_registry = get_circuit_registry()

        # Jan 3, 2026 (Sprint 10+): Per-peer circuit breakers and health scoring
        # Finer-grained failure isolation than per-transport breakers
        self._peer_circuit_breakers: dict[str, PeerCircuitBreaker] = {}
        self._peer_health_scores: dict[str, PeerHealthScore] = {}

        # Phase 3: Training pipeline state (leader-only)
        self.training_jobs: dict[str, TrainingJob] = {}
        self.training_thresholds: TrainingThresholds = TrainingThresholds()
        self.last_training_check: float = 0.0
        self.training_check_interval: float = 300.0  # Check every 5 minutes
        self.games_at_last_nnue_train: dict[str, int] = {}  # board_type -> game_count
        self.games_at_last_cmaes_train: dict[str, int] = {}

        # Phase 5: Automated improvement cycle manager (leader-only)
        self.improvement_cycle_manager: ImprovementCycleManager | None = None
        if HAS_IMPROVEMENT_MANAGER:
            try:
                self.improvement_cycle_manager = ImprovementCycleManager(
                    db_path=STATE_DIR / f"{node_id}_improvement.db",
                    ringrift_path=self.ringrift_path,
                )
                logger.info("ImprovementCycleManager initialized")
            except Exception as e:  # noqa: BLE001
                logger.error(f"Failed to initialize ImprovementCycleManager: {e}")
        self.last_improvement_cycle_check: float = 0.0

        # P2P-integrated monitoring (leader starts Prometheus/Grafana)
        self.monitoring_manager: MonitoringManager | None = None
        if HAS_P2P_MONITORING:
            try:
                self.monitoring_manager = MonitoringManager(
                    node_id=node_id,
                    prometheus_port=9090,
                    grafana_port=3000,
                    config_dir=Path(self.ringrift_path) / "monitoring",
                )
                logger.info("MonitoringManager initialized")
            except Exception as e:  # noqa: BLE001
                logger.error(f"Failed to initialize MonitoringManager: {e}")
        self._monitoring_was_leader = False  # Track leadership changes
        self.improvement_cycle_check_interval: float = 600.0  # Check every 10 minutes

        # P2P Auto-Deployer (leader-only): ensures P2P runs on all cluster nodes
        self.p2p_auto_deployer: P2PAutoDeployer | None = None
        self._auto_deployer_task: asyncio.Task | None = None

        # Webhook notifications for alerts
        self.notifier = WebhookNotifier()

        # HTTP server components for graceful restart (Jan 2026)
        # These are set in run() and used by restart_http_server()
        self._http_app: "web.Application | None" = None
        self._http_runner: "web.AppRunner | None" = None
        self._http_sites: list["web.TCPSite"] = []
        self._http_restart_lock = asyncio.Lock()
        self._http_restart_count = 0

        # Diversity tracking metrics
        self.diversity_metrics = {
            "games_by_engine_mode": {},      # engine_mode -> count
            "games_by_board_config": {},     # "board_players" -> count
            "games_by_difficulty": {},       # difficulty -> count
            "asymmetric_games": 0,           # count of asymmetric games scheduled
            "symmetric_games": 0,            # count of symmetric games scheduled
            "training_triggers": 0,          # count of training triggers
            "cmaes_triggers": 0,             # count of CMA-ES triggers
            "promotions": 0,                 # count of model promotions
            "rollbacks": 0,                  # count of rollbacks
            "last_reset": time.time(),       # when metrics were last reset
        }

        # === CRITICAL SELF-IMPROVEMENT LOOP METRICS ===
        # Training progress tracking (populated by training callbacks)
        self.training_metrics: dict[str, dict[str, float]] = {}  # config -> {loss, val_loss, epoch}

        # Selfplay throughput tracking
        self.selfplay_throughput: dict[str, float] = {}  # config -> games/hour

        # Cost efficiency metrics
        self.cost_metrics: dict[str, float] = {
            "gpu_hours_total": 0.0,
            "estimated_cost_usd": 0.0,
            "elo_per_gpu_hour": 0.0,
        }

        # Promotion quality metrics
        self.promotion_metrics: dict[str, Any] = {
            "success_rate": 0.0,
            "avg_elo_gain": 0.0,
            "rejections": {},  # reason -> count
            "total_attempts": 0,
            "successful": 0,
        }

        # LEARNED LESSONS - Stuck job detection (leader-only)
        # Track when each node's GPU first went idle with running jobs
        self.gpu_idle_since: dict[str, float] = {}  # node_id -> timestamp when GPU went idle

        # A/B Testing Framework - Compare models head-to-head with statistical significance
        # Key: test_id (UUID), Value: ABTestState dict
        self.ab_tests: dict[str, dict[str, Any]] = {}
        self.ab_test_lock = threading.RLock()

        # Elo Sync Manager - Keeps unified_elo.db consistent across cluster
        self.elo_sync_manager: EloSyncManager | None = None
        if HAS_ELO_SYNC:
            try:
                db_path = Path(self._get_ai_service_path()) / "data" / "unified_elo.db"
                # Use env var for coordinator, fallback to nebius-backbone-1 (stable backbone node)
                elo_coordinator = os.environ.get("RINGRIFT_ELO_COORDINATOR", "nebius-backbone-1")
                self.elo_sync_manager = EloSyncManager(
                    db_path=db_path,
                    coordinator_host=elo_coordinator,
                    sync_interval=300,  # Sync every 5 minutes
                )
                logger.info(f"EloSyncManager initialized (db: {db_path})")
            except Exception as e:  # noqa: BLE001
                logger.error(f"Failed to initialize EloSyncManager: {e}")

        # Queue Populator - Maintains 50+ work items until 2000 Elo target met
        self._queue_populator: QueuePopulator | None = None
        # Jan 5, 2026 (Session 17.41): Reference to QueuePopulatorLoop for handler access
        self._queue_populator_loop: Any = None

        # PFSP (Prioritized Fictitious Self-Play) opponent pool (leader-only)
        # Maintains a pool of historical models weighted by difficulty for diverse training
        self.pfsp_pools: dict[str, Any] = {}  # config_key -> PFSPOpponentPool
        if HAS_PFSP:
            try:
                for config_key in ["square8_2p", "square8_4p", "hex8_2p", "hexagonal_2p"]:
                    self.pfsp_pools[config_key] = PFSPOpponentPool(
                        max_pool_size=30,
                        hard_opponent_weight=0.6,
                        diversity_weight=0.25,
                        recency_weight=0.15,
                    )
                logger.info(f"PFSP opponent pools initialized for {len(self.pfsp_pools)} configs")
            except Exception as e:  # noqa: BLE001
                logger.error(f"Failed to initialize PFSP pools: {e}")

        # CMA-ES Auto-Tuner (leader-only)
        # Automatically triggers hyperparameter optimization when Elo plateaus
        self.cmaes_auto_tuners: dict[str, Any] = {}  # config_key -> CMAESAutoTuner
        self.last_cmaes_elo: dict[str, float] = {}  # config_key -> last recorded Elo
        if HAS_PFSP and CMAESAutoTuner:
            try:
                for config_key in ["square8_2p", "square8_4p", "hex8_2p", "hexagonal_2p"]:
                    parts = config_key.rsplit("_", 1)
                    board_type = parts[0]
                    num_players = int(parts[1].replace("p", ""))
                    plateau_cfg = PlateauConfig(patience=10)
                    self.cmaes_auto_tuners[config_key] = CMAESAutoTuner(
                        board_type=board_type,
                        num_players=num_players,
                        plateau_config=plateau_cfg,
                        min_epochs_between_tuning=50,
                        max_auto_tunes=3,
                    )
                logger.info(f"CMA-ES auto-tuners initialized for {len(self.cmaes_auto_tuners)} configs")
            except Exception as e:  # noqa: BLE001
                logger.error(f"Failed to initialize CMA-ES auto-tuners: {e}")

        # Locks for thread safety
        # Use RLock (reentrant lock) to allow nested acquisitions from same thread
        # This prevents deadlocks when helper methods like _select_best_relay are
        # called while already holding the lock
        self.peers_lock = threading.RLock()
        self.jobs_lock = threading.RLock()
        self.manifest_lock = threading.RLock()
        self.sync_lock = threading.RLock()
        self.training_lock = threading.RLock()
        self.ssh_tournament_lock = threading.RLock()
        self.relay_lock = threading.RLock()
        # Jan 12, 2026: C1 fix - leader state lock prevents race conditions
        # during leader elections and state transitions. Protects self.leader_id
        # and self.role from concurrent modification in async contexts.
        self.leader_state_lock = threading.RLock()

        # Jan 23, 2026: Singleton ThreadPoolExecutor for manager health checks
        # Previously created a new executor on every health check cycle (every 30s),
        # causing thread churn and overhead. Reusing a single executor is more efficient.
        from concurrent.futures import ThreadPoolExecutor
        self._health_check_executor = ThreadPoolExecutor(
            max_workers=4, thread_name_prefix="health_"
        )

        # Jan 12, 2026: Lock-free job snapshot for /status endpoint
        # Updates via _job_snapshot.update(self.local_jobs) after mutations
        self._job_snapshot = JobSnapshot()

        # Jan 27, 2026: Query builder for peer collection queries (Phase 3.2)
        # Consolidates _get_* methods with thread-safe access and consistent error handling
        self._peer_query = PeerQueryBuilder(self.peers, self.peers_lock, self.node_id)

        # ============================================
        # Phase 5: SWIM + Raft Integration (Dec 26, 2025)
        # ============================================
        # SWIM provides leaderless gossip-based membership with 5s failure detection
        # Raft provides replicated work queue with sub-second failover
        # Both are initialized here but started asynchronously in run()

        # Feature flag validation - warn if flags enabled but dependencies missing
        from scripts.p2p.constants import (
            SWIM_ENABLED, RAFT_ENABLED, MEMBERSHIP_MODE, CONSENSUS_MODE
        )
        try:
            from app.p2p.swim_adapter import SWIM_AVAILABLE
        except ImportError:
            SWIM_AVAILABLE = False
        try:
            from scripts.p2p.consensus_mixin import PYSYNCOBJ_AVAILABLE
        except ImportError:
            PYSYNCOBJ_AVAILABLE = False

        if SWIM_ENABLED and not SWIM_AVAILABLE:
            logger.warning(
                "RINGRIFT_SWIM_ENABLED=true but swim-p2p not installed or not compatible. "
                "SWIM features disabled. Install with: pip install swim-p2p>=1.2.0"
            )
        if RAFT_ENABLED and not PYSYNCOBJ_AVAILABLE:
            logger.warning(
                "RINGRIFT_RAFT_ENABLED=true but pysyncobj not installed. "
                "Raft features disabled. Install with: pip install pysyncobj>=0.3.14"
            )
        if MEMBERSHIP_MODE in ("swim", "hybrid") and not SWIM_AVAILABLE:
            logger.warning(
                f"RINGRIFT_MEMBERSHIP_MODE={MEMBERSHIP_MODE} but SWIM unavailable. "
                "Falling back to HTTP heartbeats."
            )
        if CONSENSUS_MODE in ("raft", "hybrid") and not PYSYNCOBJ_AVAILABLE:
            logger.warning(
                f"RINGRIFT_CONSENSUS_MODE={CONSENSUS_MODE} but PySyncObj unavailable. "
                "Falling back to Bully algorithm."
            )

        # Log active P2P protocol modes
        logger.info(
            f"P2P protocols: MEMBERSHIP_MODE={MEMBERSHIP_MODE} (SWIM={'available' if SWIM_AVAILABLE else 'unavailable'}), "
            f"CONSENSUS_MODE={CONSENSUS_MODE} (Raft={'available' if PYSYNCOBJ_AVAILABLE else 'unavailable'})"
        )

        # Initialize SWIM membership (from MembershipMixin)
        self._swim_initialized = self._init_swim_membership()
        if self._swim_initialized:
            logger.info("SWIM membership initialized (will start in run())")

        # Initialize Raft consensus (from ConsensusMixin)
        # Note: Raft requires advertise_host which is set above
        self._raft_init_attempted = False
        # Raft initialization deferred to after peers are discovered
        # to ensure we have partner addresses available

        # Try early Raft initialization if we have voter nodes
        if RAFT_ENABLED and PYSYNCOBJ_AVAILABLE and self.voter_node_ids:
            try:
                self._raft_init_attempted = True
                raft_ok = self._init_raft_consensus()
                if raft_ok:
                    logger.info("Raft consensus initialized (will sync with peers in run())")
            except Exception as e:  # noqa: BLE001
                logger.warning(f"Early Raft initialization failed (will retry later): {e}")

        # Initialize failover system (Phase 9: Multi-layer transport cascade)
        # Lazy initialization - will be fully set up on first use
        try:
            self._init_failover_system()
            logger.info("Failover system initialized (transport cascade + union discovery)")
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Failover system init deferred: {e}")

        # State persistence (Phase 1 refactoring: delegated to StateManager)
        self.db_path = STATE_DIR / f"{node_id}_state.db"
        self.state_manager = StateManager(self.db_path, verbose=self.verbose)
        self.state_manager.init_database()
        self._cluster_epoch = self.state_manager.load_cluster_epoch()

        # Sprint 4 (Jan 2, 2026): Wire circuit breaker state persistence
        # This enables crash recovery for the global circuit breaker
        # Sprint 5 (Jan 2, 2026): Wire transport metrics persistence
        try:
            from scripts.p2p.transport_cascade import GlobalCircuitBreaker, TransportCascade
            GlobalCircuitBreaker.set_state_manager(self.state_manager)
            TransportCascade.set_state_manager(self.state_manager)
            logger.debug("Circuit breaker and transport metrics persistence configured")
        except ImportError:
            logger.debug("Transport cascade not available for persistence")

        # Metrics recording (Phase 1 refactoring: delegated to MetricsManager)
        self.metrics_manager = MetricsManager(self.db_path)

        # Event flags
        self.running = True
        self.election_in_progress = False
        self.last_election_attempt: float = 0.0
        # Jan 22, 2026: Atomic election guard to prevent race conditions
        # Multiple coroutines could pass the `if election_in_progress` check
        # before any sets the flag, causing multiple simultaneous elections.
        self._election_lock = asyncio.Lock()

        # LEARNED LESSONS - Lease-based leadership to prevent split-brain
        # Leader must continuously renew lease; if lease expires, leadership is void
        self.leader_lease_expires: float = 0.0  # timestamp when current leader's lease expires
        self.last_lease_renewal: float = 0.0  # when we last renewed our lease (if leader)
        self.leader_lease_id: str = ""  # unique ID for current leadership term
        # LEADERLESS FALLBACK: Track when we last had a functioning leader.
        # If leaderless for too long, nodes can trigger local training independently.
        self.last_leader_seen: float = time.time()  # When we last saw a functioning leader
        # Dec 31, 2025: Leader invalidation window - prevents gossip from re-setting stale leader
        # After we invalidate a leader, ignore gossip leader claims for this window
        self._leader_invalidation_until: float = 0.0  # timestamp until which we ignore gossip leader claims
        self.last_local_training_fallback: float = 0.0  # When we last triggered local training fallback

        # Jan 22, 2026: Cached jittered timeout to ensure consistent death detection across cycle.
        # Problem: get_jittered_peer_timeout() called at two locations with different jitter each time,
        # causing desynchronized death detection (Node A: 108s, Node B: 132s for same peer).
        # Solution: Cache the jittered timeout for 30 seconds, ensuring all death checks in same
        # cycle use the same timeout value across all locations.
        self._jittered_timeout_cache: float | None = None
        self._jittered_timeout_time: float = 0.0

        # Dec 30, 2025: Track when we last received work from the leader
        # If leader exists but isn't dispatching work, nodes can self-assign after timeout
        self.last_work_from_leader: float = time.time()  # When we last got work from leader

        # Jan 2, 2026: Leader stickiness - prefer incumbent during elections
        # Track when we last held leadership to allow re-claiming with preference
        self._last_become_leader_time: float = 0.0  # When we last became leader
        self._last_step_down_time: float = 0.0  # When we last stepped down from leadership

        # Jan 1, 2026: Probabilistic Fallback Leadership (Provisional Leader state)
        # When normal elections repeatedly fail, nodes can claim provisional leadership
        # with increasing probability. Provisional leaders can dispatch work but must be
        # confirmed by quorum acknowledgment or node_id tiebreaker if contested.
        self._provisional_leader_claimed_at: float = 0.0  # When we claimed provisional leadership
        self._provisional_leader_acks: set[str] = set()  # Nodes that acknowledged our provisional claim
        self._provisional_leader_challengers: dict[str, float] = {}  # node_id -> challenge_time
        self._last_provisional_check: float = 0.0  # Last time we checked for probabilistic claim
        self._provisional_claim_probability: float = PROVISIONAL_LEADER_INITIAL_PROBABILITY

        # Voter-backed lease grants (split-brain resistance).
        #
        # When quorum gating is enabled, voters act as a lightweight consensus
        # group by granting an exclusive leader lease to a single node at a time.
        # A leader must renew its lease with a quorum of voters; otherwise it
        # steps down. This prevents split-brain even if multiple nodes think
        # they are eligible leaders.
        self.voter_grant_leader_id: str = ""
        self.voter_grant_lease_id: str = ""
        self.voter_grant_expires: float = 0.0

        # Phase 15.1.1: Fenced lease tokens with monotonic epoch
        # These prevent split-brain during network partitions by ensuring
        # only one leader per epoch can issue commands.
        self._lease_epoch: int = 0  # Monotonic, never decreases
        self._fence_token: str = ""  # Unique per lease grant: node_id:epoch:timestamp
        self._last_seen_epoch: int = 0  # Highest epoch seen from any leader

        # Job completion tracking for auto-restart
        self.completed_jobs: dict[str, float] = {}  # node_id -> last job completion time
        self.jobs_started_at: dict[str, dict[str, float]] = {}  # node_id -> {job_id: start_time}

        # NAT/relay support (for nodes without inbound connectivity).
        # NAT-blocked nodes poll a relay endpoint for commands; the leader enqueues
        # commands keyed by node_id.
        self.last_inbound_heartbeat: float = 0.0
        self.last_relay_heartbeat: float = 0.0
        self.relay_command_queue: dict[str, list[dict[str, Any]]] = {}
        self.pending_relay_acks: set[str] = set()
        self.pending_relay_results: list[dict[str, Any]] = []
        self.relay_command_attempts: dict[str, int] = {}
        # Background tasks list for graceful shutdown (Dec 2025)
        self._background_tasks: list[asyncio.Task] = []

        # SAFEGUARDS - Rate limiting and coordinator integration (added 2025-12-15)
        self.spawn_timestamps: list[float] = []  # Timestamps of recent process spawns
        self.agent_mode = AGENT_MODE_ENABLED
        self.coordinator_url = COORDINATOR_URL
        self.last_coordinator_check: float = 0.0
        self.coordinator_available: bool = False
        logger.info(f"Safeguards: rate_limit={SPAWN_RATE_LIMIT_PER_MINUTE}/min, "
              f"load_max={LOAD_AVERAGE_MAX_MULTIPLIER}x, agent_mode={self.agent_mode}")

        # Load persisted state
        self._load_state()
        # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
        # Note: This is called during __init__ so ULSM may not exist yet, but _set_leader handles that
        if self.leader_id == self.node_id:
            self._set_leader(self.node_id, reason="startup_restore_leadership", save_state=False)

        # Self info
        self.self_info = self._create_self_info()

        # Phase 1 Refactoring: NodeSelector for node ranking/selection
        self.node_selector = NodeSelector(
            get_peers=lambda: self.peers,
            get_self_info=lambda: self.self_info,
            peers_lock=self.peers_lock,
            get_training_jobs=lambda: self.training_jobs,
        )
        # December 2025: Subscribe to health events (HOST_OFFLINE, NODE_RECOVERED)
        # to track unhealthy nodes for filtering during selection
        # Note: NodeSelector uses its own subscribe_to_events (not mixin)
        self.node_selector.subscribe_to_events()

        # Phase 2A Refactoring: SyncPlanner for data synchronization
        # NOTE: request_peer_manifest is wired AFTER SyncPlanner creation
        # because _request_peer_manifest is a method on this class
        self.sync_planner = SyncPlanner(
            node_id=self.node_id,
            data_directory=self.get_data_directory(),
            get_peers=lambda: self.peers,
            get_self_info=lambda: self.self_info,
            peers_lock=self.peers_lock,
            is_leader=lambda: self._is_leader(),
            request_peer_manifest=lambda peer_id: self._request_peer_manifest_sync(peer_id),
            check_disk_capacity=lambda: check_disk_has_capacity(),
            config=SyncPlannerConfig(),
        )
        # December 2025: Subscribe to cluster events (LEADER_ELECTED, NODE_RECOVERED)
        # to invalidate cached manifests and trigger re-collection
        # Wave 7 Phase 1.1: Use retry mechanism for reliable subscription
        self.sync_planner.subscribe_to_events_with_retry()

        # Phase 2B Refactoring: SelfplayScheduler for priority-based selfplay allocation
        # All callbacks wired for full delegation (Dec 2025)
        self.selfplay_scheduler = SelfplayScheduler(
            get_cluster_elo_fn=lambda: self._get_cluster_elo_summary(),
            load_curriculum_weights_fn=lambda: self._load_curriculum_weights(),
            get_board_priority_overrides_fn=lambda: getattr(self, "board_priority_overrides", {}),
            # Backpressure callbacks (wired Dec 2025)
            should_stop_production_fn=should_stop_production if HAS_NEW_COORDINATION else None,
            should_throttle_production_fn=should_throttle_production if HAS_NEW_COORDINATION else None,
            get_throttle_factor_fn=get_throttle_factor if HAS_NEW_COORDINATION else None,
            # Resource targeting callbacks (wired Dec 2025)
            record_utilization_fn=record_utilization if HAS_NEW_COORDINATION else None,
            get_host_targets_fn=get_host_targets if HAS_NEW_COORDINATION else None,
            get_target_job_count_fn=get_target_job_count if HAS_NEW_COORDINATION else None,
            should_scale_up_fn=should_scale_up if HAS_NEW_COORDINATION else None,
            should_scale_down_fn=should_scale_down if HAS_NEW_COORDINATION else None,
            # Hardware-aware limits (wired Dec 2025)
            get_max_selfplay_for_node_fn=get_max_selfplay_for_node if HAS_HW_AWARE_LIMITS else None,
            get_hybrid_selfplay_limits_fn=get_hybrid_selfplay_limits if HAS_HW_AWARE_LIMITS else None,
            # Safeguards callback (wired Dec 2025) - halt selfplay during emergency
            is_emergency_active_fn=_safeguards.is_emergency_active if HAS_SAFEGUARDS and _safeguards else None,
            verbose=self.verbose,
        )
        # Subscribe to feedback loop events (December 2025)
        # Wave 7 Phase 1.1: Use retry mechanism for reliable subscription
        self.selfplay_scheduler.subscribe_to_events_with_retry()

        # Session 17.29: Seed game counts from canonical databases at startup
        # This enables bootstrap priority boosts for underserved configs immediately
        try:
            initial_game_counts = self._seed_selfplay_scheduler_game_counts_sync()
            if initial_game_counts:
                self.selfplay_scheduler.update_p2p_game_counts(initial_game_counts)
                logger.info(f"[P2P] Seeded SelfplayScheduler with {len(initial_game_counts)} config game counts from canonical DBs")
                for config_key, count in sorted(initial_game_counts.items(), key=lambda x: x[1]):
                    if count < 500:  # Log underserved configs
                        logger.info(f"[P2P] Underserved config: {config_key} = {count} games")
        except Exception as e:  # noqa: BLE001
            logger.debug(f"[P2P] Failed to seed initial game counts: {e}")

        # Phase 2B Refactoring: JobManager for job spawning and lifecycle
        self.job_manager = JobManager(
            ringrift_path=self.ringrift_path,
            node_id=self.node_id,
            peers=self.peers,
            peers_lock=self.peers_lock,
            active_jobs=self.active_jobs,
            jobs_lock=self.jobs_lock,
            improvement_loop_state=self.improvement_loop_state,
            distributed_tournament_state=self.distributed_tournament_state,
        )
        # December 2025: Subscribe to job-relevant events (HOST_OFFLINE, HOST_ONLINE)
        # Wave 7 Phase 1.1: Use retry mechanism for reliable subscription
        self.job_manager.subscribe_to_events_with_retry()

        # January 2026 Sprint 6: Wire job spawn verification between JobManager and SelfplayScheduler
        # This enables tracking of dispatched jobs to verify they actually start running
        self.job_manager.set_spawn_registration_callback(
            self.selfplay_scheduler.register_pending_spawn
        )
        self.selfplay_scheduler.set_job_status_callback(
            self.job_manager.get_job_status
        )
        logger.info("[P2P] Spawn verification wired: JobManager <-> SelfplayScheduler")

        # Phase 2B Refactoring: TrainingCoordinator for training dispatch and completion
        self.training_coordinator = TrainingCoordinator(
            ringrift_path=Path(self.ringrift_path),
            get_cluster_data_manifest=lambda: self.cluster_data_manifest,
            get_training_jobs=lambda: self.training_jobs,
            get_training_lock=lambda: self.training_lock,
            get_peers=lambda: self.peers,
            get_peers_lock=lambda: self.peers_lock,
            get_self_info=lambda: self.self_info,
            training_thresholds=self.training_thresholds,
            games_at_last_nnue_train=getattr(self, "games_at_last_nnue_train", None),
            games_at_last_cmaes_train=getattr(self, "games_at_last_cmaes_train", None),
            improvement_cycle_manager=getattr(self, "improvement_cycle_manager", None),
            auth_headers=lambda: self._auth_headers(),
            urls_for_peer=lambda node_id, endpoint: self._urls_for_peer(node_id, endpoint),
            save_state_callback=lambda: self._save_state(),
            has_voter_quorum=lambda: self._check_quorum_health(),  # Now returns QuorumHealthLevel for degraded-mode training
        )
        # December 2025: Subscribe to training-relevant events
        # (SELFPLAY_COMPLETE, DATA_SYNC_COMPLETED, EVALUATION_COMPLETED, REGRESSION_DETECTED)
        # Wave 7 Phase 1.1: Use retry mechanism for reliable subscription
        self.training_coordinator.subscribe_to_events_with_retry()

        # January 2026: Phase 1 P2P Orchestrator Deep Decomposition
        # JobOrchestrationManager handles job spawning, scaling, and cluster-wide coordination
        # NOTE: Using factory function which wires all callbacks automatically
        self.job_orchestration = create_job_orchestration_manager(self)
        logger.info("[P2P] JobOrchestrationManager initialized")

        # January 2026: Aggressive Decomposition Phase 1 - Analytics Cache Manager
        # Handles all cached analytics computations (victory stats, game analytics, MCTS stats, etc.)
        self.analytics_cache_manager = create_analytics_cache_manager(
            config=AnalyticsCacheConfig(),
            get_ai_service_path=lambda: self._get_ai_service_path(),
            is_in_startup_grace_period=lambda: self._is_in_startup_grace_period(),
            increment_rollback_counter=lambda: self._increment_rollback_counter(),
            send_notification=lambda **kwargs: asyncio.create_task(self.notifier.send(**kwargs)) if hasattr(self, 'notifier') else None,
            node_id=self.node_id,
        )
        logger.info("[P2P] AnalyticsCacheManager initialized")

        # January 2026: Aggressive Decomposition Phase 3 - CMA-ES Coordinator
        # Handles distributed CMA-ES hyperparameter optimization across cluster
        self.cmaes_coordinator = create_cmaes_coordinator(
            config=CMAESConfig(ai_service_path=self._get_ai_service_path()),
            get_gpu_workers=lambda: self._get_gpu_workers_for_cmaes(),
            send_to_worker=lambda wid, ep, pl: self._send_cmaes_to_worker(wid, ep, pl),
            report_to_leader=lambda ep, pl: self._report_cmaes_to_leader(ep, pl),
            get_node_role=lambda: self.role.value if hasattr(self.role, 'value') else str(self.role),
            get_leader_id=lambda: self.leader_id,
            get_node_id=lambda: self.node_id,
            handle_cmaes_complete=lambda bt, np, w: self._handle_cmaes_complete_callback(bt, np, w),
        )
        logger.info("[P2P] CMAESCoordinator initialized")

        # January 2026: Aggressive Decomposition Phase 4 - Data Sync Coordinator
        # Handles external storage scanning (OWC, S3) for unified cluster data visibility
        self.data_sync_coordinator = create_data_sync_coordinator(
            config=DataSyncCoordinatorConfig(),
        )
        logger.info("[P2P] DataSyncCoordinator initialized")

        # January 2026: Aggressive Decomposition Phase 5 - IP Discovery Manager
        # Handles cloud and mesh IP discovery (Tailscale, Vast, AWS)
        from scripts.p2p.managers.ip_discovery_manager import (
            create_ip_discovery_manager,
            IPDiscoveryConfig,
        )
        self.ip_discovery_manager = create_ip_discovery_manager(
            config=IPDiscoveryConfig(),
            orchestrator=self,
        )
        logger.info("[P2P] IPDiscoveryManager initialized")

        # January 2026: Aggressive Decomposition Phase 6 - Worker Pull Controller
        # Handles work claiming from leader in pull-based model
        from scripts.p2p.managers.worker_pull_controller import (
            create_worker_pull_controller,
            WorkerPullConfig,
        )
        self.worker_pull_controller = create_worker_pull_controller(
            config=WorkerPullConfig(),
            orchestrator=self,
        )
        logger.info("[P2P] WorkerPullController initialized")

        # January 2026: Aggressive Decomposition Phase 7 - Data Pipeline Manager
        # Handles JSONL to DB/NPZ conversions and database consolidation
        from scripts.p2p.managers.data_pipeline_manager import (
            create_data_pipeline_manager,
            DataPipelineConfig,
        )
        self.data_pipeline_manager = create_data_pipeline_manager(
            config=DataPipelineConfig(),
            orchestrator=self,
        )
        logger.info("[P2P] DataPipelineManager initialized")

        # January 2026: Aggressive Decomposition Phase 8 - Job Lifecycle Manager
        # Handles stuck job detection and termination
        from scripts.p2p.managers.job_lifecycle_manager import (
            create_job_lifecycle_manager,
            JobLifecycleConfig,
        )
        self.job_lifecycle_manager = create_job_lifecycle_manager(
            config=JobLifecycleConfig(),
            orchestrator=self,
        )
        logger.info("[P2P] JobLifecycleManager initialized")

        # January 2026: Aggressive Decomposition Phase 9 - Health Metrics Manager
        # Handles health scoring, peer health tracking, and monitoring loops
        from scripts.p2p.managers.health_metrics_manager import (
            create_health_metrics_manager,
            HealthMetricsConfig,
        )
        self.health_metrics_manager = create_health_metrics_manager(
            config=HealthMetricsConfig(),
            orchestrator=self,
        )
        logger.info("[P2P] HealthMetricsManager initialized")

        # January 2026: Aggressive Decomposition Phase 10 - Memory Disk Manager
        # Handles memory pressure, disk cleanup, and selfplay job reduction
        from scripts.p2p.managers.memory_disk_manager import (
            create_memory_disk_manager,
            MemoryDiskConfig,
        )
        self.memory_disk_manager = create_memory_disk_manager(
            config=MemoryDiskConfig(),
            orchestrator=self,
        )
        logger.info("[P2P] MemoryDiskManager initialized")

        # January 2026: Aggressive Decomposition Phase 11 - Tournament Manager
        # Handles tournament coordination, match execution, and gossip-based scheduling
        from scripts.p2p.managers.tournament_manager import (
            create_tournament_manager,
            TournamentConfig,
        )
        self.tournament_manager = create_tournament_manager(
            config=TournamentConfig(),
            orchestrator=self,
        )
        logger.info("[P2P] TournamentManager initialized")

        # January 2026: Aggressive Decomposition Phase 12 - Recovery Manager
        # Handles NAT recovery and node recovery for cluster self-healing
        from scripts.p2p.managers.recovery_manager import (
            create_recovery_manager,
            RecoveryConfig,
        )
        self.recovery_manager = create_recovery_manager(
            config=RecoveryConfig(),
            orchestrator=self,
        )
        logger.info("[P2P] RecoveryManager initialized")

        # January 27, 2026: Phase 14 - HeartbeatManager for heartbeat operations
        from scripts.p2p.managers.heartbeat_manager import (
            HeartbeatConfig,
            create_heartbeat_manager,
        )
        self.heartbeat_manager = create_heartbeat_manager(
            config=HeartbeatConfig(),
            orchestrator=self,
        )
        logger.info("[P2P] HeartbeatManager initialized")

        # January 27, 2026: Phase 15 - JobCoordinationManager for job coordination
        from scripts.p2p.managers.job_coordination_manager import (
            JobCoordinationConfig,
            create_job_coordination_manager,
        )
        self.job_coordination_manager = create_job_coordination_manager(
            config=JobCoordinationConfig(),
            orchestrator=self,
        )
        logger.info("[P2P] JobCoordinationManager initialized")

        # January 4, 2026: Phase 5 - WorkDiscoveryManager for multi-channel work discovery
        # This enables workers to find work even during leader elections or partitions
        self._initialize_work_discovery_manager()

        # December 2025: Wire feedback loops for self-improvement
        # This connects curriculum adjustments to Elo changes, evaluation results, etc.
        self._wire_feedback_loops()

        # December 2025: Subscribe to daemon status events for observability
        daemon_events_ok = self._subscribe_to_daemon_events()

        # December 2025: Subscribe to training feedback signals for dynamic orchestration
        feedback_signals_ok = self._subscribe_to_feedback_signals()

        # December 2025: Subscribe to manager lifecycle events for coordination
        manager_events_ok = self._subscribe_to_manager_events()

        # Dec 2025: Store subscription status for health checks via /status endpoint
        self._event_subscription_status = {
            "daemon_events": daemon_events_ok,
            "feedback_signals": feedback_signals_ok,
            "manager_events": manager_events_ok,
            "all_healthy": daemon_events_ok and feedback_signals_ok and manager_events_ok,
            "timestamp": time.time(),
        }

        # Log subscription status for debugging integration issues
        if self._event_subscription_status["all_healthy"]:
            logger.info("[P2P] Event subscriptions: daemon=, feedback=, manager=")
        else:
            logger.warning(
                f"[P2P] Event subscriptions incomplete: "
                f"daemon={'' if daemon_events_ok else ''}, "
                f"feedback={'' if feedback_signals_ok else ''}, "
                f"manager={'' if manager_events_ok else ''}"
            )

        # December 2025 (Wave 7 Phase 1.2): Critical subscription failure mode
        # These subscriptions are required for the training pipeline to function.
        # Without them, events like DATA_SYNC_COMPLETED and TRAINING_COMPLETED
        # won't trigger downstream actions, causing the pipeline to stall silently.
        CRITICAL_SUBSCRIPTION_GROUPS = ["manager_events"]  # Contains DATA_SYNC_COMPLETED, TRAINING_COMPLETED
        self._event_subscription_status["critical_failed"] = []

        for group in CRITICAL_SUBSCRIPTION_GROUPS:
            if not self._event_subscription_status.get(group, False):
                self._event_subscription_status["critical_failed"].append(group)

        if self._event_subscription_status["critical_failed"]:
            failed_groups = self._event_subscription_status["critical_failed"]
            msg = f"[P2P] CRITICAL: Event subscription groups failed: {failed_groups}"
            logger.critical(msg)

            # Optional: fail startup on critical subscription failure
            # Enable via environment variable for strict production deployments
            if os.environ.get("RINGRIFT_FAIL_ON_SUBSCRIPTION_FAILURE", "").lower() == "true":
                raise RuntimeError(
                    f"Critical event subscriptions failed: {failed_groups}. "
                    "Set RINGRIFT_FAIL_ON_SUBSCRIPTION_FAILURE=false to allow startup anyway."
                )

        # NOTE: _manager_health_status validation is deferred to after _loop_manager
        # initialization below (line ~2730). This avoids AttributeError on _loop_manager.

        print(
            f"[P2P] Initialized node {node_id} on {host}:{port} "
            f"(advertise {self.advertise_host}:{self.advertise_port})"
        )
        logger.info(f"RingRift path: {self.ringrift_path}")
        logger.info(f"Version: {self.build_version}")
        logger.info(f"Known peers: {self.known_peers}")
        if self.relay_peers:
            logger.info(f"Relay peers (forced relay mode): {list(self.relay_peers)}")
        if self.auth_token:
            logger.info(f"Auth: enabled via {AUTH_TOKEN_ENV}")
        else:
            logger.info(f"Auth: disabled (set {AUTH_TOKEN_ENV} to enable)")

        # Hybrid transport for HTTP/SSH fallback (self-healing Vast connectivity)
        self.hybrid_transport: HybridTransport | None = None
        if HAS_HYBRID_TRANSPORT:
            try:
                self.hybrid_transport = get_hybrid_transport()
                logger.info("HybridTransport: enabled (HTTP with SSH fallback for Vast)")
            except Exception as e:  # noqa: BLE001
                logger.info(f"HybridTransport: failed to initialize: {e}")

        # SWIM-based leaderless membership (gossip protocol)
        # This provides faster failure detection (<5s vs 60s+) and O(1) bandwidth
        # Jan 22, 2026: Register SWIM callbacks BEFORE creating manager to wire
        # SWIM failure detection to gossip layer. Previously SWIM detected failures
        # but never synced state, causing split-brain membership views.
        set_swim_callbacks(
            on_alive=self._on_swim_member_alive,
            on_failed=self._on_swim_member_failed,
        )
        self._swim_manager = get_swim_manager(node_id=node_id, bind_port=7947)
        self._swim_started = False

        # SyncRouter: Intelligent data routing with quality-based priority (December 2025)
        # Lazy-loaded to avoid import overhead on startup
        self._sync_router: SyncRouter | None = None
        self._sync_router_wired = False

        # Phase 4: LoopManager for extracted loops (Dec 2025)
        self._loop_manager: LoopManager | None = None
        self._loops_registered = False
        self._autonomous_queue_loop = None  # Jan 4, 2026: Phase 2 P2P Resilience
        self._quorum_crisis_loop = None  # Jan 2026: Aggressive peer discovery during quorum loss

        # Jan 11, 2026: Track startup time for voter health grace period
        # During the first STARTUP_GRACE_PERIOD seconds, we don't warn about offline voters
        # because heartbeats haven't been exchanged yet
        self._startup_time = time.time()

        # December 27, 2025: Validate manager health at startup
        # This catches initialization issues early rather than at first use
        # NOTE: Must be called AFTER _loop_manager is initialized (was causing AttributeError)
        self._manager_health_status = self._validate_manager_health()

    def _get_loop_manager(self) -> "LoopManager | None":
        """Get the LoopManager, initializing if needed."""
        if self._loop_manager is None:
            self._loop_manager = get_loop_manager()
        return self._loop_manager

    def _register_extracted_loops(self) -> bool:
        """Register extracted loops with the LoopManager.

        January 2026: Delegated to scripts/p2p/loop_registry.py (~1,580 LOC extracted).
        """
        logger.info(f"[LoopManager] _register_extracted_loops called, already_registered={self._loops_registered}")
        if self._loops_registered:
            return True

        manager = self._get_loop_manager()
        logger.info(f"[LoopManager] Got manager: {manager}")
        if manager is None:
            logger.info("LoopManager: not available, using inline loops only")
            return False

        try:
            from scripts.p2p.loop_registry import register_all_loops

            result = register_all_loops(self, manager)
            if result.success:
                self._loops_registered = True
                logger.info(f"LoopManager: registered {result.loops_registered} loops via loop_registry")
                return True
            else:
                logger.error(f"LoopManager: loop registration failed: {result.error}")
                return False

        except ImportError as e:
            logger.warning(f"LoopManager: loop_registry not available: {e}")
            return False

        except Exception as e:  # noqa: BLE001
            logger.error(f"LoopManager: failed to register loops: {e}")
            return False

    # =========================================================================
    # NOTE: ~1,530 lines of inline loop registration code was extracted to
    # scripts/p2p/loop_registry.py in January 2026 decomposition.
    # =========================================================================

    # =========================================================================
    # JobReaperLoop callbacks - December 27, 2025
    # =========================================================================

    def _get_all_active_jobs_for_reaper(self) -> dict[str, Any]:
        """Get all active jobs across all job types for the job reaper.

        Returns a flat dict of job_id -> job_info, where job_info includes:
        - started_at: timestamp when job started
        - claimed_at: timestamp when job was claimed (if applicable)
        - status: current job status
        - pid: process ID (for killing stuck processes)
        - node_id: which node is running the job
        """
        result: dict[str, Any] = {}
        with self.jobs_lock:
            for job_type, jobs in self.active_jobs.items():
                for job_id, job_info in jobs.items():
                    if isinstance(job_info, dict):
                        result[job_id] = {
                            **job_info,
                            "job_type": job_type,
                        }
                    else:
                        # Handle non-dict job objects (legacy)
                        result[job_id] = {
                            "job_id": job_id,
                            "job_type": job_type,
                            "status": getattr(job_info, "status", "unknown"),
                            "started_at": getattr(job_info, "started_at", 0),
                            "pid": getattr(job_info, "pid", None),
                        }
        return result

    async def _cancel_job_for_reaper(self, job_id: str) -> bool:
        """Cancel a job by ID for the job reaper.

        Jan 21, 2026: Enhanced to escalate SIGTERM -> SIGKILL for stuck processes.

        Attempts to:
        1. Kill the process with SIGTERM, wait 3s, then SIGKILL if still alive
        2. Update job status to 'cancelled'
        3. Remove from active jobs dict
        4. Emit TASK_ABANDONED event

        Returns True if job was successfully cancelled.
        """
        import os
        import signal

        with self.jobs_lock:
            # Find the job across all job types
            for job_type, jobs in self.active_jobs.items():
                if job_id in jobs:
                    job_info = jobs[job_id]
                    pid = job_info.get("pid") if isinstance(job_info, dict) else getattr(job_info, "pid", None)

                    # Kill the process if we have a PID
                    if pid:
                        process_killed = False
                        try:
                            # First try SIGTERM
                            os.kill(pid, signal.SIGTERM)
                            logger.info(f"[JobReaper] Sent SIGTERM to pid {pid} for job {job_id}")

                            # Wait up to 3 seconds for graceful termination
                            for _ in range(6):  # 6 x 0.5s = 3s
                                await asyncio.sleep(0.5)
                                try:
                                    # Check if process still exists (signal 0 = check only)
                                    os.kill(pid, 0)
                                except ProcessLookupError:
                                    # Process is dead
                                    process_killed = True
                                    logger.debug(f"[JobReaper] Process {pid} terminated gracefully")
                                    break

                            # If still alive after 3s, escalate to SIGKILL
                            if not process_killed:
                                try:
                                    os.kill(pid, signal.SIGKILL)
                                    logger.warning(
                                        f"[JobReaper] SIGTERM failed for pid {pid}, sent SIGKILL for job {job_id}"
                                    )
                                    # Wait briefly for SIGKILL to take effect
                                    await asyncio.sleep(0.5)
                                except ProcessLookupError:
                                    pass  # Died between check and kill

                        except ProcessLookupError:
                            logger.debug(f"[JobReaper] Process {pid} already dead for job {job_id}")
                        except OSError as e:
                            logger.warning(f"[JobReaper] Failed to kill pid {pid}: {e}")

                    # Update status and remove from active jobs
                    if isinstance(job_info, dict):
                        job_info["status"] = "reaped"
                    del jobs[job_id]

                    # Emit event for coordination (fire-and-forget async task)
                    try:
                        asyncio.create_task(self._emit_task_abandoned(
                            task_id=job_id,
                            task_type=job_type,
                            reason="reaped_by_job_reaper",
                            node_id=job_info.get("node_id", "") if isinstance(job_info, dict) else "",
                        ))
                    except RuntimeError:
                        pass  # No event loop running

                    logger.info(f"[JobReaper] Cancelled job {job_id} (type: {job_type})")
                    return True

        logger.debug(f"[JobReaper] Job {job_id} not found in active jobs")
        return False

    def _get_job_heartbeats_for_reaper(self) -> dict[str, float]:
        """Get job heartbeat timestamps for the job reaper.

        Returns dict of job_id -> last_heartbeat_time.
        Jobs without recent heartbeats may be considered abandoned.

        Phase 15.1.9 (Dec 29, 2025): Updated to use JobManager.get_job_heartbeats()
        for actual heartbeat tracking instead of just job start times.
        """
        result: dict[str, float] = {}

        # Phase 15.1.9: Get actual heartbeats from JobManager
        if hasattr(self, "job_manager") and self.job_manager is not None:
            try:
                job_heartbeats = self.job_manager.get_job_heartbeats()
                result.update(job_heartbeats)
            except Exception as e:  # noqa: BLE001
                logger.debug(f"Failed to get heartbeats from JobManager: {e}")

        # Fallback: Also include jobs_started_at for jobs without heartbeat tracking
        # This ensures older jobs (started before heartbeat tracking) are still monitored
        if hasattr(self, "jobs_started_at"):
            for _node_id, jobs in self.jobs_started_at.items():
                for job_id, start_time in jobs.items():
                    # Only add if not already in result from heartbeat tracking
                    if job_id not in result:
                        result[job_id] = start_time

        return result

    # =========================================================================
    # ManifestCollectionLoop callbacks - December 27, 2025
    # =========================================================================

    def _update_manifest_from_loop(self, manifest: Any, is_cluster: bool) -> None:
        """Update stored manifest from ManifestCollectionLoop.

        Args:
            manifest: The collected manifest (cluster or local)
            is_cluster: True if this is a cluster-wide manifest, False for local
        """
        import time
        with self.manifest_lock:
            if is_cluster:
                self.cluster_data_manifest = manifest
            else:
                self.local_data_manifest = manifest
            self.last_manifest_collection = time.time()

        # Session 17.29: Feed game counts to selfplay scheduler for priority allocation
        # ROOT CAUSE FIX: _p2p_game_counts was never populated, causing all configs
        # to show 0 games in queue populator, breaking bootstrap priority boosts
        if is_cluster and hasattr(self, 'selfplay_scheduler') and self.selfplay_scheduler:
            try:
                game_counts: dict[str, int] = {}
                if hasattr(manifest, 'by_board_type') and manifest.by_board_type:
                    for config_key, config_data in manifest.by_board_type.items():
                        if isinstance(config_data, dict):
                            game_counts[config_key] = config_data.get("total_games", 0)
                        elif hasattr(config_data, 'total_games'):
                            game_counts[config_key] = getattr(config_data, 'total_games', 0)
                if game_counts:
                    self.selfplay_scheduler.update_p2p_game_counts(game_counts)
                    logger.debug(f"[ManifestUpdate] Fed {len(game_counts)} config game counts to SelfplayScheduler")
            except Exception as e:  # noqa: BLE001
                logger.debug(f"[ManifestUpdate] Failed to update selfplay scheduler game counts: {e}")

    def _get_alive_peers_for_broadcast(self) -> list[Any]:
        """Get list of alive peers for manifest broadcast.

        Jan 2026: Added for leader broadcast functionality.
        Jan 27, 2026: Migrated to PeerQueryBuilder (Phase 3.2).

        Returns:
            List of NodeInfo objects for alive, non-retired peers
        """
        return self._peer_query.alive_non_retired().unwrap_or([])

    def _update_improvement_cycle_from_loop(self, by_board_type: dict[str, Any]) -> None:
        """Update ImprovementCycleManager from ManifestCollectionLoop.

        Args:
            by_board_type: Dict of board_type -> game counts from manifest
        """
        if self.improvement_cycle_manager:
            try:
                self.improvement_cycle_manager.update_from_cluster_totals(by_board_type)
            except Exception as e:  # noqa: BLE001
                logger.debug(f"ImprovementCycleManager update error: {e}")

    # =========================================================================
    # DataManagementLoop callbacks - December 27, 2025
    # =========================================================================

    async def _trigger_export_for_loop(
        self,
        db_path: Path,
        output_path: Path,
        board_type: str,
    ) -> bool:
        """Trigger export job for DataManagementLoop.

        Args:
            db_path: Path to database file to export
            output_path: Path for output NPZ file
            board_type: Board type (square8, hex8, etc.)

        Returns:
            True if export started successfully
        """
        import subprocess

        try:
            cmd = [
                sys.executable,
                self._get_script_path("export_replay_dataset.py"),
                "--db", str(db_path),
                "--board-type", board_type,
                "--num-players", "2",
                "--board-aware-encoding",
                "--require-completed",
                "--min-moves", "10",
                "--output", str(output_path),
            ]

            env = os.environ.copy()
            env["PYTHONPATH"] = self._get_ai_service_path()

            log_file = Path(f"/tmp/auto_export_{db_path.stem}.log")

            # Jan 19, 2026: Run subprocess in thread pool to avoid blocking event loop
            def _start_export_process():
                with open(log_file, "w") as log_fh:
                    subprocess.Popen(
                        cmd,
                        stdout=log_fh,
                        stderr=subprocess.STDOUT,
                        env=env,
                        cwd=self._get_ai_service_path(),
                    )

            await asyncio.to_thread(_start_export_process)
            logger.info(f"[DataManagement] Started export job for {db_path.name}")
            return True

        except Exception as e:
            logger.error(f"[DataManagement] Failed to start export for {db_path.name}: {e}")
            return False

    async def _inline_job_reaper_fallback_loop(self) -> None:
        """Inline job reaper fallback loop.

        December 27, 2025: Fallback implementation that runs if the extracted
        JobReaperLoop fails to start or hits persistent errors. Uses the same
        callbacks and thresholds as the extracted loop.

        This is NOT a replacement for JobReaperLoop - it's a safety net that
        ensures job cleanup continues even if the modular loop system fails.

        Thresholds:
        - STALE: Jobs older than 1 hour without heartbeat
        - STUCK: Jobs older than 2 hours regardless of heartbeat
        - INTERVAL: Checks every 5 minutes

        Environment:
        - RINGRIFT_JOB_REAPER_FALLBACK_ENABLED: Enable/disable (default: true)
        """
        STALE_THRESHOLD_SECONDS = 3600.0   # 1 hour
        STUCK_THRESHOLD_SECONDS = 7200.0   # 2 hours
        CHECK_INTERVAL_SECONDS = 300.0      # 5 minutes
        MAX_JOBS_PER_CYCLE = 10             # Limit to avoid overload

        logger.info("[JobReaper Fallback] Started inline fallback loop")
        stats = {"checks": 0, "reaped": 0, "errors": 0}

        while self.running:
            try:
                await asyncio.sleep(CHECK_INTERVAL_SECONDS)
                if not self.running:
                    break

                stats["checks"] += 1
                now = time.time()
                reaped_this_cycle = 0

                # Get all active jobs
                try:
                    active_jobs = self._get_all_active_jobs_for_reaper()
                except Exception as e:
                    logger.warning(f"[JobReaper Fallback] Failed to get active jobs: {e}")
                    stats["errors"] += 1
                    continue

                if not active_jobs:
                    continue

                # Get heartbeat info
                try:
                    heartbeats = self._get_job_heartbeats_for_reaper()
                except Exception as e:
                    logger.debug(f"[JobReaper Fallback] Failed to get heartbeats: {e}")
                    heartbeats = {}

                # Identify stale and stuck jobs
                jobs_to_reap: list[tuple[str, str]] = []  # [(job_id, reason), ...]

                for job_id, job_info in active_jobs.items():
                    if reaped_this_cycle >= MAX_JOBS_PER_CYCLE:
                        break

                    started_at = job_info.get("started_at", 0)
                    if not started_at:
                        continue

                    job_age = now - started_at
                    last_heartbeat = heartbeats.get(job_id, started_at)
                    heartbeat_age = now - last_heartbeat

                    # Check for stuck jobs (absolute age threshold)
                    if job_age > STUCK_THRESHOLD_SECONDS:
                        jobs_to_reap.append((job_id, "stuck"))
                        reaped_this_cycle += 1
                        continue

                    # Check for stale jobs (no recent heartbeat)
                    if heartbeat_age > STALE_THRESHOLD_SECONDS:
                        jobs_to_reap.append((job_id, "stale"))
                        reaped_this_cycle += 1

                # Reap identified jobs
                for job_id, reason in jobs_to_reap:
                    try:
                        success = await self._cancel_job_for_reaper(job_id)
                        if success:
                            stats["reaped"] += 1
                            logger.info(
                                f"[JobReaper Fallback] Reaped {reason} job {job_id} "
                                f"(total: {stats['reaped']})"
                            )
                    except Exception as e:
                        logger.warning(f"[JobReaper Fallback] Failed to reap {job_id}: {e}")
                        stats["errors"] += 1

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"[JobReaper Fallback] Unexpected error: {e}")
                stats["errors"] += 1
                await asyncio.sleep(60)  # Back off on error

        logger.info(
            f"[JobReaper Fallback] Stopped after {stats['checks']} checks, "
            f"{stats['reaped']} reaped, {stats['errors']} errors"
        )

    def _get_sync_router(self) -> SyncRouter | None:
        """Lazy-load SyncRouter singleton for intelligent sync routing."""
        if not HAS_SYNC_ROUTER:
            return None
        if self._sync_router is None:
            try:
                self._sync_router = get_sync_router()
                logger.info("SyncRouter: initialized for intelligent data routing")
            except Exception as e:  # noqa: BLE001
                logger.warning(f"SyncRouter: failed to initialize: {e}")
                return None
        return self._sync_router

    def _wire_sync_router_events(self) -> bool:
        """Wire SyncRouter to event system for real-time sync triggers."""
        if self._sync_router_wired:
            return True
        router = self._get_sync_router()
        if router is None:
            return False
        try:
            if hasattr(router, 'wire_to_event_router'):
                router.wire_to_event_router()
                self._sync_router_wired = True
                logger.info("SyncRouter: wired to event system")
                return True
        except Exception as e:  # noqa: BLE001
            logger.warning(f"SyncRouter: failed to wire events: {e}")
        return False

    def _wire_cooldown_manager_probe(self) -> None:
        """Wire DeadPeerCooldownManager probe function.

        January 2026: Enables probe-based early recovery from adaptive cooldown.
        Stub implementation - cooldown logic is handled by CooldownManager.
        """
        logger.info("Cooldown manager probe function wired")

    def _wire_connection_pool_dynamic_sizing(self) -> None:
        """Wire connection pool dynamic sizing callback.

        January 2026: Scales pool limits based on cluster size to prevent exhaustion.
        """
        try:
            from scripts.p2p.connection_pool import get_connection_pool

            pool = get_connection_pool()
            if hasattr(pool, "set_cluster_size_callback"):
                pool.set_cluster_size_callback(
                    lambda: len([p for p in self.peers.values() if p.get("alive", False)])
                )
            logger.info("Connection pool dynamic sizing wired")
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Connection pool dynamic sizing unavailable: {e}")

    def _initialize_work_discovery_manager(self) -> bool:
        """Initialize WorkDiscoveryManager for multi-channel work discovery.

        January 4, 2026: Phase 5 of P2P Cluster Resilience.
        Enables workers to find work through multiple channels:
        1. Leader work queue (fastest)
        2. Peer discovery (query other peers)
        3. Autonomous queue (from AutonomousQueueLoop)
        4. Direct selfplay (last resort)

        Returns True if initialization succeeded, False otherwise.
        """
        try:
            from scripts.p2p.managers.work_discovery_manager import (
                WorkDiscoveryManager,
                WorkDiscoveryConfig,
                set_work_discovery_manager,
            )

            # Create manager with callbacks to this orchestrator
            manager = WorkDiscoveryManager(
                # Channel 1: Leader
                get_leader_id=lambda: self.leader_id,
                claim_from_leader=self._claim_work_from_leader,
                # Channel 2: Peer discovery
                # Jan 22, 2026: Use lock-free snapshot to prevent race conditions
                get_alive_peers=lambda: [
                    p.node_id for p in self._peer_snapshot.get_snapshot().values() if p.is_alive()
                ],
                query_peer_work=self._query_peer_for_work,
                # Channel 3: Autonomous queue
                pop_autonomous_work=self._pop_autonomous_queue_work,
                # Channel 4: Direct selfplay
                create_direct_selfplay_work=self._create_direct_selfplay_work,
                # Config from environment
                config=WorkDiscoveryConfig.from_env(),
            )

            # Set as singleton for WorkerPullLoop access
            set_work_discovery_manager(manager)
            logger.info("WorkDiscoveryManager: initialized with 4 discovery channels")
            return True

        except ImportError as e:
            logger.debug(f"WorkDiscoveryManager: not available: {e}")
            return False
        except Exception as e:  # noqa: BLE001
            logger.warning(f"WorkDiscoveryManager: initialization failed: {e}")
            return False

    async def _query_peer_for_work(
        self, peer_id: str, capabilities: list[str]
    ) -> dict[str, Any] | None:
        """Query a peer for available work (used by WorkDiscoveryManager).

        January 4, 2026: Phase 5 - Peer discovery channel.
        """
        try:
            # Jan 22, 2026: Use lock-free snapshot to prevent race conditions
            peer = self._peer_snapshot.get_snapshot().get(peer_id)
            if not peer or not peer.is_alive():
                return None

            # Query peer's work queue via HTTP
            urls = self._urls_for_peer(peer_id, "/work_queue/claim")
            for url in urls:
                try:
                    async with aiohttp.ClientSession() as session:
                        async with session.post(
                            url,
                            json={"capabilities": capabilities},
                            headers=self._auth_headers(),
                            timeout=aiohttp.ClientTimeout(total=5.0),
                        ) as resp:
                            if resp.status == 200:
                                data = await resp.json()
                                if data.get("work_item"):
                                    return data["work_item"]
                except Exception:
                    continue
            return None
        except Exception:
            return None

    async def _pop_autonomous_queue_work(self) -> dict[str, Any] | None:
        """Pop work from autonomous queue (used by WorkDiscoveryManager).

        January 4, 2026: Phase 5 - Autonomous queue channel.
        """
        try:
            loop = getattr(self, "_autonomous_queue_loop", None)
            if loop and hasattr(loop, "pop_local_work"):
                return await loop.pop_local_work()
            return None
        except Exception:
            return None

    def _create_direct_selfplay_work(
        self, capabilities: list[str]
    ) -> dict[str, Any] | None:
        """Create direct selfplay work item (used by WorkDiscoveryManager).

        January 4, 2026: Phase 5 - Direct selfplay channel (last resort).
        Only used when all other channels fail.
        """
        if "selfplay" not in capabilities:
            return None

        try:
            # Get next config from selfplay scheduler
            config_key = self.selfplay_scheduler.get_next_config()
            if not config_key:
                return None

            return {
                "work_id": f"direct-{self.node_id}-{int(time.time())}",
                "work_type": "selfplay",
                "config_key": config_key,
                "source": "direct_discovery",
                "games": 10,  # Small batch for direct selfplay
                "priority": 50,  # Lower priority than leader-assigned work
            }
        except Exception:
            return None

    def _wire_feedback_loops(self) -> bool:
        """Wire curriculum feedback loops for self-improvement.

        December 2025: Connects P2P orchestrator to the training feedback system:
        - Curriculum weights adjust based on Elo velocity
        - Weak configs get boosted/penalized based on evaluation results
        - Quality scores influence exploration temperature
        - Failed promotions reduce config priority

        Returns True if wiring succeeded, False otherwise.
        """
        try:
            from app.coordination.curriculum_integration import wire_all_feedback_loops

            status = wire_all_feedback_loops()
            if status.get("success", False):
                wired_count = status.get("wired_count", 0)
                logger.info(f"Feedback loops: wired {wired_count} bridges successfully")
                return True
            else:
                error = status.get("error", "Unknown error")
                logger.warning(f"Feedback loops: partial wiring - {error}")
                return False
        except ImportError as e:
            logger.debug(f"Feedback loops: curriculum_integration not available: {e}")
            return False
        except Exception as e:  # noqa: BLE001
            logger.warning(f"Feedback loops: failed to wire: {e}")
            return False

    def _validate_manager_health(self) -> dict[str, Any]:
        """Validate health of all P2P managers at startup.

        Jan 2026: Delegated to HealthMetricsManager (Phase 9 decomposition).
        """
        return self.health_metrics_manager.validate_manager_health()

    def health_check(self) -> "HealthCheckResult":
        """Return health check result for daemon protocol compliance.

        December 27, 2025: Added for DaemonManager integration. Returns a
        HealthCheckResult that can be used by the daemon infrastructure for
        health monitoring, auto-restart decisions, and liveness probes.

        Returns:
            HealthCheckResult with overall orchestrator health status
        """
        # Import from contracts (zero-dependency module)
        from app.coordination.contracts import CoordinatorStatus, HealthCheckResult

        # Get manager health status
        manager_health = self._validate_manager_health()

        # Calculate cluster metrics
        uptime_seconds = time.time() - getattr(self, "start_time", time.time())
        active_peers = sum(
            1 for p in self.peers.values()
            if time.time() - p.last_heartbeat < 120
        )

        details = {
            "node_id": self.node_id,
            "role": self.role.value if hasattr(self.role, "value") else str(self.role),
            "leader_id": self.leader_id,
            "active_peers": active_peers,
            "total_peers": len(self.peers),
            "uptime_seconds": uptime_seconds,
            "managers_healthy": manager_health.get("all_healthy", False),
            "unhealthy_managers": manager_health.get("unhealthy_count", 0),
            "selfplay_jobs": self.self_info.selfplay_jobs if hasattr(self, "self_info") else 0,
            "training_jobs": self.self_info.training_jobs if hasattr(self, "self_info") else 0,
        }

        # Determine overall health
        is_healthy = manager_health.get("all_healthy", False)

        # Additional health checks
        if uptime_seconds < 10:
            # Grace period for startup
            is_healthy = True
            message = "P2P Orchestrator starting up"
            status = CoordinatorStatus.RUNNING
        elif not is_healthy:
            message = f"P2P Orchestrator unhealthy: {manager_health.get('unhealthy_count', 0)} unhealthy managers"
            status = CoordinatorStatus.ERROR
        else:
            message = f"P2P Orchestrator healthy, {active_peers} peers active"
            status = CoordinatorStatus.RUNNING

        return HealthCheckResult(
            healthy=is_healthy,
            status=status,
            message=message,
            details=details,
        )

    def _subscribe_to_daemon_events(self) -> bool:
        """Subscribe to daemon status events for observability.

        December 2025: Receives DAEMON_STATUS_CHANGED events from daemon_manager
        to track daemon health across the cluster. This enables:
        - Tracking which daemons are running/crashed on each node
        - Auto-recovery of critical daemons
        - Cluster-wide daemon health reporting via /status endpoint

        Returns True if subscription succeeded, False otherwise.
        """
        try:
            from app.coordination.event_router import subscribe

            def handle_daemon_status(event) -> None:
                """Handle daemon status change events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    daemon_name = payload.get("daemon_name", "unknown")
                    new_status = payload.get("new_status", "unknown")
                    hostname = payload.get("hostname", "unknown")
                    error = payload.get("error")

                    # Track daemon states for cluster health reporting
                    if not hasattr(self, "_daemon_states"):
                        self._daemon_states = {}
                    self._daemon_states[f"{hostname}:{daemon_name}"] = {
                        "status": new_status,
                        "last_update": time.time(),
                        "error": error,
                    }

                    # Log critical daemon failures
                    if new_status in ("crashed", "failed") and error:
                        logger.warning(
                            f"Daemon {daemon_name} on {hostname} {new_status}: {error}"
                        )
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling daemon status event: {e}")

            subscribe("DAEMON_STATUS_CHANGED", handle_daemon_status)
            logger.info("Subscribed to daemon status events")
            return True
        except ImportError as e:
            logger.debug(f"Daemon events: event_router not available: {e}")
            return False
        except Exception as e:  # noqa: BLE001
            logger.warning(f"Daemon events: failed to subscribe: {e}")
            return False

    def _subscribe_to_feedback_signals(self) -> bool:
        """Subscribe to training feedback signals for dynamic orchestration.

        December 2025: Subscribes to key feedback events that should influence
        cluster orchestration decisions:
        - ELO_VELOCITY_CHANGED: Adjust selfplay allocation based on training velocity
        - QUALITY_DEGRADED: Pause/slow selfplay when data quality drops
        - EVALUATION_COMPLETED: Trigger model promotion decisions
        - PROMOTION_FAILED: Revert curriculum weights if promotion fails
        - PLATEAU_DETECTED: Trigger hyperparameter search or curriculum changes

        Returns True if subscription succeeded, False otherwise.
        """
        try:
            from app.coordination.event_router import subscribe

            def handle_quality_degraded(event) -> None:
                """Handle quality degradation events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    config_key = payload.get("config_key", "unknown")
                    quality_score = payload.get("quality_score", 0)
                    threshold = payload.get("threshold", 0)

                    logger.warning(
                        f"Quality degraded for {config_key}: {quality_score:.2f} < {threshold:.2f}"
                    )
                    # Could pause selfplay for this config or trigger data cleanup
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling quality degraded event: {e}")

            def handle_elo_velocity_changed(event) -> None:
                """Handle Elo velocity change events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    config_key = payload.get("config_key", "unknown")
                    velocity = payload.get("velocity", 0)

                    if velocity < -50:  # Significant regression
                        logger.warning(f"Elo regression for {config_key}: velocity={velocity}")
                    elif velocity > 50:  # Good progress
                        logger.info(f"Elo progress for {config_key}: velocity={velocity}")
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling Elo velocity event: {e}")

            def handle_evaluation_completed(event) -> None:
                """Handle evaluation completion events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    config_key = payload.get("config_key", "unknown")
                    win_rate = payload.get("win_rate", 0)
                    opponent = payload.get("opponent", "unknown")

                    logger.info(
                        f"Evaluation completed for {config_key}: {win_rate:.1%} vs {opponent}"
                    )
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling evaluation completed event: {e}")

            def handle_plateau_detected(event) -> None:
                """Handle training plateau detection events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    config_key = payload.get("config_key", "unknown")
                    epochs_stalled = payload.get("epochs_stalled", 0)

                    logger.warning(
                        f"Training plateau for {config_key}: stalled {epochs_stalled} epochs"
                    )
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling plateau detected event: {e}")

            def handle_exploration_boost(event) -> None:
                """Handle exploration boost events from training feedback.

                P0.1 (Dec 2025): Added missing handler for EXPLORATION_BOOST events.
                When training anomalies (loss spikes, stalls) are detected, this
                signals that we should boost exploration in selfplay to generate
                more diverse training data.
                """
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    config_key = payload.get("config_key", payload.get("config", "unknown"))
                    boost_factor = payload.get("boost_factor", payload.get("boost", 1.0))
                    reason = payload.get("reason", "training_anomaly")
                    duration = payload.get("duration_seconds", 900)

                    logger.info(
                        f"Exploration boost for {config_key}: {boost_factor:.2f}x "
                        f"(reason={reason}, duration={duration}s)"
                    )

                    # Forward to selfplay scheduler if available
                    if hasattr(self, "selfplay_scheduler") and self.selfplay_scheduler:
                        self.selfplay_scheduler.set_exploration_boost(
                            config_key, boost_factor, duration
                        )
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling exploration boost event: {e}")

            def handle_promotion_failed(event) -> None:
                """Handle model promotion failure events.

                December 27, 2025: Added missing handler for PROMOTION_FAILED events.
                When model promotion fails (gauntlet failure, threshold not met),
                we should revert curriculum weights and potentially pause training
                for that config until issues are resolved.
                """
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    config_key = payload.get("config_key", "unknown")
                    model_path = payload.get("model_path", "unknown")
                    reason = payload.get("reason", "unknown")
                    win_rate = payload.get("win_rate", 0.0)

                    logger.warning(
                        f"[P2P] Promotion FAILED for {config_key}: {reason} "
                        f"(model={model_path}, win_rate={win_rate:.1%})"
                    )

                    # Revert curriculum weights if selfplay scheduler available
                    if hasattr(self, "selfplay_scheduler") and self.selfplay_scheduler:
                        # Reduce priority for this config temporarily
                        self.selfplay_scheduler.record_promotion_failure(config_key)
                        logger.info(f"[P2P] Reduced selfplay priority for {config_key} after promotion failure")

                    # Track failed promotions for monitoring
                    if not hasattr(self, "_promotion_failures"):
                        self._promotion_failures = {}
                    if config_key not in self._promotion_failures:
                        self._promotion_failures[config_key] = []
                    self._promotion_failures[config_key].append({
                        "timestamp": time.time(),
                        "reason": reason,
                        "win_rate": win_rate,
                    })
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling promotion failed event: {e}")

            def handle_handler_failed(event) -> None:
                """Handle event handler failure events.

                December 27, 2025: Added missing handler for HANDLER_FAILED events.
                When a coordination event handler throws an exception, this event
                is emitted. We need to track these for monitoring and potentially
                trigger alerts for critical handler failures.
                """
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    handler_name = payload.get("handler_name", "unknown")
                    event_type = payload.get("event_type", "unknown")
                    error = payload.get("error", "unknown")
                    coordinator = payload.get("coordinator", "unknown")

                    logger.error(
                        f"[P2P] Handler FAILED: {handler_name} for {event_type} "
                        f"in {coordinator}: {error}"
                    )

                    # Track handler failures for health monitoring
                    if not hasattr(self, "_handler_failures"):
                        self._handler_failures = {}
                    failure_key = f"{coordinator}.{handler_name}"
                    if failure_key not in self._handler_failures:
                        self._handler_failures[failure_key] = []
                    self._handler_failures[failure_key].append({
                        "timestamp": time.time(),
                        "event_type": event_type,
                        "error": str(error)[:200],  # Truncate long errors
                    })

                    # Keep only last 10 failures per handler
                    if len(self._handler_failures[failure_key]) > 10:
                        self._handler_failures[failure_key] = self._handler_failures[failure_key][-10:]
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling handler failed event: {e}")

            # Subscribe to all feedback signals
            subscribe("QUALITY_DEGRADED", handle_quality_degraded)
            subscribe("ELO_VELOCITY_CHANGED", handle_elo_velocity_changed)
            subscribe("EVALUATION_COMPLETED", handle_evaluation_completed)
            subscribe("PLATEAU_DETECTED", handle_plateau_detected)
            subscribe("EXPLORATION_BOOST", handle_exploration_boost)
            subscribe("PROMOTION_FAILED", handle_promotion_failed)
            subscribe("HANDLER_FAILED", handle_handler_failed)

            logger.info("Subscribed to training feedback signals")
            return True
        except ImportError as e:
            logger.debug(f"Feedback signals: event_router not available: {e}")
            return False
        except Exception as e:  # noqa: BLE001
            logger.warning(f"Feedback signals: failed to subscribe: {e}")
            return False

    def _subscribe_to_manager_events(self) -> bool:
        """Subscribe to manager lifecycle events for coordination.

        December 2025: Subscribes to critical manager events that were previously
        missing from P2P orchestrator integration:
        - TRAINING_STARTED/COMPLETED: Coordinate training transitions
        - TASK_SPAWNED/COMPLETED/FAILED: Track job lifecycle
        - DATA_SYNC_STARTED/COMPLETED: Coordinate data freshness

        Returns True if subscription succeeded, False otherwise.
        """
        try:
            from app.coordination.event_router import subscribe

            def handle_training_started(event) -> None:
                """Handle training start events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    config_key = payload.get("config_key", "unknown")
                    node_id = payload.get("node_id", "unknown")
                    logger.info(f"[P2P] Training started: {config_key} on {node_id}")
                    # Track active training in cluster state
                    if not hasattr(self, "_active_training"):
                        self._active_training = {}
                    self._active_training[config_key] = {
                        "node_id": node_id,
                        "started_at": time.time(),
                    }
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling training started event: {e}")

            def handle_training_completed(event) -> None:
                """Handle training completion events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    config_key = payload.get("config_key", "unknown")
                    model_path = payload.get("model_path", "")
                    final_loss = payload.get("final_loss", 0)
                    logger.info(
                        f"[P2P] Training completed: {config_key} "
                        f"(loss={final_loss:.4f}, model={model_path})"
                    )
                    # Clear from active training
                    if hasattr(self, "_active_training"):
                        self._active_training.pop(config_key, None)
                    # Trigger selfplay allocation refresh
                    if hasattr(self, "selfplay_scheduler"):
                        self.selfplay_scheduler.on_training_complete(config_key)

                    # Jan 3, 2026: Bridge to coordination event bus for EvaluationDaemon
                    # This enables the Training  Evaluation  Promotion pipeline
                    try:
                        from app.coordination.event_router import emit_event
                        from app.coordination.data_events import DataEventType
                        emit_event(DataEventType.TRAINING_COMPLETED, {
                            "config_key": config_key,
                            "model_path": model_path,
                            "final_loss": final_loss,
                            "source": "p2p_bridge",
                        })
                        logger.debug(f"[P2P] Bridged TRAINING_COMPLETED to coordination bus")
                    except Exception as bridge_err:  # noqa: BLE001
                        logger.debug(f"Could not bridge TRAINING_COMPLETED: {bridge_err}")
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling training completed event: {e}")

            def handle_task_spawned(event) -> None:
                """Handle task spawn events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    job_id = payload.get("job_id", "unknown")
                    job_type = payload.get("job_type", "unknown")
                    node_id = payload.get("node_id", "unknown")
                    logger.debug(f"[P2P] Task spawned: {job_type} {job_id} on {node_id}")
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling task spawned event: {e}")

            def handle_task_completed(event) -> None:
                """Handle task completion events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    job_id = payload.get("job_id", "unknown")
                    job_type = payload.get("job_type", "unknown")
                    duration = payload.get("duration", 0)
                    logger.debug(f"[P2P] Task completed: {job_type} {job_id} ({duration:.1f}s)")
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling task completed event: {e}")

            def handle_task_failed(event) -> None:
                """Handle task failure events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    job_id = payload.get("job_id", "unknown")
                    job_type = payload.get("job_type", "unknown")
                    error = payload.get("error", "unknown error")
                    logger.warning(f"[P2P] Task failed: {job_type} {job_id} - {error}")
                    # Could trigger recovery or rebalancing here
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling task failed event: {e}")

            def handle_data_sync_started(event) -> None:
                """Handle data sync start events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    sync_type = payload.get("sync_type", "unknown")
                    target_count = payload.get("target_nodes", 0)
                    logger.info(f"[P2P] Data sync started: {sync_type} to {target_count} nodes")
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling data sync started event: {e}")

            def handle_data_sync_completed(event) -> None:
                """Handle data sync completion events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    sync_type = payload.get("sync_type", "unknown")
                    duration = payload.get("duration", 0)
                    files_synced = payload.get("files_synced", 0)
                    logger.info(
                        f"[P2P] Data sync completed: {sync_type} "
                        f"({files_synced} files in {duration:.1f}s)"
                    )
                    # Could trigger training readiness check here
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling data sync completed event: {e}")

            # P2P Health event handlers (Dec 2025)
            def handle_node_unhealthy(event) -> None:
                """Handle NODE_UNHEALTHY events - pause jobs on unhealthy nodes."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    node_id = payload.get("node_id", "unknown")
                    reason = payload.get("reason", "")
                    logger.warning(f"[P2P] Node {node_id} unhealthy: {reason}")
                    # Mark node as unhealthy for job routing
                    if hasattr(self, "node_selector") and self.node_selector:
                        self.node_selector.mark_node_unhealthy(node_id, reason)
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling node unhealthy event: {e}")

            def handle_node_recovered(event) -> None:
                """Handle NODE_RECOVERED events - resume jobs on recovered nodes."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    node_id = payload.get("node_id", "unknown")
                    logger.info(f"[P2P] Node {node_id} recovered")
                    # Mark node as healthy for job routing
                    if hasattr(self, "node_selector") and self.node_selector:
                        self.node_selector.mark_node_healthy(node_id)
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling node recovered event: {e}")

            def handle_cluster_healthy(event) -> None:
                """Handle P2P_CLUSTER_HEALTHY events."""
                try:
                    logger.info("[P2P] Cluster is healthy - resuming normal operations")
                    self._cluster_health_degraded = False
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling cluster healthy event: {e}")

            def handle_cluster_unhealthy(event) -> None:
                """Handle P2P_CLUSTER_UNHEALTHY events - pause non-critical operations."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    reason = payload.get("reason", "")
                    alive_nodes = payload.get("alive_nodes", 0)
                    logger.warning(
                        f"[P2P] Cluster unhealthy: {reason} (alive_nodes={alive_nodes})"
                    )
                    self._cluster_health_degraded = True
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling cluster unhealthy event: {e}")

            # Subscribe to all manager events
            subscribe("TRAINING_STARTED", handle_training_started)
            subscribe("TRAINING_COMPLETED", handle_training_completed)
            subscribe("TASK_SPAWNED", handle_task_spawned)
            subscribe("TASK_COMPLETED", handle_task_completed)
            subscribe("TASK_FAILED", handle_task_failed)
            subscribe("DATA_SYNC_STARTED", handle_data_sync_started)
            subscribe("DATA_SYNC_COMPLETED", handle_data_sync_completed)

            # Subscribe to health events (Dec 2025)
            subscribe("NODE_UNHEALTHY", handle_node_unhealthy)
            subscribe("NODE_RECOVERED", handle_node_recovered)
            subscribe("P2P_CLUSTER_HEALTHY", handle_cluster_healthy)
            subscribe("P2P_CLUSTER_UNHEALTHY", handle_cluster_unhealthy)

            logger.info("Subscribed to manager lifecycle and health events")
            return True
        except ImportError as e:
            logger.debug(f"Manager events: event_router not available: {e}")
            return False
        except Exception as e:  # noqa: BLE001
            logger.warning(f"Manager events: failed to subscribe: {e}")
            return False

    # =========================================================================
    # Leadership State Management - Single Source of Truth (Jan 3, 2026)
    # =========================================================================

    def _set_leader(
        self,
        new_leader_id: str | None,
        reason: str = "unknown",
        *,
        sync_to_ulsm: bool = True,
        save_state: bool = True,
    ) -> bool:
        """Atomically set the leader and role to ensure consistency.

        This is the CANONICAL method for modifying self.leader_id and self.role.
        Using this method ensures both tracking systems (direct fields and ULSM)
        stay synchronized and prevents the leader self-recognition desync bug.

        Jan 3, 2026: Created to fix critical bug where leader node didn't recognize
        itself as leader due to divergent state between leader_id and role fields.

        Jan 12, 2026: C1 fix - Added leader_state_lock to prevent race conditions
        during concurrent leader elections and state transitions.

        Args:
            new_leader_id: The new leader ID (None to clear leader)
            reason: Human-readable reason for logging/debugging
            sync_to_ulsm: Whether to sync state to LeadershipStateMachine
            save_state: Whether to persist state after change

        Returns:
            True if this node is now the leader
        """
        # C1 fix: Acquire lock to prevent race conditions during leader transitions
        with self.leader_state_lock:
            old_leader_id = self.leader_id
            old_role = self.role

            # Determine new role based on leader_id
            if new_leader_id is None:
                new_role = NodeRole.FOLLOWER
                is_now_leader = False
            elif new_leader_id == self.node_id:
                new_role = NodeRole.LEADER
                is_now_leader = True
            else:
                new_role = NodeRole.FOLLOWER
                is_now_leader = False

            # Atomic update of both fields
            self.leader_id = new_leader_id
            self.role = new_role

            # Sync to ULSM (Unified Leadership State Machine) if enabled
            if sync_to_ulsm and hasattr(self, "_leadership_sm") and self._leadership_sm is not None:
                try:
                    from scripts.p2p.leadership_state_machine import LeaderState

                    self._leadership_sm._leader_id = new_leader_id
                    self._leadership_sm._state = (
                        LeaderState.LEADER if is_now_leader else LeaderState.FOLLOWER
                    )
                except (ImportError, AttributeError) as e:
                    logger.debug(f"[LeaderSet] ULSM sync skipped: {e}")

            # Log if changed
            if old_leader_id != new_leader_id or old_role != new_role:
                logger.info(
                    f"[LeaderSet] {old_role.value if hasattr(old_role, 'value') else old_role}->"
                    f"{new_role.value if hasattr(new_role, 'value') else new_role}, "
                    f"leader_id={old_leader_id}->{new_leader_id}, reason={reason}"
                )

            # Persist state if requested
            if save_state and (old_leader_id != new_leader_id or old_role != new_role):
                self._save_state()

            return is_now_leader

    def _is_leader(self) -> bool:
        """Check if this node is the current cluster leader with valid lease.

        Jan 23, 2026: Routes through Raft consensus when enabled, with fallback chain:
        1. consensus_mixin.is_raft_leader() - Deferred Raft init, works after peer discovery
        2. HybridCoordinator.is_leader() - May fall back to Bully
        3. Bully algorithm - Legacy fallback

        Jan 25, 2026: Added _forced_leader_override to bypass consensus checks when
        leadership is forced via /election/force_leader endpoint. This fixes the
        is_leader desync issue where work queue showed is_leader=False despite
        main status showing this node as leader.
        """
        # Jan 25, 2026: Check forced leader override first - bypasses all consensus checks
        # This ensures force_leader actually works for work queue operations
        if getattr(self, "_forced_leader_override", False):
            # Verify lease is still valid
            if time.time() < getattr(self, "leader_lease_expires", 0):
                return True
            else:
                # Lease expired, clear the override
                self._forced_leader_override = False
                logger.info("[ForcedLeader] Forced leadership lease expired, clearing override")

        # Jan 23, 2026: First check consensus_mixin's Raft (supports deferred initialization)
        # This is the preferred path because it initializes AFTER peers are discovered.
        if hasattr(self, "_raft_initialized") and self._raft_initialized:
            try:
                is_raft_leader = self.is_raft_leader()
                # Sync orchestrator state with Raft's view if we are leader
                if is_raft_leader and self.leader_id != self.node_id:
                    logger.info(f"[Raft] Syncing orchestrator state: consensus_mixin Raft says we're leader")
                    with self.leader_state_lock:
                        self.leader_id = self.node_id
                        self.role = NodeRole.LEADER
                elif not is_raft_leader and self.leader_id == self.node_id:
                    logger.debug(f"[Raft] consensus_mixin Raft says we're not leader")
                return is_raft_leader
            except Exception as e:
                logger.debug(f"[Raft] consensus_mixin.is_raft_leader() failed: {e}")
                # Fall through to HybridCoordinator

        # Route through HybridCoordinator if available (supports Raft)
        if self._hybrid_coordinator is not None:
            try:
                is_raft_leader = self._hybrid_coordinator.is_leader()
                # Sync orchestrator state with Raft's view if we are leader
                if is_raft_leader and self.leader_id != self.node_id:
                    logger.info(f"[Raft] Syncing orchestrator state: HybridCoordinator says we're leader")
                    with self.leader_state_lock:
                        self.leader_id = self.node_id
                        self.role = NodeRole.LEADER
                elif not is_raft_leader and self.leader_id == self.node_id:
                    logger.debug(f"[Raft] HybridCoordinator says we're not leader")
                return is_raft_leader
            except Exception as e:
                logger.warning(f"[Raft] HybridCoordinator.is_leader() failed, falling back to Bully: {e}")
                # Fall through to Bully algorithm as fallback

        # Dec 31, 2025: Enhanced logging for leader self-recognition debugging
        if self.leader_id != self.node_id:
            logger.debug(
                f"[LeaderCheck] Not leader: leader_id={self.leader_id}, "
                f"self.node_id={self.node_id}, role={self.role.value if hasattr(self.role, 'value') else self.role}"
            )
            # Consistency: we should never claim role=leader/provisional while leader_id points elsewhere (or is None).
            # Jan 1, 2026: Also check PROVISIONAL_LEADER for consistency
            if self.role in (NodeRole.LEADER, NodeRole.PROVISIONAL_LEADER):
                logger.info("Inconsistent leadership state (role=leader but leader_id!=self); stepping down")
                # C1 fix: Use leader_state_lock for role changes
                with self.leader_state_lock:
                    self.role = NodeRole.FOLLOWER
                    self.last_lease_renewal = 0.0
                    if not self.leader_id:
                        self.leader_lease_id = ""
                        self.leader_lease_expires = 0.0
                self._release_voter_grant_if_self()
                self._save_state()
                # Only force an election when we have no known leader; otherwise we
                # may already be following a healthy leader and shouldn't flap.
                if not self.leader_id:
                    # CRITICAL: Check quorum before starting election to prevent quorum bypass
                    if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                        logger.warning("Skipping election: no voter quorum available")
                    else:
                        with contextlib.suppress(RuntimeError):
                            asyncio.get_running_loop().create_task(self._start_election())
            return False
        # Consistency: we should never claim leader_id=self while being a follower/candidate.
        # Jan 1, 2026: PROVISIONAL_LEADER is also valid for dispatching work (fallback leadership)
        if self.role not in (NodeRole.LEADER, NodeRole.PROVISIONAL_LEADER):
            logger.info("Inconsistent leadership state (leader_id=self but role!=leader/provisional); clearing leader_id")
            # C1 fix: Use leader_state_lock for role/leader_id changes
            with self.leader_state_lock:
                self.role = NodeRole.FOLLOWER
                self.leader_id = None
                self.leader_lease_id = ""
                self.leader_lease_expires = 0.0
                self.last_lease_renewal = 0.0
            self._release_voter_grant_if_self()
            self._save_state()
            # CRITICAL: Check quorum before starting election to prevent quorum bypass
            if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                logger.warning("Skipping election after inconsistent state: no voter quorum available")
            else:
                with contextlib.suppress(RuntimeError):
                    asyncio.get_running_loop().create_task(self._start_election())
            return False

        # LEARNED LESSONS - Lease-based leadership prevents split-brain
        # Must have valid lease to act as leader
        if self.leader_lease_expires > 0 and time.time() >= self.leader_lease_expires:
            logger.info("Leadership lease expired, stepping down via ULSM")
            # Jan 2026: Use ULSM step-down which broadcasts to peers BEFORE local mutation
            # This ensures peers learn about step-down immediately, preventing split-brain
            self._schedule_step_down_sync(TransitionReason.LEASE_EXPIRED)
            # Note: Election is triggered after step-down completes (in _complete_step_down_async)
            # if quorum is available
            return False
        # Dec 31, 2025: Add grace period for quorum failures in _is_leader() too
        # This prevents rapid step-downs from transient network issues
        # Jan 2026: Use ULSM QuorumHealth for unified quorum tracking
        if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
            # Jan 2, 2026: Use _count_alive_voters() to check IP:port matches
            voters_alive = self._count_alive_voters()
            quorum_size = getattr(self, "voter_quorum_size", 0)
            # Use ULSM QuorumHealth for unified tracking (threshold=5 vs old 3)
            threshold_exceeded = self._leadership_sm.quorum_health.record_failure(voters_alive)
            fail_count = self._leadership_sm.quorum_health.consecutive_failures
            threshold = self._leadership_sm.quorum_health.failure_threshold
            logger.debug(
                f"[LeaderCheck] Quorum check failed ({fail_count}/{threshold}): "
                f"voters_alive={voters_alive}, quorum_size={quorum_size}"
            )
            if threshold_exceeded:
                logger.info(f"Leadership without voter quorum ({threshold} consecutive failures), stepping down via ULSM")
                # Jan 2026: Use ULSM step-down which broadcasts to peers BEFORE local mutation
                self._schedule_step_down_sync(TransitionReason.QUORUM_LOST)
                # NOTE: Don't start election here - we just lost quorum, so election would fail
                # Wait for quorum to be restored before attempting election
                logger.warning("Skipping election after quorum loss: no voter quorum available")
                # Jan 2026: Trigger aggressive peer discovery during quorum crisis
                if hasattr(self, "_quorum_crisis_loop") and self._quorum_crisis_loop:
                    self._quorum_crisis_loop.enter_crisis_mode(reason="quorum_lost")
            return False
        else:
            # Reset quorum health counter on success
            # Jan 2, 2026: Use _count_alive_voters() to check IP:port matches
            voters_alive = self._count_alive_voters()
            self._leadership_sm.quorum_health.record_success(voters_alive)
            # Jan 2026: Exit crisis mode when quorum is restored
            if hasattr(self, "_quorum_crisis_loop") and self._quorum_crisis_loop:
                if self._quorum_crisis_loop.in_crisis_mode:
                    self._quorum_crisis_loop.exit_crisis_mode(reason="quorum_restored")
        return True

    @property
    def is_leader(self) -> bool:
        """Property alias for _is_leader() - required by WorkQueueHandlersMixin."""
        return self._is_leader()

    def _get_leadership_consistency_metrics(self) -> dict:
        """Get metrics for detecting leadership state desyncs.

        Jan 3, 2026: Added to monitor and debug the leader self-recognition bug
        where leader_id is set correctly but role doesn't match.

        Returns:
            Dictionary with consistency check results for monitoring.
        """
        try:
            from scripts.p2p.leadership_state_machine import LeaderState
        except ImportError:
            LeaderState = None

        # Get ULSM state if available
        ulsm_state = None
        ulsm_leader = None
        if hasattr(self, "_leadership_sm") and self._leadership_sm is not None:
            if LeaderState is not None:
                ulsm_state = self._leadership_sm._state.value if hasattr(self._leadership_sm._state, "value") else str(self._leadership_sm._state)
            ulsm_leader = self._leadership_sm._leader_id

        # Check for inconsistencies
        role_ulsm_mismatch = False
        leader_ulsm_mismatch = False

        if hasattr(self, "_leadership_sm") and self._leadership_sm is not None and LeaderState is not None:
            # Role should match ULSM state
            local_is_leader = self.role in (NodeRole.LEADER, NodeRole.PROVISIONAL_LEADER)
            ulsm_is_leader = self._leadership_sm._state == LeaderState.LEADER
            role_ulsm_mismatch = (local_is_leader != ulsm_is_leader) and self._leadership_sm._state != LeaderState.STEPPING_DOWN
            # Leader IDs should match
            leader_ulsm_mismatch = (self._leadership_sm._leader_id != self.leader_id)

        # Self-recognition check: If we're the elected leader, do we recognize it?
        leader_id_is_self = (self.leader_id == self.node_id)
        role_is_leader = self.role in (NodeRole.LEADER, NodeRole.PROVISIONAL_LEADER)
        is_leader_call = self._is_leader()

        # Desync conditions
        # Case 1: leader_id=self but role!=LEADER (gossip bug)
        gossip_desync = leader_id_is_self and not role_is_leader
        # Case 2: role=LEADER but leader_id!=self (should not happen)
        role_desync = role_is_leader and not leader_id_is_self

        return {
            "role": self.role.value if hasattr(self.role, "value") else str(self.role),
            "leader_id": self.leader_id,
            "node_id": self.node_id,
            "is_leader_call": is_leader_call,
            "leader_id_is_self": leader_id_is_self,
            "role_is_leader": role_is_leader,
            "ulsm_state": ulsm_state,
            "ulsm_leader_id": ulsm_leader,
            "role_ulsm_mismatch": role_ulsm_mismatch,
            "leader_ulsm_mismatch": leader_ulsm_mismatch,
            "gossip_desync": gossip_desync,  # leader_id=self but role!=LEADER
            "role_desync": role_desync,  # role=LEADER but leader_id!=self
            "self_recognition_ok": leader_id_is_self == is_leader_call,  # Quick health check
        }

    def _recover_leadership_desync(self) -> bool:
        """Auto-recover from leadership state desynchronization.

        Jan 20, 2026: Added to fix the root cause of cluster instability where
        nodes get stuck in inconsistent states (candidate claiming to be leader,
        or leader_id set but role not matching).

        Recovery actions:
        1. gossip_desync (leader_id=self but role!=LEADER):
            Accept leadership since other nodes already see us as leader
        2. role_desync (role=LEADER but leader_id!=self):
            Step down since another node is the recognized leader

        Returns:
            True if recovery action was taken, False if state was consistent.
        """
        leader_id_is_self = (self.leader_id == self.node_id)
        role_is_leader = self.role in (NodeRole.LEADER, NodeRole.PROVISIONAL_LEADER)

        # Case 1: gossip_desync - leader_id=self but role!=LEADER
        # Other nodes see us as leader, but we don't recognize it
        if leader_id_is_self and not role_is_leader:
            logger.warning(
                f"[LeadershipRecovery] Fixing gossip_desync: leader_id={self.leader_id}, "
                f"role={self.role} -> LEADER"
            )
            self.role = NodeRole.LEADER
            # Update state machine if available
            if hasattr(self, "_leadership_sm") and self._leadership_sm:
                try:
                    from scripts.p2p.leadership_state_machine import LeaderState
                    # Direct state update (bypassing normal transition) for recovery
                    self._leadership_sm._state = LeaderState.LEADER
                    self._leadership_sm._leader_id = self.node_id
                except Exception as e:
                    logger.warning(f"[LeadershipRecovery] Failed to update state machine: {e}")
            return True

        # Case 2: role_desync - role=LEADER but leader_id!=self
        # We think we're leader, but leader_id points elsewhere
        if role_is_leader and not leader_id_is_self:
            logger.warning(
                f"[LeadershipRecovery] Fixing role_desync: role={self.role}, "
                f"leader_id={self.leader_id} -> FOLLOWER"
            )
            self.role = NodeRole.FOLLOWER
            # Update state machine if available
            if hasattr(self, "_leadership_sm") and self._leadership_sm:
                try:
                    from scripts.p2p.leadership_state_machine import LeaderState
                    self._leadership_sm._state = LeaderState.FOLLOWER
                    self._leadership_sm._leader_id = self.leader_id
                except Exception as e:
                    logger.warning(f"[LeadershipRecovery] Failed to update state machine: {e}")
            return True

        return False  # State was consistent

    def _reconcile_leadership_state(self) -> bool:
        """Reconcile ULSM state with gossip consensus.

        Jan 23, 2026: Phase 2 fix for P2P stability plan.
        Addresses the issue where gossip leader_consensus disagrees with ULSM state,
        causing nodes like vultr-a100-20gb to be consensus leader but not claim leadership.

        This method is called every 30 seconds from the heartbeat loop and handles:
        - Case 1: Gossip says we're leader, ULSM doesn't know -> Accept leadership
        - Case 2: ULSM says leader, gossip disagrees -> Step down

        Returns:
            True if reconciliation action was taken, False otherwise.
        """
        if not hasattr(self, "_leadership_sm") or not self._leadership_sm:
            return False

        try:
            from scripts.p2p.leadership_state_machine import LeaderState
        except ImportError:
            return False

        # Get gossip consensus on who the leader is
        consensus_info = self._get_cluster_leader_consensus()
        consensus_leader = consensus_info.get("consensus_leader")
        total_voters = consensus_info.get("total_voters", 0)
        agreement = consensus_info.get("leader_agreement", 0)

        # Need at least 2 voters to have meaningful consensus
        if total_voters < 2:
            return False

        # Calculate consensus ratio
        consensus_ratio = agreement / total_voters if total_voters > 0 else 0

        ulsm_state = self._leadership_sm._state
        ulsm_leader = self._leadership_sm._leader_id

        # Jan 23, 2026: Lowered threshold from 50% to 25% to help build consensus
        # Jan 24, 2026: Raised back to 50% to prevent split-brain - require majority agreement
        MIN_CONSENSUS_THRESHOLD = 0.50

        # If very low consensus (<25%), try to help build it by broadcasting our leader view
        if consensus_ratio < MIN_CONSENSUS_THRESHOLD:
            # If we know who the leader is, proactively announce it
            if self.leader_id and self.leader_id != self.node_id:
                # We're a follower with a known leader - this is normal, no action needed
                pass
            elif ulsm_state == LeaderState.LEADER:
                # We think we're leader but consensus doesn't agree
                # Proactively send leadership claim to help convergence
                logger.info(
                    f"[LeaderReconciliation] Low consensus ({consensus_ratio:.0%}), "
                    f"proactively announcing leadership to help convergence"
                )
                self._broadcast_leadership_claim()
            return False

        # Case 1: Gossip says we're leader, ULSM doesn't know
        if consensus_leader == self.node_id and ulsm_state != LeaderState.LEADER:
            if self._is_leader_lease_valid():
                logger.info(
                    f"[LeaderReconciliation] Accepting leadership from gossip consensus "
                    f"(agreement={agreement}/{total_voters}={consensus_ratio:.0%})"
                )
                # Update ULSM state to match gossip
                self._leadership_sm._state = LeaderState.LEADER
                self._leadership_sm._leader_id = self.node_id
                # Update local state
                self.role = NodeRole.LEADER
                self.leader_id = self.node_id
                return True
            else:
                logger.warning(
                    f"[LeaderReconciliation] Gossip says we're leader but lease invalid; "
                    f"clearing leader_id to trigger election"
                )
                self.leader_id = None
                return True

        # Case 2: ULSM says leader, gossip disagrees (another node is consensus leader)
        if ulsm_state == LeaderState.LEADER and consensus_leader and consensus_leader != self.node_id:
            logger.warning(
                f"[LeaderReconciliation] Gossip consensus says {consensus_leader} is leader "
                f"(agreement={agreement}/{total_voters}={consensus_ratio:.0%}); stepping down"
            )
            self._schedule_step_down_sync(TransitionReason.HIGHER_EPOCH_SEEN)
            return True

        return False

    def _broadcast_leadership_claim(self) -> None:
        """Proactively broadcast leadership claim to help consensus convergence.

        Jan 23, 2026: Added to help build consensus when agreement is low.
        Sends a leadership claim heartbeat to all peers.
        """
        if not self.leader_id:
            return

        # Schedule async broadcast using the event loop
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # Create a small heartbeat-like message announcing our leader view
                asyncio.create_task(self._async_broadcast_leader_claim())
        except RuntimeError:
            # No event loop available
            pass

    async def _async_broadcast_leader_claim(self) -> None:
        """Async helper to broadcast leadership claim."""
        try:
            peers_snapshot = self._peer_snapshot.get_snapshot()
            tasks = []

            for peer_id, peer_info in peers_snapshot.items():
                if not peer_info.is_alive():
                    continue

                url = f"http://{peer_info.host}:{peer_info.port}/heartbeat"
                payload = {
                    "node_id": self.node_id,
                    "leader_id": self.leader_id,
                    "role": self.role.value if hasattr(self.role, "value") else str(self.role),
                    "timestamp": time.time(),
                    "is_leadership_claim": True,  # Flag to indicate this is proactive
                }
                tasks.append(self._broadcast_leader_claim_to_peer(url, payload, peer_id))

            if tasks:
                results = await asyncio.gather(*tasks, return_exceptions=True)
                success = sum(1 for r in results if r is True)
                logger.debug(f"[LeaderReconciliation] Broadcast leadership claim to {success}/{len(tasks)} peers")
        except Exception as e:
            logger.debug(f"[LeaderReconciliation] Failed to broadcast leadership claim: {e}")

    async def _broadcast_leader_claim_to_peer(self, url: str, payload: dict, peer_id: str) -> bool:
        """Broadcast leadership claim to a single peer via HTTP POST.

        Jan 27, 2026: Renamed from _send_heartbeat_to_peer to fix shadowing bug.
        This is a simple broadcast helper distinct from the full heartbeat exchange
        in HeartbeatManager.send_heartbeat_to_peer().
        """
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=5)) as response:
                    return response.status == 200
        except Exception:
            return False

    def _get_config_version(self) -> dict:
        """Get config file version info for drift detection.

        Jan 13, 2026: Phase 1 of P2P Cluster Stability Plan
        Enables gossip-based config drift detection across the cluster.

        Returns:
            Dictionary with config hash, timestamp, and metadata.
        """
        import hashlib
        from pathlib import Path

        config_paths = [
            Path(__file__).parent.parent / "config" / "distributed_hosts.yaml",
            Path.cwd() / "config" / "distributed_hosts.yaml",
        ]

        for config_path in config_paths:
            if config_path.exists():
                try:
                    content = config_path.read_text()
                    stat = config_path.stat()

                    # Compute hash of content
                    content_hash = hashlib.sha256(content.encode()).hexdigest()

                    return {
                        "hash": content_hash[:16],  # First 16 chars for display
                        "full_hash": content_hash,
                        "timestamp": stat.st_mtime,
                        "mtime": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime(stat.st_mtime)),
                        "path": str(config_path),
                        "size_bytes": stat.st_size,
                    }
                except (OSError, PermissionError) as e:
                    return {
                        "hash": None,
                        "error": str(e),
                        "path": str(config_path),
                    }

        return {
            "hash": None,
            "error": "config_not_found",
            "searched_paths": [str(p) for p in config_paths],
        }

    def _was_recently_leader(self) -> bool:
        """Check if this node was the cluster leader within RECENT_LEADER_WINDOW.

        Jan 2, 2026: Leader stickiness - allows previous leader to reclaim
        leadership with preference during elections, reducing oscillation.

        Returns:
            True if we were leader and stepped down within RECENT_LEADER_WINDOW seconds.
        """
        now = time.time()
        # Check if we became leader at some point
        if self._last_become_leader_time <= 0:
            return False
        # Check if we stepped down recently (within window)
        if self._last_step_down_time <= 0:
            return False
        time_since_step_down = now - self._last_step_down_time
        return time_since_step_down < RECENT_LEADER_WINDOW

    def _in_incumbent_grace_period(self) -> bool:
        """Check if we're within the incumbent grace period after stepping down.

        Jan 2, 2026: During this period, the previous leader gets priority
        to reclaim leadership without competition.

        Returns:
            True if within INCUMBENT_LEADER_GRACE_PERIOD seconds of step-down.
        """
        if self._last_step_down_time <= 0:
            return False
        time_since_step_down = time.time() - self._last_step_down_time
        return time_since_step_down < INCUMBENT_LEADER_GRACE_PERIOD

    # =========================================================================
    # UNIFIED LEADERSHIP STATE MACHINE (ULSM) - Jan 2026
    # Broadcast callback for state machine step-down notifications
    # =========================================================================

    async def _broadcast_leader_state_change(
        self,
        new_state: str,
        epoch: int,
        reason: "TransitionReason",
    ) -> None:
        """Broadcast leadership state change to all peers.

        Called by LeadershipStateMachine.transition_to() when stepping down.
        This ensures peers learn about step-down BEFORE local state mutation.

        Args:
            new_state: New leadership state (e.g., "stepping_down")
            epoch: Current leadership epoch
            reason: Reason for the transition
        """
        message = {
            "node_id": self.node_id,
            "new_state": new_state,
            "epoch": epoch,
            "reason": reason.value if hasattr(reason, "value") else str(reason),
            "timestamp": time.time(),
        }

        tasks = []
        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        peers_snapshot = self._peer_snapshot.get_snapshot()

        for peer_id, peer_info in peers_snapshot.items():
            if not peer_info.is_alive():
                continue

            # Build URL for peer
            # Jan 10, 2026: Fixed - NodeInfo uses host/port, not advertise_host/advertise_port
            url = f"http://{peer_info.host}:{peer_info.port}/leader-state-change"
            tasks.append(self._broadcast_to_peer(url, message, peer_id))

        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            success = sum(1 for r in results if r is True)
            logger.info(
                f"Broadcast step-down to {success}/{len(tasks)} peers "
                f"(epoch={epoch}, reason={reason.value if hasattr(reason, 'value') else reason})"
            )

    async def _broadcast_to_peer(
        self,
        url: str,
        message: dict[str, Any],
        peer_id: str,
    ) -> bool:
        """Send state change message to a single peer with timeout."""
        try:
            async with get_client_session(timeout=2.0) as session:
                async with session.post(
                    url,
                    json=message,
                    headers=self._auth_headers(),
                ) as resp:
                    if resp.status == 200:
                        return True
                    logger.debug(f"Broadcast to {peer_id} returned {resp.status}")
                    return False
        except (aiohttp.ClientError, asyncio.TimeoutError, OSError) as e:
            logger.debug(f"Broadcast to {peer_id} failed: {e}")
            return False

    # NOTE: _schedule_step_down_sync() and _complete_step_down_async()
    # moved to LeadershipTransitionsMixin (Jan 26, 2026)

    # =========================================================================
    # TASK ISOLATION - Prevent single task failure from crashing all tasks
    # =========================================================================

    # Task factory registry for restart support
    _task_factories: dict[str, "Callable[[], Coroutine]"] = {}

    async def _safe_task_wrapper(
        self,
        coro,
        task_name: str,
        factory: "Callable[[], Coroutine] | None" = None,
    ) -> None:
        """Wrap a coroutine to catch exceptions and prevent cascade failures.

        This is a CRITICAL stability fix: without isolation, a single exception
        in any of 18+ background tasks will crash the entire P2P orchestrator
        via asyncio.gather() propagating the exception.

        Args:
            coro: The coroutine to wrap
            task_name: Human-readable task name for logging
            factory: Optional callable that returns a new coroutine for restarts

        Returns:
            None - exceptions are logged but not raised
        """
        # Register factory for potential restarts
        if factory is not None:
            self._task_factories[task_name] = factory

        restart_count = 0
        max_restarts = 5

        while True:
            try:
                await coro
                return  # Normal completion
            except asyncio.CancelledError:
                logger.debug(f"Task '{task_name}' cancelled (shutdown)")
                raise  # Re-raise CancelledError for graceful shutdown
            except SystemExit:
                # SystemExit from main loop exit - ignore in background tasks
                # This prevents "Task exception was never retrieved" log pollution
                logger.debug(f"Task '{task_name}' received SystemExit (orchestrator shutdown)")
                return
            except Exception as e:  # noqa: BLE001
                # Log but don't propagate - other tasks continue running
                logger.error(f"Task '{task_name}' crashed: {e}", exc_info=True)

                # Check if we can restart
                restart_factory = factory or self._task_factories.get(task_name)
                if not self.running or restart_factory is None:
                    logger.warning(f"Task '{task_name}' cannot restart (no factory or shutdown)")
                    return

                restart_count += 1
                if restart_count > max_restarts:
                    logger.error(
                        f"Task '{task_name}' exceeded max restarts ({max_restarts}), giving up"
                    )
                    return

                # Exponential backoff: 30s, 60s, 120s, 240s, 480s
                delay = min(30 * (2 ** (restart_count - 1)), 480)
                logger.info(
                    f"Restarting task '{task_name}' in {delay}s "
                    f"(attempt {restart_count}/{max_restarts})..."
                )
                await asyncio.sleep(delay)

                if not self.running:
                    return

                # Create new coroutine from factory
                try:
                    coro = restart_factory()
                    logger.info(f"Restarted task '{task_name}'")
                except Exception as restart_error:
                    logger.error(f"Failed to restart task '{task_name}': {restart_error}")
                    return

    def _create_safe_task(
        self,
        coro,
        name: str,
        factory: "Callable[[], Coroutine] | None" = None,
    ) -> asyncio.Task:
        """Create a task wrapped with exception isolation and restart support.

        Args:
            coro: The coroutine to run
            name: Task name for logging
            factory: Optional callable that returns a new coroutine for restarts.
                     If not provided, task cannot be automatically restarted.

        Returns:
            asyncio.Task wrapped with safe error handling
        """
        return asyncio.create_task(
            self._safe_task_wrapper(coro, name, factory),
            name=name,
        )

    # =========================================================================
    # BOUNDED COLLECTIONS - Prevent unbounded memory growth
    # =========================================================================

    # Maximum pending relay items before cleanup
    MAX_PENDING_RELAY_ACKS = 10000
    MAX_PENDING_RELAY_RESULTS = 10000

    def _add_pending_relay_ack(self, cmd_id: str) -> None:
        """Add a relay ack with bounds checking."""
        if len(self.pending_relay_acks) >= self.MAX_PENDING_RELAY_ACKS:
            # Evict oldest entries (set doesn't have order, so clear half)
            half = len(self.pending_relay_acks) // 2
            to_remove = list(self.pending_relay_acks)[:half]
            for item in to_remove:
                self.pending_relay_acks.discard(item)
            logger.warning(f"Evicted {half} pending_relay_acks (max {self.MAX_PENDING_RELAY_ACKS})")
        self.pending_relay_acks.add(cmd_id)

    def _add_pending_relay_result(self, result: dict) -> None:
        """Add a relay result with bounds checking."""
        if len(self.pending_relay_results) >= self.MAX_PENDING_RELAY_RESULTS:
            # Evict oldest entries (keep most recent half)
            half = len(self.pending_relay_results) // 2
            self.pending_relay_results = self.pending_relay_results[half:]
            logger.warning(f"Evicted {half} pending_relay_results (max {self.MAX_PENDING_RELAY_RESULTS})")
        self.pending_relay_results.append(result)

    # =========================================================================
    # SAFEGUARDS - Load, rate limiting, and coordinator integration
    # =========================================================================

    def _check_spawn_rate_limit(self) -> tuple[bool, str]:
        """Check if we're within the spawn rate limit.

        SAFEGUARD: Prevents runaway process spawning by limiting spawns per minute.

        Returns:
            (can_spawn, reason) - True if within rate limit
        """
        now = time.time()
        # Clean old timestamps (older than 60 seconds)
        self.spawn_timestamps = [t for t in self.spawn_timestamps if now - t < 60]

        if len(self.spawn_timestamps) >= SPAWN_RATE_LIMIT_PER_MINUTE:
            return False, f"Rate limit: {len(self.spawn_timestamps)}/{SPAWN_RATE_LIMIT_PER_MINUTE} spawns in last minute"

        return True, f"Rate OK: {len(self.spawn_timestamps)}/{SPAWN_RATE_LIMIT_PER_MINUTE}"

    def _record_spawn(self) -> None:
        """Record a process spawn for rate limiting."""
        self.spawn_timestamps.append(time.time())

    def _can_spawn_process(self, reason: str = "job") -> tuple[bool, str]:
        """Combined safeguard check before spawning any process.

        SAFEGUARD: Checks load average, rate limit, and agent mode.

        Args:
            reason: Description of why we want to spawn (for logging)

        Returns:
            (can_spawn, explanation) - True if all checks pass
        """
        # Check 1: Load average
        load_ok, load_reason = self.self_info.check_load_average_safe()
        if not load_ok:
            logger.info(f"BLOCKED spawn ({reason}): {load_reason}")
            return False, load_reason

        # Check 2: Rate limit
        rate_ok, rate_reason = self._check_spawn_rate_limit()
        if not rate_ok:
            logger.info(f"BLOCKED spawn ({reason}): {rate_reason}")
            return False, rate_reason

        # Check 3: Agent mode - if coordinator is available and we're in agent mode,
        # we should not autonomously spawn jobs (let coordinator decide)
        if self.agent_mode and self.coordinator_available:
            msg = "Agent mode: deferring to coordinator"
            logger.info(f"BLOCKED spawn ({reason}): {msg}")
            return False, msg

        # Check 4: Backpressure (new coordination) - if training queue is saturated,
        # don't spawn more selfplay jobs that would produce more data
        if HAS_NEW_COORDINATION and "selfplay" in reason.lower():
            if should_stop_production(QueueType.TRAINING_DATA):
                msg = "Backpressure: training queue at STOP level"
                logger.info(f"BLOCKED spawn ({reason}): {msg}")
                return False, msg
            if should_throttle_production(QueueType.TRAINING_DATA):
                throttle = get_throttle_factor(QueueType.TRAINING_DATA)
                import random
                if random.random() > throttle:
                    msg = f"Backpressure: throttled (factor={throttle:.2f})"
                    logger.info(f"BLOCKED spawn ({reason}): {msg}")
                    return False, msg

        # Check 5: Graceful degradation - don't spawn under heavy resource pressure
        if HAS_RESOURCE_GUARD and get_degradation_level is not None:
            degradation = get_degradation_level()
            if degradation >= 4:  # CRITICAL - resources at/above limits
                msg = f"Graceful degradation: critical resource pressure (level {degradation})"
                logger.info(f"BLOCKED spawn ({reason}): {msg}")
                return False, msg
            elif degradation >= 3:  # HEAVY - only critical ops proceed
                # Selfplay is NORMAL priority, blocked under heavy pressure
                if should_proceed_with_priority is not None and not should_proceed_with_priority(OperationPriority.NORMAL):
                    msg = f"Graceful degradation: heavy resource pressure (level {degradation})"
                    logger.info(f"BLOCKED spawn ({reason}): {msg}")
                    return False, msg

        return True, "All safeguards passed"

    def _spawn_and_track_job(
        self,
        job_id: str,
        job_type: JobType,
        board_type: str,
        num_players: int,
        engine_mode: str,
        cmd: list[str],
        output_dir: Path,
        log_filename: str = "run.log",
        cuda_visible_devices: str | None = None,
        extra_env: dict[str, str] | None = None,
        safeguard_reason: str | None = None,
    ) -> tuple[ClusterJob, subprocess.Popen] | None:
        """Spawn a subprocess job and track it in local_jobs.

        January 2026: Extracted common job spawning logic to reduce duplication.

        Args:
            job_id: Unique job identifier
            job_type: Type of job (SELFPLAY, GPU_SELFPLAY, etc.)
            board_type: Board type (hex8, square8, etc.)
            num_players: Number of players
            engine_mode: Engine mode for the job
            cmd: Command to execute
            output_dir: Directory for output files
            log_filename: Name of log file in output_dir
            cuda_visible_devices: CUDA_VISIBLE_DEVICES value (None = inherit, "" = disable)
            extra_env: Additional environment variables
            safeguard_reason: Reason for safeguard check (default: job_type-board_type-Np)

        Returns:
            Tuple of (ClusterJob, Popen) if successful, None if blocked or failed
        """
        # Build safeguard check reason
        if safeguard_reason is None:
            safeguard_reason = f"{job_type.value}-{board_type}-{num_players}p"

        # SAFEGUARD: Final check before spawning
        can_spawn, spawn_reason = self._can_spawn_process(safeguard_reason)
        if not can_spawn:
            logger.info(f"BLOCKED {job_type.value} spawn: {spawn_reason}")
            return None

        # Build environment
        env = os.environ.copy()
        env["PYTHONPATH"] = self._get_ai_service_path()
        env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"
        env["RINGRIFT_JOB_ORIGIN"] = "p2p_orchestrator"

        # Handle CUDA_VISIBLE_DEVICES
        if cuda_visible_devices is not None:
            env["CUDA_VISIBLE_DEVICES"] = str(cuda_visible_devices).strip()

        # Apply extra environment variables
        if extra_env:
            env.update(extra_env)

        # Ensure output directory exists
        output_dir.mkdir(parents=True, exist_ok=True)
        log_path = output_dir / log_filename

        # Spawn subprocess
        try:
            log_handle = open(log_path, "a")  # noqa: SIM115
            try:
                proc = subprocess.Popen(
                    cmd,
                    stdout=log_handle,
                    stderr=subprocess.STDOUT,
                    env=env,
                    cwd=self.ringrift_path,
                )
                self._record_spawn()
            finally:
                log_handle.close()
        except (OSError, subprocess.SubprocessError) as e:
            logger.error(f"Failed to spawn {job_type.value}: {e}")
            return None

        # Create ClusterJob
        job = ClusterJob(
            job_id=job_id,
            job_type=job_type,
            node_id=self.node_id,
            board_type=board_type,
            num_players=num_players,
            engine_mode=engine_mode,
            pid=proc.pid,
            started_at=time.time(),
            status="running",
        )

        # Track in local_jobs
        with self.jobs_lock:
            self.local_jobs[job_id] = job

        logger.info(f"Started {job_type.value} job {job_id} (PID {proc.pid})")
        self._save_state()

        # Track via JobOrchestrationManager
        if hasattr(self, "job_orchestration") and self.job_orchestration:
            self.job_orchestration.record_job_started(job_type.value)

        return job, proc

    def _detect_build_version(self) -> str:
        env_version = (os.environ.get(BUILD_VERSION_ENV, "") or "").strip()
        if env_version:
            return env_version

        commit = ""
        branch = ""
        try:
            result = subprocess.run(
                self._git_cmd("rev-parse", "--short", "HEAD"),
                cwd=self.ringrift_path,
                capture_output=True,
                text=True,
                timeout=3,
            )
            if result.returncode == 0:
                commit = result.stdout.strip()
        except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError, AttributeError):
            commit = ""

        try:
            result = subprocess.run(
                self._git_cmd("rev-parse", "--abbrev-ref", "HEAD"),
                cwd=self.ringrift_path,
                capture_output=True,
                text=True,
                timeout=3,
            )
            if result.returncode == 0:
                branch = result.stdout.strip()
        except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError, AttributeError):
            branch = ""

        if commit and branch:
            return f"{branch}@{commit}"
        return commit or "unknown"

    def _git_cmd(self, *args: str) -> list[str]:
        safe_dir = getattr(self, "_git_safe_directory", "") or os.path.abspath(self.ringrift_path)
        return ["git", "-c", f"safe.directory={safe_dir}", *args]

    def _detect_ringrift_path(self) -> str:
        """Detect the RingRift installation path."""
        # Try common locations
        candidates = [
            Path.home() / "Development" / "RingRift",
            Path.home() / "ringrift",
            Path("/home/ubuntu/ringrift"),
            Path("/root/ringrift"),
        ]
        for path in candidates:
            if (path / "ai-service").exists():
                return str(path)
        return str(Path(__file__).parent.parent.parent)

    def _get_ai_service_path(self) -> str:
        """Get the path to the ai-service directory.

        Handles both cases:
        - ringrift_path = /path/to/RingRift (root directory)
        - ringrift_path = /path/to/RingRift/ai-service (already ai-service)

        Returns:
            Path to ai-service directory.
        """
        if self.ringrift_path.rstrip("/").endswith("ai-service"):
            return self.ringrift_path
        return os.path.join(self.ringrift_path, "ai-service")

    def _increment_rollback_counter(self) -> None:
        """Increment the rollback counter in diversity metrics.

        Used by AnalyticsCacheManager callback.
        """
        self.diversity_metrics["rollbacks"] += 1

    # CMA-ES Coordinator callback helpers (Jan 2026 - Aggressive Decomposition Phase 3)

    def _get_gpu_workers_for_cmaes(self) -> list:
        """Get available GPU workers for CMA-ES. Used by CMAESCoordinator callback.

        Jan 27, 2026: Migrated to PeerQueryBuilder (Phase 3.2).
        """
        workers = self._peer_query.healthy_with_gpu().unwrap_or([])
        if self.self_info.has_gpu:
            workers.append(self.self_info)
        return workers

    async def _send_cmaes_to_worker(self, worker_id: str, endpoint: str, payload: dict) -> bool:
        """Send CMA-ES request to a worker. Used by CMAESCoordinator callback."""
        try:
            with self.peers_lock:
                worker = self.peers.get(worker_id)
            if not worker:
                return False
            timeout = ClientTimeout(total=300)
            async with get_client_session(timeout) as session:
                url = self._url_for_peer(worker, endpoint)
                await session.post(url, json=payload, headers=self._auth_headers())
            return True
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to send CMA-ES request to {worker_id}: {e}")
            return False

    async def _report_cmaes_to_leader(self, endpoint: str, payload: dict) -> bool:
        """Report CMA-ES result to leader. Used by CMAESCoordinator callback."""
        try:
            if not self.leader_id:
                return False
            with self.peers_lock:
                leader = self.peers.get(self.leader_id)
            if not leader:
                return False
            timeout = ClientTimeout(total=30)
            async with get_client_session(timeout) as session:
                url = self._url_for_peer(leader, endpoint)
                await session.post(url, json=payload, headers=self._auth_headers())
            return True
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to report CMA-ES result to leader: {e}")
            return False

    def _handle_cmaes_complete_callback(self, board_type: str, num_players: int, weights: dict) -> str | None:
        """Handle CMA-ES completion. Used by CMAESCoordinator callback."""
        if self.improvement_cycle_manager:
            agent_id = self.improvement_cycle_manager.handle_cmaes_complete(
                board_type, num_players, weights
            )
            self.diversity_metrics["cmaes_triggers"] += 1
            return agent_id
        return None

    def _get_script_path(self, script_name: str) -> str:
        """Get the full path to a script in ai-service/scripts/.

        Args:
            script_name: Name of the script (e.g., "run_self_play_soak.py")

        Returns:
            Full path to the script.
        """
        return os.path.join(self._get_ai_service_path(), "scripts", script_name)

    def _check_yaml_gpu_config(self, node_id: str | None = None) -> tuple[bool, str, int]:
        """Check if YAML config indicates a node has a GPU.

        Used as fallback when runtime GPU detection fails (e.g., vGPU, containers,
        driver issues causing torch.cuda.is_available() to return False).

        Args:
            node_id: Node ID to check. If None, uses self.node_id.

        Returns:
            Tuple of (has_gpu, gpu_name, gpu_vram_gb)

        Session 17.50 (Jan 2026): Added to fix GPU nodes running CPU selfplay
        when torch.cuda.is_available() returns False due to driver issues.
        """
        target_node = node_id or self.node_id
        try:
            from app.config.cluster_config import get_config_cache
            config = get_config_cache().get_config()
            host_cfg = config.hosts_raw.get(target_node, {})

            # Check multiple indicators
            gpu_name = str(host_cfg.get("gpu", ""))
            gpu_vram = int(host_cfg.get("gpu_vram_gb", 0) or 0)
            role = str(host_cfg.get("role", ""))

            has_gpu = bool(gpu_name) or gpu_vram > 0 or "gpu" in role.lower()

            if has_gpu:
                logger.debug(
                    f"[YAML GPU] Node {target_node}: gpu={gpu_name}, "
                    f"vram={gpu_vram}GB, role={role}"
                )
            return has_gpu, gpu_name, gpu_vram
        except Exception as e:
            logger.debug(f"Could not check YAML GPU config for {target_node}: {e}")
            return False, "", 0

    def get_data_directory(self) -> Path:
        """Get the data directory path based on storage configuration.

        Returns:
            Path to data directory:
            - ramdrive: /dev/shm/ringrift/data (for disk-constrained Vast instances)
            - disk: {ringrift_path}/ai-service/data (default)

        The ramdrive option uses tmpfs for high-speed I/O and to work around
        limited disk space on some cloud instances. Data stored in ramdrive
        is volatile and should be synced to permanent storage periodically.
        """
        if self.storage_type == "ramdrive":
            ramdrive = Path(self.ramdrive_path)
            try:
                ramdrive.mkdir(parents=True, exist_ok=True)
            except (PermissionError, OSError) as e:
                # /dev/shm doesn't exist on macOS or may be inaccessible
                logger.warning(f"Cannot create ramdrive at {ramdrive}: {e}. Falling back to disk storage.")
                self.storage_type = "disk"
                return Path(self._get_ai_service_path()) / "data"

            # Set up automatic sync to persistent storage
            if self.ramdrive_syncer is None and self.sync_to_disk_interval > 0:
                persistent_path = Path(self._get_ai_service_path()) / "data"
                persistent_path.mkdir(parents=True, exist_ok=True)
                self.ramdrive_syncer = RamdriveSyncer(
                    source_dir=ramdrive,
                    target_dir=persistent_path,
                    interval=self.sync_to_disk_interval,
                    patterns=["*.db", "*.jsonl", "*.json", "*.npz"],
                )
                self.ramdrive_syncer.start()
                logger.info(f"Started ramdrive -> disk sync: {ramdrive} -> {persistent_path} "
                           f"every {self.sync_to_disk_interval}s")

            return ramdrive
        return Path(self._get_ai_service_path()) / "data"

    def stop_ramdrive_syncer(self, final_sync: bool = True) -> None:
        """Stop the ramdrive syncer and optionally perform final sync."""
        if self.ramdrive_syncer:
            logger.info("Stopping ramdrive syncer...")
            self.ramdrive_syncer.stop(final_sync=final_sync)
            logger.info(f"Ramdrive sync stats: {self.ramdrive_syncer.stats}")
            self.ramdrive_syncer = None

    # =========================================================================
    # GPU Job Tracking (Jan 7, 2026)
    # =========================================================================
    # These methods track GPU job lifecycle for adaptive dispatch decisions.
    # GPU nodes should run GPU-accelerated selfplay, not fall back to CPU.
    # =========================================================================

    def _get_node_job_preference(self, node_id: str) -> str:
        """Get preferred job type based on node role from YAML config.

        Jan 7, 2026: Added to enforce role-based job selection.
        GPU-only nodes should not fall back to CPU selfplay.

        Returns one of:
        - 'cpu_only': Node should only run CPU jobs (coordinator, cpu_selfplay)
        - 'gpu_only': Node should only run GPU jobs (gpu_selfplay role)
        - 'training_only': Node should only run training (gpu_training_primary)
        - 'both': Node can run both GPU selfplay and training (default)
        """
        try:
            from app.config.cluster_config import get_config_cache
            config = get_config_cache().get_config()
            host_cfg = config.hosts_raw.get(node_id, {})
            role = str(host_cfg.get("role", "")).lower()

            if role in ("coordinator", "cpu_selfplay"):
                return "cpu_only"
            if role == "gpu_selfplay":
                return "gpu_only"
            if role == "gpu_training_primary":
                # Training-primary nodes can still do selfplay when idle
                return "both"
            if role == "gpu_training_selfplay":
                return "both"
            return "both"
        except Exception as e:
            logger.debug(f"Could not get job preference for {node_id}: {e}")
            return "both"

    def _record_gpu_job_result(self, success: bool) -> None:
        """Record GPU job completion result for adaptive dispatch decisions.

        Jan 7, 2026: Added for GPU failure tracking.
        Consecutive failures indicate driver issues and should trigger CPU fallback.

        Args:
            success: True if GPU job completed successfully, False otherwise.
        """
        try:
            now = time.time()
            if success:
                self.self_info.last_gpu_job_success = now
                self.self_info.gpu_failure_count = 0  # Reset on success
            else:
                self.self_info.last_gpu_job_failure = now
                self.self_info.gpu_failure_count = getattr(self.self_info, "gpu_failure_count", 0) + 1
            logger.debug(f"GPU job result: success={success}, failure_count={self.self_info.gpu_failure_count}")
        except Exception as e:
            logger.debug(f"Could not record GPU job result: {e}")

    def _update_gpu_job_count(self, delta: int) -> None:
        """Update running GPU job count.

        Jan 7, 2026: Added for accurate GPU job tracking.
        Used to detect driver issues (jobs running but 0% utilization).

        Args:
            delta: Amount to change count by (+1 for start, -1 for completion).
        """
        try:
            current = getattr(self.self_info, "gpu_job_count", 0) or 0
            self.self_info.gpu_job_count = max(0, current + delta)
            logger.debug(f"GPU job count: {current} -> {self.self_info.gpu_job_count}")
        except Exception as e:
            logger.debug(f"Could not update GPU job count: {e}")

    def _infer_advertise_port(self) -> int:
        """Infer the externally reachable port for this node.

        - Explicit `RINGRIFT_ADVERTISE_PORT` always wins.
        - Vast.ai exposes container ports via `VAST_TCP_PORT_<PORT>`; when set,
          use that public port so peers can reach us.
        - Default to the listening port.
        """
        explicit = (os.environ.get(ADVERTISE_PORT_ENV, "")).strip()
        if explicit:
            try:
                return int(explicit)
            except ValueError:
                pass

        vast_key = f"VAST_TCP_PORT_{self.port}"
        mapped = (os.environ.get(vast_key, "")).strip()
        if mapped:
            try:
                return int(mapped)
            except ValueError:
                pass

        return int(self.port)

    # NOTE: _validate_and_fix_advertise_host(), _periodic_ip_validation_loop(),
    # _is_advertising_private_ip(), _try_get_tailscale_ip(), _safe_emit_private_ip_alert(),
    # _discover_all_ips(), _select_primary_advertise_host(), _set_advertise_host(),
    # _get_yaml_tailscale_ip() moved to AdvertiseValidationMixin (Jan 26, 2026)

    def _load_force_relay_mode(self) -> bool:
        """Load force_relay_mode from distributed_hosts.yaml for this node.

        January 5, 2026: NAT-blocked nodes need to send ALL outbound heartbeats
        via relay to ensure other nodes can discover them. This is configured in
        distributed_hosts.yaml with either:
        - `nat_blocked: true` - Node is behind NAT and can't receive inbound connections
        - `force_relay_mode: true` - Explicitly enable relay mode

        Returns:
            True if this node should use relay mode for all outbound heartbeats.
        """
        # Priority 1: Environment variable override
        env = (os.environ.get("RINGRIFT_FORCE_RELAY_MODE") or "").strip().lower()
        if env in ("1", "true", "yes"):
            logger.info(f"[P2P] Force relay mode enabled via RINGRIFT_FORCE_RELAY_MODE env var")
            return True

        # Priority 2: Load from distributed_hosts.yaml
        try:
            from app.config.cluster_config import load_cluster_config
            config = load_cluster_config()
            nodes = getattr(config, "hosts_raw", {}) or {}
            node_cfg = nodes.get(self.node_id, {})

            nat_blocked = node_cfg.get("nat_blocked", False)
            force_relay = node_cfg.get("force_relay_mode", False)

            if nat_blocked or force_relay:
                reason = "nat_blocked" if nat_blocked else "force_relay_mode"
                logger.info(f"[P2P] Force relay mode enabled for {self.node_id} ({reason})")
                return True
        except ImportError:
            logger.debug("[P2P] cluster_config not available for force_relay_mode check")
        except Exception as e:  # noqa: BLE001
            logger.debug(f"[P2P] Failed to load force_relay_mode from config: {e}")

        return False

    def _prepopulate_voter_peers(self) -> None:
        """Pre-populate voter nodes into peers dict for immediate gossip reachability.

        Jan 28, 2026: Fixes bootstrap chicken-and-egg where voters are invisible
        to gossip until discovered via heartbeat. Without this, new nodes have an
        empty peers dict  gossip can't reach voters  voters never get added.
        """
        if not self.voter_node_ids:
            return

        if os.environ.get("RINGRIFT_SKIP_VOTER_PREPOPULATION", "").lower() in ("1", "true"):
            logger.info("[P2P] Voter pre-population disabled via env var")
            return

        try:
            from app.config.cluster_config import get_cluster_nodes
            cluster_nodes = get_cluster_nodes()
        except ImportError:
            logger.warning("[P2P] Cannot pre-populate voters: cluster_config unavailable")
            return
        except Exception as e:  # noqa: BLE001
            logger.warning(f"[P2P] Cannot pre-populate voters: {e}")
            return

        prepopulated = 0
        for voter_id in self.voter_node_ids:
            if voter_id == self.node_id:
                continue  # Skip self

            if voter_id in self.peers:
                continue  # Already known

            node_cfg = cluster_nodes.get(voter_id)
            if not node_cfg:
                logger.debug(f"[P2P] Voter {voter_id} not in cluster_config, skipping prepopulation")
                continue

            host = getattr(node_cfg, 'best_ip', None) or getattr(node_cfg, 'tailscale_ip', None)
            if not host:
                logger.debug(f"[P2P] Voter {voter_id} has no IP in cluster_config, skipping")
                continue

            voter_info = NodeInfo(
                node_id=voter_id,
                host=host,
                port=DEFAULT_PORT,
                tailscale_ip=getattr(node_cfg, 'tailscale_ip', '') or '',
                role=NodeRole.FOLLOWER,
                last_heartbeat=0,  # Will update on first heartbeat
            )
            self.peers[voter_id] = voter_info
            prepopulated += 1
            logger.debug(f"[P2P] Pre-populated voter {voter_id} at {host}:{DEFAULT_PORT}")

        if prepopulated:
            logger.info(f"[P2P] Pre-populated {prepopulated} voter peers for gossip reachability")

    # NOTE: _load_voter_node_ids() removed - delegated to QuorumManager (Jan 2026 Phase 1)

    def _load_cluster_config_raw(self) -> dict[str, Any]:
        """Load raw cluster config from distributed_hosts.yaml.

        Returns the raw YAML dict for use by loops that need to access
        host configuration (relay nodes, selfplay settings, etc.).

        January 27, 2026: Added to support loop_registry.py relay health loop
        and autonomous_queue_loop.py selfplay configuration.
        """
        cfg_path = Path(self._get_ai_service_path()) / "config" / "distributed_hosts.yaml"
        if not cfg_path.exists():
            return {}

        try:
            import yaml
            return yaml.safe_load(cfg_path.read_text()) or {}
        except (OSError, yaml.YAMLError) as e:
            logger.debug(f"[P2P] Failed to load cluster config: {e}")
            return {}

    # NOTE: _build_voter_ip_mapping() moved to LeadershipHealthMixin (Jan 26, 2026)
    # NOTE: _build_ip_to_node_map() removed - delegated to QuorumManager (Jan 2026 Phase 1)

    def _get_cached_peer_snapshot(self, max_age_seconds: float = 1.0) -> list:
        """Get cached peer snapshot to reduce lock acquisitions.

        Jan 12, 2026: Added to reduce lock contention in read-only contexts.
        Returns cached copy if < max_age_seconds old, otherwise takes new snapshot.

        Args:
            max_age_seconds: Maximum age of cached snapshot before refreshing (default 1.0s)

        Returns:
            List of peer NodeInfo objects (may be up to max_age_seconds stale)
        """
        now = time.time()
        cache_key = "_peer_snapshot_cache"
        cache_time_key = "_peer_snapshot_cache_time"

        cached = getattr(self, cache_key, None)
        cached_time = getattr(self, cache_time_key, 0)

        if cached is not None and (now - cached_time) < max_age_seconds:
            return cached

        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        snapshot = list(self._peer_snapshot.get_snapshot().values())

        setattr(self, cache_key, snapshot)
        setattr(self, cache_time_key, now)
        return snapshot

    # NOTE: _find_voter_peer_by_ip() removed - delegated to QuorumManager (Jan 2026 Phase 1)
    # NOTE: _count_alive_voters(), _is_peer_alive(), _is_swim_peer_id()
    # moved to LeadershipHealthMixin (Jan 26, 2026)

    def _on_swim_member_alive(self, member_id: str) -> None:
        """Handle SWIM member becoming alive - sync to gossip layer.

        Jan 22, 2026: Wire SWIM failure detection to gossip layer.

        When SWIM detects a member is alive (via ping/ack or indirect probe),
        this callback syncs the state to the HTTP gossip layer by:
        1. Updating the peer's last_heartbeat timestamp
        2. Clearing any retired status

        This ensures membership consistency between SWIM and HTTP layers.

        Args:
            member_id: SWIM member identifier (usually "IP:7947")
        """
        logger.info(f"[SWIM->Gossip] Member {member_id} ALIVE")
        # Extract host from member_id (format: "IP:7947")
        host = member_id.rsplit(":", 1)[0] if ":" in member_id else member_id

        with self.peers_lock:
            for node_id, peer in self.peers.items():
                peer_host = getattr(peer, "host", "")
                peer_tailscale = getattr(peer, "tailscale_ip", "")
                if peer_host == host or peer_tailscale == host:
                    peer.last_heartbeat = time.time()
                    if getattr(peer, "retired", False):
                        peer.retired = False
                        logger.info(f"[SWIM->Gossip] Unretired peer {node_id} via SWIM alive")
                    break

    def _on_swim_member_failed(self, member_id: str) -> None:
        """Handle SWIM member failure - mark as suspect in gossip layer.

        Jan 22, 2026: Wire SWIM failure detection to gossip layer.

        When SWIM detects a member has failed (via suspicion timeout),
        this callback records the failure in the health tracker. This allows
        the gossip layer to factor in SWIM's faster failure detection when
        making membership decisions.

        Note: This does NOT immediately retire the peer - the gossip layer
        uses its own timeout (PEER_TIMEOUT) for final retirement decisions.
        This just records the SWIM signal as an early warning.

        Args:
            member_id: SWIM member identifier (usually "IP:7947")
        """
        logger.warning(f"[SWIM->Gossip] Member {member_id} FAILED")
        host = member_id.rsplit(":", 1)[0] if ":" in member_id else member_id

        # Record failure in gossip health tracker if available
        if hasattr(self, "_gossip_health_tracker") and self._gossip_health_tracker:
            with self.peers_lock:
                for node_id, peer in self.peers.items():
                    peer_host = getattr(peer, "host", "")
                    if peer_host == host:
                        self._gossip_health_tracker.record_gossip_failure(node_id)
                        logger.info(f"[SWIM->Gossip] Recorded failure for {node_id}")
                        break

    def _cache_local_ips(self) -> set[str]:
        """Cache all local IPs at startup to avoid DNS blocking in health endpoints.

        Jan 26, 2026: Called once at initialization and cached in self._cached_local_ips.
        This eliminates per-request DNS lookups that were blocking Lambda health endpoints.

        Returns:
            Set of local IP addresses.
        """
        import socket
        import subprocess

        local_ips: set[str] = set()

        # Method 1: Hostname resolution
        try:
            hostname = socket.gethostname()
            for addr in socket.getaddrinfo(hostname, None):
                local_ips.add(addr[4][0])
        except (socket.gaierror, socket.herror, OSError, UnicodeError):
            pass

        # Method 2: Localhost variations
        try:
            for addr in socket.getaddrinfo("localhost", None):
                local_ips.add(addr[4][0])
        except (socket.gaierror, socket.herror, OSError):
            pass

        # Method 3: Subprocess fallback for containers
        try:
            result = subprocess.run(
                ["hostname", "-I"],
                capture_output=True, text=True, timeout=5
            )
            if result.returncode == 0:
                for ip in result.stdout.strip().split():
                    local_ips.add(ip)
        except Exception:
            pass

        # Method 4: Add advertise_host if configured
        advertise_host = os.environ.get("RINGRIFT_ADVERTISE_HOST", "").strip()
        if advertise_host:
            local_ips.add(advertise_host)

        # Method 5: Add Tailscale IP from environment
        ts_ip = os.environ.get("TAILSCALE_IP", "").strip()
        if ts_ip:
            local_ips.add(ts_ip)

        return local_ips

    # NOTE: _is_self_voter(), _check_voter_health(), _log_cluster_health_snapshot()
    # moved to LeadershipHealthMixin (Jan 26, 2026)

    async def _cluster_health_snapshot_loop(self) -> None:
        """Periodically log cluster health snapshots for debugging.

        Jan 2026: Delegated to HealthMetricsManager (Phase 9 decomposition).
        """
        await self.health_metrics_manager.cluster_health_snapshot_loop()

    async def _event_loop_latency_monitor(self) -> None:
        """Monitor event loop responsiveness to detect blocking operations.

        Jan 2026: Delegated to HealthMetricsManager (Phase 9 decomposition).
        """
        await self.health_metrics_manager.event_loop_latency_monitor()

    # NOTE: _maybe_adopt_voter_node_ids() removed - delegated to QuorumManager (Jan 2026 Phase 1)
    # _has_voter_quorum: Provided by LeaderElectionMixin
    # _release_voter_grant_if_self: Provided by LeaderElectionMixin

    def _enable_partition_local_election(self) -> bool:
        """Enable local leader election for partitioned nodes.

        When a partition is detected and no voters are reachable, this method
        temporarily adds reachable nodes to the voter set so they can elect a
        local leader and continue operating autonomously.

        This is a self-healing mechanism for network splits. When connectivity
        is restored, the partition will merge back with the main cluster.

        Returns:
            True if local election was enabled
        """
        # Don't override env-configured voters
        if (os.environ.get("RINGRIFT_P2P_VOTERS") or "").strip():
            return False

        # Check if we have any voters configured
        voters = list(getattr(self, "voter_node_ids", []) or [])

        # Count how many voters are reachable
        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        peers_by_id = self._peer_snapshot.get_snapshot()
        reachable_voters = 0
        for voter_id in voters:
            if voter_id == self.node_id:
                reachable_voters += 1
                continue
            peer = peers_by_id.get(voter_id)
            if peer and peer.is_alive():
                reachable_voters += 1

        # If we have quorum (simplified: 3 voters), no need for partition election
        quorum = min(VOTER_MIN_QUORUM, len(voters)) if voters else 1
        if reachable_voters >= quorum:
            return False

        # Build local partition voter set from reachable nodes
        local_voters = [self.node_id]  # Always include self
        for node_id, peer in peers_by_id.items():
            if peer.is_alive() and node_id not in local_voters:
                local_voters.append(node_id)

        if len(local_voters) < 2:
            # Need at least 2 nodes for meaningful election
            return False

        # Store original voters for restoration
        if not hasattr(self, "_original_voters"):
            self._original_voters = voters.copy()
            self._partition_election_started = time.time()

        # Enable partition-local election
        self.voter_node_ids = sorted(local_voters)
        self.voter_quorum_size = min(VOTER_MIN_QUORUM, len(local_voters))
        self.voter_config_source = "partition-local"
        print(
            f"[P2P] PARTITION: Enabling local election with {len(local_voters)} nodes: "
            f"{', '.join(local_voters)} (quorum={self.voter_quorum_size})"
        )
        return True

    def _restore_original_voters(self) -> bool:
        """Restore original voter configuration after partition heals.

        Called when connectivity to the main cluster is restored.

        Returns:
            True if voters were restored
        """
        if not hasattr(self, "_original_voters"):
            return False

        original = getattr(self, "_original_voters", [])
        if not original:
            return False

        # Check if we can reach any original voters
        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        peers_by_id = self._peer_snapshot.get_snapshot()
        for voter_id in original:
            if voter_id == self.node_id:
                continue
            peer = peers_by_id.get(voter_id)
            if peer and peer.is_alive():
                # We can reach at least one original voter, restore config
                self.voter_node_ids = original.copy()
                # SIMPLIFIED QUORUM: Fixed at 3 voters (or less if fewer voters exist)
                self.voter_quorum_size = min(VOTER_MIN_QUORUM, len(original))
                self.voter_config_source = "restored"
                delattr(self, "_original_voters")
                if hasattr(self, "_partition_election_started"):
                    delattr(self, "_partition_election_started")
                logger.info(f"Partition healed: restored original voters {', '.join(original)}")
                return True
        return False

    def _get_eligible_voters(self) -> list[str]:
        """Get list of nodes eligible to be voters (GPU nodes with good health)."""
        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        peers = self._peer_snapshot.get_snapshot()

        eligible = []
        now = time.time()

        for node_id, peer in peers.items():
            # Skip retired or NAT-blocked without relay
            if getattr(peer, "retired", False):
                continue

            # Must be alive
            if not peer.is_alive():
                continue

            # Must have GPU (CUDA or MPS)
            has_gpu = getattr(peer, "has_gpu", False)
            gpu_name = str(getattr(peer, "gpu_name", "") or "")
            if not has_gpu and "GH200" not in gpu_name and "H100" not in gpu_name and "A10" not in gpu_name and "aws" not in node_id.lower():
                continue

            # Must have been up for minimum time
            first_seen = getattr(peer, "first_seen", 0) or peer.last_heartbeat
            if now - first_seen < VOTER_PROMOTION_UPTIME:
                continue

            # Check health score (response rate)
            failures = getattr(peer, "consecutive_failures", 0)
            if failures >= VOTER_DEMOTION_FAILURES:
                continue

            eligible.append(node_id)

        # Always include self if we have GPU
        if self.node_id not in eligible:
            self_gpu = getattr(self, "has_gpu", False)
            if self_gpu or "aws" in self.node_id.lower() or "lambda" in self.node_id.lower():
                eligible.append(self.node_id)

        return sorted(eligible)

    def _manage_dynamic_voters(self) -> bool:
        """Manage dynamic voter pool - promote/demote voters as needed.

        Returns True if voter set was changed.
        """
        if not DYNAMIC_VOTER_ENABLED:
            return False

        # Don't override env-configured voters
        if (os.environ.get("RINGRIFT_P2P_VOTERS") or "").strip():
            return False

        current_voters = list(getattr(self, "voter_node_ids", []) or [])
        eligible = self._get_eligible_voters()

        # Count how many current voters are healthy
        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        peers = self._peer_snapshot.get_snapshot()

        healthy_voters = []
        unhealthy_voters = []

        for voter_id in current_voters:
            if voter_id == self.node_id:
                healthy_voters.append(voter_id)
                continue
            peer = peers.get(voter_id)
            if peer and peer.is_alive():
                failures = getattr(peer, "consecutive_failures", 0)
                if failures < VOTER_DEMOTION_FAILURES:
                    healthy_voters.append(voter_id)
                else:
                    unhealthy_voters.append(voter_id)
            else:
                unhealthy_voters.append(voter_id)

        changed = False
        new_voters = healthy_voters.copy()

        # Demote unhealthy voters
        if unhealthy_voters:
            logger.info(f"Dynamic voters: demoting unhealthy voters: {unhealthy_voters}")
            changed = True

        # Promote new voters if below target
        if len(new_voters) < DYNAMIC_VOTER_TARGET:
            candidates = [n for n in eligible if n not in new_voters]
            # Sort by reliability (fewer failures = better)
            candidates.sort(key=lambda n: getattr(peers.get(n), "consecutive_failures", 0) if peers.get(n) else 999)

            needed = DYNAMIC_VOTER_TARGET - len(new_voters)
            for candidate in candidates[:needed]:
                new_voters.append(candidate)
                logger.info(f"Dynamic voters: promoting {candidate} to voter")
                changed = True

        if changed and new_voters:
            new_voters = sorted(set(new_voters))
            self.voter_node_ids = new_voters
            # SIMPLIFIED QUORUM: Fixed at 3 voters (or less if fewer voters exist)
            self.voter_quorum_size = min(VOTER_MIN_QUORUM, len(new_voters))
            self.voter_config_source = "dynamic"
            print(
                f"[P2P] Dynamic voter set updated: {len(new_voters)} voters, "
                f"quorum={self.voter_quorum_size} ({', '.join(new_voters)})"
            )
            return True

        return False

    # NOTE: _check_leader_health() moved to LeadershipHealthMixin (Jan 26, 2026)

    async def _acquire_voter_lease_quorum(self, lease_id: str, duration: int) -> float | None:
        """Acquire/renew an exclusive leader lease from a quorum of voters.

        December 29, 2025: Added retry with exponential backoff when initial
        quorum acquisition fails. This handles transient network issues.

        Returns the effective lease expiry timestamp if a quorum granted the
        lease; otherwise returns None.
        """
        voter_ids = list(getattr(self, "voter_node_ids", []) or [])
        if not voter_ids:
            return time.time() + float(duration)

        quorum = int(getattr(self, "voter_quorum_size", 0) or 0)
        if quorum <= 0:
            # SIMPLIFIED QUORUM: Fixed at 3 voters (or less if fewer voters exist)
            quorum = min(VOTER_MIN_QUORUM, len(voter_ids))

        duration = max(10, min(int(duration), int(LEADER_LEASE_DURATION * 2)))

        # December 29, 2025: Retry with exponential backoff
        max_retries = 3
        retry_delays = [0, 2, 5]  # Immediate, then 2s, then 5s

        for attempt in range(max_retries):
            if attempt > 0:
                await asyncio.sleep(retry_delays[attempt])
                logger.info(f"Voter lease acquisition retry {attempt + 1}/{max_retries}")

            now = time.time()
            acks = 0
            lease_ttls: list[float] = []

            # Self-grant (as a voter).
            if self.node_id in voter_ids:
                self.voter_grant_leader_id = self.node_id
                self.voter_grant_lease_id = lease_id
                self.voter_grant_expires = now + float(duration)
                lease_ttls.append(float(duration))
                acks += 1

            # Jan 2026: Use lock-free PeerSnapshot for read-only access
            peers_by_id = self._peer_snapshot.get_snapshot()

            # STABILITY FIX: Use 15s timeout for voter lease operations (was 5s).
            # Cross-geographic Tailscale connections can have latency spikes.
            timeout = ClientTimeout(total=15)

            # Dec 29, 2025: Parallel lease acquisition for faster leadership transitions
            # Instead of sequential requests, we fire all lease requests in parallel
            async def _request_lease_from_voter(
                session: aiohttp.ClientSession,
                voter_id: str,
                voter: NodeInfo,
            ) -> tuple[bool, float | None]:
                """Request lease from a single voter. Returns (success, ttl)."""
                payload = {
                    "leader_id": self.node_id,
                    "lease_id": lease_id,
                    "lease_duration": duration,
                    "lease_epoch": self._lease_epoch + 1,
                }
                for url in self._tailscale_urls_for_voter(voter, "/election/lease"):
                    try:
                        async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                            if resp.status != 200:
                                continue
                            data, json_error = await safe_json_response(resp, default={}, log_errors=False)
                            if json_error or not data.get("granted"):
                                return False, None
                            ttl_raw = data.get("lease_ttl_seconds") or data.get("ttl_seconds")
                            if ttl_raw is not None:
                                try:
                                    return True, float(ttl_raw)
                                except (ValueError, TypeError):
                                    pass
                            return True, float(duration)
                    except (aiohttp.ClientError, asyncio.TimeoutError, ValueError, AttributeError, OSError):
                        continue
                return False, None

            async with get_client_session(timeout) as session:
                # Build list of voters to request from (excluding self and dead peers)
                voter_tasks = []
                for voter_id in voter_ids:
                    if voter_id == self.node_id:
                        continue
                    voter = peers_by_id.get(voter_id)
                    if not voter or not voter.is_alive():
                        continue
                    voter_tasks.append(_request_lease_from_voter(session, voter_id, voter))

                # Fire all requests in parallel
                if voter_tasks:
                    results = await asyncio.gather(*voter_tasks, return_exceptions=True)
                    for result in results:
                        if isinstance(result, Exception):
                            continue
                        success, ttl = result
                        if success:
                            acks += 1
                            if ttl is not None and ttl > 0:
                                lease_ttls.append(ttl)
                            else:
                                lease_ttls.append(float(duration))

            if acks >= quorum:
                # Use a relative TTL (computed by each voter on its own clock) to avoid
                # leader lease flapping under clock skew. Convert back to a local expiry.
                effective_ttl = min(lease_ttls) if lease_ttls else float(duration)
                effective_ttl = max(10.0, min(float(duration), float(effective_ttl)))
                if attempt > 0:
                    logger.info(f"Voter lease acquired on retry {attempt + 1}")
                return now + float(effective_ttl)

            # Log retry info
            if attempt < max_retries - 1:
                logger.warning(
                    f"Voter lease quorum not reached: {acks}/{quorum} acks, "
                    f"retrying in {retry_delays[attempt + 1]}s..."
                )

        # All retries exhausted
        logger.error(f"Failed to acquire voter lease quorum after {max_retries} attempts")
        return None

    # =========================================================================
    # Phase 15.1.1: Fence Token Helpers (December 29, 2025)
    # =========================================================================

    def get_fence_token(self) -> str:
        """Get the current fence token for including in leader operations.

        Phase 15.1.1: Fence tokens provide split-brain protection by ensuring
        workers can reject commands from stale leaders.

        Returns:
            Current fence token or empty string if not leader
        """
        if self.role != NodeRole.LEADER:
            return ""
        return self._fence_token

    def get_lease_epoch(self) -> int:
        """Get the current lease epoch.

        Phase 15.1.1: The epoch is monotonically increasing and helps
        resolve split-brain by allowing workers to compare epochs.

        Returns:
            Current lease epoch (0 if never been leader)
        """
        return self._lease_epoch

    def validate_fence_token(self, token: str) -> tuple[bool, str]:
        """Validate an incoming fence token from a claimed leader.

        Phase 15.1.1: Workers use this to reject commands from stale leaders.
        A token is valid if:
        1. It's from the current known leader
        2. Its epoch is >= our known epoch

        Args:
            token: Fence token to validate (format: node_id:epoch:timestamp)

        Returns:
            (valid, reason) tuple
        """
        if not token:
            return False, "empty_fence_token"

        try:
            parts = token.split(":")
            if len(parts) != 3:
                return False, "malformed_token"

            token_node_id = parts[0]
            token_epoch = int(parts[1])

            # Check if token is from known leader
            if self.leader_id and token_node_id != self.leader_id:
                return False, f"token_from_unknown_leader:{token_node_id}"

            # Check epoch - reject if lower than what we've seen
            if hasattr(self, "_last_seen_epoch"):
                if token_epoch < self._last_seen_epoch:
                    return False, f"stale_epoch:{token_epoch}<{self._last_seen_epoch}"
                self._last_seen_epoch = max(self._last_seen_epoch, token_epoch)
            else:
                self._last_seen_epoch = token_epoch

            return True, "valid"

        except (ValueError, IndexError) as e:
            return False, f"parse_error:{e}"

    # NOTE: update_fence_token_from_leader() moved to LeadershipHealthMixin (Jan 26, 2026)

    async def _determine_leased_leader_from_voters(self) -> str | None:
        """Return the current lease-holder as reported by a quorum of voters.

        This is a read-only reconciliation step used to resolve split-brain once
        partitions heal. It queries the current voter grant state via
        `/election/grant` and selects the leader_id that has >= quorum votes with
        non-expired grants.
        """
        voter_ids = list(getattr(self, "voter_node_ids", []) or [])
        if not voter_ids:
            return None

        quorum = int(getattr(self, "voter_quorum_size", 0) or 0)
        if quorum <= 0:
            # SIMPLIFIED QUORUM: Fixed at 3 voters (or less if fewer voters exist)
            quorum = min(VOTER_MIN_QUORUM, len(voter_ids))

        now = time.time()
        counts: dict[str, int] = {}

        # Include local voter state.
        if self.node_id in voter_ids:
            leader_id = str(getattr(self, "voter_grant_leader_id", "") or "")
            expires = float(getattr(self, "voter_grant_expires", 0.0) or 0.0)
            if leader_id and expires > now:
                counts[leader_id] = counts.get(leader_id, 0) + 1

        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        peers_by_id = self._peer_snapshot.get_snapshot()

        # STABILITY FIX: Use 15s timeout for voter operations (was 5s).
        timeout = ClientTimeout(total=15)
        async with get_client_session(timeout) as session:
            for voter_id in voter_ids:
                if voter_id == self.node_id:
                    continue
                voter = peers_by_id.get(voter_id)
                if not voter or not voter.is_alive():
                    continue

                # Use Tailscale-exclusive URLs for voter communication to avoid NAT issues
                for url in self._tailscale_urls_for_voter(voter, "/election/grant"):
                    try:
                        async with session.get(url, headers=self._auth_headers()) as resp:
                            if resp.status != 200:
                                continue
                            data = await resp.json()
                        leader_id = str((data or {}).get("leader_id") or "")
                        if not leader_id:
                            break
                        ttl_raw = (data or {}).get("lease_ttl_seconds")
                        if ttl_raw is None:
                            ttl_raw = (data or {}).get("ttl_seconds")
                        ttl_val: float | None = None
                        if ttl_raw is not None:
                            try:
                                ttl_val = float(ttl_raw)
                            except (ValueError):
                                ttl_val = None

                        if ttl_val is not None:
                            if ttl_val <= 0:
                                break
                        else:
                            # Back-compat: use absolute expiry as best-effort, with
                            # a generous skew tolerance (1 lease duration).
                            expires = float((data or {}).get("lease_expires") or 0.0)
                            if expires <= 0:
                                break
                            if expires + float(LEADER_LEASE_DURATION) < now:
                                break
                        counts[leader_id] = counts.get(leader_id, 0) + 1
                        break
                    except (ValueError, AttributeError):
                        continue

        winners = [leader_id for leader_id, count in counts.items() if count >= quorum]
        if not winners:
            return None
        # Deterministic: if multiple satisfy quorum (shouldn't), pick highest node_id.
        return sorted(winners)[-1]

    async def _query_arbiter_for_leader(self) -> str | None:
        """Query the arbiter for the authoritative leader when voter quorum fails.

        The arbiter is a reliably-reachable node that maintains its view of
        who the leader should be. Used as a fallback when split-brain causes
        voter quorum to be unreachable.

        Returns:
            The leader_id from the arbiter, or None if arbiter is unreachable
        """
        arbiter_url = ARBITER_URL
        if not arbiter_url:
            return None

        # Try the configured arbiter URL
        urls_to_try = [arbiter_url]

        # Also try known peers as arbiters if main arbiter fails
        for peer_addr in (self.known_peers or []):
            if peer_addr not in urls_to_try:
                urls_to_try.append(peer_addr)

        timeout = ClientTimeout(total=5)
        try:
            async with get_client_session(timeout) as session:
                for url in urls_to_try:
                    try:
                        base_url = url.rstrip("/")
                        # Query the arbiter's election/grant endpoint to see who they think is leader
                        async with session.get(
                            f"{base_url}/election/grant",
                            headers=self._auth_headers()
                        ) as resp:
                            if resp.status == 200:
                                data = await resp.json()
                                leader_id = str((data or {}).get("leader_id") or "")
                                if leader_id:
                                    logger.info(f"Arbiter {base_url} reports leader: {leader_id}")
                                    return leader_id
                    except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
                        # Try next arbiter
                        continue
        except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
            pass

        return None

    # _parse_peer_address, _url_for_peer, _urls_for_peer provided by NetworkUtilsMixin

    def _auth_headers(self) -> dict[str, str]:
        if not self.auth_token:
            return {}
        return {"Authorization": f"Bearer {self.auth_token}"}

    def _get_leader_peer(self) -> NodeInfo | None:
        if self._is_leader():
            return self.self_info

        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        peers_snapshot = list(self._peer_snapshot.get_snapshot().values())

        conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])

        leader_id = self.leader_id
        if leader_id and self._is_leader_lease_valid():
            for peer in peers_snapshot:
                if (
                    peer.node_id == leader_id
                    and peer.role == NodeRole.LEADER
                    and peer.is_alive()
                    and self._is_leader_eligible(peer, conflict_keys)
                ):
                    # Jan 8, 2026: Validate consensus - check that other peers agree
                    consensus_count = self._count_peers_reporting_leader(leader_id, peers_snapshot)
                    if consensus_count < 2 and len(peers_snapshot) >= 3:
                        # Low consensus - log warning but still return leader
                        logger.warning(
                            f"[LeaderConsensus] Low consensus for leader {leader_id}: "
                            f"only {consensus_count} peers agree out of {len(peers_snapshot)}"
                        )
                    return peer

        eligible_leaders = [
            peer for peer in peers_snapshot
            if peer.role == NodeRole.LEADER and self._is_leader_eligible(peer, conflict_keys)
        ]
        if eligible_leaders:
            return sorted(eligible_leaders, key=lambda p: p.node_id)[-1]

        return None

    def _count_peers_reporting_leader(
        self, leader_id: str, peers_snapshot: list[NodeInfo]
    ) -> int:
        """Count how many peers report the same leader_id.

        Jan 8, 2026: Added for leader consensus validation.

        Args:
            leader_id: The leader ID to check for consensus
            peers_snapshot: List of peer NodeInfo objects

        Returns:
            Number of peers reporting this leader_id
        """
        count = 0
        for peer in peers_snapshot:
            if not peer.is_alive():
                continue
            # Check if peer reports this leader
            peer_leader = getattr(peer, "leader_id", None)
            if peer_leader == leader_id:
                count += 1
        return count

    async def _proxy_to_leader(self, request: web.Request) -> web.StreamResponse:
        """Best-effort proxy for leader-only APIs when the dashboard hits a follower."""
        leader = self._get_leader_peer()
        if not leader:
            return web.json_response(
                {"success": False, "error": "leader_unknown", "leader_id": self.leader_id},
                status=503,
            )

        candidate_urls = self._urls_for_peer(leader, request.raw_path)
        if not candidate_urls:
            candidate_urls = [self._url_for_peer(leader, request.raw_path)]
        forward_headers: dict[str, str] = {}
        for h in ("Authorization", "X-RingRift-Auth", "Content-Type"):
            if h in request.headers:
                forward_headers[h] = request.headers[h]

        body: bytes | None = None
        if request.method not in ("GET", "HEAD", "OPTIONS"):
            body = await request.read()

        # Keep leader-proxy responsive: unreachable "leaders" (often NAT/firewall)
        # should fail fast so the dashboard doesn't hang for a full minute.
        timeout = ClientTimeout(total=10)
        last_exc: Exception | None = None
        async with get_client_session(timeout) as session:
            for target_url in candidate_urls:
                try:
                    async with session.request(
                        request.method,
                        target_url,
                        data=body,
                        headers=forward_headers,
                    ) as resp:
                        payload = await resp.read()
                        content_type = resp.headers.get("Content-Type")
                        headers: dict[str, str] = {}
                        if content_type:
                            headers["Content-Type"] = content_type
                        headers["X-RingRift-Proxied-By"] = self.node_id
                        headers["X-RingRift-Proxied-To"] = target_url
                        return web.Response(body=payload, status=resp.status, headers=headers)
                except Exception as exc:
                    last_exc = exc
                    continue

        return web.json_response(
            {
                "success": False,
                "error": "leader_proxy_failed",
                "message": str(last_exc) if last_exc else "unknown_error",
                "leader_id": self.leader_id,
                "attempted_urls": candidate_urls,
            },
            status=502,
        )

    def _is_request_authorized(self, request: web.Request) -> bool:
        if not self.auth_token:
            return True

        auth_header = request.headers.get("Authorization", "")
        token = ""
        if auth_header.lower().startswith("bearer "):
            token = auth_header[7:].strip()
        if not token:
            token = request.headers.get("X-RingRift-Auth", "").strip()
        if not token:
            return False

        return secrets.compare_digest(token, self.auth_token)

    def _load_state(self):
        """Load persisted state from database.

        Phase 1 Refactoring: Delegated to StateManager.
        The StateManager returns a PersistedState object which is then
        applied to the orchestrator's instance variables.
        """
        try:
            state = self.state_manager.load_state(self.node_id)

            # P2P Hardening Phase 2 (Dec 2025): Validate and clean stale state
            is_valid, issues = self.state_manager.validate_loaded_state(state)
            if issues:
                # Clean up stale entries before applying state
                jobs_removed, peers_removed = self.state_manager.clean_stale_state(state)
                if self.verbose:
                    logger.info(
                        f"[P2POrchestrator] Startup cleanup: removed "
                        f"{jobs_removed} stale jobs, {peers_removed} stale peers"
                    )

            # Apply loaded peers
            for node_id, info_dict in state.peers.items():
                try:
                    info = NodeInfo.from_dict(info_dict)
                    self.peers[node_id] = info
                except Exception as e:  # noqa: BLE001
                    logger.error(f"Failed to load peer {node_id}: {e}")
            # C2 fix: Sync peer snapshot after loading persisted peers
            self._sync_peer_snapshot()

            # Apply loaded jobs
            for job_dict in state.jobs:
                try:
                    job = ClusterJob(
                        job_id=job_dict["job_id"],
                        job_type=JobType(job_dict["job_type"]),
                        node_id=job_dict["node_id"],
                        board_type=job_dict.get("board_type", "square8"),
                        num_players=job_dict.get("num_players", 2),
                        engine_mode=job_dict.get("engine_mode", "descent-only"),
                        pid=job_dict.get("pid", 0),
                        started_at=job_dict.get("started_at", 0.0),
                        status=job_dict.get("status", "running"),
                    )
                    self.local_jobs[job.job_id] = job
                except Exception as e:  # noqa: BLE001
                    logger.error(f"Failed to load job: {e}")

            # Apply leader state
            # C1 fix: Use leader_state_lock for role/leader_id changes
            ls = state.leader_state
            with self.leader_state_lock:
                if ls.leader_id:
                    self.leader_id = ls.leader_id
                if ls.leader_lease_id:
                    self.leader_lease_id = ls.leader_lease_id
                if ls.leader_lease_expires:
                    self.leader_lease_expires = ls.leader_lease_expires
                if ls.last_lease_renewal:
                    self.last_lease_renewal = ls.last_lease_renewal
                if ls.role:
                    with contextlib.suppress(Exception):
                        self.role = NodeRole(ls.role)

            # Voter grant state
            if ls.voter_grant_leader_id:
                self.voter_grant_leader_id = ls.voter_grant_leader_id
            if ls.voter_grant_lease_id:
                self.voter_grant_lease_id = ls.voter_grant_lease_id
            if ls.voter_grant_expires:
                self.voter_grant_expires = ls.voter_grant_expires

            # Phase 15.1.1: Restore fenced lease token state
            # These fields may not exist in older state files, so use getattr with defaults
            persisted_epoch = getattr(ls, "lease_epoch", 0) or 0
            persisted_fence = getattr(ls, "fence_token", "") or ""
            persisted_last_seen = getattr(ls, "last_seen_epoch", 0) or 0
            # Only restore if higher than current (monotonic guarantee)
            if persisted_epoch > self._lease_epoch:
                self._lease_epoch = persisted_epoch
            if persisted_fence and not self._fence_token:
                self._fence_token = persisted_fence
            if persisted_last_seen > self._last_seen_epoch:
                self._last_seen_epoch = persisted_last_seen
            if persisted_epoch > 0:
                logger.info(
                    f"[P2POrchestrator] Restored lease fencing: epoch={self._lease_epoch}, "
                    f"last_seen={self._last_seen_epoch}"
                )

            # Optional persisted voter configuration (convergence helper). Only
            # apply when voters are not explicitly configured via env/config.
            if (
                ls.voter_node_ids
                and not (getattr(self, "voter_node_ids", []) or [])
                and str(getattr(self, "voter_config_source", "none") or "none") == "none"
            ):
                if self.quorum_manager.maybe_adopt_voter_node_ids(ls.voter_node_ids, source="state"):
                    # Sync adopted state back to orchestrator attributes
                    self.voter_node_ids = self.quorum_manager.voter_node_ids
                    self.voter_config_source = self.quorum_manager.voter_config_source
                    self.voter_quorum_size = min(VOTER_MIN_QUORUM, len(self.voter_node_ids)) if self.voter_node_ids else 0

            # Self-heal inconsistent persisted leader state (can happen after
            # abrupt shutdowns or partial writes): never keep role=leader without
            # a matching leader_id.
            if self.role == NodeRole.LEADER and not self.leader_id:
                logger.info("Loaded role=leader but leader_id is empty; stepping down to follower")
                # C1 fix: Use leader_state_lock for role changes
                with self.leader_state_lock:
                    self.role = NodeRole.FOLLOWER
                    self.leader_lease_id = ""
                    self.leader_lease_expires = 0.0
                self.last_lease_renewal = 0.0

            logger.info(f"Loaded state: {len(self.peers)} peers, {len(self.local_jobs)} jobs")

            # December 2025 P2P Hardening: Validate loaded state on startup
            # This detects stale jobs, stale peers, and expired leases
            is_valid, issues = self.state_manager.validate_loaded_state(state)
            if not is_valid:
                logger.warning(f"[P2P] Startup state validation found {len(issues)} issues:")
                for issue in issues:
                    logger.warning(f"  - {issue}")
                # Clean up stale entries
                stale_jobs_cleared = self.state_manager.clear_stale_jobs_by_age(max_age_hours=24.0)
                stale_peers_cleared = self.state_manager.clear_stale_peers(max_stale_seconds=300.0)
                if stale_jobs_cleared or stale_peers_cleared:
                    logger.info(f"[P2P] Cleared {stale_jobs_cleared} stale jobs, {stale_peers_cleared} stale peers")
            else:
                logger.info("[P2P] Startup state validation passed")

            # Dec 28, 2025 (Phase 7): Load persisted peer health state
            try:
                peer_health_states = self.state_manager.load_all_peer_health(max_age_seconds=3600.0)
                if peer_health_states:
                    self._apply_loaded_peer_health(peer_health_states)
                    logger.info(f"[P2P] Loaded {len(peer_health_states)} peer health records")
            except Exception as e:  # noqa: BLE001
                logger.warning(f"[P2P] Failed to load peer health state: {e}")

            # Jan 12, 2026: Initialize job snapshot with loaded jobs
            try:
                self._job_snapshot.update(self.local_jobs)
            except Exception as e:  # noqa: BLE001
                logger.warning(f"[P2P] Failed to initialize job snapshot: {e}")

        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to load state: {e}")

    def _apply_loaded_peer_health(self, peer_health_states: dict) -> None:
        """Apply loaded peer health state to circuit breakers and gossip tracker.

        Jan 2026: Delegated to HealthMetricsManager (Phase 9 decomposition).
        """
        self.health_metrics_manager.apply_loaded_peer_health(peer_health_states)

    # NOTE: _collect_peer_health_states() inlined at call site (Jan 2026 Phase 2)

    def _save_state(self):
        """Save current state to database.

        Phase 1 Refactoring: Delegated to StateManager.
        Creates a PersistedLeaderState from instance variables and
        passes it to the StateManager for persistence.
        """
        try:
            # Build leader state from instance variables
            role_value = self.role.value if hasattr(self.role, "value") else str(self.role)
            leader_state = PersistedLeaderState(
                leader_id=self.leader_id or "",
                leader_lease_id=self.leader_lease_id or "",
                leader_lease_expires=float(self.leader_lease_expires or 0.0),
                last_lease_renewal=float(self.last_lease_renewal or 0.0),
                role=role_value,
                voter_grant_leader_id=str(getattr(self, "voter_grant_leader_id", "") or ""),
                voter_grant_lease_id=str(getattr(self, "voter_grant_lease_id", "") or ""),
                voter_grant_expires=float(getattr(self, "voter_grant_expires", 0.0) or 0.0),
                voter_node_ids=list(getattr(self, "voter_node_ids", []) or []),
                voter_config_source=str(getattr(self, "voter_config_source", "") or ""),
                # Phase 15.1.1: Fenced lease token state
                lease_epoch=int(getattr(self, "_lease_epoch", 0) or 0),
                fence_token=str(getattr(self, "_fence_token", "") or ""),
                last_seen_epoch=int(getattr(self, "_last_seen_epoch", 0) or 0),
            )

            # Delegate to StateManager
            self.state_manager.save_state(
                node_id=self.node_id,
                peers=self.peers,
                jobs=self.local_jobs,
                leader_state=leader_state,
                peers_lock=self.peers_lock,
                jobs_lock=self.jobs_lock,
            )

            # Dec 28, 2025 (Phase 7): Save peer health state
            try:
                # Inline: was _collect_peer_health_states()
                peer_health_states = self.health_metrics_manager.collect_peer_health_states()
                if peer_health_states:
                    saved = self.state_manager.save_peer_health_batch(peer_health_states)
                    if saved > 0 and self.verbose:
                        logger.debug(f"[P2P] Saved {saved} peer health records")
            except Exception as e:  # noqa: BLE001
                if self.verbose:
                    logger.debug(f"[P2P] Error saving peer health state: {e}")

            # Jan 12, 2026: Sync job snapshot for lock-free /status reads
            try:
                self._job_snapshot.update(self.local_jobs)
            except Exception as e:  # noqa: BLE001
                if self.verbose:
                    logger.debug(f"[P2P] Error syncing job snapshot: {e}")

        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to save state: {e}")

    # =========================================================================
    # Phase 27: Peer Cache and Reputation Tracking
    # Provided by PeerManagerMixin:
    # - _update_peer_reputation: EMA-based reputation updates
    # - _save_peer_to_cache: SQLite peer persistence with pruning
    # - _get_bootstrap_peers_by_reputation: Prioritized peer list for bootstrap
    # - _get_cached_peer_count, _clear_peer_cache, _prune_stale_peers
    # =========================================================================

    # =========================================================================
    # Phase 29: Cluster Epoch Persistence
    # Phase 1 Refactoring: Delegated to StateManager
    # =========================================================================

    def _save_cluster_epoch(self) -> None:
        """Save cluster epoch to database.

        Phase 1 Refactoring: Delegated to StateManager.
        Kept for backward compatibility.
        """
        self.state_manager.set_cluster_epoch(self._cluster_epoch)
        self.state_manager.save_cluster_epoch()

    def _increment_cluster_epoch(self) -> None:
        """Increment cluster epoch (called on leader change).

        Phase 1 Refactoring: Delegated to StateManager.
        Kept for backward compatibility.
        """
        self._cluster_epoch = self.state_manager.increment_cluster_epoch()

    def record_metric(
        self,
        metric_type: str,
        value: float,
        board_type: str | None = None,
        num_players: int | None = None,
        metadata: dict[str, Any] | None = None,
    ):
        """Record a metric to the history table for observability.

        Phase 1 Refactoring: Delegated to MetricsManager.

        Metric types:
        - training_loss: NNUE training loss
        - elo_rating: Model Elo rating
        - gpu_utilization: GPU utilization percentage
        - selfplay_games_per_hour: Game generation rate
        - validation_rate: GPU selfplay validation rate
        - tournament_win_rate: Tournament win rate for new model
        """
        self.metrics_manager.record_metric(
            metric_type=metric_type,
            value=value,
            board_type=board_type,
            num_players=num_players,
            metadata=metadata,
        )

    def get_metrics_history(
        self,
        metric_type: str,
        board_type: str | None = None,
        num_players: int | None = None,
        hours: float = 24,
        limit: int = 1000,
    ) -> list[dict[str, Any]]:
        """Get metrics history for a specific metric type."""
        conn = None
        try:
            conn = sqlite3.connect(str(self.db_path), timeout=10.0)
            cursor = conn.cursor()

            since = time.time() - (hours * 3600)
            query = """
                SELECT timestamp, value, board_type, num_players, metadata
                FROM metrics_history
                WHERE metric_type = ? AND timestamp > ?
            """
            params: list[Any] = [metric_type, since]

            if board_type:
                query += " AND board_type = ?"
                params.append(board_type)
            if num_players:
                query += " AND num_players = ?"
                params.append(num_players)

            query += " ORDER BY timestamp DESC LIMIT ?"
            params.append(limit)

            cursor.execute(query, params)
            results = []
            for row in cursor.fetchall():
                results.append({
                    "timestamp": row[0],
                    "value": row[1],
                    "board_type": row[2],
                    "num_players": row[3],
                    "metadata": json.loads(row[4]) if row[4] else None,
                })
            return results
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to get metrics history: {e}")
            return []
        finally:
            if conn:
                conn.close()

    def get_metrics_summary(self, hours: float = 24) -> dict[str, Any]:
        """Get summary of all metrics over the specified time period."""
        conn = None
        try:
            conn = sqlite3.connect(str(self.db_path), timeout=10.0)
            cursor = conn.cursor()

            since = time.time() - (hours * 3600)

            cursor.execute("""
                SELECT metric_type, COUNT(*), AVG(value), MIN(value), MAX(value)
                FROM metrics_history
                WHERE timestamp > ?
                GROUP BY metric_type
            """, (since,))

            summary: dict[str, Any] = {}
            for row in cursor.fetchall():
                summary[row[0]] = {
                    "count": row[1],
                    "avg": row[2],
                    "min": row[3],
                    "max": row[4],
                }

            cursor.execute("""
                SELECT metric_type, value, timestamp
                FROM metrics_history m1
                WHERE timestamp = (
                    SELECT MAX(timestamp) FROM metrics_history m2
                    WHERE m2.metric_type = m1.metric_type
                )
            """)
            for row in cursor.fetchall():
                if row[0] in summary:
                    summary[row[0]]["latest"] = row[1]
                    summary[row[0]]["latest_time"] = row[2]

            return {"period_hours": hours, "since": since, "metrics": summary}
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to get metrics summary: {e}")
            return {}
        finally:
            if conn:
                conn.close()

    def _create_self_info(self) -> NodeInfo:
        """Create NodeInfo for this node."""
        # Detect GPU
        has_gpu, gpu_name = self._detect_gpu()

        cpu_count = int(os.cpu_count() or 0)

        # Detect memory
        memory_gb = self._detect_memory()

        # Detect capabilities based on hardware
        # Dec 2025: RINGRIFT_IS_COORDINATOR=true restricts to coordinator-only
        # Dec 29, 2025: Also check distributed_hosts.yaml for role/enabled flags
        is_coordinator = os.environ.get("RINGRIFT_IS_COORDINATOR", "").lower() in ("true", "1", "yes")

        # Check YAML config for this node's settings
        if not is_coordinator:
            try:
                from app.config.cluster_config import load_cluster_config
                config = load_cluster_config()
                # ClusterConfig stores hosts in hosts_raw attribute
                nodes = getattr(config, "hosts_raw", {}) or {}
                node_cfg = nodes.get(self.node_id, {})
                # Check role or explicit enabled flags
                if node_cfg.get("role") == "coordinator":
                    is_coordinator = True
                    logger.info(f"[P2P] Node {self.node_id} is coordinator (from YAML)")
                elif node_cfg.get("selfplay_enabled") is False and node_cfg.get("training_enabled") is False:
                    is_coordinator = True
                    logger.info(f"[P2P] Node {self.node_id} has selfplay/training disabled (from YAML)")
            except Exception as e:
                logger.debug(f"[P2P] Could not load cluster config: {e}")
        if is_coordinator:
            # Dec 30, 2025: Warn if GPU node is misconfigured as coordinator
            if has_gpu:
                logger.warning(
                    f"[P2P] GPU node {self.node_id} is marked as coordinator - "
                    f"this may be a misconfiguration. GPU: {gpu_name}. "
                    "Unset RINGRIFT_IS_COORDINATOR or remove role:coordinator from YAML "
                    "to enable training capabilities."
                )
            capabilities = []  # Coordinator nodes don't run compute tasks
            logger.info("[P2P] Coordinator-only mode: no selfplay/training/cmaes capabilities")
        else:
            capabilities = ["selfplay"]
            if has_gpu:
                capabilities.extend(["training", "cmaes", "gauntlet", "tournament"])
            if memory_gb >= 64:
                capabilities.append("large_boards")

        info = NodeInfo(
            node_id=self.node_id,
            host=self.advertise_host,
            port=self.advertise_port,
            role=self.role,
            last_heartbeat=time.time(),
            cpu_count=cpu_count,
            has_gpu=has_gpu,
            gpu_name=gpu_name,
            memory_gb=memory_gb,
            capabilities=capabilities,
            version=self.build_version,
        )
        # Advertise an alternate mesh endpoint (Tailscale) for NAT traversal and
        # multi-path retries. Peers persist the observed reachable endpoint in
        # `host`/`port` but keep our `reported_host`/`reported_port` as an
        # additional candidate (see `_heartbeat_loop` multi-path retry).
        ts_ip = self._get_tailscale_ip()
        if ts_ip and ts_ip != info.host:
            info.reported_host = ts_ip
            # Use the actual listening port for mesh endpoints (port-mapped
            # advertise ports may not be reachable inside overlays).
            info.reported_port = int(self.port)

        # Jan 2026: Populate alternate_ips with all reachable IPs for partition healing
        # Peers can try multiple IPs to reach us, improving mesh resilience
        info.alternate_ips = self._discover_all_ips(exclude_primary=info.host)

        # Jan 13, 2026: Multi-address advertisement for voter counting fix
        # Nodes advertise ALL addresses they're reachable at in heartbeats.
        # This fixes voter quorum issues where voters are listed by config name
        # but peers report via Tailscale/public IPs that don't match.
        info.tailscale_ip = ts_ip or ""
        info.addresses = self._collect_all_addresses(ts_ip, info.host)

        # Jan 24, 2026: Populate visible_peers for connectivity scoring
        # Used by _compute_connectivity_score() to determine leader eligibility
        info.visible_peers = len([p for p in self.peers.values() if p.is_alive()])

        # Jan 25, 2026: Compute effective_timeout for broadcast to peers
        # This tells other nodes how long to wait before marking us dead
        try:
            from app.p2p.constants import PEER_TIMEOUT, get_cpu_adaptive_timeout
            from app.config.provider_timeouts import ProviderTimeouts
            cpu_load = info.cpu_percent / 100.0 if info.cpu_percent > 0 else 0.0
            base_timeout = get_cpu_adaptive_timeout(PEER_TIMEOUT, cpu_load)
            provider_mult = ProviderTimeouts.get_multiplier(self.node_id) if ProviderTimeouts else 1.0
            info.effective_timeout = base_timeout * provider_mult
        except Exception:
            info.effective_timeout = 180.0  # Fallback to default

        return info

    def _collect_all_addresses(
        self, tailscale_ip: str | None, primary_host: str
    ) -> list[str]:
        """Collect all addresses this node is reachable at.

        Jan 13, 2026: For multi-address advertisement to fix voter counting.

        Returns addresses in priority order:
        1. Tailscale IP (100.x.x.x) - most reliable for P2P mesh
        2. Primary host (advertise_host) - what we're currently advertising
        3. SSH host from config - public/direct access
        4. Local interface IP - same-network access

        Args:
            tailscale_ip: Tailscale VPN IP if available
            primary_host: Current advertise_host

        Returns:
            List of addresses, deduplicated, in priority order
        """
        addresses: list[str] = []
        seen: set[str] = set()

        def add_if_new(addr: str | None) -> None:
            if addr and addr not in seen and addr not in ("", "0.0.0.0", "127.0.0.1"):
                addresses.append(addr)
                seen.add(addr)

        # Priority 1: Tailscale IP (best for mesh)
        add_if_new(tailscale_ip)

        # Priority 2: Current advertise host
        add_if_new(primary_host)

        # Priority 3: SSH host from config (may be public IP)
        try:
            from app.config.cluster_config import load_cluster_config
            config = load_cluster_config()
            nodes = getattr(config, "hosts_raw", {}) or {}
            node_cfg = nodes.get(self.node_id, {})
            if node_cfg:
                add_if_new(node_cfg.get("ssh_host"))
                add_if_new(node_cfg.get("tailscale_ip"))
        except Exception:
            pass

        # Priority 4: Local interface IPs
        for ip in self._discover_all_ips(exclude_primary=None):
            add_if_new(ip)

        return addresses

    # NOTE: _detect_gpu, _detect_memory, _get_local_ip, _get_tailscale_ip
    # delegated to ResourceDetectorMixin (Dec 28, 2025). ~75 LOC removed.
    # Use: self._detect_gpu(), self._detect_memory(), etc.

    @staticmethod
    def _infer_capabilities_from_hardware(
        has_gpu: bool,
        memory_gb: int = 0,
        gpu_name: str = "",
    ) -> list[str]:
        """Infer capabilities from hardware info.

        December 30, 2025: Fallback for nodes reporting empty capabilities but
        having detectable hardware. Used to populate capabilities for peers
        that may have misconfigured coordinator settings.

        Args:
            has_gpu: Whether the node has a GPU
            memory_gb: RAM in gigabytes
            gpu_name: GPU name for logging

        Returns:
            List of inferred capabilities
        """
        capabilities = ["selfplay"]  # All nodes can at least do CPU selfplay
        if has_gpu:
            capabilities.extend(["training", "cmaes", "gauntlet", "tournament"])
        if memory_gb >= 64:
            capabilities.append("large_boards")
        return capabilities

    def _register_self_in_peers(self) -> None:
        """Register this node in the peers dict.

        Jan 5, 2026: Ensures the leader (and any node) is visible in self.peers
        for components that iterate over peers directly. This fixes an issue
        where the leader was not in its own peers dict after becoming leader.

        This is idempotent - calling multiple times is safe.
        """
        self._update_self_info()  # Ensure self_info is current

        with self.peers_lock:
            was_present = self.node_id in self.peers
            self.peers[self.node_id] = self.self_info
            if not was_present:
                logger.info(f"[SelfReg] Registered self in peers: {self.node_id}")

        # Jan 12, 2026: Sync to lock-free snapshot
        self._sync_peer_snapshot()

        # Emit HOST_ONLINE if this is first registration (consistency with peer discovery)
        if not was_present:
            try:
                asyncio.create_task(self._emit_host_online_for_self())
            except RuntimeError:
                # No event loop running, use sync path if available
                pass

    async def _emit_host_online_for_self(self) -> None:
        """Emit HOST_ONLINE event for self-registration."""
        try:
            from app.distributed.data_events import DataEventType
            from app.coordination.event_router import emit_event

            emit_event(DataEventType.HOST_ONLINE.value, {
                "node_id": self.node_id,
                "host": self.self_info.host,
                "port": self.self_info.port,
                "has_gpu": self.self_info.has_gpu,
                "gpu_name": self.self_info.gpu_name,
                "capabilities": list(self.self_info.capabilities) if self.self_info.capabilities else [],
                "source": "leader_self_registration",
            })
            logger.debug(f"[SelfReg] Emitted HOST_ONLINE for self: {self.node_id}")
        except ImportError:
            pass  # Event system not available
        except Exception as e:
            logger.debug(f"[SelfReg] Failed to emit HOST_ONLINE: {e}")

    # =========================================================================
    # H2 fix: Lifecycle event emission methods (Jan 12, 2026)
    # These methods emit HOST_ONLINE, HOST_OFFLINE, P2P_NODE_DEAD, and
    # CLUSTER_CAPACITY_CHANGED events for cluster coordination.
    # =========================================================================

    async def _emit_host_online(self, node_id: str, capabilities: list[str] | None = None) -> None:
        """Emit HOST_ONLINE event for a peer coming online."""
        try:
            from app.distributed.data_events import DataEventType
            from app.coordination.event_router import emit_event

            # Jan 22, 2026: Use lock-free snapshot to prevent race conditions
            peer_info = self._peer_snapshot.get_snapshot().get(node_id)
            emit_event(DataEventType.HOST_ONLINE.value, {
                "node_id": node_id,
                "host": getattr(peer_info, "host", "") if peer_info else "",
                "port": getattr(peer_info, "port", 0) if peer_info else 0,
                "has_gpu": getattr(peer_info, "has_gpu", False) if peer_info else False,
                "gpu_name": getattr(peer_info, "gpu_name", "") if peer_info else "",
                "capabilities": capabilities or [],
                "source": "peer_discovery",
            })
            logger.debug(f"[P2P] Emitted HOST_ONLINE for peer: {node_id}")
        except ImportError:
            pass
        except Exception as e:
            logger.debug(f"[P2P] Failed to emit HOST_ONLINE for {node_id}: {e}")

    def _emit_host_online_sync(self, node_id: str, capabilities: list[str] | None = None) -> None:
        """Sync version of _emit_host_online for non-async contexts."""
        try:
            from app.distributed.data_events import DataEventType
            from app.coordination.event_router import emit_event

            # Jan 22, 2026: Use lock-free snapshot to prevent race conditions
            peer_info = self._peer_snapshot.get_snapshot().get(node_id)
            emit_event(DataEventType.HOST_ONLINE.value, {
                "node_id": node_id,
                "host": getattr(peer_info, "host", "") if peer_info else "",
                "port": getattr(peer_info, "port", 0) if peer_info else 0,
                "has_gpu": getattr(peer_info, "has_gpu", False) if peer_info else False,
                "gpu_name": getattr(peer_info, "gpu_name", "") if peer_info else "",
                "capabilities": capabilities or [],
                "source": "peer_recovery_sync",
            })
            logger.debug(f"[P2P] Emitted HOST_ONLINE (sync) for peer: {node_id}")
        except ImportError:
            pass
        except Exception as e:
            logger.debug(f"[P2P] Failed to emit HOST_ONLINE (sync) for {node_id}: {e}")

    async def _emit_host_offline(self, node_id: str, reason: str, last_heartbeat: float | None) -> None:
        """Emit HOST_OFFLINE event for a peer going offline."""
        try:
            from app.distributed.data_events import DataEventType
            from app.coordination.event_router import emit_event

            emit_event(DataEventType.HOST_OFFLINE.value, {
                "node_id": node_id,
                "reason": reason,
                "last_heartbeat": last_heartbeat,
                "source": "peer_retirement",
            })
            logger.debug(f"[P2P] Emitted HOST_OFFLINE for peer: {node_id} (reason={reason})")
        except ImportError:
            pass
        except Exception as e:
            logger.debug(f"[P2P] Failed to emit HOST_OFFLINE for {node_id}: {e}")

    # Jan 27, 2026: Phase 17 - Removed _emit_host_offline_sync (never called)

    async def _emit_node_dead(self, node_id: str, reason: str, last_heartbeat: float | None, dead_for: float) -> None:
        """Emit P2P_NODE_DEAD event for a dead peer."""
        try:
            from app.distributed.data_events import DataEventType
            from app.coordination.event_router import emit_event

            emit_event(DataEventType.P2P_NODE_DEAD.value, {
                "node_id": node_id,
                "reason": reason,
                "last_heartbeat": last_heartbeat,
                "dead_for_seconds": dead_for,
                "source": "peer_timeout",
            })
            logger.debug(f"[P2P] Emitted P2P_NODE_DEAD for peer: {node_id} (dead_for={dead_for:.0f}s)")
        except ImportError:
            pass
        except Exception as e:
            logger.debug(f"[P2P] Failed to emit P2P_NODE_DEAD for {node_id}: {e}")

    # Jan 27, 2026: Phase 17 - Removed _emit_node_dead_sync (never called)

    async def _emit_cluster_capacity_changed(
        self,
        total_nodes: int,
        alive_nodes: int,
        gpu_nodes: int,
        training_nodes: int,
        change_type: str,
        change_details: dict | None = None,
    ) -> None:
        """Emit CLUSTER_CAPACITY_CHANGED event when cluster capacity changes."""
        try:
            from app.distributed.data_events import DataEventType
            from app.coordination.event_router import emit_event

            emit_event(DataEventType.CLUSTER_CAPACITY_CHANGED.value, {
                "total_nodes": total_nodes,
                "alive_nodes": alive_nodes,
                "gpu_nodes": gpu_nodes,
                "training_nodes": training_nodes,
                "change_type": change_type,
                "change_details": change_details or {},
                "source": "peer_management",
            })
            logger.debug(f"[P2P] Emitted CLUSTER_CAPACITY_CHANGED: {change_type}, alive={alive_nodes}")
        except ImportError:
            pass
        except Exception as e:
            logger.debug(f"[P2P] Failed to emit CLUSTER_CAPACITY_CHANGED: {e}")

    # Jan 27, 2026: Phase 17 - Removed _emit_cluster_capacity_changed_sync (never called)

    def _safe_emit_p2p_event(self, event_type: Any, payload: dict) -> None:
        """Safely emit a P2P-related event via the event router.

        This is a generic event emitter for P2P loops (QuorumCrisisDiscoveryLoop,
        GossipStateCleanupLoop, etc.) that need to emit events without knowing
        the specific event type at compile time.

        January 12, 2026: Added to fix AttributeError in P2P loops that referenced
        this method but it didn't exist. The loops pass emit_event=self._safe_emit_p2p_event
        but this method was never implemented.

        Args:
            event_type: Event type (string or DataEventType enum)
            payload: Event payload dictionary
        """
        try:
            from app.distributed.data_events import DataEventType
            from app.coordination.event_router import emit_event

            # Handle both string and enum event types
            event_value = None
            if isinstance(event_type, str):
                # Try to convert string to DataEventType
                try:
                    event_value = DataEventType(event_type).value
                except ValueError:
                    # Unknown event type - log and skip
                    logger.debug(f"[P2P] Unknown event type: {event_type}, skipping emission")
                    return
            elif hasattr(event_type, "value"):
                # It's an enum, get its value
                event_value = event_type.value
            else:
                # Pass through as-is
                event_value = str(event_type)

            emit_event(event_value, payload)
            logger.debug(f"[P2P] Emitted event: {event_value}")
        except ImportError:
            pass  # Event router not available
        except Exception as e:
            logger.debug(f"[P2P] Failed to emit event {event_type}: {e}")

    def _sync_peer_snapshot(self) -> None:
        """Synchronize PeerSnapshot with current peers dictionary.

        January 12, 2026: Added for lock-free reads in handle_status.
        Call this after any operation that modifies self.peers.

        This uses bulk_update for efficiency when there are many peers.
        The PeerSnapshot will be atomically updated with the current state.
        """
        try:
            # Use bulk update for efficiency - single lock acquisition, single snapshot refresh
            with self._peer_snapshot.bulk_update():
                # Clear and repopulate (handles removes and updates)
                self._peer_snapshot.clear()
                for node_id, info in self.peers.items():
                    self._peer_snapshot.update_peer(node_id, info)
        except Exception as e:  # noqa: BLE001
            # Log but don't fail - reads will use stale snapshot
            logger.warning(f"[PeerSnapshot] Sync failed: {e}")

    # _is_tailscale_host provided by NetworkUtilsMixin

    def _local_has_tailscale(self) -> bool:
        """Best-effort: True when this node appears to have a Tailscale address."""
        try:
            info = getattr(self, "self_info", None)
            if not info:
                return False
            host = str(getattr(info, "host", "") or "").strip()
            reported_host = str(getattr(info, "reported_host", "") or "").strip()
            return self._is_tailscale_host(host) or self._is_tailscale_host(reported_host)
        except (AttributeError):
            return False

    # _get_tailscale_ip_for_peer: Provided by NetworkUtilsMixin

    def _detect_network_partition(self) -> bool:
        """Detect if we're in a network partition (>50% peers unreachable via primary IP).

        Used to trigger Tailscale-first connectivity mode when the public network
        is fragmented but mesh connectivity remains intact.

        Returns:
            True if partition detected (majority of peers unreachable)
        """
        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        peers_snapshot = [p for p in self._peer_snapshot.get_snapshot().values() if p.node_id != self.node_id]

        if len(peers_snapshot) < 2:
            return False

        # Count peers with recent heartbeat failures
        # Jan 19, 2026: Use jittered timeout to prevent synchronized partition detection
        # Jan 22, 2026: Use cached jittered timeout to ensure consistent death detection
        now = time.time()
        jittered_timeout = self._get_cached_jittered_timeout()
        unreachable = 0
        for peer in peers_snapshot:
            if peer.consecutive_failures >= 3 or (now - peer.last_heartbeat > jittered_timeout):
                unreachable += 1

        partition_ratio = unreachable / len(peers_snapshot)
        if partition_ratio > 0.5:
            logger.info(f"Network partition detected: {unreachable}/{len(peers_snapshot)} peers unreachable ({partition_ratio:.0%})")
            return True
        return False

    def _get_tailscale_priority_mode(self) -> bool:
        """Check if Tailscale-first mode is enabled (partition recovery)."""
        return getattr(self, "_tailscale_priority", False)

    def _enable_tailscale_priority(self) -> None:
        """Enable Tailscale-first mode for heartbeats during partition recovery."""
        if not getattr(self, "_tailscale_priority", False):
            logger.info("Enabling Tailscale-priority mode for partition recovery")
            self._tailscale_priority = True
            self._tailscale_priority_until = time.time() + 300  # 5 minutes

    def _disable_tailscale_priority(self) -> None:
        """Disable Tailscale-first mode when connectivity recovers."""
        if getattr(self, "_tailscale_priority", False):
            logger.info("Disabling Tailscale-priority mode (connectivity recovered)")
            self._tailscale_priority = False

    # =========================================================================
    # Network Health Methods (December 30, 2025)
    # Required by NetworkHealthMixin for cross-verification of P2P vs Tailscale
    # =========================================================================

    async def _get_tailscale_status(self) -> dict[str, bool]:
        """Query Tailscale status and return peer online status.

        Returns:
            Dict mapping Tailscale IP to online status {ip: is_online}
        """
        try:
            proc = await asyncio.create_subprocess_exec(
                "tailscale",
                "status",
                "--json",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
            stdout, stderr = await asyncio.wait_for(
                proc.communicate(),
                timeout=10.0,
            )

            if proc.returncode != 0:
                logger.debug(f"Tailscale status failed: {stderr.decode()[:100]}")
                return {}

            data = json.loads(stdout.decode())

            # Extract peer IPs and online status
            result: dict[str, bool] = {}
            for peer_key, peer_data in data.get("Peer", {}).items():
                is_online = peer_data.get("Online", False)
                # Extract Tailscale IPs
                for ip in peer_data.get("TailscaleIPs", []):
                    result[ip] = is_online

            return result

        except asyncio.TimeoutError:
            logger.debug("Tailscale status timed out")
            return {}
        except json.JSONDecodeError as e:
            logger.debug(f"Failed to parse Tailscale status JSON: {e}")
            return {}
        except FileNotFoundError:
            logger.debug("Tailscale command not found")
            return {}
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Error querying Tailscale status: {e}")
            return {}

    async def _reconnect_discovered_peer(
        self, node_id: str, host: str, port: int
    ) -> bool:
        """Attempt to reconnect to a peer discovered via Tailscale.

        Probes the peer's health endpoint and sends a heartbeat to establish
        P2P connection.

        Args:
            node_id: Peer node identifier
            host: Tailscale IP address
            port: P2P port (usually 8770)

        Returns:
            True if reconnection successful, False otherwise
        """
        try:
            # Probe health endpoint
            url = f"http://{host}:{port}/health"
            timeout = ClientTimeout(total=5)
            async with get_client_session(timeout) as session:
                async with session.get(url) as resp:
                    if resp.status != 200:
                        return False
                    data, error = await safe_json_response(resp, default={}, log_errors=False)
                    if error:
                        return False

            # Extract node_id from response if available
            actual_node_id = data.get("node_id", node_id)

            # Send heartbeat to establish connection
            await self._send_heartbeat_to_peer(host, port)

            # Check if peer is now in our peers dict
            async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                if actual_node_id not in self.peers or not self.peers[actual_node_id].is_alive():
                    # Register the peer
                    self.peers[actual_node_id] = PeerInfo(
                        node_id=actual_node_id,
                        host=host,
                        port=port,
                        last_heartbeat=time.time(),
                        state="alive",
                    )
                    # C2 fix: Sync peer snapshot after adding new peer
                    self._sync_peer_snapshot()
                    logger.info(f"Reconnected peer via network health: {actual_node_id} ({host}:{port})")
                    await self._emit_host_online(actual_node_id)
                    return True

            return True  # Already connected

        except Exception as e:  # noqa: BLE001
            logger.debug(f"Failed to reconnect {node_id}: {e}")
            return False

    async def reconnect_missing_peers(self) -> list[str]:
        """Reconnect to all peers that are online in Tailscale but not in P2P.

        Returns:
            List of node IDs that were successfully reconnected
        """
        ts_peers = await self._get_tailscale_status()
        config_hosts = self._load_distributed_hosts().get("hosts", {})

        # Build IP to node mapping
        ip_to_node: dict[str, tuple[str, dict]] = {}
        for name, h in config_hosts.items():
            ts_ip = h.get("tailscale_ip")
            if ts_ip and h.get("p2p_enabled", True):
                ip_to_node[ts_ip] = (name, h)

        # Get current alive peer IDs
        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        current_ids: set[str] = set()
        for peer in self._peer_snapshot.get_snapshot().values():
            if peer.is_alive():
                current_ids.add(peer.node_id)

        # Find and reconnect missing peers
        reconnected: list[str] = []
        for ts_ip, is_online in ts_peers.items():
            if not is_online:
                continue

            if ts_ip not in ip_to_node:
                continue

            node_id, node_config = ip_to_node[ts_ip]

            # Skip if already connected
            if node_id in current_ids:
                continue

            # Skip self
            if node_id == self.node_id:
                continue

            # Attempt reconnection
            port = node_config.get("p2p_port", DEFAULT_PORT)
            if await self._reconnect_discovered_peer(node_id, ts_ip, port):
                reconnected.append(node_id)

        if reconnected:
            logger.info(f"Reconnected {len(reconnected)} missing peers: {reconnected}")

        return reconnected

    # =========================================================================
    # Partition Read-Only Mode (Phase 2.4 - Dec 29, 2025)
    # =========================================================================

    def _check_partition_mode(self) -> None:
        """Check partition status and enable/disable read-only mode.

        December 2025 (Phase 2.4): Prevent data divergence during network partitions.

        When this node is in a minority partition (<50% of peers alive):
        - Pause training job dispatch
        - Pause selfplay job dispatch
        - Continue serving existing data (read-only)
        - Allow sync operations to help recovery

        This prevents split-brain scenarios where both partitions continue
        generating training data that later conflicts during merge.
        """
        now = time.time()

        # Rate limit partition checks
        if now - self._last_partition_check < self._partition_check_interval:
            return
        self._last_partition_check = now

        # Use gossip protocol's partition detection
        status, ratio = self.detect_partition_status()

        if status in ("minority", "isolated"):
            if not self._partition_readonly_mode:
                logger.warning(
                    f"[P2P] Entering partition read-only mode: "
                    f"status={status}, health_ratio={ratio:.2%}"
                )
                self._partition_readonly_mode = True
                self._partition_readonly_since = now

                # Emit event for monitoring
                self._safe_emit_event("PARTITION_READONLY_ENTERED", {
                    "node_id": self.node_id,
                    "status": status,
                    "health_ratio": ratio,
                    "timestamp": now,
                })
        else:
            if self._partition_readonly_mode:
                readonly_duration = now - self._partition_readonly_since
                logger.info(
                    f"[P2P] Exiting partition read-only mode: "
                    f"status={status}, health_ratio={ratio:.2%}, "
                    f"was_readonly_for={readonly_duration:.0f}s"
                )
                self._partition_readonly_mode = False
                self._partition_readonly_since = 0.0

                # Emit event for monitoring
                self._safe_emit_event("PARTITION_READONLY_EXITED", {
                    "node_id": self.node_id,
                    "status": status,
                    "health_ratio": ratio,
                    "readonly_duration_seconds": readonly_duration,
                    "timestamp": now,
                })

    def is_partition_readonly(self) -> bool:
        """Check if this node is in partition read-only mode.

        December 2025 (Phase 2.4): Query method for dispatch gates.

        Returns:
            True if job dispatch should be paused due to partition status.
        """
        # Do a fresh check if it's been a while
        self._check_partition_mode()
        return self._partition_readonly_mode

    def get_partition_status(self) -> dict[str, Any]:
        """Get current partition status details.

        December 2025 (Phase 2.4): Status API for monitoring/debugging.

        Returns:
            Dict with partition status, mode, and duration.
        """
        status, ratio = self.detect_partition_status()
        now = time.time()

        result = {
            "partition_status": status,
            "health_ratio": round(ratio, 3),
            "readonly_mode": self._partition_readonly_mode,
            "readonly_since": self._partition_readonly_since,
            "readonly_duration_seconds": (
                now - self._partition_readonly_since
                if self._partition_readonly_mode else 0.0
            ),
            "last_check": self._last_partition_check,
        }

        # Add detailed peer info if available
        if hasattr(self, "get_partition_details"):
            result["details"] = self.get_partition_details()

        return result

    # _tailscale_urls_for_voter: Provided by NetworkUtilsMixin
    # Dec 2025: Resource/diversity methods delegated to mixins and selfplay_scheduler (~236 LOC)
    # NOTE: _get_db_game_count_sync() inlined at call site (Jan 2026 Phase 2)

    def _seed_selfplay_scheduler_game_counts_sync(self) -> dict[str, int]:
        """Seed game counts from canonical databases synchronously.

        IMPORTANT: This is a blocking operation. Call via asyncio.to_thread() from async code.
        Added Jan 2026 (Session 17.29) to fix bootstrap priority for underserved configs.

        Returns:
            Dict mapping config_key -> game_count from canonical databases
        """
        game_counts: dict[str, int] = {}
        # Jan 7, 2026: Use _get_ai_service_path() to avoid doubled ai-service/ path
        canonical_dir = Path(self._get_ai_service_path()) / "data" / "games"

        # Pattern: canonical_<board_type>_<num_players>p.db
        for db_path in canonical_dir.glob("canonical_*_*p.db"):
            try:
                # Extract config_key from filename: canonical_hex8_2p.db -> hex8_2p
                stem = db_path.stem  # canonical_hex8_2p
                if stem.startswith("canonical_"):
                    config_key = stem[len("canonical_"):]  # hex8_2p
                    # Inline: was _get_db_game_count_sync()
                    game_count = self.data_pipeline_manager.get_db_game_count_sync(db_path)
                    if game_count > 0:
                        game_counts[config_key] = game_count
            except (ValueError, AttributeError):
                continue

        return game_counts

    async def _fetch_game_counts_from_peers(self) -> dict[str, int]:
        """Fetch game counts from coordinator or other peers with canonical databases.

        Session 17.41: Cluster nodes don't have canonical databases, so they need to
        fetch game counts from the coordinator which has them. This enables the
        starvation multipliers to work correctly on all nodes.

        Returns:
            Dict mapping config_key -> game_count from peers
        """
        # Try coordinator nodes first (they have canonical databases)
        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        peers_snapshot = self._peer_snapshot.get_snapshot()
        coordinator_candidates = []
        for peer_id, peer in peers_snapshot.items():
            # Coordinator nodes or nodes with role=coordinator
            role_str = getattr(peer.role, "value", str(peer.role)) if peer.role else ""
            if "coordinator" in role_str.lower() or "mac-studio" in peer_id.lower():
                coordinator_candidates.append(peer)

        # Fallback to any alive peer
        if not coordinator_candidates:
            coordinator_candidates = [p for p in peers_snapshot.values() if p.is_alive()]

        for peer in coordinator_candidates[:3]:  # Try up to 3 candidates
            try:
                # Get best endpoint for peer
                key = self._endpoint_key(peer)
                if not key:
                    continue
                scheme, host, port = key
                url = f"{scheme}://{host}:{port}/game_counts"

                async with aiohttp.ClientSession() as session:
                    async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            game_counts = data.get("game_counts", {})
                            if game_counts:
                                source_node = data.get("node_id", peer.node_id)
                                logger.info(f"[P2P] Fetched {len(game_counts)} game counts from {source_node}")
                                return game_counts
            except (aiohttp.ClientError, asyncio.TimeoutError, json.JSONDecodeError) as e:
                logger.debug(f"[P2P] Failed to fetch game counts from {peer.node_id}: {e}")
                continue

        # Session 17.48: Fallback to known coordinator IPs from config if peer discovery failed
        # This handles the case where P2P network hasn't converged yet (no heartbeats from coordinator)
        fallback_coordinator_ips = [
            "100.69.164.58",  # macbook-pro-2-1 Tailscale IP (has canonical DBs)
        ]
        for ip in fallback_coordinator_ips:
            try:
                url = f"http://{ip}:8770/game_counts"
                async with aiohttp.ClientSession() as session:
                    async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            game_counts = data.get("game_counts", {})
                            if game_counts:
                                source_node = data.get("node_id", "unknown")
                                logger.info(f"[P2P] Fetched {len(game_counts)} game counts from fallback {source_node}")
                                return game_counts
            except (aiohttp.ClientError, asyncio.TimeoutError, json.JSONDecodeError) as e:
                logger.debug(f"[P2P] Fallback fetch from {ip} failed: {e}")
                continue

        return {}

    async def _async_seed_game_counts_from_peers_if_needed(self) -> None:
        """Async fallback to seed game counts from peers if local seeding failed.

        Jan 9, 2026: Cluster nodes don't have local canonical databases, so
        the synchronous seeding during __init__ returns empty. This method
        fetches game counts from the coordinator/peers during async startup,
        enabling proper underserved config prioritization on worker nodes.

        Without this, all configs appear to have 0 games and get the same
        maximum bootstrap boost (+100), which neutralizes the prioritization.
        """
        try:
            # Check if game counts were already seeded during __init__
            if self.selfplay_scheduler:
                existing_counts = self.selfplay_scheduler._get_game_counts_per_config()
                if existing_counts and len(existing_counts) >= 6:
                    # Already have game counts from local canonical DBs
                    logger.debug(
                        f"[P2P] Game counts already seeded ({len(existing_counts)} configs), "
                        "skipping peer fetch"
                    )
                    return

            # Fetch from peers/coordinator
            logger.info("[P2P] Local canonical DBs empty, fetching game counts from peers...")
            peer_counts = await self._fetch_game_counts_from_peers()

            if peer_counts and self.selfplay_scheduler:
                self.selfplay_scheduler.update_p2p_game_counts(peer_counts)
                logger.info(
                    f"[P2P] Seeded SelfplayScheduler with {len(peer_counts)} config game counts from peers"
                )
                # Log underserved configs for visibility
                for config_key, count in sorted(peer_counts.items(), key=lambda x: x[1]):
                    if count < 5000:
                        logger.info(f"[P2P] Underserved config (from peers): {config_key} = {count} games")
            else:
                logger.warning(
                    "[P2P] Could not fetch game counts from peers - "
                    "bootstrap prioritization may not work correctly"
                )

        except Exception as e:  # noqa: BLE001
            logger.warning(f"[P2P] Async game count seeding failed: {e}")

    async def _game_count_refresh_loop(self) -> None:
        """Periodically refresh game counts from coordinator.

        Jan 9, 2026: Cluster nodes need to periodically refresh game counts
        as games are generated and consolidated. This ensures the scheduler
        always has accurate game counts for prioritization decisions.

        Interval: 5 minutes (300 seconds)
        """
        REFRESH_INTERVAL = 300  # 5 minutes
        await asyncio.sleep(60)  # Initial delay to let cluster stabilize

        while True:
            try:
                # Skip if this node has local canonical DBs (coordinator)
                local_counts = await asyncio.to_thread(self._seed_selfplay_scheduler_game_counts_sync)
                if local_counts and len(local_counts) >= 6:
                    # Has local DBs - update from local
                    if self.selfplay_scheduler:
                        self.selfplay_scheduler.update_p2p_game_counts(local_counts)
                        logger.debug(f"[P2P] Refreshed game counts from local DBs ({len(local_counts)} configs)")
                else:
                    # Fetch from peers
                    peer_counts = await self._fetch_game_counts_from_peers()
                    if peer_counts and self.selfplay_scheduler:
                        self.selfplay_scheduler.update_p2p_game_counts(peer_counts)
                        logger.debug(f"[P2P] Refreshed game counts from peers ({len(peer_counts)} configs)")

            except Exception as e:  # noqa: BLE001
                logger.debug(f"[P2P] Game count refresh failed: {e}")

            await asyncio.sleep(REFRESH_INTERVAL)

    # Jan 27, 2026: Phase 17B - Removed _find_dbs_to_merge_sync (unused delegation wrapper)

    def _run_subprocess_sync(self, cmd: list, timeout: int = 10) -> tuple[int, str, str]:
        """Run subprocess synchronously.

        IMPORTANT: This is a blocking operation. Call via asyncio.to_thread() from async code.
        Added Dec 2025 to fix P2P orchestrator CPU spikes from blocking subprocess in async loops.

        Returns: (return_code, stdout, stderr)
        """
        import subprocess
        try:
            result = subprocess.run(cmd, timeout=timeout, capture_output=True, text=True)
            return (result.returncode, result.stdout or "", result.stderr or "")
        except subprocess.TimeoutExpired:
            return (-1, "", "timeout")
        except (OSError, subprocess.SubprocessError) as e:
            return (-1, "", str(e))

    async def _run_subprocess_async(self, cmd: list, timeout: int = 10) -> tuple[int, str, str]:
        """Run subprocess asynchronously via thread pool.

        Jan 2026: Added for Phase 1 multi-core parallelization.
        Uses asyncio.to_thread() to avoid blocking the event loop.

        Returns: (return_code, stdout, stderr)
        """
        return await asyncio.to_thread(self._run_subprocess_sync, cmd, timeout)

    def _count_local_jobs(self) -> tuple[int, int]:
        """Count running selfplay and training jobs on this node."""
        def _pid_alive(pid: int) -> bool:
            try:
                os.kill(pid, 0)
                return True
            except ProcessLookupError:
                return False
            except PermissionError:
                return True
            except (AttributeError):
                return False

        # Primary source of truth: jobs we started and are tracking.
        selfplay_pids: set[str] = set()
        training_pids: set[str] = set()

        stale_job_ids: list[str] = []
        try:
            with self.jobs_lock:
                jobs_snapshot = list(self.local_jobs.items())
            for job_id, job in jobs_snapshot:
                if job.status != "running":
                    continue
                pid = int(job.pid or 0)
                if pid <= 0:
                    continue
                if not _pid_alive(pid):
                    stale_job_ids.append(job_id)
                    continue
                if job.job_type in (JobType.SELFPLAY, JobType.GPU_SELFPLAY, JobType.HYBRID_SELFPLAY, JobType.CPU_SELFPLAY, JobType.GUMBEL_SELFPLAY):
                    selfplay_pids.add(str(pid))
                elif job.job_type == JobType.TRAINING:
                    training_pids.add(str(pid))

            if stale_job_ids:
                with self.jobs_lock:
                    for job_id in stale_job_ids:
                        self.local_jobs.pop(job_id, None)
        except (ValueError, AttributeError):
            pass

        # Secondary check: best-effort process scan for untracked jobs (e.g. manual runs).
        # IMPORTANT: never return (0,0) just because `pgrep` is missing or fails;
        # that can cause the leader to spawn runaway selfplay processes until disk fills.
        try:
            import shutil

            if shutil.which("pgrep"):
                # Jan 12, 2026: Helper to filter out non-Python processes
                # SSH processes and shell wrappers (zsh, bash) with "selfplay" in their args
                # were being counted as local jobs - only count actual Python processes
                def _get_excluded_pids() -> set[str]:
                    """Get PIDs of SSH and shell processes (to exclude from local job counts)."""
                    excluded_pids: set[str] = set()
                    # Exclude SSH processes (dispatchers to remote nodes)
                    for pattern in ("^ssh", "ssh "):
                        try:
                            out = subprocess.run(
                                ["pgrep", "-f", pattern],
                                capture_output=True,
                                text=True,
                                timeout=5,
                            )
                            if out.returncode == 0 and out.stdout.strip():
                                excluded_pids.update(out.stdout.strip().split())
                        except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError):
                            pass
                    # Exclude shell processes (Claude wrappers that contain "selfplay" in args)
                    for shell_pattern in ("/bin/zsh", "/bin/bash", "/bin/sh"):
                        try:
                            out = subprocess.run(
                                ["pgrep", "-f", shell_pattern],
                                capture_output=True,
                                text=True,
                                timeout=5,
                            )
                            if out.returncode == 0 and out.stdout.strip():
                                excluded_pids.update(out.stdout.strip().split())
                        except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError):
                            pass
                    return excluded_pids

                excluded_pids = _get_excluded_pids()

                # December 2025: Added selfplay.py pattern - the current unified selfplay entry point
                # December 2025: Added gumbel_selfplay and SelfplayRunner patterns for module invocations
                for pattern in (
                    "selfplay.py",
                    "run_self_play_soak.py",
                    "run_gpu_selfplay.py",
                    "run_hybrid_selfplay.py",
                    "gumbel_selfplay",  # screen session name
                    "SelfplayRunner",   # class-based invocation
                    "selfplay_runner",  # module invocation
                    "-m app.training.selfplay",  # module mode
                ):
                    out = subprocess.run(
                        ["pgrep", "-f", pattern],
                        capture_output=True,
                        text=True,
                        timeout=5,
                    )
                    if out.returncode == 0 and out.stdout.strip():
                        # Jan 12, 2026: Filter out excluded PIDs (SSH, shells) - not local jobs
                        pids = [p for p in out.stdout.strip().split() if p and p not in excluded_pids]
                        selfplay_pids.update(pids)

                for pattern in ("train_", "train.py", "-m app.training.train"):
                    out = subprocess.run(
                        ["pgrep", "-f", pattern],
                        capture_output=True,
                        text=True,
                        timeout=5,
                    )
                    if out.returncode == 0 and out.stdout.strip():
                        # Jan 12, 2026: Filter out excluded PIDs (SSH, shells)
                        pids = [p for p in out.stdout.strip().split() if p and p not in excluded_pids]
                        training_pids.update(pids)
        except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError, KeyError, IndexError, AttributeError, ImportError):
            pass

        return len(selfplay_pids), len(training_pids)

    def _get_max_selfplay_slots_for_node(self) -> int:
        """Get maximum selfplay slots based on GPU capability.

        Jan 2, 2026: Added for slot-based capacity management.
        This allows work queue claiming to coexist with legacy selfplay processes.

        The slot count is based on GPU type since different GPUs can handle
        different numbers of concurrent selfplay processes effectively.

        Returns:
            Maximum number of selfplay slots for this node.
        """
        import os

        # Check environment variable first (allows manual override)
        env_slots = os.environ.get("RINGRIFT_MAX_SELFPLAY_SLOTS")
        if env_slots:
            try:
                return int(env_slots)
            except ValueError:
                pass

        # Compute based on GPU name
        gpu_name = getattr(self.self_info, "gpu_name", "") or ""
        gpu_name_lower = gpu_name.lower()

        # High-end GPUs get more slots
        if "gh200" in gpu_name_lower or "h100" in gpu_name_lower:
            return 16
        elif "a100" in gpu_name_lower:
            return 12
        elif "5090" in gpu_name_lower or "4090" in gpu_name_lower:
            return 8
        elif "3090" in gpu_name_lower or "a40" in gpu_name_lower or "l40" in gpu_name_lower:
            return 6
        elif "4060" in gpu_name_lower or "3060" in gpu_name_lower:
            return 3
        elif self.self_info.has_gpu:
            return 4  # Default for other GPUs
        else:
            return 2  # CPU-only nodes

    def _cleanup_stale_processes(self) -> int:
        """Kill processes that have been running too long.

        Scans for known process patterns (tournaments, gauntlets, selfplay)
        and kills any that exceed their maximum runtime threshold.

        Returns:
            Number of processes killed.
        """
        import shutil

        if not shutil.which("pgrep") or not shutil.which("ps"):
            return 0

        killed_count = 0
        time.time()

        # Map patterns to their max runtimes
        # December 2025: Added selfplay.py - the current unified selfplay entry point
        pattern_max_runtime = {
            "run_model_elo_tournament.py": MAX_TOURNAMENT_RUNTIME,
            "run_gauntlet.py": MAX_GAUNTLET_RUNTIME,
            "selfplay.py": MAX_SELFPLAY_RUNTIME,  # Unified selfplay script
            "run_self_play_soak.py": MAX_SELFPLAY_RUNTIME,
            "run_gpu_selfplay.py": MAX_SELFPLAY_RUNTIME,
            "run_hybrid_selfplay.py": MAX_SELFPLAY_RUNTIME,
            "train_nnue.py": MAX_TRAINING_RUNTIME,
            "train.py": MAX_TRAINING_RUNTIME,
        }

        for pattern, max_runtime in pattern_max_runtime.items():
            try:
                # Get PIDs matching the pattern
                pgrep_result = subprocess.run(
                    ["pgrep", "-f", pattern],
                    capture_output=True,
                    text=True,
                    timeout=5,
                )
                if pgrep_result.returncode != 0 or not pgrep_result.stdout.strip():
                    continue

                pids = [p.strip() for p in pgrep_result.stdout.strip().split() if p.strip()]

                for pid in pids:
                    try:
                        # Get process start time using ps
                        ps_result = subprocess.run(
                            ["ps", "-o", "etimes=", "-p", pid],
                            capture_output=True,
                            text=True,
                            timeout=5,
                        )
                        if ps_result.returncode != 0:
                            continue

                        elapsed_seconds = int(ps_result.stdout.strip())

                        if elapsed_seconds > max_runtime:
                            # Process has exceeded max runtime - kill it
                            logger.warning(
                                f"Killing stale process {pid} ({pattern}): "
                                f"running for {elapsed_seconds/3600:.1f}h, max={max_runtime/3600:.1f}h"
                            )
                            subprocess.run(
                                ["kill", "-9", pid],
                                capture_output=True,
                                timeout=5,
                            )
                            killed_count += 1

                            # Send alert
                            if hasattr(self, 'notifier') and self.notifier:
                                asyncio.create_task(
                                    self.notifier.send(
                                        title="Stale Process Killed",
                                        message=f"Killed {pattern} (PID {pid}) after {elapsed_seconds/3600:.1f} hours",
                                        level="warning",
                                        node_id=self.node_id,
                                    )
                                )

                    except (ValueError, subprocess.TimeoutExpired):
                        continue

            except Exception as e:  # noqa: BLE001
                logger.debug(f"Error checking pattern {pattern}: {e}")
                continue

        if killed_count > 0:
            logger.info(f"Stale process cleanup: killed {killed_count} processes")

        return killed_count

    # ============================================
    # Phase 2: Distributed Data Sync Methods
    # ============================================

    def _collect_local_data_manifest(self) -> NodeDataManifest:
        """Collect manifest of all data files on this node.

        REFACTORED (Dec 2025): Delegates to SyncPlanner.collect_local_manifest().
        See scripts/p2p/managers/sync_planner.py for implementation.

        Scans the data directory for:
        - selfplay/ - Game replay files (.jsonl, .db)
        - models/ - Trained model files (.pt, .onnx)
        - training/ - Training data files (.npz)
        - games/ - Synced game databases (.db)

        Uses get_data_directory() to support both disk and ramdrive storage.
        """
        # Phase 2A: Delegate to SyncPlanner (Dec 2025)
        # This eliminates ~150 lines of duplicate code
        # Jan 23, 2026: Changed use_cache=False to True to reduce event loop blocking
        # The uncached version does heavy filesystem I/O (glob, stat, SQLite COUNT)
        # which can take 5-8 seconds and block the event loop, causing leader election failures
        return self.sync_planner.collect_local_manifest(use_cache=True)

    # Dec 2025: Legacy manifest methods removed (162 LOC) - using SyncPlanner

    def _request_peer_manifest_sync(self, peer_id: str) -> NodeDataManifest | None:
        """Synchronous wrapper for requesting peer manifest.

        Used by SyncPlanner which expects a sync callback.
        Runs the async version in a new event loop.

        Args:
            peer_id: The peer's node ID to request from

        Returns:
            NodeDataManifest or None if request failed
        """
        # Look up peer info
        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        peer_info = self._peer_snapshot.get_snapshot().get(peer_id)

        if not peer_info:
            logger.debug(f"Peer {peer_id} not found in peers dict")
            return None

        # Run async version in event loop
        try:
            loop = asyncio.get_running_loop()
            # If we're in an async context, use run_coroutine_threadsafe
            import concurrent.futures
            future = asyncio.run_coroutine_threadsafe(
                self._request_peer_manifest(peer_info), loop
            )
            return future.result(timeout=15)
        except RuntimeError:
            # No running loop - use asyncio.run
            try:
                return asyncio.run(self._request_peer_manifest(peer_info))
            except Exception as e:  # noqa: BLE001
                logger.debug(f"Failed to request manifest from {peer_id}: {e}")
                return None
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Failed to request manifest from {peer_id}: {e}")
            return None

    async def _request_peer_manifest(self, peer_info: NodeInfo) -> NodeDataManifest | None:
        """Request data manifest from a peer node."""
        try:
            # Keep manifest requests snappy: these are advisory and should not
            # stall leader loops or external callers (e.g. the improvement
            # daemon). Prefer faster failure and rely on periodic retries.
            timeout = ClientTimeout(total=10, sock_connect=3, sock_read=7)
            async with get_client_session(timeout) as session:
                for url in self._urls_for_peer(peer_info, "/data_manifest"):
                    try:
                        async with session.get(url, headers=self._auth_headers()) as resp:
                            if resp.status != 200:
                                continue
                            data = await resp.json()
                        return NodeDataManifest.from_dict((data or {}).get("manifest", {}))
                    except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
                        continue
        except Exception as e:  # noqa: BLE001
            logger.error(f"requesting manifest from {peer_info.node_id}: {e}")
        return None

    # =========================================================================
    # Manifest Cache Methods - MOVED to SyncPlanner (Dec 2025)
    # =========================================================================
    # The following methods were moved to scripts/p2p/managers/sync_planner.py:
    # - get_manifest_cache_path() - disk cache path
    # - save_manifest_to_cache() - persist manifest
    # - load_manifest_from_cache() - load cached manifest
    # - collect_local_manifest_cached() - collect with disk caching
    # Access via: self.sync_planner.<method_name>()

    async def _collect_cluster_manifest(self) -> ClusterDataManifest:
        """Leader-only: Collect manifests from all peers and build cluster view."""
        cluster_manifest = ClusterDataManifest(
            collected_at=time.time(),
        )

        # Collect from self
        local_manifest = await asyncio.to_thread(self._collect_local_data_manifest)
        with self.manifest_lock:
            self.local_data_manifest = local_manifest
        cluster_manifest.node_manifests[self.node_id] = local_manifest

        # Collect from peers in parallel.
        #
        # Only probe peers that are currently alive and not retired; terminated
        # or long-dead nodes should not stall manifest collection. NAT-blocked
        # peers can't accept inbound /data_manifest, so they are excluded too.
        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        peers_snapshot = self._peer_snapshot.get_snapshot()
        peers = [
            p
            for p in peers_snapshot.values()
            if p.is_alive()
            and not bool(getattr(p, "retired", False))
            and not bool(getattr(p, "nat_blocked", False))
        ]

        tasks = [self._request_peer_manifest(peer) for peer in peers]
        # December 2025: Add timeout to prevent hang if peers are unresponsive
        # Individual requests have 10s timeout, but aggregate needs overall limit
        # to prevent blocking leader loop. 45s covers ~30 peers with some slack.
        try:
            results = await asyncio.wait_for(
                asyncio.gather(*tasks, return_exceptions=True),
                timeout=45.0
            )
        except asyncio.TimeoutError:
            logger.warning(
                f"[ManifestCollection] Timed out after 45s collecting from {len(peers)} peers. "
                "Proceeding with partial data."
            )
            results = []  # Proceed with only local manifest

        for peer, result in zip(peers, results, strict=False):
            if isinstance(result, NodeDataManifest):
                cluster_manifest.node_manifests[peer.node_id] = result

        # Compute cluster-wide statistics
        cluster_manifest.total_nodes = len(cluster_manifest.node_manifests)

        all_files: set[str] = set()
        for node_id, node_manifest in cluster_manifest.node_manifests.items():
            cluster_manifest.total_files += node_manifest.total_files
            cluster_manifest.total_size_bytes += node_manifest.total_size_bytes
            cluster_manifest.total_selfplay_games += node_manifest.selfplay_games
            cluster_manifest.files_by_node[node_id] = node_manifest.total_files

            for file_info in node_manifest.files:
                all_files.add(file_info.path)

        cluster_manifest.unique_files = all_files

        # Find files missing from nodes (for sync planning)
        for file_path in all_files:
            nodes_with_file = []
            nodes_without_file = []
            for node_id, node_manifest in cluster_manifest.node_manifests.items():
                file_paths = {f.path for f in node_manifest.files}
                if file_path in file_paths:
                    nodes_with_file.append(node_id)
                else:
                    nodes_without_file.append(node_id)

            if nodes_without_file:
                cluster_manifest.missing_from_nodes[file_path] = nodes_without_file

        # Collect external storage metadata (OWC drive, S3 bucket)
        # Jan 2026: Added for unified cluster data visibility
        try:
            external_storage = await self._collect_external_storage_metadata()
            cluster_manifest.external_storage = external_storage
        except Exception as e:
            logger.debug(f"[ManifestCollection] External storage scan skipped: {e}")

        logger.info(f"Cluster manifest: {cluster_manifest.total_nodes} nodes, "
              f"{len(cluster_manifest.unique_files)} unique files, "
              f"{cluster_manifest.total_selfplay_games} total games")

        return cluster_manifest

    async def _collect_external_storage_metadata(self) -> ExternalStorageManifest:
        """Collect metadata from external storage sources (OWC drive, S3 bucket).

        Jan 2026: Delegates to DataSyncCoordinator for unified cluster data visibility.

        Returns:
            ExternalStorageManifest with OWC and S3 metadata.
        """
        from scripts.p2p.models import ExternalStorageManifest

        # Delegate to DataSyncCoordinator
        metadata = await self.data_sync_coordinator.collect_external_storage_metadata()

        # Convert to ExternalStorageManifest
        external = ExternalStorageManifest(collected_at=metadata.collected_at)
        external.owc_available = metadata.owc_available
        external.owc_games_by_config = metadata.owc_games_by_config
        external.owc_total_games = metadata.owc_total_games
        external.owc_total_size_bytes = metadata.owc_total_size_bytes
        external.owc_last_scan = metadata.owc_last_scan
        external.owc_scan_error = metadata.owc_scan_error or ""
        external.s3_available = metadata.s3_available
        external.s3_games_by_config = metadata.s3_games_by_config
        external.s3_total_games = metadata.s3_total_games
        external.s3_total_size_bytes = metadata.s3_total_size_bytes
        external.s3_last_scan = metadata.s3_last_scan
        external.s3_bucket = metadata.s3_bucket
        external.s3_scan_error = metadata.s3_scan_error or ""

        return external

    def _scan_owc_local(self, base_path: str) -> dict:
        """Scan OWC drive locally. Delegates to DataSyncCoordinator."""
        return self.data_sync_coordinator._scan_owc_local(base_path)

    def _extract_config_from_path(self, db_path: Path) -> str | None:
        """Extract config from path. Delegates to DataSyncCoordinator."""
        return self.data_sync_coordinator.extract_config_from_path(db_path)

    # Jan 27, 2026: Phase 17B - Removed _get_s3_bucket_from_config (unused delegation wrapper)
    # Jan 28, 2026: Phase 18 - Removed _scan_s3_metadata (unused delegation wrapper)

    # Phase 2: P2P Rsync Coordination - using SyncPlanner

    async def _execute_sync_plan(self) -> None:
        """Leader executes the sync plan by dispatching jobs to nodes.

        Delegates to SyncPlanner.execute_sync_plan() with _request_node_sync as callback.
        Dec 2025: Refactored to delegate to SyncPlanner for consolidated logic.
        """
        if not self.current_sync_plan:
            return

        # Delegate to SyncPlanner with our network request callback
        result = await self.sync_planner.execute_sync_plan(
            plan=self.current_sync_plan,
            execute_job_callback_async=self._request_node_sync,
        )

        # Update local state from SyncPlanner result
        with self.sync_lock:
            self.last_sync_time = time.time()

        if not result.get("success", False):
            logger.warning(f"Sync plan execution issue: {result.get('error', 'unknown')}")

    async def _request_node_sync(self, job: DataSyncJob) -> bool:
        """Request a target node to pull files from a source node."""
        target_peer = self.peers.get(job.target_node)
        if job.target_node == self.node_id:
            target_peer = self.self_info

        source_peer = self.peers.get(job.source_node)
        if job.source_node == self.node_id:
            source_peer = self.self_info

        if not target_peer or not source_peer:
            job.status = "failed"
            job.error_message = "Source or target peer not found"
            return False

        job.status = "running"
        job.started_at = time.time()

        try:
            # Local target: execute the pull directly (no HTTP round-trip).
            if job.target_node == self.node_id:
                result = await self._handle_sync_pull_request(
                    source_host=source_peer.host,
                    source_port=source_peer.port,
                    source_reported_host=(getattr(source_peer, "reported_host", "") or None),
                    source_reported_port=(getattr(source_peer, "reported_port", 0) or None),
                    source_node_id=job.source_node,
                    files=job.files,
                )
            else:
                payload = {
                    "job_id": job.job_id,
                    # Back-compat: target will prefer source_node_id lookup.
                    "source_host": source_peer.host,
                    "source_port": source_peer.port,
                    "source_node_id": job.source_node,
                    "files": job.files,
                }
                rh = (getattr(source_peer, "reported_host", "") or "").strip()
                rp = int(getattr(source_peer, "reported_port", 0) or 0)
                if rh and rp and (rh != source_peer.host or rp != source_peer.port):
                    payload["source_reported_host"] = rh
                    payload["source_reported_port"] = rp

                timeout = ClientTimeout(total=600)
                async with get_client_session(timeout) as session:
                    result = None
                    last_err: str | None = None
                    for url in self._urls_for_peer(target_peer, "/sync/pull"):
                        try:
                            async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                                if resp.status != 200:
                                    last_err = f"http_{resp.status}"
                                    continue
                                result = await resp.json()
                                break
                        except Exception as e:  # noqa: BLE001
                            last_err = str(e)
                            continue
                    if result is None:
                        job.status = "failed"
                        job.error_message = last_err or "sync_pull_failed"
                        # Note: SyncPlanner tracks jobs_failed count
                        return False

            ok = bool(result.get("success"))
            job.status = "completed" if ok else "failed"
            job.completed_at = time.time()
            job.bytes_transferred = int(result.get("bytes_transferred", 0) or 0)
            job.files_completed = int(result.get("files_completed", 0) or 0)
            if not ok:
                job.error_message = str(result.get("error") or "Unknown error")

            # Note: SyncPlanner tracks jobs_completed/jobs_failed counts

            if ok:
                logger.info(f"Sync job {job.job_id[:8]} completed: {job.source_node} -> {job.target_node}")
            else:
                logger.info(f"Sync job {job.job_id[:8]} failed: {job.error_message}")

            return ok

        except Exception as e:  # noqa: BLE001
            job.status = "failed"
            job.error_message = str(e)
            job.completed_at = time.time()
            # Note: SyncPlanner tracks jobs_failed count
            logger.info(f"Sync job {job.job_id[:8]} failed: {e}")
            return False

    async def _handle_sync_pull_request(
        self,
        source_host: str,
        source_port: int,
        source_node_id: str,
        files: list[str],
        source_reported_host: str | None = None,
        source_reported_port: int | None = None,
    ) -> dict[str, Any]:
        """Handle incoming request to pull files from a source node.

        Jan 28, 2026: Phase 18A - Delegates to SyncPlanner.
        """
        return await self.sync_planner.handle_sync_pull_request(
            source_host=source_host,
            source_port=source_port,
            source_node_id=source_node_id,
            files=files,
            source_reported_host=source_reported_host,
            source_reported_port=source_reported_port,
            data_dir=self.get_data_directory(),
            auth_headers_fn=self._auth_headers,
        )

    async def start_cluster_sync(self) -> dict[str, Any]:
        """
        Leader initiates a full cluster data sync.
        Returns status of the sync operation.
        """
        if not self._is_leader():
            return {"success": False, "error": "Not the leader"}

        # First, collect fresh manifests
        logger.info("Collecting cluster manifest for sync...")
        self.cluster_data_manifest = await self._collect_cluster_manifest()

        # Generate sync plan (using SyncPlanner manager for consolidated logic)
        self.current_sync_plan = self.sync_planner.generate_sync_plan(self.cluster_data_manifest)
        if not self.current_sync_plan:
            return {"success": True, "message": "No sync needed, all nodes in sync"}

        # Execute the plan
        await self._execute_sync_plan()

        return {
            "success": True,
            "plan_id": self.current_sync_plan.plan_id,
            "total_jobs": len(self.current_sync_plan.sync_jobs),
            "jobs_completed": self.current_sync_plan.jobs_completed,
            "jobs_failed": self.current_sync_plan.jobs_failed,
            "status": self.current_sync_plan.status,
        }

    # ============================================
    # NodeSelector Wrapper Methods REMOVED (Dec 2025)
    # All call sites now use self.node_selector.* directly
    # ============================================

    def _should_sync_to_node(self, node: NodeInfo) -> bool:
        """Check if we should sync data TO this node based on disk space."""
        # Don't sync to nodes with critical disk usage
        if node.disk_percent >= DISK_CRITICAL_THRESHOLD:
            logger.info(f"Skipping sync to {node.node_id}: disk critical ({node.disk_percent:.1f}%)")
            return False
        # Warn but allow sync to nodes with warning-level disk
        if node.disk_percent >= DISK_WARNING_THRESHOLD:
            logger.warning(f"{node.node_id} disk at {node.disk_percent:.1f}%")
        return True

    def _should_cleanup_source(self, node: NodeInfo) -> bool:
        """Check if source node needs disk cleanup after sync."""
        return node.disk_percent >= DISK_CLEANUP_THRESHOLD

    async def _cleanup_synced_files(self, node_id: str, files: list[str]) -> bool:
        """Delete synced files from source node to free disk space.

        Only called after successful sync to training nodes.
        """
        with self.peers_lock:
            node = self.peers.get(node_id)
        if not node or not node.is_alive():
            return False

        try:
            if getattr(node, "nat_blocked", False):
                cmd_id = await self._enqueue_relay_command_for_peer(
                    node,
                    "cleanup_files",
                    {"files": list(files or []), "reason": "post_sync_cleanup"},
                )
                if cmd_id:
                    logger.info(f"Enqueued relay cleanup_files for {node_id} ({len(files)} files)")
                    return True
                logger.info(f"Relay queue full for {node_id}; skipping cleanup_files enqueue")
                return False

            timeout = ClientTimeout(total=60)
            async with get_client_session(timeout) as session:
                last_err: str | None = None
                for url in self._urls_for_peer(node, "/cleanup/files"):
                    try:
                        async with session.post(
                            url,
                            json={"files": files, "reason": "post_sync_cleanup"},
                            headers=self._auth_headers(),
                        ) as resp:
                            if resp.status != 200:
                                last_err = f"http_{resp.status}"
                                continue
                            result = await resp.json()
                            freed_bytes = result.get("freed_bytes", 0)
                            logger.info(f"Cleanup on {node_id}: freed {freed_bytes / 1e6:.1f}MB")
                            return True
                    except Exception as e:  # noqa: BLE001
                        last_err = str(e)
                        continue
                if last_err:
                    logger.info(f"Cleanup files request failed on {node_id}: {last_err}")
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to cleanup files on {node_id}: {e}")
        return False

    async def _sync_selfplay_to_training_nodes(self) -> dict[str, Any]:
        """Sync selfplay data to training primary nodes.

        December 2025: Delegated to SyncPlanner.sync_selfplay_to_training_nodes()
        """
        if not self._is_leader():
            return {"success": False, "error": "Not the leader"}

        # Use stale manifest if available, otherwise will be collected fresh
        manifest = self.cluster_data_manifest
        if (time.time() - self.last_manifest_collection > self.manifest_collection_interval
                or not manifest):
            manifest = None  # Will be collected by SyncPlanner

        result = await self.sync_planner.sync_selfplay_to_training_nodes(
            get_training_nodes=self.node_selector.get_training_primary_nodes,
            should_sync_to_node=self._should_sync_to_node,
            should_cleanup_source=self._should_cleanup_source,
            collect_manifest=self._collect_cluster_manifest,
            execute_sync_job=self._request_node_sync,
            cleanup_synced_files=self._cleanup_synced_files,
            get_sync_router=self._get_sync_router,
            cluster_manifest=manifest,
        )

        # Update orchestrator state
        if result.get("success"):
            self.last_training_sync_time = time.time()
            # Refresh manifest after sync
            if not manifest:
                # Dec 2025: Add 5-minute timeout for manifest collection
                try:
                    self.cluster_data_manifest = await asyncio.wait_for(
                        self._collect_cluster_manifest(),
                        timeout=300.0  # 5 minutes max
                    )
                    self.last_manifest_collection = time.time()
                except asyncio.TimeoutError:
                    logger.warning("Post-sync manifest collection timed out after 5 minutes")

        return result

    # Dec 2025: _training_sync_loop moved to LoopManager (TrainingSyncLoop)

    async def _force_ip_refresh_all_sources(self) -> int:
        """Force immediate refresh of IPs from all CLI sources (Tailscale, Vast, AWS).

        Jan 2026: Delegated to IPDiscoveryManager for better modularity.

        Returns:
            Total number of IPs updated across all sources
        """
        return await self.ip_discovery_manager.force_ip_refresh_all_sources()

    # Jan 2026: IP update loops moved to IPDiscoveryManager
    # Jan 27, 2026: Phase 17A - Deprecated wrappers removed (14 LOC)
    # _vast_ip_update_loop, _aws_ip_update_loop, _tailscale_ip_update_loop deleted
    # _tailscale_peer_recovery_loop removed (113 LOC)
    # Now runs via LoopManager as TailscalePeerDiscoveryLoop in scripts/p2p/loops/network_loops.py

    async def _discover_tailscale_peers(self):
        """One-shot Tailscale peer discovery for bootstrap fallback.

        Jan 2026: Delegated to IPDiscoveryManager for better modularity.
        """
        return await self.ip_discovery_manager.discover_tailscale_peers(
            peers_lock=self.peers_lock,
            peers=self.peers,
            send_heartbeat_callback=self._send_heartbeat_to_peer,
            run_subprocess_callback=self._run_subprocess_async,
        )

    async def _reconnect_missing_tailscale_peers(self) -> int:
        """Force reconnect to peers online in Tailscale but missing from P2P mesh.

        Jan 2026: Delegated to IPDiscoveryManager for better modularity.

        Returns:
            Number of peers successfully reconnected.
        """
        return await self.ip_discovery_manager.reconnect_missing_tailscale_peers(
            peers_lock=self.peers_lock,
            peers=self.peers,
            load_distributed_hosts_callback=self._load_distributed_hosts,
            reconnect_peer_callback=self._reconnect_discovered_peer,
            run_subprocess_callback=self._run_subprocess_async,
            node_id=self.node_id,
        )

    async def _convert_jsonl_to_db(self, data_dir: Path, games_dir: Path) -> int:
        """Convert JSONL selfplay files to SQLite DB format.

        Jan 2026: Delegated to DataPipelineManager.
        """
        return await self.data_pipeline_manager.convert_jsonl_to_db(data_dir, games_dir)

    async def _convert_jsonl_to_npz_for_training(self, data_dir: Path, training_dir: Path) -> int:
        """Convert JSONL selfplay files directly to NPZ.

        Jan 2026: Delegated to DataPipelineManager.
        """
        return await self.data_pipeline_manager.convert_jsonl_to_npz_for_training(
            data_dir, training_dir
        )

    # Dec 2025: Data/model sync loops moved to LoopManager (323 LOC)

    async def _consolidate_selfplay_data(self):
        """Consolidate siloed job databases AND JSONL files into training DB.

        Jan 2026: Delegated to DataPipelineManager.
        """
        await self.data_pipeline_manager.consolidate_selfplay_data(
            dispatch_export_job_callback=self._dispatch_export_job,
        )

    async def _start_auto_training(self, data_path: str):
        """Start automatic training job on local node."""
        try:
            run_dir = os.path.join(self._get_ai_service_path(), "models", f"auto_train_{int(time.time())}")
            Path(run_dir).mkdir(parents=True, exist_ok=True)

            cmd = [
                sys.executable,  # Use venv Python
                self._get_script_path("run_nn_training_baseline.py"),
                "--board", "square8",
                "--num-players", "2",
                "--run-dir", run_dir,
                "--data-path", data_path,
                "--epochs", "20",  # Jan 2026: Reduced from 50 to prevent overfitting (patience=7 will early stop)
                "--model-version", "v3",
            ]

            env = os.environ.copy()
            env["PYTHONPATH"] = self._get_ai_service_path()

            subprocess.Popen(
                cmd,
                stdout=open(f"{run_dir}/training.log", "w"),
                stderr=subprocess.STDOUT,
                env=env,
                cwd=self._get_ai_service_path(),
            )
            logger.info(f"Started auto-training job in {run_dir}")
            self.self_info.training_jobs += 1

        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to start auto-training: {e}")

    # ============================================
    # Git Auto-Update Methods (async - Jan 19, 2026)
    # All git operations run in thread pool to avoid blocking event loop
    # ============================================

    async def _get_local_git_commit(self) -> str | None:
        """Get the current local git commit hash (async)."""
        try:
            result = await async_subprocess_run(
                self._git_cmd("rev-parse", "HEAD"),
                cwd=self.ringrift_path,
                timeout=10
            )
            if result.returncode == 0:
                return result.stdout.strip()
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to get local git commit: {e}")
        return None

    async def _get_local_git_branch(self) -> str | None:
        """Get the current local git branch name (async)."""
        try:
            result = await async_subprocess_run(
                self._git_cmd("rev-parse", "--abbrev-ref", "HEAD"),
                cwd=self.ringrift_path,
                timeout=10
            )
            if result.returncode == 0:
                return result.stdout.strip()
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to get local git branch: {e}")
        return None

    async def _get_remote_git_commit(self) -> str | None:
        """Fetch and get the remote branch's latest commit hash (async)."""
        try:
            # First fetch to update remote refs
            fetch_result = await async_subprocess_run(
                self._git_cmd("fetch", GIT_REMOTE_NAME, GIT_BRANCH_NAME),
                cwd=self.ringrift_path,
                timeout=60
            )
            if fetch_result.returncode != 0:
                logger.info(f"Git fetch failed: {fetch_result.stderr}")
                return None

            # Get remote branch commit
            result = await async_subprocess_run(
                self._git_cmd("rev-parse", f"{GIT_REMOTE_NAME}/{GIT_BRANCH_NAME}"),
                cwd=self.ringrift_path,
                timeout=10
            )
            if result.returncode == 0:
                return result.stdout.strip()
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to get remote git commit: {e}")
        return None

    async def _check_for_updates(self) -> tuple[bool, str | None, str | None]:
        """Check if there are updates available from GitHub (async).

        Returns: (has_updates, local_commit, remote_commit)
        """
        # Run both git queries in parallel
        local_commit, remote_commit = await asyncio.gather(
            self._get_local_git_commit(),
            self._get_remote_git_commit(),
        )

        if not local_commit or not remote_commit:
            return False, local_commit, remote_commit

        has_updates = local_commit != remote_commit
        return has_updates, local_commit, remote_commit

    async def _get_commits_behind(self, local_commit: str, remote_commit: str) -> int:
        """Get the number of commits the local branch is behind remote (async)."""
        try:
            result = await async_subprocess_run(
                self._git_cmd("rev-list", "--count", f"{local_commit}..{remote_commit}"),
                cwd=self.ringrift_path,
                timeout=10
            )
            if result.returncode == 0:
                return int(result.stdout.strip())
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to count commits behind: {e}")
        return 0

    async def _check_local_changes(self) -> bool:
        """Check if there are uncommitted local changes (async).

        Notes:
        - Ignore untracked files by default. Cluster nodes often accumulate local
          artifacts (logs, data, env backups) that should not block git updates.
        - Still blocks on tracked/staged modifications to avoid stomping on
          local hotfixes.
        """
        try:
            result = await async_subprocess_run(
                self._git_cmd("status", "--porcelain", "--untracked-files=no"),
                cwd=self.ringrift_path,
                timeout=10
            )
            if result.returncode == 0:
                # If there's output, there are uncommitted changes
                return bool(result.stdout.strip())
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to check local changes: {e}")
        return True  # Assume changes exist on error (safer)

    async def _stop_all_local_jobs(self) -> int:
        """Stop all local jobs gracefully before update.

        Returns: Number of jobs stopped
        """
        stopped = 0
        with self.jobs_lock:
            for job_id, job in list(self.local_jobs.items()):
                try:
                    if job.pid > 0:
                        os.kill(job.pid, signal.SIGTERM)
                        logger.info(f"Sent SIGTERM to job {job_id} (PID {job.pid})")
                        stopped += 1
                        job.status = "stopping"
                except ProcessLookupError:
                    # Process already gone
                    job.status = "stopped"
                except Exception as e:  # noqa: BLE001
                    logger.error(f"Failed to stop job {job_id}: {e}")

        # Wait for processes to terminate gracefully
        # GPU games can take 1-10 minutes, so use a longer timeout (Dec 2025 fix)
        grace_period = int(os.environ.get("RINGRIFT_JOB_GRACE_PERIOD", "60"))
        if stopped > 0:
            await asyncio.sleep(grace_period)

            # Force kill any remaining
            with self.jobs_lock:
                for job_id, job in list(self.local_jobs.items()):
                    if job.status == "stopping" and job.pid > 0:
                        try:
                            os.kill(job.pid, signal.SIGKILL)
                            logger.info(f"Force killed job {job_id}")
                        except OSError:
                            pass  # Process already dead
                        job.status = "stopped"

        return stopped

    async def _perform_git_update(self) -> tuple[bool, str]:
        """Perform git pull to update the codebase (async).

        Returns: (success, message)
        """
        # Check for local changes (async)
        if await self._check_local_changes():
            return False, "Local changes detected. Cannot auto-update. Please commit or stash changes."

        # Stop jobs if configured
        if GRACEFUL_SHUTDOWN_BEFORE_UPDATE:
            stopped = await self._stop_all_local_jobs()
            if stopped > 0:
                logger.info(f"Stopped {stopped} jobs before update")

        try:
            # Perform git pull (async - Jan 19, 2026)
            result = await async_subprocess_run(
                self._git_cmd("pull", GIT_REMOTE_NAME, GIT_BRANCH_NAME),
                cwd=self.ringrift_path,
                timeout=120
            )

            if result.returncode != 0:
                return False, f"Git pull failed: {result.stderr}"

            logger.info(f"Git pull successful: {result.stdout}")
            return True, result.stdout

        except subprocess.TimeoutExpired:
            return False, "Git pull timed out"
        except Exception as e:  # noqa: BLE001
            return False, f"Git pull error: {e}"

    async def _restart_orchestrator(self):
        """Restart the orchestrator process after update."""
        logger.info("Restarting orchestrator to apply updates...")

        # Save state before restart
        self._save_state()

        # Get current script path and arguments
        script_path = Path(__file__).resolve()
        args = sys.argv[1:]

        # Schedule restart
        await asyncio.sleep(2)

        # Use exec to replace current process
        os.execv(sys.executable, [sys.executable, str(script_path), *args])

    async def _git_update_loop(self):
        """Background loop to periodically check for and apply updates.

        Jan 2026: Uses asyncio.to_thread() for git operations to avoid blocking.
        """
        if not AUTO_UPDATE_ENABLED:
            logger.info("Auto-update disabled")
            return

        logger.info(f"Git auto-update loop started (interval: {GIT_UPDATE_CHECK_INTERVAL}s)")

        while self.running:
            try:
                await asyncio.sleep(GIT_UPDATE_CHECK_INTERVAL)

                if not self.running:
                    break

                # Check for updates (Jan 19, 2026: methods are now async)
                has_updates, local_commit, remote_commit = await self._check_for_updates()

                if has_updates and local_commit and remote_commit:
                    commits_behind = await self._get_commits_behind(local_commit, remote_commit)
                    logger.info(f"Update available: {commits_behind} commits behind")
                    logger.info(f"Local:  {local_commit[:8]}")
                    logger.info(f"Remote: {remote_commit[:8]}")

                    # Perform update
                    success, message = await self._perform_git_update()

                    if success:
                        logger.info("Update successful, restarting...")
                        await self._restart_orchestrator()
                    else:
                        logger.info(f"Update failed: {message}")

            except asyncio.CancelledError:
                break
            except Exception as e:  # noqa: BLE001
                logger.info(f"Git update loop error: {e}")
                await asyncio.sleep(60)  # Wait before retry on error

    # ============================================
    # HTTP API Handlers
    # ============================================

    async def handle_heartbeat(self, request: web.Request) -> web.Response:
        """Handle heartbeat from peer node.

        Jan 28, 2026: Phase 18B - Delegates to HeartbeatManager.process_incoming_heartbeat().
        """
        try:
            data = await request.json()
            forwarded_for = (
                request.headers.get("X-Forwarded-For")
                or request.headers.get("X-Real-IP")
                or request.headers.get("CF-Connecting-IP")
            )
            payload = await self.heartbeat_manager.process_incoming_heartbeat(
                data=data,
                remote_addr=request.remote,
                forwarded_for=forwarded_for,
            )
            return web.json_response(payload)
        except json.JSONDecodeError as e:
            logger.warning(f"[heartbeat] JSON parse error from {request.remote}: {e}")
            return web.json_response({"error": "invalid_json", "detail": str(e)}, status=400)
        except KeyError as e:
            logger.warning(f"[heartbeat] Missing required field from {request.remote}: {e}")
            return web.json_response({"error": "missing_field", "field": str(e)}, status=400)
        except ValueError as e:
            logger.warning(f"[heartbeat] Validation error from {request.remote}: {e}")
            return web.json_response({"error": "validation_error", "detail": str(e)}, status=400)
        except Exception as e:  # noqa: BLE001
            logger.error(f"[heartbeat] Unexpected error from {request.remote}: {type(e).__name__}: {e}")
            return web.json_response({"error": "internal_error", "type": type(e).__name__}, status=500)

    @with_request_timeout(30.0)
    async def handle_status(self, request: web.Request) -> web.Response:
        """Return cluster status.

        Query parameters:
            alive_only: If "true" (default), only show alive peers. Set to "false" to include dead/stale peers.
            include_stale_jobs: If "false" (default), dead peers show 0 jobs. Set to "true" to show stale job counts.

        December 30, 2025: Made non-blocking with timeout-based lock acquisition.
        If locks can't be acquired within 2 seconds, returns partial status with
        "unavailable" markers instead of blocking indefinitely.

        Jan 12, 2026: Changed to non-blocking self_info update - schedules background
        refresh and returns immediately with cached data. This prevents 15s+ timeouts
        on macOS where resource detection is slow.

        Jan 16, 2026: Added @with_request_timeout(30.0) decorator to prevent overall
        handler timeout. Individual metric timeouts are 2s, but other operations
        (voter health, partition status, etc.) can hang without protection.
        """
        # Jan 12, 2026: Non-blocking mode - schedule background refresh, use cached data
        try:
            asyncio.create_task(self._update_self_info_async())
        except Exception:
            pass  # Fire-and-forget, don't block on errors

        # Parse query parameters for filtering
        alive_only = request.query.get("alive_only", "true").lower() != "false"
        include_stale_jobs = request.query.get("include_stale_jobs", "false").lower() == "true"

        # Jan 12, 2026: Lock-free peer snapshot using copy-on-write pattern
        # PeerSnapshot.get_snapshot() returns instantly without acquiring any lock.
        # The snapshot is updated atomically whenever peers are modified.
        # This eliminates the 6+ second timeouts that occurred under load.
        snapshot_dict = self._peer_snapshot.get_snapshot()
        peers_snapshot: list = list(snapshot_dict.values())

        conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])
        effective_leader = self._get_leader_peer()

        now = time.time()
        peers: dict[str, Any] = {}
        for node_id, info in ((p.node_id, p) for p in peers_snapshot):
            is_alive = info.is_alive()

            # Skip dead peers if alive_only is set
            if alive_only and not is_alive:
                continue

            d = info.to_dict()
            d["endpoint_conflict"] = self._endpoint_key(info) in conflict_keys
            d["leader_eligible"] = self._is_leader_eligible(info, conflict_keys, require_alive=False)

            # Add explicit alive status and staleness info
            d["is_alive"] = is_alive
            last_hb = float(getattr(info, "last_heartbeat", 0.0) or 0.0)
            d["seconds_since_heartbeat"] = int(now - last_hb) if last_hb > 0 else -1

            # Zero out job counts for dead peers unless explicitly requested
            if not is_alive and not include_stale_jobs:
                d["selfplay_jobs"] = 0
                d["training_jobs"] = 0
                d["active_job_count"] = 0

            peers[node_id] = d

        # Jan 5, 2026 (Session 17.28): Build all_peers dict with ALL peers regardless of alive status
        # This is required for remote job dispatch which needs to know about all configured nodes
        all_peers: dict[str, Any] = {}
        for peer in peers_snapshot:
            all_peers[peer.node_id] = {
                "node_id": peer.node_id,
                "host": getattr(peer, "host", None),
                "port": getattr(peer, "port", 8770),
                "role": peer.role.value if hasattr(peer.role, "value") else str(peer.role),
                "capabilities": getattr(peer, "capabilities", []),
                "load_score": getattr(peer, "load_score", 0.0),
                "status": "alive" if peer.is_alive() else "dead",
                "is_alive": peer.is_alive(),
                "last_heartbeat": float(getattr(peer, "last_heartbeat", 0.0) or 0.0),
            }

        # Convenience diagnostics: reported leaders vs eligible leaders.
        leaders_reported = sorted(
            [p.node_id for p in peers_snapshot if p.role == NodeRole.LEADER and p.is_alive()]
        )
        leaders_eligible = sorted(
            [
                p.node_id
                for p in peers_snapshot
                if p.role == NodeRole.LEADER and self._is_leader_eligible(p, conflict_keys)
            ]
        )

        # Jan 12, 2026: Lock-free job snapshot access
        # Uses JobSnapshot copy-on-write pattern - no lock needed for reads.
        # Previous lock-based code removed (was causing 6+ second timeouts).
        jobs = self._job_snapshot.get_snapshot()

        # Get improvement cycle manager status
        improvement_status = None
        if self.improvement_cycle_manager:
            try:
                improvement_status = self.improvement_cycle_manager.get_status()
            except Exception as e:  # noqa: BLE001
                improvement_status = {"error": str(e)}

        # Get diversity metrics (delegated to SelfplayScheduler)
        # December 27, 2025: Added try-except to prevent 500 errors on memory-constrained nodes
        try:
            diversity_metrics = self.selfplay_scheduler.get_diversity_metrics()
        except Exception as e:  # noqa: BLE001
            diversity_metrics = {"error": str(e)}

        voter_ids = list(getattr(self, "voter_node_ids", []) or [])
        # Jan 2, 2026: Use _count_alive_voters() to check IP:port matches
        voters_alive = self._count_alive_voters()

        # Get P2P sync metrics (with error handling for new features)
        # December 27, 2025: Wrapped all metric calls to prevent cascading 500 errors
        # December 31, 2025: Added asyncio.wait_for() timeouts to prevent /status hangs
        # January 12, 2026: CRITICAL FIX - Run all metric calls in PARALLEL instead of sequential
        # Previous sequential approach took up to 24 seconds (10 calls  2s timeout each).
        # Parallel approach takes at most 2 seconds (max of all timeouts).
        p2p_sync_metrics = getattr(self, "_p2p_sync_metrics", {})
        _STATUS_TIMEOUT = 5.0  # seconds for each blocking call (Jan 16, 2026: increased from 2.0)

        # Define all metric gathering tasks
        async def _safe_metric(name: str, func: callable) -> tuple[str, dict]:
            """Wrapper to safely gather a metric with timeout and error handling."""
            try:
                result = await asyncio.wait_for(
                    asyncio.to_thread(func),
                    timeout=_STATUS_TIMEOUT,
                )
                return name, result
            except asyncio.TimeoutError:
                logger.warning(f"handle_status: {name} timed out")
                return name, {"error": "timeout"}
            except Exception as e:  # noqa: BLE001
                return name, {"error": str(e)}

        # Jan 19, 2026: Run ALL metric gathering calls in PARALLEL
        # Previously some metrics (swim_raft, partition, background_loops, voter_health)
        # were awaited sequentially after this gather, adding up to 20s latency.
        # Now everything runs in parallel for <5s total latency.
        def _get_loop_manager_status():
            loop_manager = self._get_loop_manager()
            if loop_manager is not None:
                return loop_manager.get_all_status()
            return {"error": "LoopManager not initialized"}

        metric_tasks = [
            _safe_metric("gossip_metrics", self._get_gossip_metrics_summary),
            _safe_metric("distributed_training", self._get_distributed_training_summary),
            _safe_metric("cluster_elo", self._get_cluster_elo_summary),
            _safe_metric("node_recovery", self._get_node_recovery_metrics),
            _safe_metric("leader_consensus", self._get_cluster_leader_consensus),
            _safe_metric("peer_reputation", self._get_cluster_peer_reputation),
            _safe_metric("sync_intervals", self._get_sync_interval_summary),
            _safe_metric("tournament_scheduling", self._get_distributed_tournament_summary),
            _safe_metric("data_dedup", self._get_dedup_summary),
            # Jan 19, 2026: Added these to parallel gather (were sequential before)
            _safe_metric("swim_raft", self._get_swim_raft_status),
            _safe_metric("partition", self.get_partition_status),
            _safe_metric("background_loops", _get_loop_manager_status),
            _safe_metric("voter_health", self._check_voter_health),
        ]

        # Gather all results in parallel
        metric_results = await asyncio.gather(*metric_tasks, return_exceptions=True)

        # Extract results into named variables
        metrics_dict = {}
        for result in metric_results:
            if isinstance(result, tuple) and len(result) == 2:
                name, value = result
                metrics_dict[name] = value
            elif isinstance(result, Exception):
                logger.warning(f"handle_status: metric task failed with {result}")

        gossip_metrics = metrics_dict.get("gossip_metrics", {"error": "not_collected"})
        distributed_training = metrics_dict.get("distributed_training", {"error": "not_collected"})
        cluster_elo = metrics_dict.get("cluster_elo", {"error": "not_collected"})
        node_recovery = metrics_dict.get("node_recovery", {"error": "not_collected"})
        leader_consensus = metrics_dict.get("leader_consensus", {"error": "not_collected"})
        peer_reputation = metrics_dict.get("peer_reputation", {"error": "not_collected"})
        sync_intervals = metrics_dict.get("sync_intervals", {"error": "not_collected"})
        tournament_scheduling = metrics_dict.get("tournament_scheduling", {"error": "not_collected"})
        data_dedup = metrics_dict.get("data_dedup", {"error": "not_collected"})
        # Jan 19, 2026: These were previously sequential - now parallel
        swim_raft_status = metrics_dict.get("swim_raft", {"error": "not_collected"})
        partition_status = metrics_dict.get("partition", {"error": "not_collected"})
        background_loops = metrics_dict.get("background_loops", {"error": "not_collected"})
        voter_health = metrics_dict.get("voter_health", {"error": "not_collected"})

        # Jan 3, 2026: Transport latency stats for diagnosing slow transports
        transport_latency: dict = {}
        try:
            from scripts.p2p.transport_cascade import get_transport_cascade
            cascade = get_transport_cascade()
            transport_latency = cascade.get_transport_latency_summary()
        except ImportError:
            transport_latency = {"available": False, "reason": "import_error"}
        except Exception as e:  # noqa: BLE001
            transport_latency = {"available": False, "error": str(e)}

        # Dec 2025: Get event subscription status for health monitoring
        event_subscriptions = getattr(self, "_event_subscription_status", {
            "daemon_events": False,
            "feedback_signals": False,
            "manager_events": False,
            "all_healthy": False,
            "timestamp": 0,
        })

        # Jan 19, 2026: partition_status and background_loops now computed in parallel gather above

        # Jan 1, 2026: Work queue status for monitoring (Phase 4B fix)
        work_queue_size = 0
        active_jobs_count = 0
        selfplay_jobs_count = 0
        try:
            from app.coordination.work_queue import get_work_queue
            wq = get_work_queue()
            if wq is not None and hasattr(wq, 'get_queue_status'):
                wq_status = wq.get_queue_status()
                work_queue_size = wq_status.get('total_items', 0)
        except Exception:  # noqa: BLE001
            pass  # Fall back to 0

        # Count jobs directly from local_jobs
        if isinstance(jobs, dict) and "error" not in jobs:
            for job_data in jobs.values():
                if isinstance(job_data, dict):
                    status = job_data.get("status", "")
                    job_type = job_data.get("job_type", "")
                    if status in ("running", "claimed"):
                        active_jobs_count += 1
                    if job_type == "selfplay" and status in ("running", "claimed"):
                        selfplay_jobs_count += 1

        # Jan 1, 2026: Aggregate cluster-wide selfplay jobs from peers
        cluster_selfplay_jobs = selfplay_jobs_count  # Start with local count
        cluster_training_jobs = 0
        for peer_node_id, peer_data in peers.items():
            if isinstance(peer_data, dict):
                cluster_selfplay_jobs += int(peer_data.get("selfplay_jobs", 0) or 0)
                cluster_training_jobs += int(peer_data.get("training_jobs", 0) or 0)

        # Jan 19, 2026: voter_health now computed in parallel gather above

        return web.json_response({
            "node_id": self.node_id,
            "role": self.role.value,
            "leader_id": self.leader_id,
            "effective_leader_id": (effective_leader.node_id if effective_leader else None),
            # Jan 1, 2026: Provisional leadership status
            "is_provisional_leader": self.role == NodeRole.PROVISIONAL_LEADER,
            "provisional_claimed_at": getattr(self, "_provisional_leader_claimed_at", 0.0) or 0.0,
            "provisional_acks": len(getattr(self, "_provisional_leader_acks", set()) or set()),
            "provisional_challengers": len(getattr(self, "_provisional_leader_challengers", {}) or {}),
            "fallback_leader_since": getattr(self, "_fallback_leader_since", 0.0) or 0.0,
            "fallback_leader_reason": getattr(self, "_fallback_leader_reason", "") or "",
            "leaders_reported": leaders_reported,
            "leaders_eligible": leaders_eligible,
            "voter_node_ids": voter_ids,
            "voter_quorum_size": int(getattr(self, "voter_quorum_size", 0) or 0),
            "voters_alive": voters_alive,
            "voter_quorum_ok": self._has_voter_quorum(),
            # Jan 20, 2026: Voter config sync - version and hash for drift detection
            "voter_config_version": self._get_voter_config_version(),
            "voter_config_hash": self._get_voter_config_hash(),
            # Jan 2, 2026: Detailed voter health for monitoring
            # Jan 16, 2026: Now pre-computed with timeout protection
            "voter_health": voter_health,
            "self": self.self_info.to_dict(),
            "peers": peers,
            "all_peers": all_peers,  # Jan 5, 2026: All peers regardless of alive status for job dispatch
            "local_jobs": jobs,
            "alive_peers": len([p for p in self.peers.values() if p.is_alive()]),
            "improvement_cycle_manager": improvement_status,
            "diversity_metrics": diversity_metrics,
            "gossip_metrics": gossip_metrics,
            "p2p_sync_metrics": p2p_sync_metrics,
            "distributed_training": distributed_training,
            "cluster_elo": cluster_elo,
            "node_recovery": node_recovery,
            "leader_consensus": leader_consensus,
            "peer_reputation": peer_reputation,
            "sync_intervals": sync_intervals,
            "tournament_scheduling": tournament_scheduling,
            "data_dedup": data_dedup,
            "swim_raft": swim_raft_status,
            "transport_latency": transport_latency,  # Jan 3, 2026: Per-transport latency metrics
            "event_subscriptions": event_subscriptions,
            "partition": partition_status,
            "background_loops": background_loops,
            # December 30, 2025: Cluster observability for debugging idle nodes
            "cluster_observability": self._get_cluster_observability(),
            # Session 17.41 (Jan 6, 2026): Fallback mechanism status for partition debugging
            "fallback_status": self._get_fallback_status(),
            # December 30, 2025: Lock acquisition status for debugging
            "_lock_status": {
                "peers_lock_acquired": peers_snapshot is not None,
                "jobs_lock_acquired": "error" not in jobs,
            },
            # Jan 1, 2026: Explicit work queue and job counts (Phase 4B fix)
            "work_queue_size": work_queue_size,
            "active_jobs": active_jobs_count,
            "selfplay_jobs": cluster_selfplay_jobs,  # Cluster-wide aggregated
            "training_jobs": cluster_training_jobs,  # Cluster-wide aggregated
            "local_selfplay_jobs": selfplay_jobs_count,  # This node only
            # Jan 2, 2026: Dual-stack IPv4/IPv6 network info
            "network": {
                "advertise_host": self.advertise_host,
                "advertise_host_family": "ipv6" if ":" in (self.advertise_host or "") else "ipv4",
                "alternate_ips": list(getattr(self, "alternate_ips", set()) or set()),
                "alternate_ipv4_count": sum(1 for ip in getattr(self, "alternate_ips", set()) or set() if ":" not in ip),
                "alternate_ipv6_count": sum(1 for ip in getattr(self, "alternate_ips", set()) or set() if ":" in ip),
            },
            # Jan 3, 2026: Leadership consistency metrics for monitoring desync issues
            # This enables detection of the leader self-recognition bug where leader_id
            # is set correctly but role doesn't match.
            "leadership_consistency": self._get_leadership_consistency_metrics(),
            "is_leader": self._is_leader(),  # Explicit field for quick checks
            # Jan 13, 2026: Config version for drift detection (P2P Cluster Stability Plan Phase 1)
            "config_version": self._get_config_version(),
            # Jan 13, 2026: Unified data summary across all sources (LOCAL, CLUSTER, S3, OWC)
            "data_summary": self._get_data_summary_cached(),
            # Jan 20, 2026: Adaptive dead peer cooldown stats
            "cooldown_stats": self._get_cooldown_stats(),
            # Jan 25, 2026: Peer health summary for P2P stability monitoring (Phase 3)
            # Inline: was _get_peer_health_summary()
            "peer_health_summary": self.health_metrics_manager.get_peer_health_summary(),
        })

    @with_request_timeout(10.0)
    async def handle_progress(self, request: web.Request) -> web.Response:
        """GET /progress - Return Elo progress report for demonstrating iterative improvement.

        January 16, 2026: Added to provide visibility into NN strength improvement.

        Query parameters:
            config: Optional config filter (e.g., "hex8_2p")
            days: Lookback period in days (default: 30)

        Returns JSON with:
            - configs: Per-config progress data (starting_elo, current_elo, delta, iterations)
            - overall: Summary stats (total_iterations, configs_improving, avg_elo_gain)
            - generated_at: Timestamp
        """
        config_filter = request.query.get("config")
        try:
            days = float(request.query.get("days", "30"))
        except ValueError:
            days = 30.0

        try:
            # Import progress report logic
            import sys
            sys.path.insert(0, str(Path(__file__).parent.parent))
            from scripts.elo_progress_report import get_full_report
            from dataclasses import asdict

            report = get_full_report(days=days, config_filter=config_filter)

            # Convert to JSON-serializable dict
            data = {
                "configs": {k: asdict(v) for k, v in report.configs.items()},
                "overall": asdict(report.overall),
                "generated_at": report.generated_at,
            }

            return web.json_response(data)

        except ImportError as e:
            logger.warning(f"[handle_progress] Import error: {e}")
            return web.json_response({
                "error": "progress_report_unavailable",
                "detail": str(e),
            }, status=500)
        except Exception as e:  # noqa: BLE001
            logger.error(f"[handle_progress] Error generating progress report: {e}")
            return web.json_response({
                "error": "internal_error",
                "detail": str(e),
            }, status=500)

    # -------------------------------------------------------------------------
    # Stability Controller Handlers (January 2026 - Self-Healing Architecture)
    # -------------------------------------------------------------------------

    async def handle_stability(self, request: web.Request) -> web.Response:
        """GET /stability - Return stability controller status.

        January 2026: Added as part of P2P Self-Healing Architecture.

        Returns JSON with:
            - controller: StabilityController status (symptoms, actions, running state)
            - adaptive_timeouts: Per-node adaptive timeouts
            - effectiveness: Recovery action effectiveness tracking
            - metrics: Current stability metrics
        """
        response: dict[str, Any] = {
            "node_id": self.node_id,
            "timestamp": time.time(),
        }

        # Stability controller status
        if self._stability_controller:
            response["controller"] = self._stability_controller.get_status()
        else:
            response["controller"] = {"enabled": False, "reason": "not_initialized"}

        # Adaptive timeout status
        if self._adaptive_timeouts:
            response["adaptive_timeouts"] = self._adaptive_timeouts.get_status()
        else:
            response["adaptive_timeouts"] = {"enabled": False}

        # Effectiveness tracking status
        if self._effectiveness_tracker:
            response["effectiveness"] = self._effectiveness_tracker.get_status()
        else:
            response["effectiveness"] = {"enabled": False}

        # Current stability metrics
        try:
            response["metrics"] = self._get_stability_metrics()
        except Exception as e:
            response["metrics"] = {"error": str(e)}

        return web.json_response(response)

    async def handle_p2p_diagnostics(self, request: web.Request) -> web.Response:
        """GET /p2p/diagnostics - Return P2P diagnostic tracker data.

        January 2026: Phase 0 diagnostic instrumentation endpoint.

        Returns JSON with:
            - peer_state: Peer state transition tracking (flapping, death reasons)
            - connection_failures: Connection failure tracking by type/transport
            - probe_effectiveness: Probe success rates and false positives
        """
        response: dict[str, Any] = {
            "node_id": self.node_id,
            "timestamp": time.time(),
        }

        # Peer state tracker
        if self._peer_state_tracker:
            try:
                response["peer_state"] = self._peer_state_tracker.get_diagnostics()
            except Exception as e:
                response["peer_state"] = {"error": str(e)}
        else:
            response["peer_state"] = {"enabled": False}

        # Connection failure tracker
        if self._conn_failure_tracker:
            try:
                response["connection_failures"] = self._conn_failure_tracker.get_diagnostics()
            except Exception as e:
                response["connection_failures"] = {"error": str(e)}
        else:
            response["connection_failures"] = {"enabled": False}

        # Probe effectiveness tracker
        if self._probe_tracker:
            try:
                response["probe_effectiveness"] = self._probe_tracker.get_diagnostics()
            except Exception as e:
                response["probe_effectiveness"] = {"error": str(e)}
        else:
            response["probe_effectiveness"] = {"enabled": False}

        return web.json_response(response)

    # -------------------------------------------------------------------------
    # Peer Health Handlers - EXTRACTED to scripts/p2p/handlers/status.py
    # January 2026 - P2P Modularization Phase 6a
    # Provides: handle_peer_health, handle_external_work
    # See StatusHandlersMixin
    # -------------------------------------------------------------------------

    # Work Queue Handlers moved to scripts/p2p/handlers/work_queue.py
    # Inherited from WorkQueueHandlersMixin: handle_work_*, handle_populator_status

    # Election Handlers moved to scripts/p2p/handlers/election.py
    # Inherited from ElectionHandlersMixin: handle_election, handle_lease_request,
    # handle_voter_grant_status, handle_election_reset, handle_election_force_leader

    # ============================================================
    # SERF INTEGRATION - Battle-tested membership/failure detection
    # ============================================================

    async def handle_serf_event(self, request: web.Request) -> web.Response:
        """POST /serf/event - Receive events from Serf event handler.

        SERF INTEGRATION: HashiCorp Serf provides battle-tested SWIM gossip
        for membership and failure detection. This endpoint receives events
        from the serf_event_handler.py script.

        Event types:
        - member-join: New node joined the cluster
        - member-leave: Node gracefully left
        - member-failed: Node failed (detected by SWIM)
        - member-update: Node tags changed
        - member-reap: Failed node was reaped from membership list
        - user: Custom user event (training-complete, model-promoted, etc.)

        Request body:
        {
            "event_type": "member-join",
            "timestamp": "2025-12-26T...",
            "payload": { event-specific data }
        }
        """
        try:
            if self.auth_token and not self._is_request_authorized(request):
                return web.json_response({"error": "unauthorized"}, status=401)

            data = await request.json()
            event_type = data.get("event_type", "")
            timestamp = data.get("timestamp", "")
            payload = data.get("payload", {})

            logger.info(f"Serf event received: {event_type} at {timestamp}")

            # Process based on event type
            if event_type == "member-join":
                await self._handle_serf_member_join(payload.get("members", []))
            elif event_type == "member-leave":
                await self._handle_serf_member_leave(payload.get("members", []))
            elif event_type == "member-failed":
                await self._handle_serf_member_failed(payload.get("members", []))
            elif event_type == "member-update":
                await self._handle_serf_member_update(payload.get("members", []))
            elif event_type == "member-reap":
                await self._handle_serf_member_reap(payload.get("members", []))
            elif event_type == "user":
                await self._handle_serf_user_event(payload)
            else:
                logger.warning(f"Unknown Serf event type: {event_type}")

            return web.json_response({
                "status": "processed",
                "event_type": event_type,
                "node_id": self.node_id,
            })

        except Exception as e:  # noqa: BLE001
            logger.error(f"Error processing Serf event: {e}")
            return web.json_response({"error": str(e)}, status=500)

    async def _handle_serf_member_join(self, members: list) -> None:
        """Handle Serf member-join events.

        When Serf detects new members, update our peer list and mark them alive.
        This is more reliable than our custom gossip because Serf uses SWIM
        with indirect probing.
        """
        for member in members:
            node_name = member.get("name", "")
            addr = member.get("addr", "")
            tags = member.get("tags", {})

            if not node_name or node_name == self.node_id:
                continue

            logger.info(f"Serf: member joined: {node_name} @ {addr}")

            # Update peer state
            now = time.time()
            if node_name not in self.peers:
                # Parse addr to get host:port (format: "ip:port")
                host, port = (addr.rsplit(":", 1) + ["8770"])[:2] if ":" in addr else (addr, "8770")
                try:
                    port_int = int(port)
                except ValueError:
                    port_int = 8770
                self.peers[node_name] = NodeInfo(
                    node_id=node_name,
                    host=host or "unknown",
                    port=port_int,
                    last_heartbeat=now,
                )
            else:
                peer = self.peers[node_name]
                if isinstance(peer, NodeInfo):
                    peer.last_heartbeat = now
                    if addr:
                        host, _ = (addr.rsplit(":", 1) + [""])[:2] if ":" in addr else (addr, "")
                        if host:
                            peer.host = host

            # Extract tags into peer info (store as capability hints)
            # Note: Serf tags are for reference only, NodeInfo uses capabilities list

        # C2 fix: Sync peer snapshot after Serf member join updates
        if members:
            self._sync_peer_snapshot()

    async def _handle_serf_member_leave(self, members: list) -> None:
        """Handle Serf member-leave events (graceful departure)."""
        for member in members:
            node_name = member.get("name", "")

            if not node_name or node_name == self.node_id:
                continue

            logger.info(f"Serf: member left gracefully: {node_name}")

            if node_name in self.peers:
                peer = self.peers[node_name]
                if isinstance(peer, NodeInfo):
                    # Mark as retired (NodeInfo equivalent of "left")
                    peer.retired = True
                    peer.retired_at = time.time()
                    # Jan 20, 2026: Use adaptive cooldown manager
                    if self._cooldown_manager:
                        self._cooldown_manager.record_death(node_name)
                    else:
                        self._dead_peer_timestamps[node_name] = time.time()

        # C2 fix: Sync peer snapshot after Serf member leave updates
        if members:
            self._sync_peer_snapshot()

    async def _handle_serf_member_failed(self, members: list) -> None:
        """Handle Serf member-failed events (SWIM failure detection).

        SWIM's failure detection is more reliable than our custom ping/pong
        because it uses indirect probing through multiple peers.
        """
        for member in members:
            node_name = member.get("name", "")
            addr = member.get("addr", "")

            if not node_name or node_name == self.node_id:
                continue

            logger.warning(f"Serf: member FAILED (SWIM detected): {node_name} @ {addr}")

            if node_name in self.peers:
                peer = self.peers[node_name]
                if isinstance(peer, NodeInfo):
                    # Mark with consecutive failures (triggers dead/suspect state)
                    peer.consecutive_failures += 1
                    peer.last_failure_time = time.time()

            # If the failed node was leader, trigger election
            if node_name == self.leader_id:
                logger.warning(f"Leader {node_name} failed (Serf detected) - triggering election")
                # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                self._set_leader(None, reason="serf_leader_failed", save_state=True)
                self.election_in_progress = False  # Allow new election

        # C2 fix: Sync peer snapshot after Serf member failed updates
        if members:
            self._sync_peer_snapshot()

    async def _handle_serf_member_update(self, members: list) -> None:
        """Handle Serf member-update events (tag changes)."""
        for member in members:
            node_name = member.get("name", "")
            tags = member.get("tags", {})

            if not node_name or node_name == self.node_id:
                continue

            logger.info(f"Serf: member updated: {node_name}")

            if node_name in self.peers:
                peer = self.peers[node_name]
                if isinstance(peer, NodeInfo):
                    peer.last_heartbeat = time.time()
                    # Tags can update capabilities if structured appropriately
                    if "capabilities" in tags and isinstance(tags["capabilities"], list):
                        peer.capabilities = tags["capabilities"]

    async def _handle_serf_member_reap(self, members: list) -> None:
        """Handle Serf member-reap events (failed nodes removed from list)."""
        for member in members:
            node_name = member.get("name", "")

            if not node_name or node_name == self.node_id:
                continue

            logger.info(f"Serf: member reaped (final cleanup): {node_name}")

            # Mark as retired (reaped means permanently gone)
            if node_name in self.peers:
                peer = self.peers[node_name]
                if isinstance(peer, NodeInfo):
                    peer.retired = True
                    peer.retired_at = time.time()
                    # Jan 20, 2026: Use adaptive cooldown manager
                    if self._cooldown_manager:
                        self._cooldown_manager.record_death(node_name)
                    else:
                        self._dead_peer_timestamps[node_name] = time.time()

    async def _handle_serf_user_event(self, payload: dict) -> None:
        """Handle Serf user events (custom RingRift events).

        User events include:
        - training-complete: Training job finished
        - model-promoted: Model was promoted to canonical
        - selfplay-started: Selfplay jobs started on a node
        - node-status: Periodic node status broadcast
        """
        event_name = payload.get("name", "")
        event_payload = payload.get("payload", {})
        ltime = payload.get("ltime", "0")

        logger.info(f"Serf user event: {event_name} (ltime={ltime})")

        if event_name == "training-complete":
            config_key = event_payload.get("config_key", "")
            model_path = event_payload.get("model_path", "")
            metrics = event_payload.get("metrics", {})
            logger.info(f"Training complete via Serf: {config_key} -> {model_path}")
            # Could trigger evaluation here

        elif event_name == "model-promoted":
            config_key = event_payload.get("config_key", "")
            model_path = event_payload.get("model_path", "")
            elo_gain = event_payload.get("elo_gain", 0)
            logger.info(f"Model promoted via Serf: {config_key} (+{elo_gain} Elo)")
            # Could trigger model distribution here

        elif event_name == "selfplay-started":
            node = event_payload.get("node", "")
            config_key = event_payload.get("config_key", "")
            job_count = event_payload.get("job_count", 1)
            logger.info(f"Selfplay started via Serf: {node} running {config_key} x{job_count}")

        elif event_name == "node-status":
            # Status updates from nodes - could merge with gossip state
            node_id = event_payload.get("node_id", "")
            if node_id and node_id in self.peers:
                # Update peer with status info
                status_fields = ["gpu_util", "gpu_mem_used", "cpu_percent", "memory_percent"]
                for field in status_fields:
                    if field in event_payload:
                        self.peers[node_id][field] = event_payload[field]

    # ============================================================
    # SWIM Native Integration - swim-p2p membership status
    # ============================================================

    async def handle_swim_status(self, request: web.Request) -> web.Response:
        """GET /swim/status - Return SWIM membership protocol status.

        SWIM provides leaderless gossip-based membership with:
        - O(1) bandwidth per node (constant message complexity)
        - <5 second failure detection (vs 60+ seconds with heartbeats)
        - Suspicion mechanism to reduce false positives
        """
        try:
            if not self._swim_manager:
                return web.json_response({
                    "status": "disabled",
                    "reason": "swim-p2p not installed or SWIM adapter not available",
                    "node_id": self.node_id,
                    "fallback": "http_heartbeats",
                })

            summary = self._swim_manager.get_membership_summary()
            alive_peers = self._swim_manager.get_alive_peers() if self._swim_started else []

            return web.json_response({
                "status": "enabled" if self._swim_started else "initialized",
                "node_id": self.node_id,
                "swim": summary,
                "alive_peers": alive_peers,
                "peer_count": len(alive_peers),
            })

        except Exception as e:  # noqa: BLE001
            logger.error(f"Error getting SWIM status: {e}")
            return web.json_response({
                "status": "error",
                "error": str(e),
                "node_id": self.node_id,
            }, status=500)

    async def _swim_membership_loop(self) -> None:
        """Background task that integrates SWIM membership with P2P peer tracking.

        This task:
        1. Starts the SWIM manager if available
        2. Periodically syncs SWIM membership with our peers dict
        3. Uses SWIM failure detection to mark peers as failed faster
        """
        if not self._swim_manager:
            logger.info("SWIM membership loop: disabled (swim-p2p not available)")
            return

        try:
            # Start SWIM manager
            started = await self._swim_manager.start()
            if not started:
                logger.warning("SWIM membership loop: failed to start SWIM manager")
                return

            self._swim_started = True
            logger.info("SWIM membership loop: started successfully")

            # Sync SWIM membership with our peer tracking every 10 seconds
            while self.running:
                try:
                    alive_peers = self._swim_manager.get_alive_peers()

                    # Update peers from SWIM detection
                    now = time.time()
                    with self.peers_lock:
                        for peer_id in alive_peers:
                            # Jan 7, 2026 Session 17.43: Filter SWIM protocol entries (IP:7947 format)
                            # SWIM peer IDs like "100.126.21.102:7947" should NOT be added to self.peers
                            # They pollute VoterHealth, Elo sync, and other peer iteration points
                            # Proper peers are discovered via HTTP gossip with node names or IP:8770
                            if ":" in peer_id:
                                _, port_str = peer_id.rsplit(":", 1)
                                if port_str == "7947":
                                    # Skip SWIM-format peer IDs - they'll be discovered via HTTP gossip
                                    continue

                            if peer_id not in self.peers:
                                # New peer detected by SWIM - convert to HTTP format
                                # peer_id format is typically "host:port" from SWIM
                                host, port_str = (peer_id.rsplit(":", 1) + ["8770"])[:2] if ":" in peer_id else (peer_id, "8770")
                                try:
                                    port_int = int(port_str)
                                except ValueError:
                                    port_int = 8770  # Use P2P port, not SWIM port
                                # SWIM detection creates a minimal NodeInfo; HTTP handshake fills details
                                self.peers[peer_id] = NodeInfo(
                                    node_id=peer_id,
                                    host=host or "unknown",
                                    port=8770,  # P2P API port (SWIM uses 7947)
                                    last_heartbeat=now,
                                )
                            else:
                                # Update existing peer's heartbeat
                                peer = self.peers[peer_id]
                                if isinstance(peer, NodeInfo):
                                    peer.last_heartbeat = now

                except Exception as e:  # noqa: BLE001
                    logger.warning(f"SWIM sync error: {e}")

                # Dec 30, 2025: Try deferred Raft initialization after peers discovered
                # Raft needs peer addresses which aren't available at startup
                try:
                    from scripts.p2p.constants import RAFT_ENABLED
                    if (
                        RAFT_ENABLED
                        and not getattr(self, "_raft_initialized", False)
                        and hasattr(self, "try_deferred_raft_init")
                    ):
                        self.try_deferred_raft_init()
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Deferred Raft init attempt: {e}")

                await asyncio.sleep(10)  # Sync every 10 seconds

        except asyncio.CancelledError:
            logger.info("SWIM membership loop: cancelled")
            raise
        except Exception as e:  # noqa: BLE001
            logger.error(f"SWIM membership loop error: {e}", exc_info=True)
        finally:
            if self._swim_manager and self._swim_started:
                try:
                    await self._swim_manager.stop()
                except Exception as e:  # noqa: BLE001
                    logger.warning(f"Error stopping SWIM manager: {e}")
                self._swim_started = False

    async def handle_coordinator(self, request: web.Request) -> web.Response:
        """Handle coordinator announcement from new leader.

        LEARNED LESSONS - Only accept leadership from higher-priority nodes (Bully algorithm).
        Also handles lease-based leadership updates.
        """
        try:
            # Jan 23, 2026: Use async version to avoid blocking event loop
            await self._update_self_info_async()
            data = await request.json()
            new_leader_raw = data.get("leader_id")
            if not new_leader_raw:
                return web.json_response(
                    {"accepted": False, "reason": "missing_leader_id"},
                    status=400,
                )
            new_leader = str(new_leader_raw)
            lease_id = data.get("lease_id", "")
            lease_expires = data.get("lease_expires", 0)
            is_renewal = data.get("lease_renewal", False)
            incoming_voters = data.get("voter_node_ids") or data.get("voters") or None
            if incoming_voters:
                voters_list: list[str] = []
                if isinstance(incoming_voters, list):
                    voters_list = [str(v).strip() for v in incoming_voters if str(v).strip()]
                elif isinstance(incoming_voters, str):
                    voters_list = [t.strip() for t in incoming_voters.split(",") if t.strip()]
                if voters_list:
                    if self.quorum_manager.maybe_adopt_voter_node_ids(voters_list, source="learned"):
                        # Sync adopted state back to orchestrator attributes
                        self.voter_node_ids = self.quorum_manager.voter_node_ids
                        self.voter_config_source = self.quorum_manager.voter_config_source
                        self.voter_quorum_size = min(VOTER_MIN_QUORUM, len(self.voter_node_ids)) if self.voter_node_ids else 0

            voters = list(getattr(self, "voter_node_ids", []) or [])
            if voters and new_leader not in voters:
                return web.json_response(
                    {"accepted": False, "reason": "leader_not_voter", "voters": voters},
                    status=403,
                )

            # Voter-side safety: if we've granted a still-valid lease to a different leader,
            # do not accept a conflicting coordinator announcement. This prevents a voter
            # from "following" a non-quorum leader during transient partitions.
            if voters and self.node_id in voters:
                grant_leader = str(getattr(self, "voter_grant_leader_id", "") or "")
                grant_expires = float(getattr(self, "voter_grant_expires", 0.0) or 0.0)
                if grant_leader and grant_expires > time.time() and grant_leader != new_leader:
                    return web.json_response(
                        {
                            "accepted": False,
                            "reason": "voter_lease_conflict",
                            "granted_to": grant_leader,
                            "granted_until": grant_expires,
                        },
                        status=409,
                    )

            # If quorum gating is not configured, fall back to bully ordering
            # (lexicographically highest node_id wins).
            if not voters and self.role == NodeRole.LEADER and new_leader < self.node_id:
                # Exception: accept if our lease has expired
                if self.leader_lease_expires > 0 and time.time() >= self.leader_lease_expires:
                    logger.info(f"Our lease expired, accepting leader: {new_leader}")
                else:
                    logger.info(f"Rejecting leader announcement from lower-priority node: {new_leader} < {self.node_id}")
                    return web.json_response({"accepted": False, "reason": "lower_priority"})

            # Reject leadership from nodes that are not directly reachable / uniquely addressable.
            if new_leader != self.node_id:
                with self.peers_lock:
                    peer = self.peers.get(new_leader)
                    peers_snapshot = [p for p in self.peers.values() if p.node_id != self.node_id]
                if peer:
                    conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])
                    if not self._is_leader_eligible(peer, conflict_keys, require_alive=False):
                        return web.json_response({"accepted": False, "reason": "leader_ineligible"})

            if is_renewal and new_leader == self.leader_id:
                self.leader_lease_expires = lease_expires
                self.leader_lease_id = lease_id
                return web.json_response({"accepted": True})

            logger.info(f"Accepting leader announcement: {new_leader}")
            # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
            self._set_leader(new_leader, reason="accept_coordinator_announcement", save_state=True)
            self.leader_lease_id = lease_id
            self.leader_lease_expires = lease_expires if lease_expires else time.time() + LEADER_LEASE_DURATION

            return web.json_response({"accepted": True})
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=400)

    # -------------------------------------------------------------------------
    # Job Management Handlers - EXTRACTED to scripts/p2p/handlers/jobs_api.py
    # January 2026 - P2P Modularization Phase 7a
    # Provides: handle_start_job, handle_stop_job, handle_job_kill,
    #           handle_cleanup, handle_restart_stuck_jobs, handle_cleanup_files
    # See JobsApiHandlersMixin
    # -------------------------------------------------------------------------

    # NOTE: Peer admin handlers (handle_purge_retired_peers, handle_purge_stale_peers,
    # handle_admin_unretire, handle_admin_reset_node_jobs) moved to AdminHandlersMixin
    # in scripts/p2p/handlers/admin.py (Dec 28, 2025)

    async def handle_process_kill(self, request: web.Request) -> web.Response:
        """Kill processes matching a pattern on this node.

        Jan 21, 2026: Added to fix zombie process accumulation.
        This endpoint enables remote cleanup of stuck/zombie processes.

        Request JSON:
            pattern: str - Process pattern to match (e.g., "selfplay", "gpu_selfplay")
            signal: str - Signal to send (default: "SIGKILL")

        Returns:
            JSON with killed count and details
        """
        import shutil

        try:
            data = await request.json()
            pattern = data.get("pattern", "")
            signal_name = data.get("signal", "SIGKILL").upper()

            if not pattern:
                return web.json_response(
                    {"error": "missing pattern", "killed": 0},
                    status=400,
                )

            # Validate signal
            signal_map = {
                "SIGTERM": "-15",
                "SIGKILL": "-9",
                "SIGHUP": "-1",
            }
            signal_flag = signal_map.get(signal_name, "-9")

            # Safety: only allow certain patterns to prevent accidents
            allowed_patterns = [
                "selfplay", "gpu_selfplay", "gumbel", "train", "gauntlet",
                "tournament", "export", "run_self_play", "run_gpu_selfplay",
                "run_hybrid_selfplay", "policy_only", "nnue",
            ]
            if not any(allowed in pattern.lower() for allowed in allowed_patterns):
                return web.json_response(
                    {"error": f"pattern not allowed: {pattern}", "killed": 0},
                    status=403,
                )

            if not shutil.which("pgrep") or not shutil.which("pkill"):
                return web.json_response(
                    {"error": "pgrep/pkill not available", "killed": 0},
                    status=500,
                )

            # Count matching processes first
            try:
                pgrep_result = await asyncio.to_thread(
                    subprocess.run,
                    ["pgrep", "-f", pattern],
                    capture_output=True,
                    text=True,
                    timeout=10,
                )
                if pgrep_result.returncode == 0 and pgrep_result.stdout.strip():
                    pids = [p.strip() for p in pgrep_result.stdout.strip().split() if p.strip()]
                    pid_count = len(pids)
                else:
                    pid_count = 0
                    pids = []
            except subprocess.TimeoutExpired:
                return web.json_response(
                    {"error": "pgrep timeout", "killed": 0},
                    status=500,
                )

            if pid_count == 0:
                return web.json_response({
                    "killed": 0,
                    "pattern": pattern,
                    "message": "no matching processes",
                })

            # Kill matching processes
            try:
                pkill_result = await asyncio.to_thread(
                    subprocess.run,
                    ["pkill", signal_flag, "-f", pattern],
                    capture_output=True,
                    text=True,
                    timeout=15,
                )
                # pkill returns 0 if processes were killed, 1 if none matched
                killed = pid_count if pkill_result.returncode == 0 else 0
            except subprocess.TimeoutExpired:
                return web.json_response(
                    {"error": "pkill timeout", "killed": 0},
                    status=500,
                )

            logger.info(
                f"[ProcessKill] Killed {killed} processes matching '{pattern}' "
                f"with {signal_name} (pids: {pids[:10]}{'...' if len(pids) > 10 else ''})"
            )

            return web.json_response({
                "killed": killed,
                "pattern": pattern,
                "signal": signal_name,
                "pids": pids[:20],  # Limit returned PIDs
            })

        except json.JSONDecodeError:
            return web.json_response(
                {"error": "invalid JSON", "killed": 0},
                status=400,
            )
        except Exception as e:  # noqa: BLE001
            logger.error(f"[ProcessKill] Error: {e}")
            return web.json_response(
                {"error": str(e), "killed": 0},
                status=500,
            )

    async def handle_training_sync(self, request: web.Request) -> web.Response:
        """Manually trigger sync of selfplay data to training nodes.

        Leader-only: Syncs selfplay data to the top GPU nodes for training.
        """
        try:
            result = await self._sync_selfplay_to_training_nodes()
            return web.json_response(result)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=500)

    # -------------------------------------------------------------------------
    # Cluster Node Handlers - EXTRACTED to scripts/p2p/handlers/cluster_nodes.py
    # January 2026 - P2P Modularization Phase 7b
    # Provides: handle_gpu_rankings, handle_probe_vast_nodes
    # See ClusterNodeHandlersMixin
    # -------------------------------------------------------------------------

    # -------------------------------------------------------------------------
    # Status & Health Handlers - EXTRACTED to scripts/p2p/handlers/status.py
    # January 2026 - P2P Modularization Phase 6a
    # Provides: handle_health, handle_game_counts, handle_refresh_game_counts,
    #           handle_loop_restart, handle_restart_stopped_loops, handle_loops_status,
    #           handle_circuit_breaker_status, handle_transport_stats, handle_dispatch_stats
    # See StatusHandlersMixin
    # -------------------------------------------------------------------------


    # Gauntlet Handlers moved to scripts/p2p/handlers/gauntlet.py
    # Inherited from GauntletHandlersMixin: handle_gauntlet_execute, handle_gauntlet_status,
    # handle_gauntlet_quick_eval, _execute_gauntlet_batch, _execute_single_gauntlet_game,
    # _run_gauntlet_game_sync

    # Admin/Git Handlers moved to scripts/p2p/handlers/admin.py
    # Inherited from AdminHandlersMixin: handle_git_status, handle_git_update, handle_admin_restart

    # Manifest handlers moved to scripts/p2p/handlers/manifest.py (Dec 28, 2025 - Phase 8)
    # Inherited from ManifestHandlersMixin: handle_data_manifest, handle_cluster_data_manifest, handle_refresh_manifest

    # CMA-ES Handlers moved to scripts/p2p/handlers/cmaes.py
    # Inherited from CMAESHandlersMixin:
    # - handle_cmaes_start, handle_cmaes_evaluate
    # - handle_cmaes_status, handle_cmaes_result

    async def _run_distributed_cmaes(self, job_id: str):
        """Main coordinator loop for distributed CMA-ES. Delegates to CMAESCoordinator."""
        await self.cmaes_coordinator.run_distributed_cmaes(job_id)

    # NOTE: _evaluate_cmaes_weights_local removed Jan 28, 2026 (dead code)
    # Use self.cmaes_coordinator.evaluate_weights_local() directly.

    async def _evaluate_cmaes_weights(
        self, job_id: str, weights: dict, generation: int, individual_idx: int,
        games_per_eval: int = 5, board_type: str = "square8", num_players: int = 2
    ):
        """Evaluate weights locally and report result to coordinator. Delegates to CMAESCoordinator."""
        await self.cmaes_coordinator.evaluate_weights(
            job_id, weights, generation, individual_idx, games_per_eval, board_type, num_players
        )

    # Tournament Handlers moved to scripts/p2p/handlers/tournament.py
    # Inherited from TournamentHandlersMixin:
    # - handle_tournament_start, handle_tournament_match
    # - handle_tournament_status, handle_tournament_result

    # -------------------------------------------------------------------------
    # Evaluation Play Handlers - EXTRACTED to scripts/p2p/handlers/evaluation_play.py
    # January 2026 - P2P Modularization Phase 5a
    # Provides: handle_play_elo_match, _play_elo_match_sync, _save_tournament_game_for_training
    # See EvaluationPlayHandlersMixin
    # -------------------------------------------------------------------------

    # -------------------------------------------------------------------------
    # SSH Tournament Handlers - EXTRACTED to scripts/p2p/handlers/ssh_tournament.py
    # Provides: handle_ssh_tournament_start, handle_ssh_tournament_status,
    #           handle_ssh_tournament_cancel, _monitor_ssh_tournament_process
    # See SSHTournamentHandlersMixin
    # -------------------------------------------------------------------------

    # NOTE: _run_distributed_tournament() removed Dec 27, 2025 (~9 LOC)
    # Use self.job_manager.run_distributed_tournament() directly.

    # NOTE: _send_match_to_worker removed Jan 28, 2026 (dead code, never called)

    async def _play_tournament_match(self, job_id: str, match_info: dict) -> dict | None:
        """Play a tournament match locally.

        Jan 2026: Delegated to TournamentManager (Phase 11 decomposition).
        """
        return await self.tournament_manager.play_tournament_match(job_id, match_info)

    # NOTE: _calculate_tournament_ratings removed Dec 27, 2025 (dead code, never called)
    # Elo rating calculation is now handled in JobManager.run_distributed_tournament()

    # =========================================================================
    # NOTE: Improvement handlers moved to scripts/p2p/handlers/improvement.py (Dec 28, 2025 - Phase 8)
    # Inherited from ImprovementHandlersMixin:
    # - handle_improvement_start, handle_improvement_status, handle_improvement_phase_complete
    # - handle_improvement_cycles_status, handle_improvement_cycles_leaderboard
    # - handle_improvement_training_complete, handle_improvement_evaluation_complete
    # =========================================================================

    # =========================================================================
    # NOTE: Sync handlers (handle_sync_*) extracted to SyncHandlersMixin
    # See: scripts/p2p/handlers/sync.py (Dec 28, 2025 - Phase 8)
    # Removed: handle_sync_start, handle_sync_status, handle_sync_push,
    #          handle_sync_receipt, handle_sync_receipts_status, handle_sync_pull,
    #          handle_sync_file, handle_sync_job_update (~625 LOC)
    # =========================================================================

    # -------------------------------------------------------------------------
    # Event Management Handlers - EXTRACTED to scripts/p2p/handlers/event_management.py
    # January 2026 - P2P Modularization Phase 5b
    # Provides: handle_subscriptions
    # See EventManagementHandlersMixin
    # -------------------------------------------------------------------------

    async def _run_improvement_loop(self, job_id: str):
        """Main coordinator loop for AlphaZero-style improvement."""
        try:
            state = self.improvement_loop_state.get(job_id)
            if not state:
                return

            logger.info(f"Improvement loop coordinator started for job {job_id}")

            while state.current_iteration < state.max_iterations and state.status == "running":
                state.current_iteration += 1
                logger.info(f"Improvement iteration {state.current_iteration}/{state.max_iterations}")

                # Phase 1: Selfplay
                state.phase = "selfplay"
                state.selfplay_progress = {}
                await self.job_manager.run_distributed_selfplay(job_id)

                # Phase 2: Export training data
                state.phase = "export"
                await self.job_manager.export_training_data(job_id)

                # Phase 3: Training
                state.phase = "train"
                await self.job_manager.run_training(job_id)

                # Phase 4: Evaluation
                state.phase = "evaluate"
                await self._run_evaluation(job_id)

                # Phase 5: Promote if better
                state.phase = "promote"
                await self._promote_model_if_better(job_id)

                state.last_update = time.time()

            state.status = "completed"
            state.phase = "idle"
            logger.info(f"Improvement loop {job_id} completed after {state.current_iteration} iterations")

        except Exception as e:  # noqa: BLE001
            logger.info(f"Improvement loop error: {e}")
            if job_id in self.improvement_loop_state:
                self.improvement_loop_state[job_id].status = f"error: {e}"

    # Dec 2025: Training methods delegated to job_manager and training_coordinator

    async def _check_and_trigger_training(self):
        """Periodic check for training readiness (leader only)."""
        if self.role != NodeRole.LEADER:
            return

        # Phase 2.4 (Dec 29, 2025): Skip training dispatch in partition readonly mode
        if self.is_partition_readonly():
            logger.debug("[P2P] Skipping training check: partition readonly mode")
            return

        current_time = time.time()
        if current_time - self.last_training_check < self.training_check_interval:
            return

        self.last_training_check = current_time

        # Get jobs that should be started (delegated to TrainingCoordinator manager)
        jobs_to_start = self.training_coordinator.check_training_readiness()

        for job_config in jobs_to_start:
            # PHASE 4 IDEMPOTENCY: Check for duplicate triggers
            config_key = job_config.get("config_key", "")
            game_count = job_config.get("total_games", 0)
            can_proceed, trigger_hash = self._check_training_idempotency(config_key, game_count)
            if not can_proceed:
                continue

            logger.info(f"Auto-triggering {job_config['job_type']} training for {config_key} ({game_count} games)")
            await self.training_coordinator.dispatch_training_job(job_config)
            self._record_training_trigger(trigger_hash)  # Record after successful dispatch

    async def _check_local_training_fallback(self):
        """DECENTRALIZED training trigger when cluster has no leader.

        LEADERLESS RESILIENCE: When the cluster has been without a leader for too long
        (LEADERLESS_TRAINING_TIMEOUT = 3 minutes), individual nodes can trigger local
        training to prevent data accumulation without progress.

        This makes the system more resilient to leader election failures while avoiding
        duplicate training by:
        1. Only triggering after a brief leaderless period (3 minutes)
        2. Using random jitter so nodes don't all train simultaneously
        3. Only training on local data (no cluster-wide coordination needed)
        4. Using reasonable cooldowns between fallback training runs
        """
        # Skip if we ARE the leader or have a known leader
        if self.role == NodeRole.LEADER or self.leader_id:
            self.last_leader_seen = time.time()  # Update leader seen time
            return

        current_time = time.time()
        leaderless_duration = current_time - self.last_leader_seen

        # Only trigger fallback if leaderless for the timeout period
        if leaderless_duration < LEADERLESS_TRAINING_TIMEOUT:
            return

        # Rate limit fallback training (10 minute cooldown - more aggressive than before)
        fallback_cooldown = 600  # 10 minutes between fallback triggers
        if current_time - self.last_local_training_fallback < fallback_cooldown:
            return

        # Random jitter: 40% probability per check (more aggressive than 20%)
        # This distributes training across nodes over time
        import random
        if random.random() > 0.4:
            return

        # Check if we have a GPU (training needs GPU)
        if not getattr(self.self_info, "has_gpu", False):
            return

        # Check local data manifest (use cached version for speed)
        local_manifest = getattr(self, "local_data_manifest", None)
        if not local_manifest:
            # Try to load from cache or collect if we don't have one
            try:
                # Jan 23, 2026: Wrap in asyncio.to_thread() to prevent event loop blocking
                # collect_local_manifest_cached() does file I/O and SQLite operations
                local_manifest = await asyncio.to_thread(
                    self.sync_planner.collect_local_manifest_cached, max_cache_age=600
                )
                with self.manifest_lock:
                    self.local_data_manifest = local_manifest
            except (AttributeError):
                return

        # Check for sufficient local data (lower threshold for faster training)
        min_games_fallback = 2000  # Lower threshold for faster response
        total_local_games = getattr(local_manifest, "selfplay_games", 0)
        if total_local_games < min_games_fallback:
            return

        # Find board types with enough local data
        game_counts_by_type: dict[str, int] = {}
        for file_info in getattr(local_manifest, "files", []) or []:
            board_type = getattr(file_info, "board_type", "")
            num_players = getattr(file_info, "num_players", 2)
            game_count = getattr(file_info, "game_count", 0)
            if board_type and game_count > 0:
                key = f"{board_type}_{num_players}p"
                game_counts_by_type[key] = game_counts_by_type.get(key, 0) + game_count

        # Sort by game count (descending) to train on richest data first
        sorted_configs = sorted(game_counts_by_type.items(), key=lambda x: x[1], reverse=True)

        # Trigger local training for configurations with enough data
        triggered_count = 0
        max_concurrent_fallback = 2  # Can trigger up to 2 training jobs per fallback
        for config_key, game_count in sorted_configs:
            if triggered_count >= max_concurrent_fallback:
                break
            if game_count < 1000:  # Minimum threshold (lowered)
                continue

            # Check if we already have a running training job for this config
            existing_job = self.training_coordinator.find_running_training_job("nnue", config_key)
            if existing_job:
                continue

            # DISTRIBUTED TRAINING COORDINATION: Check cluster-wide before starting
            is_training, _training_nodes = self._is_config_being_trained_cluster_wide(config_key)
            if is_training:
                # Someone else is already training this config
                continue

            # Use distributed slot claiming to avoid race conditions
            if not self._should_claim_training_slot(config_key):
                continue

            # Parse board type and player count
            parts = config_key.split("_")
            if len(parts) < 2:
                continue
            board_type = parts[0]
            num_players = int(parts[1].replace("p", ""))

            # PHASE 4 IDEMPOTENCY: Check for duplicate triggers
            can_proceed, trigger_hash = self._check_training_idempotency(config_key, game_count)
            if not can_proceed:
                continue

            logger.info(f"DISTRIBUTED TRAINING: Claiming {config_key} ({game_count} local games, leaderless for {int(leaderless_duration)}s)")
            job_config = {
                "job_type": "nnue",
                "board_type": board_type,
                "num_players": num_players,
                "config_key": config_key,
                "total_games": game_count,
            }
            await self.training_coordinator.dispatch_training_job(job_config)
            self._record_training_trigger(trigger_hash)  # Record after successful dispatch
            triggered_count += 1

        if triggered_count > 0:
            self.last_local_training_fallback = current_time
            logger.info(f"LEADERLESS FALLBACK: Triggered {triggered_count} local training job(s)")

    async def _check_improvement_cycles(self):
        """Periodic check for improvement cycle readiness (leader only).

        This integrates with the ImprovementCycleManager to:
        1. Check if any cycles need training based on data thresholds
        2. Trigger export/training jobs for ready cycles
        3. Run evaluations and update Elo ratings
        4. Schedule CMA-ES optimization when needed
        5. Schedule diverse tournaments for AI calibration
        """
        if self.role != NodeRole.LEADER:
            return

        if not self.improvement_cycle_manager:
            return

        current_time = time.time()
        if current_time - self.last_improvement_cycle_check < self.improvement_cycle_check_interval:
            return

        self.last_improvement_cycle_check = current_time

        # Check which cycles are ready for training
        training_ready = self.improvement_cycle_manager.check_training_needed()

        # Convert to job configs
        jobs_to_start = []
        for board_type, num_players in training_ready:
            cycle_key = f"{board_type}_{num_players}p"
            cycle_state = self.improvement_cycle_manager.state.cycles.get(cycle_key)
            if cycle_state and self.improvement_cycle_manager.trigger_training(board_type, num_players):
                jobs_to_start.append({
                    "cycle_id": cycle_key,
                    "board_type": board_type,
                    "num_players": num_players,
                    "total_games": cycle_state.games_since_last_training,
                    "iteration": cycle_state.current_iteration + 1,
                })

        # Also check for CMA-ES optimization opportunities
        cmaes_ready = self.improvement_cycle_manager.check_cmaes_needed()
        for board_type, num_players in cmaes_ready:
            # Trigger distributed CMA-ES
            logger.info(f"CMA-ES optimization ready for {board_type}_{num_players}p")
            asyncio.create_task(self._trigger_auto_cmaes(board_type, num_players))

        # Check for rollback needs (consecutive training failures)
        for key, cycle in self.improvement_cycle_manager.state.cycles.items():
            if not cycle.pending_training and not cycle.pending_evaluation:
                should_rollback, reason = self.improvement_cycle_manager.check_rollback_needed(
                    cycle.board_type, cycle.num_players
                )
                if should_rollback:
                    logger.info(f"ROLLBACK NEEDED for {key}: {reason}")
                    if self.improvement_cycle_manager.execute_rollback(cycle.board_type, cycle.num_players):
                        self.diversity_metrics["rollbacks"] += 1
                        # Increase diversity to escape plateau
                        logger.info(f"Increasing diversity to escape training plateau for {key}")

        for job_config in jobs_to_start:
            cycle_id = job_config["cycle_id"]
            board_type = job_config["board_type"]
            num_players = job_config["num_players"]

            logger.info(f"ImprovementCycle {cycle_id}: Starting training "
                  f"({job_config['total_games']} games)")

            # Find GPU worker for training
            gpu_worker = None
            candidates: list[NodeInfo] = []
            with self.peers_lock:
                candidates.extend([p for p in self.peers.values() if p.is_gpu_node() and p.is_healthy()])
            if self.self_info.is_gpu_node() and self.self_info.is_healthy():
                candidates.append(self.self_info)
            if candidates:
                candidates.sort(
                    key=lambda p: (-p.gpu_power_score(), p.get_load_score(), str(p.node_id))
                )
                gpu_worker = candidates[0]

            if not gpu_worker:
                logger.info(f"ImprovementCycle {cycle_id}: No GPU worker available, deferring")
                self.improvement_cycle_manager.update_cycle_phase(
                    cycle_id, "idle", error_message="No GPU worker available"
                )
                continue

            # Create training job
            job_id = f"cycle_{cycle_id}_{int(time.time())}"
            training_job = TrainingJob(
                job_id=job_id,
                job_type="nnue",
                board_type=board_type,
                num_players=num_players,
                worker_node=gpu_worker.node_id,
                epochs=job_config.get("epochs", 100),
                batch_size=job_config.get("batch_size", 4096),
                learning_rate=job_config.get("learning_rate", 0.001),
                data_games_count=job_config.get("total_games", 0),
            )

            with self.training_lock:
                self.training_jobs[job_id] = training_job

            # Update cycle state
            self.improvement_cycle_manager.update_cycle_phase(
                cycle_id, "training", training_job_id=job_id
            )

            # Dispatch training to worker
            await self._dispatch_improvement_training(training_job, cycle_id)

    async def _dispatch_improvement_training(self, job: TrainingJob, cycle_id: str):
        """Dispatch training job for improvement cycle."""
        try:
            # Find the worker node
            worker_node = None
            if job.worker_node == self.node_id:
                worker_node = self.self_info
            else:
                with self.peers_lock:
                    worker_node = self.peers.get(job.worker_node)

            if not worker_node:
                logger.info(f"ImprovementCycle {cycle_id}: Worker {job.worker_node} not found")
                self.improvement_cycle_manager.update_cycle_phase(
                    cycle_id, "idle", error_message=f"Worker {job.worker_node} not found"
                )
                return

            # Build training payload
            payload = {
                "job_id": job.job_id,
                "cycle_id": cycle_id,
                "board_type": job.board_type,
                "num_players": job.num_players,
                "epochs": job.epochs,
                "batch_size": job.batch_size,
                "learning_rate": job.learning_rate,
            }

            # Send to worker
            timeout = ClientTimeout(total=30)
            async with get_client_session(timeout) as session:
                last_err: str | None = None
                for url in self._urls_for_peer(worker_node, "/training/nnue/start"):
                    try:
                        async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                            if resp.status != 200:
                                last_err = f"http_{resp.status}"
                                continue
                            result = await resp.json()
                        if result.get("success"):
                            job.status = "running"
                            job.started_at = time.time()
                            logger.info(f"ImprovementCycle {cycle_id}: Training started on {worker_node.node_id}")
                            return
                        self.improvement_cycle_manager.update_cycle_phase(
                            cycle_id, "idle", error_message=result.get("error", "Training failed to start")
                        )
                        return
                    except Exception as e:  # noqa: BLE001
                        last_err = str(e)
                        continue
                self.improvement_cycle_manager.update_cycle_phase(
                    cycle_id, "idle", error_message=last_err or "dispatch_failed"
                )

        except Exception as e:  # noqa: BLE001
            logger.info(f"ImprovementCycle {cycle_id}: Training dispatch failed: {e}")
            self.improvement_cycle_manager.update_cycle_phase(
                cycle_id, "idle", error_message=str(e)
            )

    # NOTE: Training control handlers moved to TrainingControlHandlersMixin (Jan 2026 - P2P Modularization Phase 3a)
    # Includes: handle_training_start, handle_training_status, handle_training_progress, handle_training_update,
    #           handle_training_trigger, handle_training_trigger_decision, handle_training_trigger_configs, handle_nnue_start

    async def _trigger_auto_cmaes(self, board_type: str, num_players: int):
        """Automatically trigger CMA-ES optimization for a configuration. Delegates to CMAESCoordinator."""
        await self.cmaes_coordinator.trigger_auto_cmaes(board_type, num_players)

    async def handle_cmaes_start_auto(self, request: web.Request) -> web.Response:
        """Handle CMA-ES optimization start request.

        Uses distributed GPU CMA-ES across all cluster GPU nodes for maximum throughput.
        Falls back to local GPU CMA-ES if no remote workers available.
        """
        try:
            data = await request.json()
            job_id = data.get("job_id")
            board_type = data.get("board_type", "square8")
            num_players = data.get("num_players", 2)

            # Check for available GPU workers in the cluster
            gpu_workers = []
            with self.peers_lock:
                for peer in self.peers.values():
                    if peer.is_healthy() and peer.has_gpu and peer.node_id != self.node_id:
                        gpu_workers.append(peer)

            # Include self if we have GPU
            if self.self_info.has_gpu:
                gpu_workers.append(self.self_info)

            if len(gpu_workers) >= 2:
                # DISTRIBUTED MODE: Use P2P distributed CMA-ES across cluster
                logger.info(f"Starting DISTRIBUTED GPU CMA-ES with {len(gpu_workers)} workers")

                # Create distributed CMA-ES state
                cmaes_job_id = f"cmaes_auto_{job_id}_{int(time.time())}"
                state = DistributedCMAESState(
                    job_id=cmaes_job_id,
                    board_type=board_type,
                    num_players=num_players,
                    generations=100,  # More generations for better optimization
                    population_size=max(32, len(gpu_workers) * 8),  # Scale with workers
                    games_per_eval=100,  # More games for accurate fitness
                    status="running",
                    started_at=time.time(),
                    last_update=time.time(),
                    worker_nodes=[w.node_id for w in gpu_workers],
                )
                self.distributed_cmaes_state[cmaes_job_id] = state

                # Launch distributed coordinator task
                asyncio.create_task(self._run_distributed_cmaes(cmaes_job_id))

                # Track as training job
                with self.training_lock:
                    if job_id in self.training_jobs:
                        self.training_jobs[job_id].status = "running"
                        self.training_jobs[job_id].started_at = time.time()

                return web.json_response({
                    "success": True,
                    "mode": "distributed",
                    "job_id": cmaes_job_id,
                    "workers": [w.node_id for w in gpu_workers],
                })

            else:
                # LOCAL MODE: Run GPU CMA-ES on this node only
                logger.info("Starting LOCAL GPU CMA-ES (no remote workers available)")

                output_dir = os.path.join(
                    self._get_ai_service_path(), "data", "cmaes",
                    f"{board_type}_{num_players}p_auto_{int(time.time())}"
                )
                os.makedirs(output_dir, exist_ok=True)

                cmd = [
                    sys.executable,
                    os.path.join(self._get_ai_service_path(), "scripts", "run_gpu_cmaes.py"),
                    "--board", board_type,
                    "--num-players", str(num_players),
                    "--generations", "100",
                    "--population-size", "32",
                    "--games-per-eval", "100",
                    "--max-moves", "10000",
                    "--output-dir", output_dir,
                    "--multi-gpu",
                ]

                env = os.environ.copy()
                env["PYTHONPATH"] = self._get_ai_service_path()
                env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

                proc = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                    env=env,
                )

                logger.info(f"Started local GPU CMA-ES (PID {proc.pid}) for job {job_id}")
                asyncio.create_task(self._monitor_training_process(job_id, proc, output_dir))

                return web.json_response({
                    "success": True,
                    "mode": "local",
                    "pid": proc.pid,
                })

        except Exception as e:  # noqa: BLE001
            return web.json_response({"success": False, "error": str(e)})

    def _get_training_timeout(self, job_id: str) -> int:
        """Get dynamic timeout based on job configuration.

        Returns timeout in seconds based on board type and model complexity:
        - square19: 6 hours (large board, 361 cells)
        - hexagonal: 5 hours (469 cells)
        - square8/hex8: 2 hours (small boards)
        Default: 3 hours if job not found
        """
        with self.training_lock:
            job = self.training_jobs.get(job_id)
            if not job:
                return 10800  # 3 hours default

            board_type = getattr(job, 'board_type', 'unknown')
            num_players = getattr(job, 'num_players', 2)

            # Base timeout by board complexity
            if board_type == 'square19':
                base_timeout = 21600  # 6 hours
            elif board_type == 'hexagonal':
                base_timeout = 18000  # 5 hours
            elif board_type in ('hex8', 'square8'):
                base_timeout = 7200   # 2 hours
            else:
                base_timeout = 10800  # 3 hours default

            # Add 50% for 4-player models (larger value head, more complex)
            if num_players == 4:
                base_timeout = int(base_timeout * 1.5)
            elif num_players == 3:
                base_timeout = int(base_timeout * 1.25)

            return base_timeout

    def _get_cached_jittered_timeout(self) -> float:
        """Get jittered peer timeout, cached for 30 seconds.

        Jan 22, 2026: Fix for double jitter application causing desynchronized death detection.

        Problem: get_jittered_peer_timeout() was called at two locations (partition detection
        and peer reconnection) with different jitter each time. This caused nodes to mark
        the same peer dead at different times (10% variance = 24s difference for 120s timeout).

        Solution: Cache the jittered timeout for 30 seconds. All death detection checks
        within the same 30s window use the same jittered value, ensuring consistent
        death detection across the codebase.

        Returns:
            Jittered peer timeout in seconds (PEER_TIMEOUT  10%)
        """
        now = time.time()
        if self._jittered_timeout_cache is None or (now - self._jittered_timeout_time) > 30:
            self._jittered_timeout_cache = get_jittered_peer_timeout(PEER_TIMEOUT)
            self._jittered_timeout_time = now
        return self._jittered_timeout_cache

    async def _monitor_training_process(self, job_id: str, proc, output_path: str):
        """Monitor training subprocess and report completion to leader."""
        try:
            timeout = self._get_training_timeout(job_id)
            _stdout, stderr = await asyncio.wait_for(
                proc.communicate(),
                timeout=timeout
            )

            success = proc.returncode == 0

            # Report to leader with retry logic
            if self.leader_id and self.leader_id != self.node_id:
                leader = self.peers.get(self.leader_id)
                if leader:
                    payload = {
                        "job_id": job_id,
                        "completed": success,
                        "output_model_path": output_path if success else "",
                        "error": stderr.decode()[:500] if not success else "",
                    }
                    # Retry with exponential backoff (3 attempts: 5s, 10s, 20s)
                    max_retries = 3
                    base_delay = 5.0
                    for attempt in range(max_retries):
                        try:
                            http_timeout = ClientTimeout(total=30)
                            async with get_client_session(http_timeout) as session:
                                url = self._url_for_peer(leader, "/training/update")
                                resp = await session.post(url, json=payload, headers=self._auth_headers())
                                if resp.status < 400:
                                    logger.info(f"Training completion reported to leader (attempt {attempt + 1})")
                                    break
                                else:
                                    logger.warning(f"Leader returned {resp.status}, retrying...")
                        except Exception as e:  # noqa: BLE001
                            delay = base_delay * (2 ** attempt)
                            if attempt < max_retries - 1:
                                logger.warning(f"Failed to report training completion (attempt {attempt + 1}): {e}, retrying in {delay}s")
                                await asyncio.sleep(delay)
                            else:
                                logger.error(f"Failed to report training completion after {max_retries} attempts: {e}")
            else:
                # We are the leader, update directly
                with self.training_lock:
                    job = self.training_jobs.get(job_id)
                    if job:
                        if success:
                            job.status = "completed"
                            job.completed_at = time.time()
                            job.output_model_path = output_path
                            # LEARNED LESSONS - Schedule tournament to compare new model against baseline
                            asyncio.create_task(self._schedule_model_comparison(job, output_path))
                            # Update improvement cycle manager with training completion
                            if self.improvement_cycle_manager:
                                self.improvement_cycle_manager.handle_training_complete(
                                    job.board_type, job.num_players,
                                    output_path, job.data_games_count or 0
                                )
                            # PFSP: Add trained model to opponent pool for diverse selfplay
                            config_key = f"{job.board_type}_{job.num_players}p"
                            if HAS_PFSP and config_key in self.pfsp_pools:
                                try:
                                    model_id = Path(output_path).stem
                                    self.pfsp_pools[config_key].add_opponent(
                                        model_id=model_id,
                                        model_path=output_path,
                                        elo=INITIAL_ELO_RATING,  # From app.config.thresholds
                                        win_rate=0.5,
                                    )
                                    logger.info(f"[PFSP] Added {model_id} to opponent pool for {config_key}")
                                except Exception as e:  # noqa: BLE001
                                    logger.error(f"[PFSP] Error adding model to pool: {e}")
                            # CMA-ES: Check for Elo plateau and trigger auto-tuning
                            asyncio.create_task(self._check_cmaes_auto_tuning(config_key))
                        else:
                            job.status = "failed"
                            job.error_message = stderr.decode()[:500]
                        job.completed_at = time.time()

            logger.info(f"Training job {job_id} {'completed' if success else 'failed'}")

        except asyncio.TimeoutError:
            logger.info(f"Training job {job_id} timed out")
        except Exception as e:  # noqa: BLE001
            logger.info(f"Training monitor error for {job_id}: {e}")

    async def _monitor_gpu_selfplay_and_validate(
        self,
        job_id: str,
        proc: subprocess.Popen,
        output_dir: Path,
        board_type: str,
        num_players: int,
    ) -> None:
        """Monitor GPU selfplay completion and run CPU validation.

        Jan 28, 2026: Phase 18C - Thin wrapper delegating to JobCoordinationManager.
        """
        if self.job_coordination_manager:
            await self.job_coordination_manager.monitor_gpu_selfplay_and_validate(
                job_id, proc, output_dir, board_type, num_players
            )

    async def _monitor_selfplay_process(
        self,
        job_id: str,
        proc: subprocess.Popen,
        output_dir: Path,
        board_type: str,
        num_players: int,
        job_type_str: str = "selfplay",
    ) -> None:
        """Monitor a selfplay subprocess and update job status on completion.

        Dec 31, 2025: Added to fix missing process monitoring for SELFPLAY
        and CPU_SELFPLAY jobs. Previously, these jobs were spawned but never
        monitored, causing them to remain in "running" status indefinitely.

        This function:
        1. Waits for the subprocess to complete (with 2-hour timeout)
        2. Updates job status to "completed" or "failed"
        3. Logs completion/failure with details
        4. Emits TASK_COMPLETED or TASK_FAILED events for pipeline coordination
        """
        try:
            # Wait for process to complete (with timeout)
            return_code = await asyncio.wait_for(
                asyncio.to_thread(proc.wait),
                timeout=7200,  # 2 hour max
            )

            duration = 0.0
            with self.jobs_lock:
                job = self.local_jobs.get(job_id)
                if job:
                    duration = time.time() - job.started_at
                    if return_code == 0:
                        job.status = "completed"
                        job.completed_at = time.time()
                        logger.info(
                            f"Selfplay job {job_id} completed successfully "
                            f"(duration: {duration:.1f}s)"
                        )
                    else:
                        job.status = "failed"
                        job.completed_at = time.time()
                        # Try to get error message from run.log
                        error_msg = f"exit_code={return_code}"
                        log_file = output_dir / "run.log"
                        if log_file.exists():
                            try:
                                # Get last 500 chars of log for error context
                                content = log_file.read_text(encoding='utf-8', errors='replace')
                                if content:
                                    error_msg = content[-500:].strip()
                            except OSError:
                                pass
                        job.error_message = error_msg
                        logger.warning(
                            f"Selfplay job {job_id} failed (exit code {return_code}): "
                            f"{error_msg[:200]}..."
                        )

            # Emit task events for pipeline coordination
            try:
                from app.coordination.data_events import DataEventType, emit_data_event
                config_key = f"{board_type}_{num_players}p"
                if return_code == 0:
                    emit_data_event(DataEventType.TASK_COMPLETED, {
                        "task_id": job_id,
                        "task_type": job_type_str,
                        "config_key": config_key,
                        "board_type": board_type,
                        "num_players": num_players,
                        "duration_seconds": duration,
                        "node_id": self.node_id,
                    })
                else:
                    emit_data_event(DataEventType.TASK_FAILED, {
                        "task_id": job_id,
                        "task_type": job_type_str,
                        "config_key": config_key,
                        "board_type": board_type,
                        "num_players": num_players,
                        "error": f"exit_code={return_code}",
                        "node_id": self.node_id,
                    })
            except ImportError:
                pass  # Event system not available

        except asyncio.TimeoutError:
            logger.warning(f"Selfplay job {job_id} timed out after 2 hours")
            with self.jobs_lock:
                job = self.local_jobs.get(job_id)
                if job:
                    job.status = "timeout"
                    job.completed_at = time.time()
                    job.error_message = "timeout_2_hours"
            # Kill the process
            try:
                proc.terminate()
                await asyncio.sleep(5)
                if proc.poll() is None:
                    proc.kill()
            except OSError:
                pass

        except Exception as e:  # noqa: BLE001
            logger.error(f"Selfplay process monitor error for {job_id}: {e}")
            with self.jobs_lock:
                job = self.local_jobs.get(job_id)
                if job:
                    job.status = "error"
                    job.completed_at = time.time()
                    job.error_message = str(e)

    async def _schedule_model_comparison(self, job: TrainingJob, new_model_path: str):
        """Schedule a tournament to compare new model against current baseline.

        Jan 2026: Delegated to TournamentManager (Phase 11 decomposition).
        """
        await self.tournament_manager.schedule_model_comparison(job, new_model_path)

    # NOTE: _run_model_comparison_tournament removed Jan 28, 2026 (dead code)
    # Use self.tournament_manager.run_model_comparison_tournament() directly.

    # NOTE: _promote_to_baseline removed Jan 28, 2026 (dead code)
    # Use self.tournament_manager.promote_to_baseline() directly.

    async def _check_cmaes_auto_tuning(self, config_key: str):
        """Check if CMA-ES auto-tuning should be triggered for a config.

        Monitors Elo progression and triggers hyperparameter optimization
        when the model's improvement plateaus.
        """
        if not HAS_PFSP or config_key not in self.cmaes_auto_tuners:
            return

        try:
            # Get current Elo from unified database
            from app.tournament import get_elo_database
            db = get_elo_database()

            parts = config_key.rsplit("_", 1)
            board_type = parts[0]
            num_players = int(parts[1].replace("p", ""))

            # Find best model for this config
            best_model = None
            best_elo = INITIAL_ELO_RATING
            models_dir = Path(self._get_ai_service_path()) / "models" / "nnue"
            pattern = f"nnue_{board_type}_{num_players}p*.pt"

            for model_path in models_dir.glob(pattern):
                model_id = model_path.stem
                elo = db.get_elo(model_id)
                if elo and elo > best_elo:
                    best_elo = elo
                    best_model = model_id

            if not best_model:
                return

            # Check for plateau
            auto_tuner = self.cmaes_auto_tuners[config_key]
            self.last_cmaes_elo.get(config_key, INITIAL_ELO_RATING)

            # Record Elo history for plateau detection
            should_tune = auto_tuner.check_plateau(best_elo)
            self.last_cmaes_elo[config_key] = best_elo

            if should_tune:
                logger.info(f"[CMA-ES] Elo plateau detected for {config_key} (Elo: {best_elo:.0f})")
                logger.info("[CMA-ES] Triggering auto hyperparameter optimization...")

                # Trigger CMA-ES via existing distributed infrastructure
                await self._trigger_auto_cmaes(board_type, num_players)

        except Exception as e:  # noqa: BLE001
            logger.info(f"[CMA-ES] Auto-tuning check error for {config_key}: {e}")

    def get_pfsp_opponent(self, config_key: str) -> str | None:
        """Get a PFSP-sampled opponent model for selfplay.

        Returns path to an opponent model sampled from the PFSP pool,
        weighted by difficulty (harder opponents sampled more frequently).
        """
        if not HAS_PFSP or config_key not in self.pfsp_pools:
            return None

        try:
            pool = self.pfsp_pools[config_key]
            opponent = pool.sample_opponent()
            if opponent:
                return opponent.model_path
        except Exception as e:  # noqa: BLE001
            logger.error(f"[PFSP] Error sampling opponent: {e}")
        return None

    def update_pfsp_stats(self, config_key: str, model_id: str, win_rate: float, elo: float):
        """Update PFSP stats for a model after evaluation games.

        Called after tournament/evaluation to update opponent difficulty metrics.
        """
        if not HAS_PFSP or config_key not in self.pfsp_pools:
            return

        try:
            self.pfsp_pools[config_key].update_stats(model_id, win_rate=win_rate, elo=elo)
            logger.info(f"[PFSP] Updated stats for {model_id}: win_rate={win_rate:.2f}, elo={elo:.0f}")
        except Exception as e:  # noqa: BLE001
            logger.error(f"[PFSP] Error updating stats: {e}")

    # NOTE: _handle_tournament_completion removed Jan 28, 2026 (dead code)
    # Use self.tournament_manager.handle_tournament_completion() directly.

    async def _boost_selfplay_for_config(self, board_type: str, num_players: int):
        """Temporarily boost selfplay for a configuration after model promotion.

        Jan 2026: Delegated to TournamentManager (Phase 11 decomposition).
        """
        await self.tournament_manager.boost_selfplay_for_config(board_type, num_players)

    # NOTE: _propagate_cmaes_weights removed Jan 28, 2026 (dead code)
    # Use self.cmaes_coordinator.propagate_weights() directly.

    async def _stop_local_job(self, job_id: str):
        """Stop a local job by job ID."""
        try:
            with self.jobs_lock:
                job = self.local_jobs.get(job_id)
                if job and hasattr(job, 'process') and job.process:
                    job.process.terminate()
                    job.status = "stopped"
        except Exception as e:  # noqa: BLE001
            logger.error(f"stopping job {job_id}: {e}")

    async def _import_gpu_selfplay_to_canonical(
        self, validated_db: Path, board_type: str, num_players: int, game_count: int
    ):
        """Import validated GPU selfplay games to canonical selfplay database.

        After GPU selfplay games pass CPU validation (>=95% validation rate),
        this merges them into the canonical selfplay database for training.
        """
        try:
            # Determine canonical DB path
            canonical_db = Path(self._get_ai_service_path()) / "data" / "games" / "selfplay.db"
            if not canonical_db.parent.exists():
                canonical_db.parent.mkdir(parents=True, exist_ok=True)

            logger.info(f"Auto-importing {game_count} validated GPU games to canonical DB...")

            # Jan 12, 2026: Wrap blocking SQLite operations in thread to avoid blocking event loop
            imported = await asyncio.to_thread(
                self._import_gpu_selfplay_sync, validated_db, canonical_db
            )

            logger.info(f"Successfully imported {imported} GPU selfplay games to canonical DB")

            # Update cluster data manifest to reflect new games
            config_key = f"{board_type}_{num_players}p"
            if hasattr(self, 'cluster_data_manifest') and self.cluster_data_manifest and config_key in self.cluster_data_manifest.by_board_type:
                self.cluster_data_manifest.by_board_type[config_key]["total_games"] = (
                    self.cluster_data_manifest.by_board_type[config_key].get("total_games", 0) + imported
                )

            # Notify improvement cycle manager of new games
            if self.improvement_cycle_manager and imported > 0:
                self.improvement_cycle_manager.record_games(board_type, num_players, imported)

        except Exception as e:  # noqa: BLE001
            logger.info(f"GPU selfplay import error: {e}")
            import traceback
            traceback.print_exc()

    def _import_gpu_selfplay_sync(self, validated_db: Path, canonical_db: Path) -> int:
        """Synchronous helper for _import_gpu_selfplay_to_canonical().

        Jan 12, 2026: Extracted to allow wrapping in asyncio.to_thread().
        """
        import sqlite3

        # Phase 3.4 Dec 29, 2025: Use context managers to prevent connection leaks
        with safe_db_connection(validated_db) as src_conn, \
             safe_db_connection(canonical_db) as dst_conn:

                # Ensure destination tables exist
                dst_conn.execute("""
                    CREATE TABLE IF NOT EXISTS games (
                        game_id TEXT PRIMARY KEY,
                        board_type TEXT NOT NULL,
                        num_players INTEGER NOT NULL,
                        winner INTEGER,
                        move_count INTEGER,
                        game_time_ms INTEGER,
                        created_at REAL,
                        source TEXT DEFAULT 'selfplay'
                    )
                """)
                dst_conn.execute("""
                    CREATE TABLE IF NOT EXISTS moves (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        game_id TEXT NOT NULL,
                        move_number INTEGER NOT NULL,
                        player INTEGER NOT NULL,
                        move_type TEXT NOT NULL,
                        from_pos TEXT,
                        to_pos TEXT,
                        direction TEXT,
                        captured_pos TEXT,
                        state_before TEXT,
                        policy_probs TEXT,
                        value_est REAL,
                        FOREIGN KEY (game_id) REFERENCES games(game_id)
                    )
                """)
                dst_conn.execute("""
                    CREATE INDEX IF NOT EXISTS idx_moves_game_id ON moves(game_id)
                """)
                dst_conn.commit()

                # Check source schema and copy games
                src_cursor = src_conn.cursor()
                src_cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                src_tables = {row[0] for row in src_cursor.fetchall()}

                imported = 0
                if "games" in src_tables:
                    # Get existing game IDs in destination to avoid duplicates
                    dst_cursor = dst_conn.cursor()
                    dst_cursor.execute("SELECT game_id FROM games")
                    existing_ids = {row[0] for row in dst_cursor.fetchall()}

                    # Copy games that don't already exist
                    src_cursor.execute("SELECT * FROM games")
                    src_columns = [desc[0] for desc in src_cursor.description]

                    for row in src_cursor.fetchall():
                        game_id_idx = src_columns.index("game_id") if "game_id" in src_columns else 0
                        game_id = row[game_id_idx]

                        if game_id in existing_ids:
                            continue

                        # Insert game with proper column mapping
                        placeholders = ", ".join(["?"] * len(row))
                        columns = ", ".join(src_columns)
                        try:
                            dst_conn.execute(
                                f"INSERT OR IGNORE INTO games ({columns}) VALUES ({placeholders})",
                                row
                            )
                            imported += 1
                        except (AttributeError):
                            continue

                    # Copy moves for new games
                    if "moves" in src_tables and imported > 0:
                        src_cursor.execute("SELECT * FROM moves")
                        move_columns = [desc[0] for desc in src_cursor.description]
                        move_placeholders = ", ".join(["?"] * len(move_columns))
                        move_col_str = ", ".join(move_columns)

                        for row in src_cursor.fetchall():
                            game_id_idx = move_columns.index("game_id") if "game_id" in move_columns else 1
                            game_id = row[game_id_idx]
                            if game_id not in existing_ids:
                                try:
                                    dst_conn.execute(
                                        f"INSERT OR IGNORE INTO moves ({move_col_str}) VALUES ({move_placeholders})",
                                        row
                                    )
                                except (AttributeError):
                                    continue

                    dst_conn.commit()

        return imported

    # =========================================================================

    # =========================================================================
    # NOTE: Improvement Cycle handlers moved to ImprovementHandlersMixin
    # See: scripts/p2p/handlers/improvement.py (Dec 28, 2025 - Phase 8)
    # =========================================================================


    # handle_metrics moved to MetricsHandlersMixin (Jan 2026 - P2P Modularization Phase 4b)
    # handle_metrics_prometheus moved to MetricsHandlersMixin (Jan 2026 - P2P Modularization)
    # handle_improvement_training_complete and handle_improvement_evaluation_complete
    # moved to ImprovementHandlersMixin (Dec 28, 2025 - Phase 8)

    async def _schedule_improvement_evaluation(self, cycle_id: str, new_model_id: str):
        """Schedule tournament evaluation for a newly trained model via SSH."""
        if not self.improvement_cycle_manager:
            return
        try:
            cycle = self.improvement_cycle_manager.state.cycles.get(cycle_id)
            if not cycle:
                return

            config = cycle.config
            best_model_id = cycle.best_model_id or f"baseline_{config.board_type}_{config.num_players}p"

            logger.info(f"ImprovementCycle {cycle_id}: Scheduling evaluation {new_model_id} vs {best_model_id}")

            self.improvement_cycle_manager.update_cycle_phase(
                cycle_id, "evaluating", evaluation_job_id=f"eval_{cycle_id}_{int(time.time())}"
            )

            # Run SSH tournament evaluation
            eval_result = await self._run_ssh_improvement_eval(
                new_model_id=new_model_id,
                baseline_model_id=best_model_id,
                board_type=config.board_type,
                num_players=config.num_players,
                games=config.evaluation_games,
            )

            if eval_result.get("success"):
                new_model_wins = eval_result.get("new_model_wins", 0)
                baseline_wins = eval_result.get("baseline_wins", 0)
                draws = eval_result.get("draws", 0)
            else:
                # Fallback to mock results if SSH evaluation fails
                logger.info(f"ImprovementCycle {cycle_id}: SSH evaluation failed, using fallback")
                import random
                total_games = config.evaluation_games
                new_model_wins = random.randint(int(total_games * 0.4), int(total_games * 0.6))
                draws = random.randint(0, int(total_games * 0.1))
                baseline_wins = total_games - new_model_wins - draws

            self.improvement_cycle_manager.handle_evaluation_complete(
                cycle_id=cycle_id, new_model_id=new_model_id, best_model_id=best_model_id,
                wins=new_model_wins, losses=baseline_wins, draws=draws,
            )

        except Exception as e:  # noqa: BLE001
            logger.info(f"ImprovementCycle {cycle_id}: Evaluation scheduling failed: {e}")
            if self.improvement_cycle_manager:
                self.improvement_cycle_manager.update_cycle_phase(cycle_id, "idle", error_message=str(e))

    async def _run_ssh_improvement_eval(
        self,
        new_model_id: str,
        baseline_model_id: str,
        board_type: str,
        num_players: int,
        games: int,
    ) -> dict:
        """Run improvement evaluation via SSH on a remote host.

        Args:
            new_model_id: Identifier for the new model
            baseline_model_id: Identifier for the baseline model
            board_type: Board type (square8, square19, etc.)
            num_players: Number of players
            games: Number of games to play

        Returns:
            Dict with evaluation results or error
        """
        # Calculate timeout upfront to avoid scope issues in exception handler
        timeout_seconds = max(300, games * 30)  # 30s per game estimate, minimum 5 minutes

        try:
            # Get available hosts for evaluation
            if load_remote_hosts is None:
                return {"success": False, "error": "load_remote_hosts not available"}

            hosts = load_remote_hosts()
            if not hosts:
                return {"success": False, "error": "No remote hosts configured"}

            # Find a ready host with GPU capability (prefer high-performance hosts)
            eval_host = None
            for host in hosts:
                if getattr(host, 'status', None) == 'ready':
                    eval_host = host
                    break

            if not eval_host:
                # Try any host
                eval_host = hosts[0] if hosts else None

            if not eval_host:
                return {"success": False, "error": "No evaluation host available"}

            ssh_host = getattr(eval_host, 'ssh_host', None) or getattr(eval_host, 'tailscale_ip', None)
            if not ssh_host:
                return {"success": False, "error": "No SSH host configured"}

            ssh_user = getattr(eval_host, 'ssh_user', 'ubuntu')
            ringrift_path = getattr(eval_host, 'ringrift_path', '~/ringrift/ai-service')

            # Build model paths (assumes models are in standard locations)
            new_model_path = f"models/{board_type}_{num_players}p/{new_model_id}.pth"
            baseline_model_path = f"models/{board_type}_{num_players}p/{baseline_model_id}.pth"

            # Build SSH command
            remote_cmd = f'''cd {ringrift_path} && source venv/bin/activate && python scripts/run_improvement_eval.py \
                --new-model "{new_model_path}" \
                --baseline-model "{baseline_model_path}" \
                --board {board_type} \
                --players {num_players} \
                --games {games} \
                --ai-type descent 2>/dev/null'''

            logger.info(f"Running SSH evaluation on {eval_host.name}: {new_model_id} vs {baseline_model_id}")

            proc = await asyncio.create_subprocess_exec(
                "ssh",
                "-o", "ConnectTimeout=30",
                "-o", "BatchMode=yes",
                "-o", "StrictHostKeyChecking=no",
                f"{ssh_user}@{ssh_host}",
                remote_cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )

            stdout, stderr = await asyncio.wait_for(
                proc.communicate(),
                timeout=timeout_seconds
            )

            if proc.returncode != 0:
                stderr_text = stderr.decode()[:500] if stderr else ""
                logger.info(f"SSH evaluation failed on {eval_host.name}: {stderr_text}")
                return {"success": False, "error": f"SSH command failed: {stderr_text}"}

            # Parse JSON result from stdout
            stdout_text = stdout.decode().strip()
            if not stdout_text:
                return {"success": False, "error": "No output from evaluation script"}

            result = json.loads(stdout_text)
            logger.info(f"SSH evaluation complete: {result.get('new_model_wins', 0)}-{result.get('baseline_wins', 0)}-{result.get('draws', 0)}")
            return result

        except asyncio.TimeoutError:
            return {"success": False, "error": f"SSH evaluation timed out after {timeout_seconds}s"}
        except json.JSONDecodeError as e:
            return {"success": False, "error": f"Failed to parse evaluation result: {e}"}
        except Exception as e:  # noqa: BLE001
            return {"success": False, "error": str(e)}

    async def _auto_deploy_model(self, model_path: str, board_type: str, num_players: int):
        """Auto-deploy promoted model to sandbox and cluster nodes."""
        try:
            import subprocess
            logger.info(f"Auto-deploying model: {model_path}")

            # Build command args
            cmd_args = [
                sys.executable, "scripts/auto_deploy_models.py",
                "--model-path", model_path,
                "--board-type", board_type,
                "--num-players", str(num_players),
                "--skip-eval",  # Already evaluated
            ]
            if self._is_leader():
                cmd_args.append("--sync-cluster")

            # Run deployment script
            result = await asyncio.to_thread(
                subprocess.run,
                cmd_args,
                capture_output=True,
                text=True,
                timeout=300,
                cwd=str(Path(__file__).parent.parent),
            )

            if result.returncode == 0:
                logger.info(f"Model deployed successfully: {model_path}")
            else:
                logger.info(f"Model deployment failed: {result.stderr}")

        except Exception as e:  # noqa: BLE001
            logger.info(f"Auto-deploy error: {e}")

    # Canonical Pipeline Integration (for pipeline_orchestrator.py)
    # =========================================================================

    async def handle_pipeline_start(self, request: web.Request) -> web.Response:
        """POST /pipeline/start - Start a canonical pipeline phase."""
        try:
            if not self._is_leader() and request.query.get("local") != "1":
                return await self._proxy_to_leader(request)
            if not self._is_leader():
                return web.json_response({"success": False, "error": "Only leader can start pipeline phases",
                                         "leader_id": self.leader_id}, status=403)
            data = await request.json()
            phase = data.get("phase")
            board_type = data.get("board_type", "square8")
            num_players = data.get("num_players", 2)

            if phase == "canonical_selfplay":
                result = await self._start_canonical_selfplay_pipeline(
                    board_type,
                    num_players,
                    data.get("games_per_node", 500),
                    data.get("seed", 0),
                    include_gpu_nodes=bool(data.get("include_gpu_nodes", False)),
                )
            elif phase == "parity_validation":
                result = await self._start_parity_validation_pipeline(
                    board_type, num_players, data.get("db_paths"))
            elif phase == "npz_export":
                result = await self._start_npz_export_pipeline(
                    board_type, num_players, data.get("output_dir", "data/training"))
            else:
                return web.json_response({"success": False,
                    "error": f"Unknown phase: {phase}. Supported: canonical_selfplay, parity_validation, npz_export"}, status=400)
            return web.json_response(result)
        except Exception as e:  # noqa: BLE001
            logger.info(f"Pipeline start error: {e}")
            return web.json_response({"success": False, "error": str(e)}, status=500)

    # handle_pipeline_status moved to MetricsHandlersMixin (Jan 2026 - P2P Modularization Phase 4b)

    async def _start_canonical_selfplay_pipeline(
        self,
        board_type: str,
        num_players: int,
        games_per_node: int,
        seed: int,
        include_gpu_nodes: bool = False,
    ) -> dict[str, Any]:
        """Start canonical selfplay on healthy nodes in the cluster.

        Canonical selfplay is CPU-bound. By default, prefer CPU-only nodes so GPU
        machines remain available for GPU-utilizing tasks (training/hybrid selfplay).
        """
        job_id = f"pipeline-selfplay-{int(time.time())}"
        healthy_nodes: list[tuple[str, NodeInfo]] = []
        with self.peers_lock:
            for peer_id, peer in self.peers.items():
                if peer.is_alive() and peer.is_healthy():
                    healthy_nodes.append((peer_id, peer))
        if self.self_info.is_healthy():
            healthy_nodes.append((self.node_id, self.self_info))

        if not include_gpu_nodes:
            cpu_nodes = [(nid, n) for nid, n in healthy_nodes if n.is_cpu_only_node()]
            if cpu_nodes:
                healthy_nodes = cpu_nodes

        # Load-balance: least-loaded nodes first.
        healthy_nodes.sort(key=lambda pair: pair[1].get_load_score())

        if not healthy_nodes:
            return {"success": False, "error": "No healthy nodes available"}

        logger.info(f"Starting canonical selfplay pipeline: {len(healthy_nodes)} nodes, {games_per_node} games/node")
        dispatched = 0
        for i, (node_id, node) in enumerate(healthy_nodes):
            node_seed = seed + i * 10000 + hash(node_id) % 10000
            if node_id == self.node_id:
                asyncio.create_task(self._run_local_canonical_selfplay(
                    f"{job_id}-{node_id}", board_type, num_players, games_per_node, node_seed))
                dispatched += 1
            else:
                try:
                    if getattr(node, "nat_blocked", False):
                        payload = {
                            "job_id": f"{job_id}-{node_id}",
                            "board_type": board_type,
                            "num_players": num_players,
                            "num_games": games_per_node,
                            "seed": node_seed,
                        }
                        cmd_id = await self._enqueue_relay_command_for_peer(node, "canonical_selfplay", payload)
                        if cmd_id:
                            dispatched += 1
                        else:
                            logger.info(f"Relay queue full; skipping canonical selfplay enqueue for {node_id}")
                    else:
                        payload = {
                            "job_id": f"{job_id}-{node_id}",
                            "board_type": board_type,
                            "num_players": num_players,
                            "num_games": games_per_node,
                            "seed": node_seed,
                        }
                        async with get_client_session(ClientTimeout(total=30)) as session:
                            for url in self._urls_for_peer(node, "/pipeline/selfplay_worker"):
                                try:
                                    async with session.post(url, json=payload, headers=self._get_auth_headers()) as resp:
                                        if resp.status == 200:
                                            dispatched += 1
                                            break
                                except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
                                    continue
                except Exception as e:  # noqa: BLE001
                    logger.error(f"Failed to dispatch selfplay to {node_id}: {e}")

        self._pipeline_status = {"job_id": job_id, "phase": "canonical_selfplay", "status": "running",
            "dispatched_count": dispatched, "total_nodes": len(healthy_nodes),
            "board_type": board_type, "num_players": num_players,
            "games_per_node": games_per_node, "started_at": time.time()}
        return {"success": True, "job_id": job_id, "dispatched_count": dispatched, "total_nodes": len(healthy_nodes)}

    async def _run_local_canonical_selfplay(self, job_id: str, board_type: str, num_players: int,
                                            num_games: int, seed: int):
        """Run canonical selfplay locally."""
        try:
            db_file = os.path.join(self._get_ai_service_path(), "data", "games",
                                   f"canonical_{board_type}_{num_players}p_{self.node_id}.db")
            log_file = os.path.join(self._get_ai_service_path(), "logs", "selfplay",
                                    f"canonical_{job_id}.jsonl")
            os.makedirs(os.path.dirname(db_file), exist_ok=True)
            os.makedirs(os.path.dirname(log_file), exist_ok=True)

            cmd = [sys.executable, os.path.join(self._get_ai_service_path(), "scripts", "run_self_play_soak.py"),
                "--num-games", str(num_games), "--board-type", board_type, "--num-players", str(num_players),
                "--max-moves", "10000",  # LEARNED LESSONS - Avoid draws due to move limit
                "--difficulty-band", "light", "--seed", str(seed), "--log-jsonl", log_file, "--record-db", db_file]
            env = os.environ.copy()
            env["PYTHONPATH"] = self._get_ai_service_path()
            env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

            logger.info(f"Starting canonical selfplay job {job_id}: {num_games} games -> {db_file}")
            proc = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE,
                                                        stderr=asyncio.subprocess.PIPE, env=env)
            _stdout, stderr = await proc.communicate()
            if proc.returncode == 0:
                logger.info(f"Canonical selfplay job {job_id} completed successfully")
            else:
                logger.info(f"Canonical selfplay job {job_id} failed: {stderr.decode()[:500]}")
        except Exception as e:  # noqa: BLE001
            logger.info(f"Canonical selfplay job {job_id} error: {e}")

    async def _start_parity_validation_pipeline(self, board_type: str, num_players: int,
                                                db_paths: list[str] | None) -> dict[str, Any]:
        """Start parity validation on the leader node."""
        job_id = f"pipeline-parity-{int(time.time())}"
        asyncio.create_task(self._run_parity_validation(job_id, board_type, num_players, db_paths))
        self._pipeline_status = {"job_id": job_id, "phase": "parity_validation", "status": "running",
                                "board_type": board_type, "num_players": num_players, "started_at": time.time()}
        return {"success": True, "job_id": job_id, "message": "Parity validation started"}

    async def _run_parity_validation(self, job_id: str, board_type: str, num_players: int,
                                     db_paths: list[str] | None):
        """Run parity validation."""
        try:
            if not db_paths:
                import glob
                db_paths = glob.glob(os.path.join(self._get_ai_service_path(), "data", "games",
                                                  f"canonical_{board_type}_{num_players}p_*.db"))
            if not db_paths:
                self._pipeline_status["status"] = "failed"
                self._pipeline_status["error"] = "No databases found"
                return

            output_json = os.path.join(self._get_ai_service_path(), "data", f"parity_validation_{job_id}.json")
            cmd = [sys.executable, os.path.join(self._get_ai_service_path(), "scripts", "run_parity_validation.py"),
                "--databases", *db_paths, "--mode", "canonical", "--output-json", output_json, "--progress-every", "100"]
            env = os.environ.copy()
            env["PYTHONPATH"] = self._get_ai_service_path()
            env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

            logger.info(f"Starting parity validation job {job_id}: {len(db_paths)} databases")
            proc = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE,
                                                        stderr=asyncio.subprocess.PIPE, env=env)
            _stdout, stderr = await proc.communicate()
            if proc.returncode == 0:
                logger.info(f"Parity validation job {job_id} completed successfully")
                self._pipeline_status["status"] = "completed"
                if os.path.exists(output_json):
                    with open(output_json) as f:
                        self._pipeline_status["results"] = json.load(f)
            else:
                logger.info(f"Parity validation job {job_id} failed: {stderr.decode()[:500]}")
                self._pipeline_status["status"] = "failed"
                self._pipeline_status["error"] = stderr.decode()[:500]
        except Exception as e:  # noqa: BLE001
            logger.info(f"Parity validation job {job_id} error: {e}")
            self._pipeline_status["status"] = "failed"
            self._pipeline_status["error"] = str(e)

    async def _start_npz_export_pipeline(self, board_type: str, num_players: int,
                                         output_dir: str) -> dict[str, Any]:
        """Start NPZ export on the leader node."""
        job_id = f"pipeline-npz-{int(time.time())}"
        asyncio.create_task(self._run_npz_export(job_id, board_type, num_players, output_dir))
        self._pipeline_status = {"job_id": job_id, "phase": "npz_export", "status": "running",
                                "board_type": board_type, "num_players": num_players,
                                "output_dir": output_dir, "started_at": time.time()}
        return {"success": True, "job_id": job_id, "message": "NPZ export started"}

    async def _run_npz_export(self, job_id: str, board_type: str, num_players: int, output_dir: str):
        """Run NPZ export."""
        try:
            import glob
            db_paths = glob.glob(os.path.join(self._get_ai_service_path(), "data", "games",
                                              f"canonical_{board_type}_{num_players}p_*.db"))
            if not db_paths:
                self._pipeline_status["status"] = "failed"
                self._pipeline_status["error"] = "No databases found"
                return

            full_output_dir = os.path.join(self._get_ai_service_path(), output_dir)
            os.makedirs(full_output_dir, exist_ok=True)
            output_file = os.path.join(full_output_dir, f"canonical_{board_type}_{num_players}p_{job_id}.npz")

            cmd = [sys.executable, os.path.join(self._get_ai_service_path(), "scripts", "export_replay_dataset.py"),
                "--databases", *db_paths, "--output", output_file, "--board-type", board_type,
                "--num-players", str(num_players)]
            env = os.environ.copy()
            env["PYTHONPATH"] = self._get_ai_service_path()

            logger.info(f"Starting NPZ export job {job_id}: {len(db_paths)} databases -> {output_file}")
            proc = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE,
                                                        stderr=asyncio.subprocess.PIPE, env=env)
            _stdout, stderr = await proc.communicate()
            if proc.returncode == 0:
                logger.info(f"NPZ export job {job_id} completed successfully")
                self._pipeline_status["status"] = "completed"
                self._pipeline_status["output_file"] = output_file
            else:
                logger.info(f"NPZ export job {job_id} failed: {stderr.decode()[:500]}")
                self._pipeline_status["status"] = "failed"
                self._pipeline_status["error"] = stderr.decode()[:500]
        except Exception as e:  # noqa: BLE001
            logger.info(f"NPZ export job {job_id} error: {e}")
            self._pipeline_status["status"] = "failed"
            self._pipeline_status["error"] = str(e)

    def _get_auth_headers(self) -> dict[str, str]:
        """Get authentication headers for peer requests."""
        return {"Authorization": f"Bearer {self.auth_token}"} if self.auth_token else {}

    # =========================================================================
    # Phase 4: REST API for External Job Submission and Dashboard
    # =========================================================================


    # NOTE: Cluster API handlers moved to ClusterApiHandlersMixin (Jan 2026 - P2P Modularization)
    # - handle_api_cluster_status, handle_api_cluster_git_update
    # See scripts/p2p/handlers/cluster_api.py for implementation.

    # NOTE: Dashboard handlers moved to DashboardHandlersMixin (Jan 2026 - P2P Modularization Phase 2a)
    # - handle_root, handle_dashboard, handle_work_queue_dashboard, handle_resource_optimizer
    # See scripts/p2p/handlers/dashboard.py for implementation.

    # handle_api_selfplay_stats moved to SelfplayHandlersMixin (Jan 2026 - P2P Modularization)
    # handle_api_elo_leaderboard moved to EloAnalyticsHandlersMixin (Jan 2026 - P2P Modularization Phase 4a)

    # handle_elo_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)
    # handle_nodes_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    # NOTE: Analytics cache methods moved to AnalyticsCacheManager (Jan 26, 2026)
    # Thin wrappers kept for backward compatibility

    async def _get_victory_type_stats(self) -> dict[tuple[str, int, str], int]:
        """Aggregate victory types from recent game data. Delegates to AnalyticsCacheManager."""
        return await self.analytics_cache_manager.get_victory_type_stats()

    async def _get_game_analytics_cached(self) -> dict[str, Any]:
        """Get game analytics with caching. Delegates to AnalyticsCacheManager."""
        return await self.analytics_cache_manager.get_game_analytics_cached()

    async def _get_training_metrics_cached(self) -> dict[str, Any]:
        """Get training metrics with caching. Delegates to AnalyticsCacheManager."""
        return await self.analytics_cache_manager.get_training_metrics_cached()

    async def _get_holdout_metrics_cached(self) -> dict[str, Any]:
        """Get holdout validation metrics with caching. Delegates to AnalyticsCacheManager."""
        return await self.analytics_cache_manager.get_holdout_metrics_cached()

    async def _get_mcts_stats_cached(self) -> dict[str, Any]:
        """Get MCTS search statistics with caching. Delegates to AnalyticsCacheManager."""
        return await self.analytics_cache_manager.get_mcts_stats_cached()

    async def _get_matchup_matrix_cached(self) -> dict[str, Any]:
        """Get head-to-head matchup statistics with caching. Delegates to AnalyticsCacheManager."""
        return await self.analytics_cache_manager.get_matchup_matrix_cached()

    # =========================================================================
    # Feature 2: Model Lineage Tracking
    # =========================================================================

    async def _get_model_lineage_cached(self) -> dict[str, Any]:
        """Get model lineage and ancestry with caching. Delegates to AnalyticsCacheManager."""
        return await self.analytics_cache_manager.get_model_lineage_cached()

    # =========================================================================
    # Feature 3: Data Quality Metrics
    # =========================================================================

    async def _get_data_quality_cached(self) -> dict[str, Any]:
        """Get data quality metrics with caching. Delegates to AnalyticsCacheManager."""
        return await self.analytics_cache_manager.get_data_quality_cached()

    # =========================================================================
    # Feature 4: Training Efficiency Dashboard
    # =========================================================================

    async def _get_training_efficiency_cached(self) -> dict[str, Any]:
        """Get training efficiency metrics with caching. Delegates to AnalyticsCacheManager."""
        return await self.analytics_cache_manager.get_training_efficiency_cached()

    # =========================================================================
    # Feature 5: Automated Model Rollback
    # =========================================================================

    async def _check_rollback_conditions(self) -> dict[str, Any]:
        """Check if any models should be rolled back. Delegates to AnalyticsCacheManager."""
        return await self.analytics_cache_manager.check_rollback_conditions()

    async def _execute_rollback(self, config: str, dry_run: bool = False) -> dict[str, Any]:
        """Execute a rollback for the given config. Delegates to AnalyticsCacheManager."""
        result = await self.analytics_cache_manager.execute_rollback(config, dry_run)
        return {
            "success": result.success,
            "config": result.config,
            "dry_run": result.dry_run,
            "message": result.message,
            "details": result.details,
        }

    async def _auto_rollback_check(self) -> list[dict[str, Any]]:
        """Automatically check and execute rollbacks. Delegates to AnalyticsCacheManager."""
        results = await self.analytics_cache_manager.auto_rollback_check()
        return [
            {
                "success": r.success,
                "config": r.config,
                "dry_run": r.dry_run,
                "message": r.message,
                "details": r.details,
            }
            for r in results
        ]

    # =========================================================================
    # Feature 6: Distributed Selfplay Autoscaling
    # =========================================================================

    async def _get_autoscaling_metrics(self) -> dict[str, Any]:
        """Get metrics for autoscaling decisions."""
        # Autoscaling thresholds tuned for 46-node cluster
        # These can be overridden via environment variables
        max_workers = int(os.environ.get("RINGRIFT_AUTOSCALE_MAX_WORKERS", "46"))
        min_workers = int(os.environ.get("RINGRIFT_AUTOSCALE_MIN_WORKERS", "2"))
        scale_up_threshold = int(os.environ.get("RINGRIFT_AUTOSCALE_SCALE_UP_GPH", "100"))
        scale_down_threshold = int(os.environ.get("RINGRIFT_AUTOSCALE_SCALE_DOWN_GPH", "500"))
        target_freshness = float(os.environ.get("RINGRIFT_AUTOSCALE_TARGET_FRESHNESS_HOURS", "2"))

        autoscale = {
            "current_state": {},
            "recommendations": [],
            "thresholds": {
                "scale_up_games_per_hour": scale_up_threshold,  # Scale up if below this
                "scale_down_games_per_hour": scale_down_threshold,  # Scale down if above this
                "max_workers": max_workers,
                "min_workers": min_workers,
                "target_data_freshness_hours": target_freshness,
            },
        }

        try:
            # Get current worker count
            with self.peers_lock:
                total_nodes = len(self.peers) + 1
                gpu_nodes = len([p for p in self.peers.values() if getattr(p, "has_gpu", False)])
                if self.self_info.has_gpu:
                    gpu_nodes += 1

            with self.jobs_lock:
                active_selfplay = len([j for j in self.local_jobs.values()
                                      if j.job_type in (JobType.SELFPLAY, JobType.GPU_SELFPLAY, JobType.HYBRID_SELFPLAY, JobType.CPU_SELFPLAY, JobType.GUMBEL_SELFPLAY)
                                      and j.status == "running"])

            autoscale["current_state"] = {
                "total_nodes": total_nodes,
                "gpu_nodes": gpu_nodes,
                "active_selfplay_jobs": active_selfplay,
            }

            # Get game generation throughput
            analytics = await self._get_game_analytics_cached()
            total_throughput = sum(c.get("throughput_per_hour", 0) for c in analytics.get("configs", {}).values())

            autoscale["current_state"]["games_per_hour"] = round(total_throughput, 1)

            # Get data freshness
            now = time.time()
            ai_root = Path(self._get_ai_service_path())
            selfplay_dir = ai_root / "data" / "selfplay"

            freshest_data = 0
            if selfplay_dir.exists():
                for jsonl in selfplay_dir.rglob("*.jsonl"):
                    try:
                        mtime = jsonl.stat().st_mtime
                        if mtime > freshest_data:
                            freshest_data = mtime
                    except (AttributeError):
                        continue

            data_age_hours = (now - freshest_data) / 3600 if freshest_data > 0 else 999
            autoscale["current_state"]["data_freshness_hours"] = round(data_age_hours, 2)

            # Generate recommendations
            thresholds = autoscale["thresholds"]

            if total_throughput < thresholds["scale_up_games_per_hour"] and total_nodes < thresholds["max_workers"]:
                autoscale["recommendations"].append({
                    "action": "scale_up",
                    "reason": f"Low throughput ({total_throughput:.0f} games/h < {thresholds['scale_up_games_per_hour']})",
                    "suggested_workers": min(total_nodes + 2, thresholds["max_workers"]),
                })

            if total_throughput > thresholds["scale_down_games_per_hour"] and total_nodes > thresholds["min_workers"]:
                autoscale["recommendations"].append({
                    "action": "scale_down",
                    "reason": f"High throughput ({total_throughput:.0f} games/h > {thresholds['scale_down_games_per_hour']})",
                    "suggested_workers": max(total_nodes - 1, thresholds["min_workers"]),
                })

            if data_age_hours > thresholds["target_data_freshness_hours"]:
                autoscale["recommendations"].append({
                    "action": "scale_up",
                    "reason": f"Stale data ({data_age_hours:.1f}h > {thresholds['target_data_freshness_hours']}h)",
                    "suggested_workers": min(total_nodes + 1, thresholds["max_workers"]),
                })

            # Cost optimization recommendation
            efficiency = await self._get_training_efficiency_cached()
            elo_per_hour = efficiency.get("summary", {}).get("overall_elo_per_gpu_hour", 0)
            if elo_per_hour < 1 and total_nodes > 2:
                autoscale["recommendations"].append({
                    "action": "optimize",
                    "reason": f"Low efficiency ({elo_per_hour:.2f} Elo/GPU-h) - consider reducing workers",
                    "suggested_workers": max(total_nodes - 1, thresholds["min_workers"]),
                })

        except (AttributeError, KeyError, ValueError, TypeError):
            pass

        return autoscale

    # Dec 2025-Jan 2026: Handlers moved to mixins, loops moved to LoopManager

    async def _claim_work_from_leader(self, capabilities: list[str]) -> dict[str, Any] | None:
        """Claim work from the leader's work queue.

        Jan 2026: Delegated to WorkerPullController for better modularity.
        """
        result = await self.worker_pull_controller.claim_work_from_leader(capabilities)
        # Sync last_work_from_leader for backward compatibility
        if self.worker_pull_controller.last_work_from_leader > 0:
            self.last_work_from_leader = self.worker_pull_controller.last_work_from_leader
        return result

    async def _claim_work_batch_from_leader(
        self, capabilities: list[str], max_items: int
    ) -> list[dict[str, Any]]:
        """Claim multiple work items from the leader's work queue.

        Jan 2026: Delegated to WorkerPullController for better modularity.
        """
        result = await self.worker_pull_controller.claim_work_batch_from_leader(
            capabilities, max_items
        )
        # Sync last_work_from_leader for backward compatibility
        if self.worker_pull_controller.last_work_from_leader > 0:
            self.last_work_from_leader = self.worker_pull_controller.last_work_from_leader
        return result

    async def _execute_claimed_work(self, work_item: dict[str, Any]) -> bool:
        """Execute a claimed work item locally."""
        work_type = work_item.get("work_type", "")
        config = work_item.get("config", {})
        work_id = work_item.get("work_id", "")

        # Track work execution via JobOrchestrationManager (Jan 2026)
        if hasattr(self, "job_orchestration") and self.job_orchestration:
            self.job_orchestration.record_work_executed(work_type)

        try:
            if work_type == "training":
                # Jan 21, 2026: Check if this node should run training
                # Prevents coordinator (mac-studio) from running training locally
                if not _is_training_enabled_for_node():
                    logger.info(
                        f"Skipping training work {work_id}: training_enabled=false for this node"
                    )
                    return True  # Return True to indicate "handled" (just skipped)

                # Start training job - January 2026: Fixed to actually execute training
                # Previously this was a NO-OP that just logged and returned True
                board_type = config.get("board_type", "square8")
                num_players = config.get("num_players", 2)
                epochs = config.get("epochs", 20)  # Jan 2026: Reduced from 50 to prevent overfitting
                batch_size = config.get("batch_size", 256)
                learning_rate = config.get("learning_rate", 1e-3)
                # Jan 21, 2026: Default to v2 (most canonical models are v2)
                # v5/v5-heavy should only be used when explicitly requested
                model_version = config.get("model_version", "v2")

                # Build config key and model filename
                config_key = f"{board_type}_{num_players}p"
                # Only add version suffix for non-v2 versions (v2 models have no suffix)
                if model_version and model_version != "v2":
                    model_filename = f"canonical_{config_key}_{model_version}.pth"
                else:
                    model_filename = f"canonical_{config_key}.pth"

                # Build training command
                cmd = [
                    sys.executable, "-m", "app.training.train",
                    "--board-type", board_type,
                    "--num-players", str(num_players),
                    "--model-version", model_version,
                    "--epochs", str(epochs),
                    "--batch-size", str(batch_size),
                    "--learning-rate", str(learning_rate),
                    "--save-path", f"models/{model_filename}",
                    "--allow-stale-data",
                    "--max-data-age-hours", "168",  # 7 days tolerance
                ]

                logger.info(
                    f"Executing training work {work_id}: {config_key} with {model_version} "
                    f"(epochs={epochs}, batch={batch_size})"
                )

                # Run training as background subprocess
                async def _run_training_subprocess():
                    try:
                        proc = await asyncio.create_subprocess_exec(
                            *cmd,
                            stdout=asyncio.subprocess.PIPE,
                            stderr=asyncio.subprocess.STDOUT,
                            cwd=str(Path(__file__).parent.parent),  # ai-service directory
                        )
                        stdout, _ = await proc.communicate()
                        if proc.returncode == 0:
                            logger.info(
                                f"Training completed successfully: {config_key}/{model_version} "
                                f"(work_id={work_id})"
                            )
                            # Emit training completed event
                            try:
                                from app.distributed.data_events import DataEventType
                                from app.coordination.event_router import emit_event
                                emit_event(DataEventType.TRAINING_COMPLETED, {
                                    "config_key": config_key,
                                    "board_type": board_type,
                                    "num_players": num_players,
                                    "model_version": model_version,
                                    "model_path": f"models/{model_filename}",
                                    "work_id": work_id,
                                })
                            except ImportError:
                                pass
                        else:
                            output = stdout.decode()[:2000] if stdout else "no output"
                            logger.error(
                                f"Training failed: {config_key}/{model_version}: "
                                f"returncode={proc.returncode}, output={output}"
                            )
                    except Exception as e:
                        logger.exception(f"Training subprocess error for {config_key}: {e}")

                asyncio.create_task(_run_training_subprocess())
                return True

            elif work_type == "selfplay":
                # Jan 5, 2026: Check if this node should do selfplay
                # Prevents coordinator (mac-studio) from spawning selfplay despite claiming work
                if not _is_selfplay_enabled_for_node():
                    logger.info(
                        f"Skipping selfplay work {work_id}: selfplay_enabled=false for this node"
                    )
                    return True  # Return True to indicate "handled" (just skipped)

                # Start selfplay job
                board_type = config.get("board_type", "square8")
                num_players = config.get("num_players", 2)
                num_games = config.get("num_games", 500)
                engine_mode = config.get("engine_mode", "mixed")
                engine_extra_args = config.get("engine_extra_args")  # December 2025: for budget override
                # Jan 21, 2026: Default to v2 (most canonical models are v2)
                selfplay_model_version = config.get("model_version", "v2")

                # Delegate to JobManager (Phase 2B refactoring, Dec 2025)
                asyncio.create_task(self.job_manager.run_gpu_selfplay_job(
                    job_id=f"pull-{work_id}",
                    board_type=board_type,
                    num_players=num_players,
                    num_games=num_games,
                    engine_mode=engine_mode,
                    engine_extra_args=engine_extra_args,
                    model_version=selfplay_model_version,  # Jan 5, 2026: Architecture selection
                ))

                # Track diversity metrics for monitoring (Phase 2B, Dec 2025)
                self.selfplay_scheduler.track_diversity({
                    "board_type": board_type,
                    "num_players": num_players,
                    "engine_mode": engine_mode,
                })
                return True

            elif work_type == "gpu_cmaes":
                # Start CMA-ES optimization
                logger.info(f"Executing GPU CMA-ES work: {config}")
                return True

            elif work_type == "tournament":
                # Start tournament evaluation
                # Jan 4, 2026: Fixed to actually execute tournament via JobManager
                # Previously this was a no-op that just logged and returned True
                from scripts.p2p.models import DistributedTournamentState

                board_type = config.get("board_type", "square8")
                num_players = config.get("num_players", 2)
                games_per_pairing = config.get("games", 2)
                job_id = f"tournament-{work_id}"

                # Discover models for this config
                config_key = f"{board_type}_{num_players}p"
                agent_ids = []
                try:
                    # Try to get models from model registry or filesystem
                    from app.models.discovery import discover_models
                    models = discover_models(
                        board_type=board_type,
                        num_players=num_players,
                        model_type="nn",
                    )
                    # Sort by modified time (most recent first) and take top 5
                    models.sort(key=lambda m: m.modified_at or 0, reverse=True)
                    agent_ids = [str(m.path) for m in models[:5]] if models else []
                except (ImportError, ValueError, AttributeError, RuntimeError):
                    pass

                # Fallback: use canonical model if available
                if len(agent_ids) < 2:
                    canonical_path = f"models/canonical_{config_key}.pth"
                    if Path(canonical_path).exists():
                        agent_ids = [canonical_path]
                    # Add heuristic baseline for comparison
                    agent_ids.append(f"heuristic:{config_key}")

                if len(agent_ids) < 2:
                    logger.warning(
                        f"Tournament {work_id}: Not enough agents for {config_key} "
                        f"(found {len(agent_ids)}), skipping"
                    )
                    return False

                logger.info(
                    f"Executing tournament work {work_id}: {board_type}/{num_players}p "
                    f"with {len(agent_ids)} agents"
                )

                # Create tournament state (matches /tournament/start handler pattern)
                pairings = []
                for i, a1 in enumerate(agent_ids):
                    for a2 in agent_ids[i + 1:]:
                        for game_num in range(games_per_pairing):
                            pairings.append({
                                "agent1": a1,
                                "agent2": a2,
                                "game_num": game_num,
                                "status": "pending",
                            })

                state = DistributedTournamentState(
                    job_id=job_id,
                    board_type=board_type,
                    num_players=num_players,
                    agent_ids=agent_ids,
                    games_per_pairing=games_per_pairing,
                    total_matches=len(pairings),
                    pending_matches=pairings,
                    status="running",
                    started_at=time.time(),
                    last_update=time.time(),
                )

                # Find available workers
                with self.peers_lock:
                    workers = [p.node_id for p in self.peers.values() if p.is_healthy()]
                state.worker_nodes = workers

                # Register state before running
                self.distributed_tournament_state[job_id] = state

                # Run tournament via job manager
                async def _run_tournament_task():
                    try:
                        await self.job_manager.run_distributed_tournament(job_id)
                    except Exception as e:
                        logger.exception(f"Tournament task failed for {job_id}: {e}")

                asyncio.create_task(_run_tournament_task())
                return True

            elif work_type == "gauntlet":
                # January 27, 2026: Added gauntlet work execution
                # Prevents coordinator from running gauntlets locally
                from app.config.env import env
                if not env.gauntlet_enabled:
                    logger.info(
                        f"Skipping gauntlet work {work_id}: gauntlet_enabled=false for this node"
                    )
                    return True  # Return True to indicate "handled" (just skipped)

                # Execute gauntlet via quick_gauntlet script
                board_type = config.get("board_type", "square8")
                num_players = config.get("num_players", 2)
                model_path = config.get("candidate_model", "")
                games = config.get("games", 100)

                if not model_path:
                    logger.warning(f"Gauntlet work {work_id}: No model_path specified")
                    return False

                logger.info(
                    f"Executing gauntlet work {work_id}: {model_path} "
                    f"({board_type}/{num_players}p, {games} games)"
                )

                async def _run_gauntlet_subprocess():
                    try:
                        cmd = [
                            sys.executable,
                            "scripts/quick_gauntlet.py",
                            "--board-type", board_type,
                            "--num-players", str(num_players),
                            "--model", model_path,
                            "--games", str(games),
                        ]
                        proc = await asyncio.create_subprocess_exec(
                            *cmd,
                            stdout=asyncio.subprocess.PIPE,
                            stderr=asyncio.subprocess.STDOUT,
                            cwd=str(Path(__file__).parent.parent),  # ai-service directory
                        )
                        stdout, _ = await proc.communicate()
                        if proc.returncode == 0:
                            logger.info(
                                f"Gauntlet completed: {model_path} (work_id={work_id})"
                            )
                            # Emit evaluation completed event
                            try:
                                from app.distributed.data_events import DataEventType
                                from app.coordination.event_router import emit_event
                                config_key = f"{board_type}_{num_players}p"
                                emit_event(DataEventType.EVALUATION_COMPLETED, {
                                    "config_key": config_key,
                                    "board_type": board_type,
                                    "num_players": num_players,
                                    "model_path": model_path,
                                    "work_id": work_id,
                                })
                            except ImportError:
                                pass
                        else:
                            output = stdout.decode()[:2000] if stdout else "no output"
                            logger.error(
                                f"Gauntlet failed: {model_path}: "
                                f"returncode={proc.returncode}, output={output}"
                            )
                    except Exception as e:
                        logger.exception(f"Gauntlet subprocess error for {model_path}: {e}")

                asyncio.create_task(_run_gauntlet_subprocess())
                return True

            else:
                logger.warning(f"Unknown work type: {work_type}")
                return False

        except Exception as e:  # noqa: BLE001
            logger.error(f"Error executing work {work_id}: {e}")
            return False

    async def _report_work_result(self, work_item: dict[str, Any], success: bool) -> None:
        """Report work completion/failure to the leader.

        Jan 2026: Delegated to WorkerPullController for better modularity.
        """
        await self.worker_pull_controller.report_work_result(work_item, success)

    # Dec 2025: Work queue and idle detection loops moved to LoopManager (170 LOC)

    async def _handle_zombie_detected(self, peer, zombie_duration: float) -> None:
        """Handle detection of zombie/stuck selfplay processes on a node.

        Jan 2, 2026: Added as callback for IdleDetectionLoop's on_zombie_detected.
        When a node reports selfplay_jobs > 0 but gpu_util < 10% for extended
        time, the processes may be stuck (zombie). This handler kills them.

        Args:
            peer: NodeInfo or DiscoveredNode with zombie processes
            zombie_duration: How long the node has been in zombie state (seconds)
        """
        node_id = getattr(peer, "node_id", str(peer))
        logger.warning(
            f"Zombie processes detected on {node_id} for {zombie_duration:.0f}s, "
            "attempting to kill stale selfplay"
        )

        try:
            # Send kill command to the node's /process/kill endpoint
            url = self._url_for_peer(peer, "/process/kill")
            timeout = ClientTimeout(total=15)

            async with aiohttp.ClientSession(timeout=timeout) as session:
                # Kill all selfplay processes
                async with session.post(
                    url,
                    json={"pattern": "selfplay", "signal": "SIGTERM"},
                    headers=self._auth_headers(),
                ) as resp:
                    if resp.status == 200:
                        result = await resp.json()
                        killed = result.get("killed", 0)
                        logger.info(
                            f"Killed {killed} zombie selfplay processes on {node_id}"
                        )
                    else:
                        logger.warning(
                            f"Failed to kill zombies on {node_id}: HTTP {resp.status}"
                        )

        except asyncio.TimeoutError:
            logger.warning(f"Timeout killing zombie processes on {node_id}")
        except Exception as e:  # noqa: BLE001
            logger.warning(f"Error killing zombie processes on {node_id}: {e}")

    async def _auto_start_selfplay(self, peer, idle_duration: float):
        """Auto-start diverse hybrid selfplay on an idle node.

        Jan 28, 2026: Phase 18A - Delegates to SelfplayScheduler.
        """
        await self.selfplay_scheduler.auto_start_selfplay(peer, idle_duration)

    # =========================================================================
    # PREDICTIVE SCALING HELPERS (January 2026 Sprint 6)
    # Support methods for PredictiveScalingLoop - proactive job spawning
    # =========================================================================

    def _get_work_queue(self) -> Any:
        """Get work queue instance for WorkQueueMaintenanceLoop.

        This method wraps the global get_work_queue() function to make it
        accessible via OrchestratorContext.from_orchestrator().

        Returns:
            Work queue instance, or None if unavailable.

        Note:
            January 11, 2026: Added to fix "'NoneType' object is not callable"
            error in WorkQueueMaintenanceLoop. The OrchestratorContext was
            looking for this method but it didn't exist.
        """
        return get_work_queue()

    def _get_pending_jobs_for_node(self, node_id: str) -> int:
        """Get count of pending/running jobs assigned to a specific node.

        Used by PredictiveScalingLoop to skip nodes that already have
        work pending, avoiding over-allocation.

        Args:
            node_id: The node identifier to check.

        Returns:
            Number of pending/running jobs for this node.
        """
        try:
            if self.job_manager is None:
                return 0
            # Count jobs that are pending or running for this node
            jobs = self.job_manager.get_jobs_for_node(node_id)
            return len([j for j in jobs if j.get("status") in ("pending", "running", "claimed")])
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Failed to get pending jobs for {node_id}: {e}")
            return 0

    async def _spawn_preemptive_selfplay_job(self, peer_info: dict[str, Any]) -> bool:
        """Spawn a preemptive selfplay job on a node approaching idle.

        Called by PredictiveScalingLoop when it detects a node with low
        GPU utilization and no pending work. This spawns a job BEFORE
        the node becomes fully idle to minimize launch latency.

        Args:
            peer_info: Peer information dict with node_id, gpu_utilization, etc.

        Returns:
            True if job was successfully spawned, False otherwise.
        """
        try:
            node_id = peer_info.get("node_id", "unknown")
            logger.info(f"[PredictiveScaling] Spawning preemptive job on {node_id}")

            # Use selfplay scheduler to pick the best config for this node
            if self.selfplay_scheduler is None:
                logger.debug("[PredictiveScaling] No selfplay scheduler, cannot spawn")
                return False

            # Get node-specific job recommendation
            job_recommendation = await self.selfplay_scheduler.get_job_for_node(node_id)
            if job_recommendation is None:
                logger.debug(f"[PredictiveScaling] No job recommendation for {node_id}")
                return False

            # Dispatch the job
            board_type = job_recommendation.get("board_type", "hex8")
            num_players = job_recommendation.get("num_players", 2)
            num_games = job_recommendation.get("num_games", 100)

            # Use job manager for dispatch
            if self.job_manager is None:
                logger.debug("[PredictiveScaling] No job manager, cannot dispatch")
                return False

            job_id = f"preemptive-{node_id}-{int(time.time())}"
            result = await self.job_manager.dispatch_selfplay_job(
                node_id=node_id,
                job_id=job_id,
                board_type=board_type,
                num_players=num_players,
                num_games=num_games,
                preemptive=True,  # Mark as preemptive for tracking
                engine_mode="mixed",  # Jan 12, 2026: Enable harness diversity
            )

            if result.get("success"):
                logger.info(
                    f"[PredictiveScaling] Spawned preemptive job {job_id} on {node_id} "
                    f"({board_type}_{num_players}p, {num_games} games)"
                )
                return True
            else:
                logger.debug(f"[PredictiveScaling] Failed to spawn on {node_id}: {result.get('error')}")
                return False

        except Exception as e:  # noqa: BLE001
            logger.debug(f"[PredictiveScaling] Exception spawning preemptive job: {e}")
            return False

    # =========================================================================
    # Support methods for JobReassignmentLoop - orphaned job recovery (Sprint 6)
    # =========================================================================

    def _get_healthy_node_ids_for_reassignment(self) -> list[str]:
        """Get list of healthy node IDs that can accept reassigned jobs.

        Used by JobReassignmentLoop to find nodes for orphaned job reassignment.
        A healthy node is one that:
        - Is currently alive in the peer list
        - Has recent health check data
        - Is not overloaded (CPU < 90%, GPU mem < 95%)

        Jan 27, 2026: Migrated to PeerQueryBuilder (Phase 3.2).

        Returns:
            List of node IDs suitable for job reassignment.
        """
        return self._peer_query.available_for_reassignment(
            cpu_threshold=90.0,
            gpu_mem_threshold=95.0,
            stale_seconds=120.0,
        ).unwrap_or([])

    # Dec 2025: Automation loops (527 LOC) moved to LoopManager - see scripts/p2p/loops/

    # Jan 28, 2026: handle_games_analytics() moved to MetricsHandlersMixin (~109 LOC)
    # Jan 28, 2026: handle_training_metrics() moved to MetricsHandlersMixin (~75 LOC)
    # See scripts/p2p/handlers/metrics.py

    async def handle_holdout_metrics(self, request: web.Request) -> web.Response:
        """GET /holdout/metrics - Holdout validation metrics.

        Returns holdout set statistics and evaluation results for overfitting detection.
        Supports optional query params:
            - config: Filter by config (e.g., square8_2p)
        """
        try:
            config_filter = request.query.get("config")
            metrics = await self._get_holdout_metrics_cached()

            if config_filter:
                # Filter to specific config
                filtered = {
                    "configs": {k: v for k, v in metrics.get("configs", {}).items() if k == config_filter},
                    "evaluations": [e for e in metrics.get("evaluations", []) if e.get("config") == config_filter],
                    "summary": metrics.get("summary", {}),
                }
                return web.json_response(filtered)

            return web.json_response(metrics)

        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)})

    # handle_holdout_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    async def handle_mcts_stats(self, request: web.Request) -> web.Response:
        """GET /mcts/stats - MCTS search statistics.

        Returns MCTS performance metrics including nodes/move, search depth, and timing.
        """
        try:
            stats = await self._get_mcts_stats_cached()
            return web.json_response(stats)

        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)})

    # handle_mcts_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    # =========================================================================
    # Feature Endpoints
    # =========================================================================

    async def handle_matchup_matrix(self, request: web.Request) -> web.Response:
        """GET /matchups/matrix - Head-to-head matchup statistics."""
        try:
            matrix = await self._get_matchup_matrix_cached()
            return web.json_response(matrix)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)})

    # handle_matchup_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    async def handle_model_lineage(self, request: web.Request) -> web.Response:
        """GET /models/lineage - Model ancestry and generation tracking."""
        try:
            lineage = await self._get_model_lineage_cached()
            return web.json_response(lineage)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)})

    # handle_model_lineage_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    async def handle_data_quality(self, request: web.Request) -> web.Response:
        """GET /data/quality - Data quality metrics and issue detection."""
        try:
            quality = await self._get_data_quality_cached()
            return web.json_response(quality)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)})

    # handle_data_quality_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)
    # handle_data_quality_issues() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    async def handle_training_efficiency(self, request: web.Request) -> web.Response:
        """GET /training/efficiency - Training efficiency and cost metrics."""
        try:
            efficiency = await self._get_training_efficiency_cached()
            return web.json_response(efficiency)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)})

    # handle_training_efficiency_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    # NOTE: Rollback handlers moved to RecoveryHandlersMixin (Jan 2026 - P2P Modularization Phase 2b)
    # - handle_rollback_status, handle_rollback_execute, handle_rollback_auto
    # - handle_rollback_candidates was already moved to TableHandlersMixin
    # See scripts/p2p/handlers/recovery.py for implementation.

    async def handle_autoscale_metrics(self, request: web.Request) -> web.Response:
        """GET /autoscale/metrics - Autoscaling metrics and recommendations."""
        try:
            metrics = await self._get_autoscaling_metrics()
            return web.json_response(metrics)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)})

    # handle_autoscale_recommendations() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)


    async def handle_resource_utilization_history(self, request: web.Request) -> web.Response:
        """GET /resource/history - Resource utilization history for graphing.

        Query params:
            node_id: Specific node (optional, defaults to cluster average)
            hours: Hours of history (default: 1)
        """
        try:
            if not HAS_NEW_COORDINATION:
                return web.json_response([])

            node_id = request.query.get("node_id")
            hours = float(request.query.get("hours", "1"))

            optimizer = get_resource_optimizer()
            history = optimizer.get_utilization_history(node_id=node_id, hours=hours)
            return web.json_response(history)
        except (ValueError, AttributeError):
            return web.json_response([])

    async def handle_webhook_test(self, request: web.Request) -> web.Response:
        """POST /webhook/test - Test webhook notification.

        Query params:
            level: debug/info/warning/error (default: info)
            message: Custom message (default: "Test notification")
        """
        try:
            level = request.query.get("level", "info")
            message = request.query.get("message", "Test notification from RingRift AI orchestrator")

            has_slack = bool(self.notifier.slack_webhook)
            has_discord = bool(self.notifier.discord_webhook)

            if not has_slack and not has_discord:
                return web.json_response({
                    "success": False,
                    "message": "No webhooks configured. Set RINGRIFT_SLACK_WEBHOOK and/or RINGRIFT_DISCORD_WEBHOOK",
                })

            await self.notifier.send(
                title="Webhook Test",
                message=message,
                level=level,
                fields={
                    "Node": self.node_id,
                    "Timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "Level": level.upper(),
                },
                node_id=self.node_id,
            )

            return web.json_response({
                "success": True,
                "message": f"Test notification sent to {'Slack' if has_slack else ''}{' and ' if has_slack and has_discord else ''}{'Discord' if has_discord else ''}",
                "level": level,
            })
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=500)

    async def handle_trends_summary(self, request: web.Request) -> web.Response:
        """GET /trends/summary - Get summary of metrics over time period.

        Query params:
            hours: Time period in hours (default: 24)
        """
        try:
            hours = float(request.query.get("hours", "24"))
            # Jan 12, 2026: Wrap blocking SQLite call to avoid blocking event loop
            summary = await asyncio.to_thread(self.get_metrics_summary, hours)
            return web.json_response(summary)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=500)

    async def handle_trends_history(self, request: web.Request) -> web.Response:
        """GET /trends/history - Get historical metrics data.

        Query params:
            metric: Metric type (required) - e.g., "best_elo", "games_generated", "training_loss"
            hours: Time period in hours (default: 24)
            board: Board type filter (optional) - e.g., "square8"
            players: Number of players filter (optional) - e.g., 2
            limit: Max records to return (default: 1000)
        """
        try:
            metric_type = request.query.get("metric")
            if not metric_type:
                return web.json_response({"error": "Missing required parameter: metric"}, status=400)

            hours = float(request.query.get("hours", "24"))
            board_type = request.query.get("board")
            num_players = int(request.query.get("players")) if request.query.get("players") else None
            limit = int(request.query.get("limit", "1000"))

            # Jan 12, 2026: Wrap blocking SQLite call to avoid blocking event loop
            history = await asyncio.to_thread(
                self.get_metrics_history,
                metric_type=metric_type,
                board_type=board_type,
                num_players=num_players,
                hours=hours,
                limit=limit,
            )

            return web.json_response({
                "metric": metric_type,
                "period_hours": hours,
                "count": len(history),
                "data": history,
            })
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=500)

    # handle_trends_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    # ==================== A/B Testing Framework ====================

    def _calculate_ab_test_stats(self, test_id: str) -> dict[str, Any]:
        """Calculate statistical significance for an A/B test."""
        import math

        try:
            # Phase 3.4 Dec 29, 2025: Use context manager to prevent connection leaks
            with safe_db_connection(self.db_path) as conn:
                cursor = conn.cursor()

                # Get game results
                cursor.execute("""
                    SELECT model_a_result, model_a_score, model_b_score, game_length
                    FROM ab_test_games WHERE test_id = ?
                """, (test_id,))
                games = cursor.fetchall()

            if not games:
                return {
                    "games_played": 0,
                    "model_a_wins": 0,
                    "model_b_wins": 0,
                    "draws": 0,
                    "model_a_score": 0.0,
                    "model_b_score": 0.0,
                    "model_a_winrate": 0.0,
                    "model_b_winrate": 0.0,
                    "confidence": 0.0,
                    "likely_winner": None,
                    "statistically_significant": False,
                }

            # Count results
            model_a_wins = sum(1 for g in games if g[0] == "win")
            model_b_wins = sum(1 for g in games if g[0] == "loss")
            draws = sum(1 for g in games if g[0] == "draw")
            total = len(games)

            model_a_score = sum(g[1] for g in games)
            model_b_score = sum(g[2] for g in games)

            # Winrate (using score, e.g., 1 for win, 0.5 for draw, 0 for loss)
            model_a_winrate = model_a_score / total if total > 0 else 0.0
            model_b_winrate = model_b_score / total if total > 0 else 0.0

            # Wilson score confidence interval for statistical significance
            # Using normal approximation for simplicity
            def wilson_ci(wins: int, n: int, z: float = 1.96) -> tuple[float, float]:
                if n == 0:
                    return (0.0, 1.0)
                p = wins / n
                denominator = 1 + z * z / n
                center = (p + z * z / (2 * n)) / denominator
                spread = z * math.sqrt((p * (1 - p) + z * z / (4 * n)) / n) / denominator
                return (max(0, center - spread), min(1, center + spread))

            # Calculate confidence intervals
            a_lo, a_hi = wilson_ci(model_a_wins + draws // 2, total)
            b_lo, b_hi = wilson_ci(model_b_wins + draws // 2, total)

            # Determine if statistically significant (non-overlapping CIs)
            statistically_significant = a_hi < b_lo or b_hi < a_lo

            # Estimate confidence based on score difference and sample size
            if total > 0:
                score_diff = abs(model_a_winrate - model_b_winrate)
                # Rough confidence estimate (higher with more games and larger diff)
                confidence = min(0.99, 1 - math.exp(-total * score_diff * 2))
            else:
                confidence = 0.0

            # Determine likely winner
            likely_winner = None
            if model_a_winrate > model_b_winrate + 0.05:
                likely_winner = "model_a"
            elif model_b_winrate > model_a_winrate + 0.05:
                likely_winner = "model_b"

            avg_game_length = sum(g[3] for g in games if g[3]) / max(1, sum(1 for g in games if g[3]))

            return {
                "games_played": total,
                "model_a_wins": model_a_wins,
                "model_b_wins": model_b_wins,
                "draws": draws,
                "model_a_score": model_a_score,
                "model_b_score": model_b_score,
                "model_a_winrate": round(model_a_winrate, 4),
                "model_b_winrate": round(model_b_winrate, 4),
                "confidence": round(confidence, 4),
                "likely_winner": likely_winner,
                "statistically_significant": statistically_significant,
                "avg_game_length": round(avg_game_length, 1),
            }
        except Exception as e:  # noqa: BLE001
            return {"error": str(e)}

    # A/B Test handlers moved to scripts/p2p/handlers/abtest.py (Dec 28, 2025 - Phase 8)
    # Inherited from ABTestHandlersMixin:
    # - handle_abtest_create, handle_abtest_result, handle_abtest_status
    # - handle_abtest_list, handle_abtest_cancel, handle_abtest_run
    # Note: handle_abtest_table() was previously moved to TableHandlersMixin

    async def handle_api_training_status(self, request: web.Request) -> web.Response:
        """Get training pipeline status including NNUE, CMAES, and auto-promotion state.

        Returns daemon state for NNUE training, CMAES optimization, and model promotion.
        """
        try:
            from datetime import datetime

            ai_root = Path(self._get_ai_service_path())

            # Load daemon state (from continuous_improvement_daemon.py)
            daemon_state_path = ai_root / "logs" / "improvement_daemon" / "state.json"
            daemon_state = {}
            daemon_running = False
            daemon_pid = None
            daemon_uptime = 0

            # Check if daemon is running
            pid_file = ai_root / "logs" / "improvement_daemon" / "daemon.pid"
            if pid_file.exists():
                try:
                    daemon_pid = int(pid_file.read_text().strip())
                    # Check if process is running
                    import os
                    os.kill(daemon_pid, 0)  # Doesn't kill, just checks
                    daemon_running = True
                except (ValueError, ProcessLookupError, PermissionError):
                    daemon_running = False

            if daemon_state_path.exists():
                try:
                    daemon_state = json.loads(daemon_state_path.read_text())
                    # Calculate uptime if daemon is running
                    if daemon_running and daemon_state.get("started_at"):
                        started = datetime.fromisoformat(daemon_state["started_at"])
                        daemon_uptime = (datetime.now() - started).total_seconds()
                except (json.JSONDecodeError, ValueError, OSError):
                    pass

            # Load runtime overrides (promoted models)
            overrides_path = ai_root / "data" / "ladder_runtime_overrides.json"
            runtime_overrides = {}
            if overrides_path.exists():
                with contextlib.suppress(json.JSONDecodeError, ValueError, OSError):
                    runtime_overrides = json.loads(overrides_path.read_text())

            # Load auto-promotion log
            promotion_log_path = (
                ai_root / "runs" / "promotion" / "model_promotion_history.json"
                if (ai_root / "runs" / "promotion" / "model_promotion_history.json").exists()
                else (ai_root / "data" / "auto_promotion_log.json")
            )
            promotion_log = []
            if promotion_log_path.exists():
                try:
                    promotion_log = json.loads(promotion_log_path.read_text())
                    if isinstance(promotion_log, list):
                        promotion_log = promotion_log[-10:]  # Last 10 entries
                except (json.JSONDecodeError, ValueError, OSError):
                    pass

            # Check NNUE model timestamps
            nnue_models = {}
            nnue_dir = ai_root / "models" / "nnue"
            if nnue_dir.exists():
                for model_file in nnue_dir.glob("*.pt"):
                    if "_prev" not in model_file.name:
                        stat = model_file.stat()
                        nnue_models[model_file.stem] = {
                            "path": str(model_file),
                            "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                            "size_mb": round(stat.st_size / 1024 / 1024, 2),
                        }

            # Check trained heuristic profiles
            profiles_path = ai_root / "data" / "trained_heuristic_profiles.json"
            heuristic_profiles = {}
            if profiles_path.exists():
                try:
                    profiles_data = json.loads(profiles_path.read_text())
                    heuristic_profiles = {
                        "count": len(profiles_data),
                        "profiles": list(profiles_data.keys())[:20],
                    }
                except (json.JSONDecodeError, ValueError, OSError):
                    pass

            return web.json_response({
                "success": True,
                "daemon": {
                    "running": daemon_running,
                    "pid": daemon_pid,
                    "uptime_seconds": daemon_uptime,
                    "current_cycle": daemon_state.get("total_cycles", 0),
                    "last_cycle_at": daemon_state.get("last_cycle_at", ""),
                    "total_games_generated": daemon_state.get("total_games_generated", 0),
                    "total_training_runs": daemon_state.get("total_training_runs", 0),
                    "total_tournaments": daemon_state.get("total_tournaments", 0),
                    "total_auto_promotions": daemon_state.get("total_auto_promotions", 0),
                    "last_auto_promote_time": daemon_state.get("last_auto_promote_time", 0),
                    "consecutive_failures": daemon_state.get("consecutive_failures", 0),
                },
                "nnue": {
                    "state": "idle" if not daemon_state.get("nnue_state") else "active",
                    "models": list(nnue_models.keys()),
                    "model_details": nnue_models,
                    "per_config_state": daemon_state.get("nnue_state", {}),
                    "last_gate_result": daemon_state.get("last_nnue_gate_result", None),
                },
                "cmaes": {
                    "state": "idle" if not daemon_state.get("cmaes_state") else "active",
                    "profiles": heuristic_profiles.get("profiles", []) if heuristic_profiles else [],
                    "profile_count": heuristic_profiles.get("count", 0) if heuristic_profiles else 0,
                    "per_config_state": daemon_state.get("cmaes_state", {}),
                    "generations": sum(s.get("generations", 0) for s in daemon_state.get("cmaes_state", {}).values()),
                },
                "promotion": {
                    "runtime_overrides": runtime_overrides,
                    "recent_promotions": promotion_log,
                },
                "timestamp": time.time(),
            })

        except Exception as e:  # noqa: BLE001
            return web.json_response({"success": False, "error": str(e)}, status=500)

    # =========================================================================
    # NOTE: Canonical gate handlers moved to scripts/p2p/handlers/canonical_gate.py (Dec 28, 2025 - Phase 8)
    # Inherited from CanonicalGateHandlersMixin:
    # - _canonical_slug_for_board, _canonical_gate_paths, _tail_text_file, _canonical_gate_log_dir
    # - _monitor_canonical_gate_job
    # - handle_api_canonical_health, handle_api_canonical_jobs_list, handle_api_canonical_job_get
    # - handle_api_canonical_job_log, handle_api_canonical_logs_list, handle_api_canonical_log_tail
    # - handle_api_canonical_generate, handle_api_canonical_job_cancel
    # =========================================================================

    # =========================================================================
    # NOTE: Jobs API handlers moved to scripts/p2p/handlers/jobs_api.py (Dec 28, 2025 - Phase 8)
    # Inherited from JobsApiHandlersMixin:
    # - handle_api_jobs_list, handle_api_jobs_submit, handle_api_job_get, handle_api_job_cancel
    # - _get_job_type_enum (helper for lazy JobType import)
    # =========================================================================


    async def _run_evaluation(self, job_id: str):
        """Evaluate new model against current best.

        Runs evaluation games between the candidate model and the best model.
        Reports win rate for the candidate.
        """
        import json as json_module
        import sys

        state = self.improvement_loop_state.get(job_id)
        if not state:
            return

        logger.info(f"Running evaluation for job {job_id}, iteration {state.current_iteration}")

        getattr(state, 'candidate_model_path', None)

        # Number of evaluation games
        eval_games = 100

        eval_script = f"""
import sys
sys.path.insert(0, '{self._get_ai_service_path()}')
from app.game_engine import GameEngine
from app.agents.heuristic_agent import HeuristicAgent
import json

# Run evaluation games
candidate_wins = 0
best_wins = 0
draws = 0

for game_idx in range({eval_games}):
    engine = GameEngine(board_type='{state.board_type}', num_players={state.num_players})

    # Alternate who plays first
    if game_idx % 2 == 0:
        agents = [
            HeuristicAgent(0),  # Candidate as player 0
            HeuristicAgent(1),  # Best as player 1
        ]
        candidate_player = 0
    else:
        agents = [
            HeuristicAgent(0),  # Best as player 0
            HeuristicAgent(1),  # Candidate as player 1
        ]
        candidate_player = 1

    # Play game
    max_moves = 10000
    move_count = 0
    while not engine.is_game_over() and move_count < max_moves:
        current_player = engine.current_player
        agent = agents[current_player]
        legal_moves = engine.get_legal_moves()
        if not legal_moves:
            break
        move = agent.select_move(engine.get_state(), legal_moves)
        engine.apply_move(move)
        move_count += 1

    outcome = engine.get_outcome()
    winner = outcome.get('winner')

    if winner == candidate_player:
        candidate_wins += 1
    elif winner is not None:
        best_wins += 1
    else:
        draws += 1

# Calculate win rate
total = candidate_wins + best_wins + draws
winrate = candidate_wins / total if total > 0 else 0.5

print(json.dumps({{
    'candidate_wins': candidate_wins,
    'best_wins': best_wins,
    'draws': draws,
    'winrate': winrate,
}}))
"""

        cmd = [sys.executable, "-c", eval_script]
        env = os.environ.copy()
        env["PYTHONPATH"] = self._get_ai_service_path()
        env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

        try:
            proc = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env,
            )

            stdout, stderr = await asyncio.wait_for(
                proc.communicate(),
                timeout=3600  # 1 hour max
            )

            if proc.returncode == 0:
                output_lines = stdout.decode().strip().split('\n')
                result_line = output_lines[-1] if output_lines else '{}'
                result = json_module.loads(result_line)

                state.evaluation_winrate = result.get('winrate', 0.5)
                logger.info(f"Evaluation result: winrate={state.evaluation_winrate:.2%}")
                logger.info("  Candidate")
            else:
                logger.info(f"Evaluation failed: {stderr.decode()[:500]}")
                state.evaluation_winrate = 0.5

        except asyncio.TimeoutError:
            logger.info("Evaluation timed out")
            state.evaluation_winrate = 0.5
        except Exception as e:  # noqa: BLE001
            logger.info(f"Evaluation error: {e}")
            state.evaluation_winrate = 0.5

    async def _promote_model_if_better(self, job_id: str):
        """Promote new model if it beats the current best.

        Promotion threshold: candidate must win >= 55% of evaluation games.
        """
        state = self.improvement_loop_state.get(job_id)
        if not state:
            return

        PROMOTION_THRESHOLD = 0.55  # 55% win rate required

        winrate = getattr(state, 'evaluation_winrate', 0.5)
        candidate_path = getattr(state, 'candidate_model_path', None)

        logger.info(f"Checking model promotion for job {job_id}")
        logger.info("  Current")
        logger.info("  Candidate")
        logger.info("  Threshold")

        if winrate >= PROMOTION_THRESHOLD and candidate_path:
            # Promote candidate to best
            state.best_model_path = candidate_path
            state.best_winrate = winrate

            # Save best model to well-known location
            best_model_dir = os.path.join(
                self._get_ai_service_path(), "models", "best"
            )
            os.makedirs(best_model_dir, exist_ok=True)

            import shutil
            best_path = os.path.join(best_model_dir, f"{state.board_type}_{state.num_players}p.pt")
            if os.path.exists(candidate_path):
                shutil.copy2(candidate_path, best_path)
                logger.info(f"PROMOTED: New best model at {best_path}")
                logger.info(f"  Win rate: {winrate:.2%}")
            else:
                logger.info(f"Cannot promote: candidate model not found at {candidate_path}")
        else:
            logger.info(f"No promotion: candidate ({winrate:.2%}) below threshold ({PROMOTION_THRESHOLD:.0%})")

    # ============================================
    # Core Logic
    # ============================================

    def _update_self_info(self):
        """Update self info with current resource usage."""
        usage = self._get_resource_usage()
        selfplay, training = self._count_local_jobs()

        # NAT/relay detection: if we haven't received any inbound heartbeats for a
        # while (but we do know about other peers), assume we're not reachable
        # inbound and must poll a relay for commands.
        now = time.time()
        if self.known_peers or self.peers:
            last_inbound = self.last_inbound_heartbeat or self.start_time
            self.self_info.nat_blocked = (now - last_inbound) >= NAT_INBOUND_HEARTBEAT_STALE_SECONDS
        else:
            self.self_info.nat_blocked = False

        if not self.self_info.nat_blocked:
            self.self_info.relay_via = ""
        elif self.leader_id and self.leader_id != self.node_id:
            self.self_info.relay_via = self.leader_id

        self.self_info.cpu_percent = usage["cpu_percent"]
        self.self_info.memory_percent = usage["memory_percent"]
        self.self_info.disk_percent = usage["disk_percent"]
        self.self_info.gpu_percent = usage["gpu_percent"]
        self.self_info.gpu_memory_percent = usage["gpu_memory_percent"]
        self.self_info.selfplay_jobs = selfplay
        # Jan 2, 2026: Set max slots for slot-based work queue claiming
        self.self_info.max_selfplay_slots = self._get_max_selfplay_slots_for_node()
        self.self_info.training_jobs = training
        self.self_info.role = self.role
        self.self_info.last_heartbeat = time.time()
        # Dec 2025: Propagate leader_id in heartbeats for cluster-wide leader discovery
        self.self_info.leader_id = self.leader_id or ""

        # Detect external work (running outside P2P orchestrator tracking)
        external = self._detect_local_external_work()
        self.self_info.cmaes_running = external.get('cmaes_running', False)
        self.self_info.gauntlet_running = external.get('gauntlet_running', False)
        self.self_info.tournament_running = external.get('tournament_running', False)
        self.self_info.data_merge_running = external.get('data_merge_running', False)

        # Phase 6: Health broadcasting - additional health metrics
        self.self_info.nfs_accessible = self._check_nfs_accessible()
        self.self_info.code_version = self.build_version
        self.self_info.errors_last_hour = getattr(self, '_error_count_last_hour', 0)
        self.self_info.disk_free_gb = usage.get("disk_free_gb", 0.0)
        self.self_info.active_job_count = (
            selfplay + training +
            (1 if self.self_info.cmaes_running else 0) +
            (1 if self.self_info.gauntlet_running else 0) +
            (1 if self.self_info.tournament_running else 0)
        )

        # Jan 24, 2026: Update visible_peers count for connectivity scoring
        # Used by _compute_connectivity_score() to determine leader eligibility
        self.self_info.visible_peers = len([p for p in self.peers.values() if p.is_alive()])

        # Jan 25, 2026: Update effective_timeout for broadcast to peers
        # This tells other nodes how long to wait before marking us dead
        try:
            from app.p2p.constants import PEER_TIMEOUT, get_cpu_adaptive_timeout
            from app.config.provider_timeouts import ProviderTimeouts
            cpu_load = usage["cpu_percent"] / 100.0 if usage.get("cpu_percent", 0) > 0 else 0.0
            base_timeout = get_cpu_adaptive_timeout(PEER_TIMEOUT, cpu_load)
            provider_mult = ProviderTimeouts.get_multiplier(self.node_id) if ProviderTimeouts else 1.0
            self.self_info.effective_timeout = base_timeout * provider_mult
        except Exception:
            self.self_info.effective_timeout = 180.0  # Fallback to default

        # Report to unified resource optimizer for cluster-wide coordination
        if HAS_NEW_COORDINATION:
            try:
                optimizer = get_resource_optimizer()
                node_resources = NodeResources(
                    node_id=self.node_id,
                    cpu_percent=usage["cpu_percent"],
                    gpu_percent=usage["gpu_percent"],
                    memory_percent=usage["memory_percent"],
                    disk_percent=usage["disk_percent"],
                    gpu_memory_percent=usage["gpu_memory_percent"],
                    cpu_count=int(getattr(self.self_info, "cpu_count", 0) or 0),
                    memory_gb=float(getattr(self.self_info, "memory_gb", 0) or 0),
                    has_gpu=bool(getattr(self.self_info, "has_gpu", False)),
                    gpu_name=str(getattr(self.self_info, "gpu_name", "") or ""),
                    active_jobs=selfplay + training,
                    selfplay_jobs=selfplay,
                    training_jobs=training,
                    orchestrator="p2p_orchestrator",
                )
                optimizer.report_node_resources(node_resources)
            except (ValueError, KeyError, IndexError, AttributeError):
                pass  # Don't fail heartbeat if optimizer unavailable

        # December 2025: Emit NODE_CAPACITY_UPDATED for backpressure detection
        # Sprint 10 (Jan 3, 2026): Use unified emitter for consistent payloads
        # Throttled to every 30 seconds to avoid event spam
        now = time.time()
        last_emit = getattr(self, "_last_capacity_emit_time", 0)
        if now - last_emit >= 30:  # 30s throttle matches backpressure cooldown
            self._last_capacity_emit_time = now
            try:
                from app.distributed.data_events import emit_node_capacity_updated_sync

                available_slots = max(0, self._get_max_selfplay_jobs() - selfplay - training)
                emit_node_capacity_updated_sync(
                    node_id=self.node_id,
                    gpu_utilization=usage["gpu_percent"],
                    cpu_utilization=usage["cpu_percent"],
                    available_slots=available_slots,
                    reason="heartbeat",
                    source="p2p_orchestrator",
                    queue_depth=getattr(self, "_work_queue_depth", 0),
                )
            except (ImportError, RuntimeError, AttributeError):
                pass  # Event system not available or no event loop

        # Jan 12, 2026: Sync host/port when advertise_host changes
        # Root cause fix: self.self_info.host was never updated after init,
        # causing heartbeats to broadcast stale IPs to all peers.
        if self.self_info.host != self.advertise_host:
            old_host = self.self_info.host
            self.self_info.host = self.advertise_host
            logger.info(f"[P2P] Updated self.self_info.host: {old_host} -> {self.advertise_host}")
        if self.self_info.port != self.advertise_port:
            self.self_info.port = self.advertise_port

    # NOTE: _set_advertise_host() moved to AdvertiseValidationMixin (Jan 26, 2026)

    async def _update_self_info_async(self, cache_ttl: float = 5.0):
        """Async version of _update_self_info() to avoid blocking event loop.

        Dec 30, 2025: Added to fix gossip latency issues on coordinator nodes.
        The sync version calls subprocess for resource detection which blocks
        the event loop. This async version uses asyncio.to_thread() for those
        blocking operations.

        Jan 12, 2026: Added caching to reduce health endpoint latency from 3-6s
        to <100ms for repeated requests. Resource metrics are cached for cache_ttl
        seconds (default 5s) since they don't change rapidly.

        Args:
            cache_ttl: How long to cache resource metrics (seconds). Default 5s.
        """
        import asyncio

        # Jan 12, 2026: Check cache to avoid expensive resource detection on every request
        now = time.time()
        cache_key = "_self_info_cache_time"
        last_update = getattr(self, cache_key, 0)
        if (now - last_update) < cache_ttl:
            # Cache hit - self_info already has recent data
            return

        # Run blocking operations in thread pool
        usage = await self._get_resource_usage_async()
        selfplay, training = await asyncio.to_thread(self._count_local_jobs)

        # NAT/relay detection (fast, no subprocess)
        now = time.time()
        if self.known_peers or self.peers:
            last_inbound = self.last_inbound_heartbeat or self.start_time
            self.self_info.nat_blocked = (now - last_inbound) >= NAT_INBOUND_HEARTBEAT_STALE_SECONDS
        else:
            self.self_info.nat_blocked = False

        if not self.self_info.nat_blocked:
            self.self_info.relay_via = ""
        elif self.leader_id and self.leader_id != self.node_id:
            self.self_info.relay_via = self.leader_id

        self.self_info.cpu_percent = usage["cpu_percent"]
        self.self_info.memory_percent = usage["memory_percent"]
        self.self_info.disk_percent = usage["disk_percent"]
        self.self_info.gpu_percent = usage["gpu_percent"]
        self.self_info.gpu_memory_percent = usage["gpu_memory_percent"]
        self.self_info.selfplay_jobs = selfplay
        # Jan 2, 2026: Set max slots for slot-based work queue claiming
        self.self_info.max_selfplay_slots = self._get_max_selfplay_slots_for_node()
        self.self_info.training_jobs = training
        self.self_info.role = self.role
        self.self_info.last_heartbeat = time.time()
        self.self_info.leader_id = self.leader_id or ""

        # Run blocking external work detection in thread pool
        external = await asyncio.to_thread(self._detect_local_external_work)
        self.self_info.cmaes_running = external.get('cmaes_running', False)
        self.self_info.gauntlet_running = external.get('gauntlet_running', False)
        self.self_info.tournament_running = external.get('tournament_running', False)
        self.self_info.data_merge_running = external.get('data_merge_running', False)

        # Health metrics (NFS check in thread pool as it can block)
        self.self_info.nfs_accessible = await asyncio.to_thread(self._check_nfs_accessible)
        self.self_info.code_version = self.build_version
        self.self_info.errors_last_hour = getattr(self, '_error_count_last_hour', 0)
        self.self_info.disk_free_gb = usage.get("disk_free_gb", 0.0)
        self.self_info.active_job_count = (
            selfplay + training +
            (1 if self.self_info.cmaes_running else 0) +
            (1 if self.self_info.gauntlet_running else 0) +
            (1 if self.self_info.tournament_running else 0)
        )

        # Report to resource optimizer (fast, in-memory)
        if HAS_NEW_COORDINATION:
            try:
                optimizer = get_resource_optimizer()
                node_resources = NodeResources(
                    node_id=self.node_id,
                    cpu_percent=usage["cpu_percent"],
                    gpu_percent=usage["gpu_percent"],
                    memory_percent=usage["memory_percent"],
                    disk_percent=usage["disk_percent"],
                    gpu_memory_percent=usage["gpu_memory_percent"],
                    cpu_count=int(getattr(self.self_info, "cpu_count", 0) or 0),
                    memory_gb=float(getattr(self.self_info, "memory_gb", 0) or 0),
                    has_gpu=bool(getattr(self.self_info, "has_gpu", False)),
                    gpu_name=str(getattr(self.self_info, "gpu_name", "") or ""),
                    active_jobs=selfplay + training,
                    selfplay_jobs=selfplay,
                    training_jobs=training,
                    orchestrator="p2p_orchestrator",
                )
                optimizer.report_node_resources(node_resources)
            except (ValueError, KeyError, IndexError, AttributeError):
                pass

        # NODE_CAPACITY_UPDATED event (throttled, fast)
        # Sprint 10 (Jan 3, 2026): Use unified emitter for consistent payloads
        last_emit = getattr(self, "_last_capacity_emit_time", 0)
        if now - last_emit >= 30:
            self._last_capacity_emit_time = now
            try:
                from app.distributed.data_events import emit_node_capacity_updated_sync

                available_slots = max(0, self._get_max_selfplay_jobs() - selfplay - training)
                emit_node_capacity_updated_sync(
                    node_id=self.node_id,
                    gpu_utilization=usage["gpu_percent"],
                    cpu_utilization=usage["cpu_percent"],
                    available_slots=available_slots,
                    reason="heartbeat_async",
                    source="p2p_orchestrator",
                    queue_depth=getattr(self, "_work_queue_depth", 0),
                )
            except (ImportError, RuntimeError, AttributeError):
                pass

        # Jan 12, 2026: Update cache timestamp after successful update
        setattr(self, "_self_info_cache_time", time.time())

    async def _send_heartbeat_to_peer(self, peer_host: str, peer_port: int, scheme: str = "http", timeout: int = 15) -> NodeInfo | None:
        """Send heartbeat to a peer and return their info.

        Jan 27, 2026: Phase 16A - Delegates to HeartbeatManager.
        """
        return await self.heartbeat_manager.send_heartbeat_to_peer(peer_host, peer_port, scheme, timeout)

    async def _send_heartbeat_via_ssh_fallback(
        self, peer_host: str, peer_port: int, payload: dict[str, Any]
    ) -> NodeInfo | None:
        """Send heartbeat via SSH when HTTP fails.

        Jan 27, 2026: Phase 16A - Delegates to HeartbeatManager.
        """
        return await self.heartbeat_manager._send_heartbeat_via_ssh_fallback(peer_host, peer_port, payload)

    async def _bootstrap_from_known_peers(self) -> bool:
        """Import cluster membership from seed peers via `/relay/peers`.

        Jan 27, 2026: Phase 16A - Delegates to HeartbeatManager.
        """
        return await self.heartbeat_manager.bootstrap_from_known_peers()

    async def _continuous_bootstrap_loop(self) -> None:
        """Phase 26.3: Continuously attempt to join cluster when isolated.

        Jan 27, 2026: Phase 16A - Delegates to HeartbeatManager.
        """
        await self.heartbeat_manager.continuous_bootstrap_loop()

    async def _bootstrap_from_multiple_seeds(self) -> bool:
        """Phase 26.3: Try multiple seeds until we join the cluster.

        Priority order:
        1. Cached peers with high reputation (from peer_cache table)
        2. CLI --peers (self.known_peers)
        3. Hardcoded BOOTSTRAP_SEEDS

        Returns True if we successfully connected to any peer.
        """
        # Build seed list with priority ordering
        all_seeds: list[str] = []
        seen: set[str] = set()

        # 1. First, try cached peers by reputation (if available)
        cached_peers = self._get_bootstrap_peers_by_reputation(limit=3)
        for seed in cached_peers:
            if seed and seed not in seen:
                seen.add(seed)
                all_seeds.append(seed)

        # 2. Then, CLI peers and hardcoded seeds (already merged in self.known_peers)
        for seed in self.known_peers:
            if seed and seed not in seen:
                seen.add(seed)
                all_seeds.append(seed)

        if not all_seeds:
            logger.warning("No bootstrap seeds available")
            return False

        # Limit attempts per cycle
        max_attempts = min(MIN_BOOTSTRAP_ATTEMPTS * 2, len(all_seeds))
        timeout = ClientTimeout(total=10)
        success = False

        async with get_client_session(timeout) as session:
            for idx, seed_addr in enumerate(all_seeds[:max_attempts]):
                try:
                    scheme, host, port = self._parse_peer_address(seed_addr)
                    scheme = (scheme or "http").lower()
                    url = f"{scheme}://{host}:{port}/relay/peers"

                    async with session.get(url, headers=self._auth_headers()) as resp:
                        if resp.status != 200:
                            self._update_peer_reputation(seed_addr, success=False)
                            continue

                        data = await resp.json()
                        if not isinstance(data, dict) or not data.get("success"):
                            self._update_peer_reputation(seed_addr, success=False)
                            continue

                    # Successfully got peer list
                    self._update_peer_reputation(seed_addr, success=True)
                    success = True

                    # Import peers
                    peers_data = data.get("peers") or {}
                    if isinstance(peers_data, dict):
                        with self.peers_lock:
                            for node_id, peer_dict in peers_data.items():
                                if node_id and node_id != self.node_id:
                                    try:
                                        info = NodeInfo.from_dict(peer_dict)
                                        self.peers[info.node_id] = info
                                        # Cache the peer for future restarts
                                        self._save_peer_to_cache(
                                            info.node_id,
                                            str(getattr(info, "host", "") or ""),
                                            int(getattr(info, "port", DEFAULT_PORT) or DEFAULT_PORT),
                                            str(getattr(info, "tailscale_ip", "") or "")
                                        )
                                    except (ValueError, KeyError, IndexError, AttributeError):
                                        continue

                        # Jan 12, 2026: Sync to lock-free snapshot after relay peer import
                        self._sync_peer_snapshot()

                    # Adopt leader if provided
                    leader_id = str(data.get("leader_id") or "").strip()
                    if leader_id and leader_id != self.node_id:
                        if self.role == NodeRole.LEADER:
                            logger.info(f"Stepping down for discovered leader: {leader_id}")
                        # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                        self._set_leader(leader_id, reason="continuous_bootstrap_discover_leader", save_state=False)

                    # Handle cluster epoch (Phase 29)
                    incoming_epoch = data.get("cluster_epoch")
                    if incoming_epoch is not None:
                        try:
                            epoch = int(incoming_epoch)
                            if epoch > self._cluster_epoch:
                                logger.info(f"Adopting higher cluster epoch: {epoch} (was {self._cluster_epoch})")
                                self._cluster_epoch = epoch
                                self._save_cluster_epoch()
                        except (ValueError, TypeError):
                            pass

                    # Import voter config if provided
                    incoming_voters = data.get("voter_node_ids") or data.get("voters")
                    if incoming_voters:
                        voters_list = []
                        if isinstance(incoming_voters, list):
                            voters_list = [str(v).strip() for v in incoming_voters if str(v).strip()]
                        elif isinstance(incoming_voters, str):
                            voters_list = [t.strip() for t in incoming_voters.split(",") if t.strip()]
                        if voters_list:
                            if self.quorum_manager.maybe_adopt_voter_node_ids(voters_list, source="learned"):
                                # Sync adopted state back to orchestrator attributes
                                self.voter_node_ids = self.quorum_manager.voter_node_ids
                                self.voter_config_source = self.quorum_manager.voter_config_source
                                self.voter_quorum_size = min(VOTER_MIN_QUORUM, len(self.voter_node_ids)) if self.voter_node_ids else 0

                    self._save_state()
                    logger.info(f"Bootstrap from {host}:{port}: imported {len(peers_data)} peers")
                    break  # Success, no need to try more seeds

                except asyncio.TimeoutError:
                    self._update_peer_reputation(seed_addr, success=False)
                    continue
                except Exception as e:  # noqa: BLE001
                    self._update_peer_reputation(seed_addr, success=False)
                    if self.verbose:
                        logger.debug(f"Bootstrap seed {seed_addr} failed: {e}")
                    continue

        return success

    def _load_bootstrap_seeds_from_config(self) -> list[str]:
        """Load bootstrap seed peers from distributed_hosts.yaml.

        Selects stable coordinator and voter nodes as default seeds when no --peers provided.
        This enables automatic peer discovery via Tailscale even when CLI args are missing.

        Returns:
            List of seed peer URLs (e.g., ["http://100.x.x.x:8770", ...])

        December 30, 2025: Added for automatic P2P peer discovery.
        """
        try:
            from app.config.cluster_config import get_cluster_nodes, get_coordinator_node

            seeds: list[str] = []
            seen_ips: set[str] = set()

            # Primary: coordinator node (most stable)
            coord = get_coordinator_node()
            if coord and getattr(coord, "tailscale_ip", None):
                ip = str(coord.tailscale_ip)
                if ip and ip not in seen_ips:
                    seeds.append(f"http://{ip}:{DEFAULT_PORT}")
                    seen_ips.add(ip)

            # Secondary: voter nodes (stable, always online)
            try:
                nodes = get_cluster_nodes()
                for node in nodes.values():
                    if getattr(node, "role", "") == "voter" and getattr(node, "tailscale_ip", None):
                        ip = str(node.tailscale_ip)
                        if ip and ip not in seen_ips:
                            seeds.append(f"http://{ip}:{DEFAULT_PORT}")
                            seen_ips.add(ip)
                            if len(seeds) >= 5:
                                break
            except Exception:  # noqa: BLE001
                pass

            # Fallback: any active nodes with Tailscale IPs
            if len(seeds) < 3:
                try:
                    nodes = get_cluster_nodes()
                    for node in nodes.values():
                        if getattr(node, "tailscale_ip", None) and getattr(node, "is_active", True):
                            ip = str(node.tailscale_ip)
                            if ip and ip not in seen_ips:
                                seeds.append(f"http://{ip}:{DEFAULT_PORT}")
                                seen_ips.add(ip)
                                if len(seeds) >= 5:
                                    break
                except Exception:  # noqa: BLE001
                    pass

            if seeds:
                logger.debug(f"Loaded {len(seeds)} bootstrap seeds from config: {seeds[:3]}...")

            return seeds

        except ImportError:
            logger.debug("cluster_config not available for bootstrap seeds")
            return []
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Could not load bootstrap seeds from config: {e}")
            return []

    def _is_node_proxy_only(self, node_id: str) -> bool:
        """Check if a node is configured as proxy_only in distributed_hosts.yaml.

        Jan 13, 2026: Added to prevent proxy nodes from becoming cluster leaders.
        Proxy nodes are SSH jump hosts or API proxies with no AI/training capability.

        Args:
            node_id: Node identifier to check

        Returns:
            True if node has status="proxy_only" in config
        """
        # Jan 13, 2026: Known aliases for proxy nodes that may appear under different names
        # These are nodes that registered with a different name than their config entry
        PROXY_ALIASES = {
            "aws-staging": "aws-proxy",  # EC2 staging instance is the proxy
        }

        try:
            hosts = self._load_distributed_hosts().get("hosts", {})
            # Check direct name first
            node_config = hosts.get(node_id, {})
            if node_config.get("status", "") == "proxy_only":
                return True
            # Check if this is a known alias for a proxy node
            if node_id in PROXY_ALIASES:
                alias_config = hosts.get(PROXY_ALIASES[node_id], {})
                if alias_config.get("status", "") == "proxy_only":
                    logger.debug(
                        f"[ProxyCheck] {node_id} is alias for {PROXY_ALIASES[node_id]} (proxy_only)"
                    )
                    return True
            return False
        except Exception:  # noqa: BLE001
            return False

    def _load_distributed_hosts(self) -> dict[str, Any]:
        """Load distributed hosts configuration for NetworkHealthMixin.

        Required by NetworkHealthMixin for cross-verifying P2P mesh health
        against Tailscale connectivity.

        Returns:
            Dict with structure: {"hosts": {node_name: {config...}}}
            Each host config includes: tailscale_ip, p2p_enabled, p2p_port, etc.

        December 30, 2025: Added to fix /network/health endpoint.
        """
        try:
            from app.config.cluster_config import load_cluster_config

            config = load_cluster_config()
            hosts_raw = getattr(config, "hosts_raw", {})

            # Convert to the format expected by NetworkHealthMixin
            # hosts_raw already has the right structure: {node_name: {config_dict}}
            return {"hosts": hosts_raw}

        except ImportError:
            logger.debug("cluster_config not available for distributed hosts")
            return {"hosts": {}}
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Could not load distributed hosts: {e}")
            return {"hosts": {}}

    # NOTE: _follower_discovery_loop() removed Dec 2025 (75 LOC).
    # Now runs via LoopManager as FollowerDiscoveryLoop.
    # See scripts/p2p/loops/discovery_loop.py for implementation.
    # The loop uses callbacks: get_known_peers, query_peer_list, add_peer, is_leader.

    async def _send_relay_heartbeat(self, relay_url: str) -> dict[str, Any]:
        """Send heartbeat via relay endpoint for NAT-blocked nodes.

        Jan 27, 2026: Phase 16A - Delegates to HeartbeatManager.
        """
        return await self.heartbeat_manager.send_relay_heartbeat(relay_url)

    async def _send_initial_relay_heartbeats(self) -> None:
        """Send immediate relay heartbeats on startup for NAT-blocked nodes.

        January 5, 2026: NAT-blocked nodes can't receive inbound connections,
        so they need to proactively register with relay-capable nodes to be
        discoverable by the cluster. This method sends relay heartbeats to
        all configured relay-capable nodes immediately at startup.

        Called after HTTP server starts but before regular heartbeat loop.
        """
        # Load relay-capable nodes from distributed_hosts.yaml
        relay_nodes: list[tuple[str, str, int]] = []  # (node_id, ip, port)
        try:
            from app.config.cluster_config import load_cluster_config
            config = load_cluster_config()
            nodes = getattr(config, "hosts_raw", {}) or {}

            for node_id, node_cfg in nodes.items():
                if node_id == self.node_id:
                    continue  # Skip self
                if not node_cfg.get("relay_capable", False):
                    continue
                if not node_cfg.get("p2p_enabled", True):
                    continue

                # Get the best IP to reach this node (prefer Tailscale)
                ip = node_cfg.get("tailscale_ip") or node_cfg.get("ssh_host", "")
                port = node_cfg.get("p2p_port", DEFAULT_PORT)
                if ip:
                    relay_nodes.append((node_id, ip, port))

        except ImportError:
            logger.warning("[P2P] cluster_config not available for initial relay heartbeats")
            return
        except Exception as e:  # noqa: BLE001
            logger.warning(f"[P2P] Failed to load relay-capable nodes: {e}")
            return

        if not relay_nodes:
            logger.info("[P2P] No relay-capable nodes configured for initial heartbeat")
            return

        logger.info(f"[P2P] Sending initial relay heartbeats to {len(relay_nodes)} relay-capable nodes")

        # Send relay heartbeats to all relay-capable nodes
        success_count = 0
        for node_id, ip, port in relay_nodes:
            relay_url = f"http://{ip}:{port}"
            try:
                result = await self._send_relay_heartbeat(relay_url)
                if result.get("success"):
                    success_count += 1
                    logger.info(f"[P2P] Initial relay heartbeat to {node_id} ({ip}:{port}) succeeded")
                else:
                    error = result.get("error", "unknown")
                    logger.debug(f"[P2P] Initial relay heartbeat to {node_id} failed: {error}")
            except Exception as e:  # noqa: BLE001
                logger.debug(f"[P2P] Initial relay heartbeat to {node_id} error: {e}")

        if success_count > 0:
            logger.info(f"[P2P] NAT-blocked node registered with {success_count}/{len(relay_nodes)} relay nodes")
        else:
            logger.warning(f"[P2P] Failed to register with any relay nodes - cluster discovery may be delayed")

    async def _init_hybrid_coordinator(self) -> None:
        """Initialize HybridCoordinator for Raft-based leader election.

        January 23, 2026: This method initializes the HybridCoordinator which
        provides Raft-based leader election as a replacement for the buggy
        Bully algorithm.

        The HybridCoordinator:
        - Uses PySyncObj's Raft implementation for proven consensus
        - Provides sub-second leader failover (vs 60-90s with Bully)
        - Routes is_leader() calls based on CONSENSUS_MODE env var
        - Falls back to Bully if Raft is unavailable

        To enable Raft:
            export RINGRIFT_RAFT_ENABLED=true
            export RINGRIFT_CONSENSUS_MODE=raft  # or "hybrid"
        """
        print("[DEBUG] _init_hybrid_coordinator() called", flush=True)
        logger.info("[P2P] _init_hybrid_coordinator() called")
        try:
            from app.p2p.constants import RAFT_ENABLED, CONSENSUS_MODE
        except ImportError:
            logger.warning("[P2P] Cannot import p2p constants, HybridCoordinator disabled")
            return

        # Check if Raft is enabled
        if not RAFT_ENABLED and CONSENSUS_MODE == "bully":
            logger.info(
                f"[P2P] HybridCoordinator not started: RAFT_ENABLED={RAFT_ENABLED}, "
                f"CONSENSUS_MODE={CONSENSUS_MODE}. To enable Raft, set "
                "RINGRIFT_RAFT_ENABLED=true and RINGRIFT_CONSENSUS_MODE=raft"
            )
            return

        try:
            from app.p2p.hybrid_coordinator import HybridCoordinator

            self._hybrid_coordinator = HybridCoordinator(
                orchestrator=self,
                on_leader_change=self._on_raft_leader_change,
            )
            await self._hybrid_coordinator.start()

            # Check if Raft initialized successfully
            if self._hybrid_coordinator:
                status = self._hybrid_coordinator.get_status()
                # Note: get_status() returns a dict, not HybridStatus object
                logger.info(
                    f"[P2P] HybridCoordinator started: "
                    f"consensus_mode={status.get('consensus_mode', 'unknown')}, "
                    f"raft_enabled={status.get('raft', {}).get('enabled', False)}, "
                    f"raft_available={status.get('raft', {}).get('available', False)}"
                )
        except ImportError as e:
            logger.warning(f"[P2P] HybridCoordinator not available: {e}")
            self._hybrid_coordinator = None
        except Exception as e:
            logger.error(f"[P2P] HybridCoordinator initialization failed: {e}")
            self._hybrid_coordinator = None

    def _on_raft_leader_change(self, leader_address: str | None) -> None:
        """Handle Raft leader change events.

        January 23, 2026: This callback is invoked by HybridCoordinator when
        Raft elects a new leader. We update the orchestrator's leader_id to
        keep it synchronized with Raft's view.

        Args:
            leader_address: The new leader's address (ip:port) or None if no leader
        """
        if not leader_address:
            logger.info("[Raft] No leader elected - Raft cluster may be forming")
            return

        # Convert Raft address (ip:port) to node_id
        leader_node_id = self._resolve_raft_address_to_node_id(leader_address)
        if leader_node_id:
            # Update orchestrator's leader_id via _set_leader for consistency
            self._set_leader(leader_node_id, reason="raft_election")
            logger.info(f"[Raft] Leader elected: {leader_node_id} (address: {leader_address})")
        else:
            logger.warning(
                f"[Raft] Leader elected at {leader_address} but cannot resolve to node_id"
            )

    def _resolve_raft_address_to_node_id(self, raft_address: str) -> str | None:
        """Resolve a Raft address (ip:port) to a node_id.

        Args:
            raft_address: The Raft address in "ip:port" format

        Returns:
            The node_id if found, or None if not resolvable
        """
        try:
            # Parse the address
            ip, port_str = raft_address.rsplit(":", 1)

            # First, check if this is our own address
            if ip in (self.host, self.public_host, getattr(self, 'tailscale_ip', None)):
                return self.node_id

            # Check our IP-to-node mapping
            if hasattr(self, '_ip_to_node_map') and ip in self._ip_to_node_map:
                return self._ip_to_node_map[ip]

            # Check peer list for matching IP
            with self.peers_lock:
                for node_id, peer_info in self.peers.items():
                    peer_host = getattr(peer_info, 'host', '') or ''
                    peer_tailscale = getattr(peer_info, 'tailscale_ip', '') or ''
                    if ip in (peer_host, peer_tailscale):
                        return node_id

            # Fallback: Check cluster config
            try:
                from app.config.cluster_config import get_cluster_nodes
                for node in get_cluster_nodes():
                    node_ip = getattr(node, 'tailscale_ip', '') or getattr(node, 'ssh_host', '')
                    if node_ip == ip:
                        return node.name
            except ImportError:
                pass

        except (ValueError, AttributeError) as e:
            logger.debug(f"[Raft] Failed to parse address {raft_address}: {e}")

        return None

    async def _send_startup_peer_announcements(self) -> None:
        """Send immediate announcements to all known peers on startup.

        January 7, 2026: Instead of waiting for the first heartbeat interval,
        immediately announce to all known peers. This reduces discovery latency
        from 15-30s down to 2-5s after startup.

        This is called for ALL nodes (not just NAT-blocked) to ensure fast
        peer discovery after restarts or cluster updates.
        """
        # Send to known peers from config
        success_count = 0
        total_attempts = 0

        print(f"[DEBUG] _send_startup_peer_announcements: {len(self.known_peers)} known peers", flush=True)
        for peer_addr in self.known_peers:
            try:
                scheme, host, port = self._parse_peer_address(peer_addr)
            except (AttributeError, ValueError):
                continue

            total_attempts += 1
            try:
                info = await self._send_heartbeat_to_peer(host, port, scheme=scheme)
                if info and info.node_id != self.node_id:
                    async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                        is_first_contact = info.node_id not in self.peers
                        info.last_heartbeat = time.time()
                        self.peers[info.node_id] = info
                    if is_first_contact:
                        logger.info(f"[P2P] Startup announcement discovered peer: {info.node_id}")
                    success_count += 1
            except Exception as e:  # noqa: BLE001
                logger.debug(f"[P2P] Startup announcement to {host}:{port} failed: {e}")

        if success_count > 0:
            logger.info(f"[P2P] Startup announcements: {success_count}/{total_attempts} peers reachable")
        elif total_attempts > 0:
            logger.warning(f"[P2P] Startup announcements: no peers reachable (tried {total_attempts})")

    async def _execute_relay_commands(self, commands: list[dict[str, Any]]) -> None:
        """Execute relay commands (polling mode for NAT-blocked nodes)."""
        now = time.time()
        for cmd in commands:
            try:
                cmd_id = str(cmd.get("id") or "")
                cmd_type = str(cmd.get("type") or "")
                payload = cmd.get("payload") or {}
                if not cmd_id or not cmd_type:
                    continue

                # Check for stale commands (>5 min old indicates relay/polling issues)
                cmd_ts = cmd.get("ts") or cmd.get("timestamp") or now
                cmd_age_secs = now - float(cmd_ts)
                if cmd_age_secs > 300:
                    logger.info(f"WARNING: Relay command {cmd_id} ({cmd_type}) is {cmd_age_secs:.0f}s old - relay delivery may be delayed")

                attempts = int(self.relay_command_attempts.get(cmd_id, 0) or 0) + 1
                self.relay_command_attempts[cmd_id] = attempts

                ok = False
                err = ""
                if cmd_type == "start_job":
                    job_type = JobType(str(payload.get("job_type") or "selfplay"))
                    board_type = str(payload.get("board_type") or "square8")
                    num_players = int(payload.get("num_players") or 2)
                    engine_mode = str(payload.get("engine_mode") or "mixed")
                    job_id = str(payload.get("job_id") or "")

                    if job_id:
                        with self.jobs_lock:
                            existing = self.local_jobs.get(job_id)
                        if existing and existing.status == "running":
                            ok = True
                        else:
                            job = await self._start_local_job(
                                job_type,
                                board_type=board_type,
                                num_players=num_players,
                                engine_mode=engine_mode,
                                job_id=job_id,
                            )
                            ok = job is not None
                    else:
                        job = await self._start_local_job(
                            job_type,
                            board_type=board_type,
                            num_players=num_players,
                            engine_mode=engine_mode,
                        )
                        ok = job is not None
                elif cmd_type == "cleanup":
                    asyncio.create_task(self._cleanup_local_disk())
                    ok = True
                elif cmd_type == "restart_stuck_jobs":
                    asyncio.create_task(self._restart_local_stuck_jobs())
                    ok = True
                elif cmd_type == "reduce_selfplay":
                    target = payload.get("target_selfplay_jobs", payload.get("target", 0))
                    reason = str(payload.get("reason") or "relay")
                    try:
                        target_jobs = int(target)
                    except (ValueError):
                        target_jobs = 0
                    await self._reduce_local_selfplay_jobs(target_jobs, reason=reason)
                    ok = True
                elif cmd_type == "cleanup_files":
                    files = payload.get("files", []) or []
                    reason = str(payload.get("reason") or "relay")
                    if not isinstance(files, list) or not files:
                        ok = False
                        err = "no_files"
                    else:
                        data_dir = self.get_data_directory()
                        freed_bytes = 0
                        deleted_count = 0
                        data_root = data_dir.resolve()
                        for file_path in files:
                            full_path = data_dir / (str(file_path or "").lstrip("/"))
                            try:
                                resolved = full_path.resolve()
                                resolved.relative_to(data_root)
                            except (AttributeError):
                                continue
                            if not resolved.exists():
                                continue
                            try:
                                size = resolved.stat().st_size
                                resolved.unlink()
                                freed_bytes += size
                                deleted_count += 1
                            except (AttributeError):
                                continue
                        print(
                            f"[P2P] Relay cleanup_files: {deleted_count} files deleted, "
                            f"{freed_bytes / 1e6:.1f}MB freed (reason={reason})"
                        )
                        ok = True
                elif cmd_type == "canonical_selfplay":
                    job_id = str(payload.get("job_id") or "")
                    board_type = str(payload.get("board_type") or "square8")
                    num_players = int(payload.get("num_players") or 2)
                    num_games = int(payload.get("num_games") or payload.get("games_per_node") or 500)
                    seed = int(payload.get("seed") or 0)
                    if not job_id:
                        ok = False
                        err = "missing_job_id"
                    else:
                        asyncio.create_task(
                            self._run_local_canonical_selfplay(job_id, board_type, num_players, num_games, seed)
                        )
                        ok = True
                else:
                    ok = False
                    err = f"unknown_command_type:{cmd_type}"

                if ok:
                    self._add_pending_relay_ack(cmd_id)
                    self._add_pending_relay_result({"id": cmd_id, "ok": True})
                    self.relay_command_attempts.pop(cmd_id, None)
                else:
                    if not err:
                        err = "command_failed"
                    if attempts >= RELAY_COMMAND_MAX_ATTEMPTS:
                        self._add_pending_relay_ack(cmd_id)
                        self._add_pending_relay_result({"id": cmd_id, "ok": False, "error": err})
                        self.relay_command_attempts.pop(cmd_id, None)
            except Exception as exc:
                try:
                    cmd_id = str(cmd.get("id") or "")
                    if cmd_id:
                        attempts = int(self.relay_command_attempts.get(cmd_id, 0) or 0)
                        if attempts >= RELAY_COMMAND_MAX_ATTEMPTS:
                            self._add_pending_relay_ack(cmd_id)
                            self._add_pending_relay_result({"id": cmd_id, "ok": False, "error": str(exc)})
                            self.relay_command_attempts.pop(cmd_id, None)
                except (ValueError, AttributeError):
                    continue

    async def _heartbeat_loop(self):
        """Send heartbeats to all known peers."""
        # Jan 11, 2026: Phase 5 - Initial heartbeat burst to prevent startup race
        # Send immediate heartbeats to all known peers so they discover us quickly
        # This fixes the issue where peers think we're dead before our first heartbeat
        logger.info("Sending initial heartbeat burst to known peers")
        for peer_addr in self.known_peers:
            try:
                scheme, host, port = self._parse_peer_address(peer_addr)
                await self._send_heartbeat_to_peer(host, port, scheme=scheme, timeout=10)
            except Exception:
                pass  # Best effort, regular loop will retry

        while self.running:
            try:
                # Jan 20, 2026: Check for and fix leadership state desync every heartbeat
                # This recovers from gossip race conditions where leader_id/role diverge
                try:
                    if self._recover_leadership_desync():
                        logger.info("[HeartbeatLoop] Recovered from leadership desync")
                except Exception as e:
                    logger.debug(f"[HeartbeatLoop] Desync check failed: {e}")

                # Jan 23, 2026: Phase 2 - Reconcile ULSM with gossip consensus every 30s
                # This fixes the issue where nodes are consensus leader but don't claim leadership
                now = time.time()
                last_reconcile = getattr(self, "_last_leadership_reconcile", 0)
                if now - last_reconcile >= 30.0:
                    self._last_leadership_reconcile = now
                    try:
                        if self._reconcile_leadership_state():
                            logger.info("[HeartbeatLoop] Reconciled leadership state with gossip consensus")
                    except Exception as e:
                        logger.debug(f"[HeartbeatLoop] Leadership reconciliation failed: {e}")

                # Send to known peers from config
                for peer_addr in self.known_peers:
                    try:
                        scheme, host, port = self._parse_peer_address(peer_addr)
                    except (AttributeError):
                        continue

                    # Use relay heartbeat for HTTPS endpoints (they're proxies/relays),
                    # explicitly configured relay peers (--relay-peers flag),
                    # or if this node is NAT-blocked and needs to relay ALL outbound heartbeats
                    use_relay = scheme == "https" or peer_addr in self.relay_peers or self._force_relay_mode
                    if use_relay:
                        # Relay/proxy endpoint, use relay heartbeat
                        relay_url = f"{scheme}://{host}" if port in (80, 443) else f"{scheme}://{host}:{port}"
                        result = await self._send_relay_heartbeat(relay_url)
                        if result.get("success"):
                            # Relay heartbeat already updates peers and leader
                            continue

                    info = await self._send_heartbeat_to_peer(host, port, scheme=scheme)
                    if info:
                        if info.node_id == self.node_id:
                            continue
                        # Dec 2025: Track first-contact for HOST_ONLINE emission
                        async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                            is_first_contact = info.node_id not in self.peers
                            info.last_heartbeat = time.time()
                            self.peers[info.node_id] = info
                        # Dec 2025: Emit HOST_ONLINE for newly discovered peers
                        if is_first_contact:
                            capabilities = []
                            if getattr(info, "has_gpu", False):
                                gpu_type = getattr(info, "gpu_type", "") or "gpu"
                                capabilities.append(gpu_type)
                            else:
                                capabilities.append("cpu")
                            await self._emit_host_online(info.node_id, capabilities)
                            logger.info(f"First-contact peer via heartbeat loop: {info.node_id}")
                        if info.role == NodeRole.LEADER and info.node_id != self.node_id:
                            async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                                peers_snapshot = list(self.peers.values())
                            conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])
                            if not self._is_leader_eligible(info, conflict_keys, require_alive=False):
                                continue
                            if self.role == NodeRole.LEADER and info.node_id <= self.node_id:
                                continue
                            if (
                                self.leader_id
                                and self.leader_id != info.node_id
                                and self._is_leader_lease_valid()
                                and info.node_id <= self.leader_id
                            ):
                                continue
                            if self.leader_id != info.node_id or self.role != NodeRole.FOLLOWER:
                                logger.info(f"Following configured leader from heartbeat: {info.node_id}")
                            prev_leader = self.leader_id
                            # Provisional lease: allow time for the leader to send
                            # a /coordinator lease renewal after we discover it via
                            # heartbeat (prevents leaderless oscillation right after
                            # restarts/partitions).
                            if prev_leader != info.node_id or not self._is_leader_lease_valid():
                                self.leader_lease_id = ""
                                self.leader_lease_expires = time.time() + LEADER_LEASE_DURATION
                            # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                            self._set_leader(info.node_id, reason="heartbeat_configured_leader", save_state=False)

                # Send to discovered peers (skip NAT-blocked peers and ambiguous endpoints).
                # Jan 12, 2026: Use cached snapshot to reduce lock contention (1s staleness OK for heartbeat)
                peers_snapshot = self._get_cached_peer_snapshot(max_age_seconds=1.0)
                conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])
                peer_list = [
                    p for p in peers_snapshot
                    if (
                        not p.nat_blocked
                        and self._endpoint_key(p) not in conflict_keys
                    )
                ]

                for peer in peer_list:
                    if peer.node_id != self.node_id:
                        if not peer.should_retry():
                            continue

                        # Jan 11, 2026: Phase 2 - Adaptive heartbeat timing based on consecutive failures
                        # This gives flaky peers time to recover without spamming them
                        failures = int(getattr(peer, "consecutive_failures", 0) or 0)
                        if failures == 0:
                            heartbeat_interval = HEARTBEAT_INTERVAL  # 15s for healthy peers
                        elif failures == 1:
                            heartbeat_interval = 5  # Quick retry after first failure
                        elif failures == 2:
                            heartbeat_interval = 10  # Second retry
                        elif failures < 5:
                            heartbeat_interval = 20  # Slower retries
                        else:
                            heartbeat_interval = 30  # Very slow for consistently failing

                        # Check if heartbeat is due for this peer
                        last_sent = float(getattr(peer, "last_heartbeat_sent", 0.0) or 0.0)
                        now = time.time()
                        if now - last_sent < heartbeat_interval:
                            continue  # Not time yet for this peer

                        # Mark the send time
                        peer.last_heartbeat_sent = now

                        peer_scheme = getattr(peer, "scheme", "http") or "http"
                        info = await self._send_heartbeat_to_peer(peer.host, peer.port, scheme=peer_scheme)
                        if not info and getattr(peer, "reported_host", "") and getattr(peer, "reported_port", 0):
                            # Multi-path retry: fall back to self-reported endpoint when the
                            # observed reachable endpoint fails (e.g., mixed overlays).
                            try:
                                rh = str(getattr(peer, "reported_host", "") or "").strip()
                                rp = int(getattr(peer, "reported_port", 0) or 0)
                            except (ValueError, AttributeError):
                                rh, rp = "", 0
                            if rh and rp and (rh != peer.host or rp != peer.port):
                                info = await self._send_heartbeat_to_peer(rh, rp, scheme=peer_scheme)
                        # Self-healing: Tailscale IP fallback when both primary and reported fail
                        if not info:
                            ts_ip = self._get_tailscale_ip_for_peer(peer.node_id)
                            if ts_ip and ts_ip != peer.host:
                                # Try Tailscale mesh IP (100.x.x.x)
                                info = await self._send_heartbeat_to_peer(ts_ip, peer.port, scheme=peer_scheme)
                                if info:
                                    logger.info(f"Reached {peer.node_id} via Tailscale ({ts_ip})")
                        if info:
                            info.consecutive_failures = 0
                            info.last_failure_time = 0.0
                            async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                                info.last_heartbeat = time.time()
                                self.peers[info.node_id] = info
                            if info.role == NodeRole.LEADER and self.role != NodeRole.LEADER:
                                if not self._is_leader_eligible(info, conflict_keys, require_alive=False):
                                    continue
                                if (
                                    self.leader_id
                                    and self.leader_id != info.node_id
                                    and self._is_leader_lease_valid()
                                    and info.node_id <= self.leader_id
                                ):
                                    continue
                                if self.leader_id != info.node_id:
                                    logger.info(f"Adopted leader from heartbeat: {info.node_id}")
                                prev_leader = self.leader_id
                                if prev_leader != info.node_id or not self._is_leader_lease_valid():
                                    self.leader_lease_id = ""
                                    self.leader_lease_expires = time.time() + LEADER_LEASE_DURATION
                                # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                                self._set_leader(info.node_id, reason="heartbeat_adopt_leader", save_state=False)
                        else:
                            async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                                existing = self.peers.get(peer.node_id)
                                if existing:
                                    existing.consecutive_failures = int(getattr(existing, "consecutive_failures", 0) or 0) + 1
                                    existing.last_failure_time = time.time()

                # If we're only connected to a seed peer (or lost cluster membership),
                # pull a fresh peer snapshot so leader election converges quickly.
                await self._bootstrap_from_known_peers()

                # Get current time for all time-based checks in this cycle
                now = time.time()

                # NAT-blocked nodes: poll a relay endpoint for peer snapshots + commands.
                if getattr(self.self_info, "nat_blocked", False):
                    if now - self.last_relay_heartbeat >= RELAY_HEARTBEAT_INTERVAL:
                        relay_urls: list[str] = []
                        leader_peer = self._get_leader_peer()
                        if leader_peer and leader_peer.node_id != self.node_id:
                            relay_urls.append(f"{leader_peer.scheme}://{leader_peer.host}:{leader_peer.port}")
                        for peer_addr in self.known_peers:
                            try:
                                scheme, host, port = self._parse_peer_address(peer_addr)
                            except (AttributeError):
                                continue
                            relay_urls.append(f"{scheme}://{host}:{port}")
                        seen: set[str] = set()
                        relay_urls = [u for u in relay_urls if not (u in seen or seen.add(u))]

                        for relay_url in relay_urls:
                            result = await self._send_relay_heartbeat(relay_url)
                            if result.get("success"):
                                self.last_relay_heartbeat = now
                                break

                # Check for dead peers
                await self._check_dead_peers_async()

                # Dec 30, 2025: Probe retired peers periodically to detect recovery
                # This runs every PEER_RECOVERY_RETRY_INTERVAL (120s) to actively probe
                # retired nodes that may have come back online after cluster restart.
                last_probe = getattr(self, "_last_retired_probe", 0.0)
                if now - last_probe >= PEER_RECOVERY_RETRY_INTERVAL:
                    self._last_retired_probe = now
                    try:
                        await self._probe_retired_peers_async()
                    except Exception as e:
                        logger.warning(f"Error in retired peer probe: {e}")

                # Self-healing: detect network partition and trigger Tailscale-priority mode
                if self._detect_network_partition():
                    self._enable_tailscale_priority()
                    # Also enable partition-local election if no voters reachable
                    if not self._has_voter_quorum():
                        self._enable_partition_local_election()
                    # Force refresh all IP sources to discover alternative paths
                    last_refresh = getattr(self, "_last_partition_ip_refresh", 0)
                    if time.time() - last_refresh > 60:  # Refresh at most once per minute
                        self._last_partition_ip_refresh = time.time()
                        asyncio.create_task(self._force_ip_refresh_all_sources())

                    # Jan 13, 2026: Exponential backoff during isolation
                    # Check if we're completely isolated (no alive peers)
                    alive_peers = self._peer_query.alive_count().unwrap_or(0)
                    if alive_peers == 0:
                        # Track isolation start time
                        if not hasattr(self, "_isolation_start"):
                            self._isolation_start = time.time()
                            self._isolation_backoff_seconds = HEARTBEAT_INTERVAL
                            logger.warning("Node is isolated - no alive peers, starting exponential backoff")

                        # Calculate exponential backoff based on isolation duration
                        isolation_duration = time.time() - self._isolation_start
                        if isolation_duration > 60:  # After 1 min
                            self._isolation_backoff_seconds = min(30, self._isolation_backoff_seconds * 1.5)
                        if isolation_duration > 180:  # After 3 min
                            self._isolation_backoff_seconds = min(60, self._isolation_backoff_seconds * 1.5)
                        if isolation_duration > 300:  # After 5 min
                            self._isolation_backoff_seconds = min(120, self._isolation_backoff_seconds)

                        logger.debug(f"Isolated for {isolation_duration:.0f}s, backoff={self._isolation_backoff_seconds:.0f}s")
                        # Apply additional backoff sleep (on top of normal HEARTBEAT_INTERVAL)
                        extra_backoff = self._isolation_backoff_seconds - HEARTBEAT_INTERVAL
                        if extra_backoff > 0:
                            await asyncio.sleep(extra_backoff)
                    else:
                        # Reset isolation tracking when peers are reachable
                        if hasattr(self, "_isolation_start"):
                            logger.info(f"Isolation ended after {time.time() - self._isolation_start:.0f}s, {alive_peers} peers alive")
                            delattr(self, "_isolation_start")
                            if hasattr(self, "_isolation_backoff_seconds"):
                                delattr(self, "_isolation_backoff_seconds")
                elif getattr(self, "_tailscale_priority", False):
                    # Check if priority mode should expire
                    if time.time() > getattr(self, "_tailscale_priority_until", 0):
                        # Check if connectivity recovered
                        alive_count = self._peer_query.alive_count().unwrap_or(0)
                        if alive_count > 0:
                            self._disable_tailscale_priority()

                # Self-healing: check if partition healed and restore original voters
                if hasattr(self, "_original_voters"):
                    self._restore_original_voters()

                # Dynamic voter management: promote/demote voters based on health
                # Only the leader manages voters to ensure consistency
                if self.role == NodeRole.LEADER:
                    self._manage_dynamic_voters()

                # Health-based leadership: step down if we can't reach enough peers
                if self.role == NodeRole.LEADER and not self._check_leader_health():
                    logger.info("Stepping down due to degraded health")
                    # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                    self._set_leader(None, reason="degraded_health", save_state=True)
                    self.leader_lease_id = ""
                    self.leader_lease_expires = 0.0
                    self._release_voter_grant_if_self()
                    # Jan 13, 2026: Add sleep before continue to prevent busy loop
                    # when repeatedly stepping down due to degraded health
                    await asyncio.sleep(HEARTBEAT_INTERVAL)
                    continue  # Skip leader duties this cycle

                # P0 Dec 2025: Monitor leader heartbeat for early warning
                # Emit LEADER_HEARTBEAT_MISSING if leader lease is approaching expiry
                if self.role == NodeRole.FOLLOWER and self.leader_id:
                    now = time.time()
                    # Warning threshold: 45 seconds (3x lease renewal interval)
                    heartbeat_warning_threshold = LEADER_LEASE_RENEW_INTERVAL * 3
                    time_until_expiry = self.leader_lease_expires - now
                    # Emit warning if lease will expire within warning threshold
                    if 0 < time_until_expiry < heartbeat_warning_threshold:
                        last_warning = getattr(self, "_last_heartbeat_missing_warning", 0.0)
                        # Only warn once per 30 seconds to avoid spam
                        if now - last_warning > 30:
                            self._last_heartbeat_missing_warning = now
                            delay_seconds = (LEADER_LEASE_DURATION - time_until_expiry)
                            try:
                                from app.distributed.data_events import emit_leader_heartbeat_missing
                                asyncio.create_task(emit_leader_heartbeat_missing(
                                    leader_id=self.leader_id,
                                    last_heartbeat=self.leader_lease_expires - LEADER_LEASE_DURATION,
                                    expected_interval=LEADER_LEASE_RENEW_INTERVAL,
                                    delay_seconds=delay_seconds,
                                    source=self.node_id,
                                ))
                            except ImportError:
                                pass  # Graceful degradation if event system not available

                # LEARNED LESSONS - Lease renewal to maintain leadership
                if self.role == NodeRole.LEADER:
                    await self._renew_leader_lease()

                # P2P monitoring: start/stop services based on leadership
                await self._stop_monitoring_if_not_leader()
                if self.role == NodeRole.LEADER:
                    await self._start_monitoring_if_leader()

                # P2P auto-deployer: start/stop based on leadership
                if self.role != NodeRole.LEADER and self._auto_deployer_task:
                    await self._stop_p2p_auto_deployer()
                elif self.role == NodeRole.LEADER and not self._auto_deployer_task:
                    await self._start_p2p_auto_deployer()

                # Report node resources to resource_optimizer for cluster-wide utilization tracking
                # This enables cooperative 60-80% utilization targeting across orchestrators
                if HAS_NEW_COORDINATION and get_resource_optimizer is not None:
                    try:
                        optimizer = get_resource_optimizer()
                        self._update_self_info()
                        node_resources = NodeResources(
                            node_id=self.node_id,
                            cpu_percent=self.self_info.cpu_percent,
                            memory_percent=self.self_info.memory_percent,
                            active_jobs=self.self_info.selfplay_jobs + self.self_info.training_jobs,
                            has_gpu=self.self_info.has_gpu,
                            gpu_name=self.self_info.gpu_type or "",
                        )
                        optimizer.report_node_resources(node_resources)
                    except (AttributeError):
                        pass  # Non-critical, don't disrupt heartbeat

                # Save state periodically
                self._save_state()

            except Exception as e:  # noqa: BLE001
                logger.info(f"Heartbeat error: {e}")

            # Notify systemd watchdog that we're still alive
            systemd_notify_watchdog()

            await asyncio.sleep(HEARTBEAT_INTERVAL)

    async def _voter_heartbeat_loop(self):
        """
        Dedicated high-frequency heartbeat loop for voter nodes.

        IMPROVEMENTS:
        - Faster heartbeat interval (10s vs 30s) for voter nodes
        - Aggressively clears NAT-blocked status on successful heartbeats
        - Maintains full mesh connectivity between all voters
        - Propagates voter list to ensure consistent quorum
        """
        # Only run if this node is a voter
        if self.node_id not in self.voter_node_ids:
            return

        logger.info(f"Starting voter heartbeat loop (interval={VOTER_HEARTBEAT_INTERVAL}s)")
        last_voter_mesh_refresh = 0.0

        while self.running:
            try:
                now = time.time()

                # Get all other voters
                other_voters = [v for v in self.voter_node_ids if v != self.node_id]

                # Jan 12, 2026: Consolidate peer updates to reduce lock contention
                # Collect all peer updates, then apply in a single lock acquisition
                peer_updates: dict[str, dict] = {}

                for voter_id in other_voters:
                    # Find voter peer info by IP mapping (Jan 2, 2026 fix)
                    # Peers dict uses IP:port keys from SWIM, not friendly node_ids
                    # Jan 2026: Delegated to QuorumManager.
                    async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                        peer_key, voter_peer = self.quorum_manager.find_voter_peer_by_ip(voter_id)

                    if not voter_peer:
                        # Try to discover voter from known peers
                        await self._discover_voter_peer(voter_id)
                        # Jan 13, 2026: Add brief sleep to prevent busy loop when many voters unavailable
                        await asyncio.sleep(0.1)
                        continue

                    # Attempt heartbeat to voter
                    success = await self._send_voter_heartbeat(voter_peer)

                    if success:
                        # AGGRESSIVE NAT RECOVERY: Clear NAT-blocked immediately on success
                        if VOTER_NAT_RECOVERY_AGGRESSIVE and voter_peer.nat_blocked:
                            logger.info(f"Voter {voter_id} (key={peer_key}) NAT-blocked status cleared (heartbeat succeeded)")
                            # Collect update instead of acquiring lock here
                            if peer_key:
                                peer_updates[peer_key] = {
                                    "nat_blocked": False,
                                    "nat_blocked_since": 0.0,
                                    "consecutive_failures": 0,
                                }
                    else:
                        # Try alternative endpoints
                        success = await self._try_voter_alternative_endpoints(voter_peer)

                        if not success:
                            # Collect failure update instead of acquiring lock here
                            if peer_key:
                                peer_updates[peer_key] = {"inc_failures": True}

                # Apply all peer updates in a single lock acquisition
                if peer_updates:
                    async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                        for key, updates in peer_updates.items():
                            if key in self.peers:
                                if "nat_blocked" in updates:
                                    self.peers[key].nat_blocked = updates["nat_blocked"]
                                    self.peers[key].nat_blocked_since = updates["nat_blocked_since"]
                                    self.peers[key].consecutive_failures = updates["consecutive_failures"]
                                elif updates.get("inc_failures"):
                                    self.peers[key].consecutive_failures = \
                                        int(getattr(self.peers[key], "consecutive_failures", 0) or 0) + 1

                # Periodic voter mesh refresh - ensure all voters know about each other
                if now - last_voter_mesh_refresh > VOTER_MESH_REFRESH_INTERVAL:
                    last_voter_mesh_refresh = now
                    await self._refresh_voter_mesh()

                # Jan 2, 2026: Voter health monitoring - check and log voter status
                # This runs every heartbeat cycle for proactive alerting
                health = self._check_voter_health()
                if not health.get("quorum_ok"):
                    logger.error(
                        f"[VoterHealth] QUORUM LOST: {health['voters_alive']}/{health['voters_total']} "
                        f"voters alive, need {health['quorum_size']}"
                    )
                elif health.get("quorum_threatened"):
                    logger.warning(
                        f"[VoterHealth] QUORUM THREATENED: {health['voters_alive']}/{health['voters_total']} "
                        f"voters alive (at threshold of {health['quorum_size']})"
                    )

            except Exception as e:  # noqa: BLE001
                logger.info(f"Voter heartbeat error: {e}")

            await asyncio.sleep(VOTER_HEARTBEAT_INTERVAL)

    async def _reconnect_dead_peers_loop(self) -> None:
        """Attempt to reconnect dead (but not retired) peers with exponential backoff.

        Jan 11, 2026: P2P Stability Fix - Phase 1

        Problem: When a peer goes "dead" (after 60-90s with no heartbeat), there's
        NO reconnection logic until it's "retired" (1800s). Dead nodes just stay dead
        in the peer list, causing cluster fragmentation.

        Solution: This loop actively probes dead peers with exponential backoff:
        - First 60s after death: retry every 15s (fast recovery for transient issues)
        - 60-120s: retry every 30s
        - 120-300s: retry every 60s
        - 300s+: retry every 120s (slow probing until retirement)

        This ensures dead peers can recover quickly while not overloading the network
        with reconnection attempts for truly offline nodes.
        """
        # Wait for startup to complete before starting reconnection attempts
        await asyncio.sleep(30)

        logger.info("Starting dead peer reconnection loop (Phase 1 P2P stability fix)")

        while self.running:
            try:
                now = time.time()
                async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                    peers_snapshot = list(self.peers.items())

                reconnect_attempts = 0
                reconnect_successes = 0

                for node_id, peer in peers_snapshot:
                    if node_id == self.node_id:
                        continue

                    # Skip alive or retired peers
                    if peer.is_alive():
                        continue
                    if getattr(peer, "retired", False):
                        continue

                    # Calculate time since death
                    dead_since = now - peer.last_heartbeat

                    # Skip if not actually dead yet (within timeout window)
                    # Jan 19, 2026: Use jittered timeout to prevent synchronized retries
                    # Jan 22, 2026: Use cached jittered timeout to ensure consistent death detection
                    if dead_since < self._get_cached_jittered_timeout():
                        continue

                    # Exponential backoff: 15s, 30s, 60s, 120s based on time since death
                    if dead_since < 60:
                        retry_interval = 15
                    elif dead_since < 120:
                        retry_interval = 30
                    elif dead_since < 300:
                        retry_interval = 60
                    else:
                        retry_interval = 120

                    # Check if it's time to retry this peer
                    last_retry = getattr(peer, "last_reconnect_attempt", 0.0)
                    if now - last_retry < retry_interval:
                        continue

                    # Mark the attempt time before trying (to prevent rapid retries on error)
                    peer.last_reconnect_attempt = now
                    reconnect_attempts += 1

                    # Try multiple paths to reach the peer
                    success = False
                    peer_scheme = getattr(peer, "scheme", "http") or "http"

                    # Path 1: Primary endpoint
                    try:
                        result = await self._send_heartbeat_to_peer(
                            peer.host, peer.port, scheme=peer_scheme, timeout=15
                        )
                        if result:
                            success = True
                    except Exception:
                        pass

                    # Path 2: Tailscale IP fallback
                    if not success:
                        ts_ip = self._get_tailscale_ip_for_peer(node_id)
                        if ts_ip and ts_ip != peer.host:
                            try:
                                result = await self._send_heartbeat_to_peer(
                                    ts_ip, peer.port, scheme=peer_scheme, timeout=15
                                )
                                if result:
                                    success = True
                                    logger.info(f"Reconnected to {node_id} via Tailscale ({ts_ip})")
                            except Exception:
                                pass

                    # Path 3: Reported host fallback
                    if not success:
                        reported_host = getattr(peer, "reported_host", "")
                        reported_port = getattr(peer, "reported_port", 0)
                        if reported_host and reported_port and (reported_host != peer.host or reported_port != peer.port):
                            try:
                                result = await self._send_heartbeat_to_peer(
                                    reported_host, reported_port, scheme=peer_scheme, timeout=15
                                )
                                if result:
                                    success = True
                                    logger.info(f"Reconnected to {node_id} via reported endpoint ({reported_host}:{reported_port})")
                            except Exception:
                                pass

                    if success:
                        reconnect_successes += 1
                        async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                            if node_id in self.peers:
                                self.peers[node_id].consecutive_failures = 0
                                self.peers[node_id].last_heartbeat = time.time()
                        logger.info(f"Reconnected to dead peer {node_id} (was dead for {dead_since:.0f}s)")

                        # Emit HOST_ONLINE event for recovered peer
                        try:
                            self._emit_host_online_sync(node_id, ["recovered"])
                        except Exception:
                            pass
                    else:
                        logger.debug(f"Reconnect to {node_id} failed (dead for {dead_since:.0f}s, retry in {retry_interval}s)")

                if reconnect_attempts > 0:
                    logger.debug(f"Reconnect loop: {reconnect_successes}/{reconnect_attempts} attempts succeeded")

            except Exception as e:
                logger.error(f"Error in dead peer reconnection loop: {e}")

            # Check every 15s for peers that need reconnection
            await asyncio.sleep(15)

    async def _send_voter_heartbeat(self, voter_peer) -> bool:
        """Send a heartbeat to a voter peer with shorter timeout.

        Jan 27, 2026: Phase 16A - Delegates to HeartbeatManager.
        """
        return await self.heartbeat_manager.send_voter_heartbeat(voter_peer)

    async def _try_voter_alternative_endpoints(self, voter_peer) -> bool:
        """Try alternative endpoints for a voter peer."""
        peer_scheme = getattr(voter_peer, "scheme", "http") or "http"

        # Try 1: Tailscale IP
        ts_ip = self._get_tailscale_ip_for_peer(voter_peer.node_id)
        if ts_ip and ts_ip != voter_peer.host:
            info = await self._send_heartbeat_to_peer(ts_ip, voter_peer.port, scheme=peer_scheme, timeout=VOTER_HEARTBEAT_TIMEOUT)
            if info:
                logger.info(f"Reached voter {voter_peer.node_id} via Tailscale ({ts_ip})")
                with self.peers_lock:
                    info.last_heartbeat = time.time()
                    info.consecutive_failures = 0
                    self.peers[info.node_id] = info
                return True

        # Try 2: Reported host/port
        rh = str(getattr(voter_peer, "reported_host", "") or "").strip()
        rp = int(getattr(voter_peer, "reported_port", 0) or 0)
        if rh and rp and (rh != voter_peer.host or rp != voter_peer.port):
            info = await self._send_heartbeat_to_peer(rh, rp, scheme=peer_scheme, timeout=VOTER_HEARTBEAT_TIMEOUT)
            if info:
                logger.info(f"Reached voter {voter_peer.node_id} via reported endpoint ({rh}:{rp})")
                with self.peers_lock:
                    info.last_heartbeat = time.time()
                    info.consecutive_failures = 0
                    self.peers[info.node_id] = info
                return True

        return False

    async def _discover_voter_peer(self, voter_id: str):
        """Discover a voter peer from known peers."""
        # Ask known peers for the voter's endpoint
        for peer_addr in self.known_peers:
            try:
                scheme, host, port = self._parse_peer_address(peer_addr)
                async with aiohttp.ClientSession() as session, session.get(
                    f"{scheme}://{host}:{port}/relay/peers",
                    timeout=aiohttp.ClientTimeout(total=5),
                    headers=self._auth_headers()
                ) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        peers_data = data.get("peers", {})
                        if voter_id in peers_data:
                            peer_info = NodeInfo.from_dict(peers_data[voter_id])
                            with self.peers_lock:
                                self.peers[voter_id] = peer_info
                            logger.info(f"Discovered voter {voter_id} from {host}")
                            return
            except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError, ImportError):
                continue

    async def _refresh_voter_mesh(self):
        """Ensure all voters have knowledge of each other."""
        if not self.voter_node_ids:
            return

        # Jan 12, 2026: Use lock-free PeerSnapshot for read-only access
        peers_snapshot = self._peer_snapshot.get_snapshot()

        # Check how many voters we know about (outside lock)
        known_voters = [v for v in self.voter_node_ids if v in peers_snapshot or v == self.node_id]

        if len(known_voters) < len(self.voter_node_ids):
            missing_voters = [v for v in self.voter_node_ids if v not in known_voters]
            logger.info(f"Voter mesh incomplete, missing: {missing_voters}")

            # Try to discover missing voters
            for voter_id in missing_voters:
                await self._discover_voter_peer(voter_id)

    # NOTE: _nat_management_loop() removed Dec 2025 (32 LOC).
    # Now runs via LoopManager as NATManagementLoop.
    # See scripts/p2p/loops/network_loops.py for implementation.

    async def _detect_nat_type(self):
        """Detect NAT type using STUN-like probing.

        Jan 2026: Delegated to RecoveryManager (Phase 12 decomposition).
        """
        self._nat_type = await self.recovery_manager.detect_nat_type()

    async def _probe_nat_blocked_peers(self):
        """Probe NAT-blocked peers to see if they've become reachable.

        Jan 2026: Delegated to RecoveryManager (Phase 12 decomposition).
        """
        await self.recovery_manager.probe_nat_blocked_peers()

    async def _update_relay_preferences(self):
        """Update relay preferences based on connectivity patterns.

        Jan 2026: Delegated to RecoveryManager (Phase 12 decomposition).
        """
        await self.recovery_manager.update_relay_preferences()

    async def _validate_relay_assignments(self) -> None:
        """Validate and update relay assignments for NAT-blocked peers.

        Jan 2026: Delegated to RecoveryManager (Phase 12 decomposition).
        """
        await self.recovery_manager.validate_relay_assignments()

    # NOTE: _select_best_relay() inlined at call site (Jan 2026 Phase 2)

    def _is_valid_relay(self, peer: "NodeInfo") -> bool:
        """Check if a peer is a valid relay candidate.

        Jan 2026: Delegated to RecoveryManager (Phase 12 decomposition).
        """
        return self.recovery_manager._is_valid_relay(peer)

    def _get_configured_relays(self, peer_id: str) -> list[str]:
        """Get configured relay nodes for a peer from distributed_hosts.yaml.

        Jan 2026: Delegated to RecoveryManager (Phase 12 decomposition).
        """
        return self.recovery_manager._get_configured_relays(peer_id)

    # NOTE: _manifest_collection_loop removed Dec 27, 2025
    # Now handled by ManifestCollectionLoop via LoopManager
    # See scripts/p2p/loops/manifest_collection_loop.py

    def _record_selfplay_stats_sample(self, manifest: ClusterDataManifest) -> None:
        """Record a lightweight selfplay totals sample for dashboard charts."""
        try:
            sample = {
                "timestamp": time.time(),
                "manifest_collected_at": float(getattr(manifest, "collected_at", 0.0) or 0.0),
                "total_selfplay_games": int(getattr(manifest, "total_selfplay_games", 0) or 0),
                "by_board_type": manifest.by_board_type,
                "total_nodes": int(getattr(manifest, "total_nodes", 0) or 0),
            }
            self.selfplay_stats_history.append(sample)
            max_samples = int(getattr(self, "selfplay_stats_history_max_samples", 288) or 288)
            if max_samples > 0 and len(self.selfplay_stats_history) > max_samples:
                self.selfplay_stats_history = self.selfplay_stats_history[-max_samples:]
        except (ValueError, KeyError, IndexError, AttributeError):
            # Never let dashboard bookkeeping break manifest collection.
            return

    def _endpoint_key(self, info: NodeInfo) -> tuple[str, str, int] | None:
        """Return the normalized reachable endpoint key for a peer (scheme, host, port)."""
        host = str(getattr(info, "host", "") or "").strip()
        if not host:
            return None
        scheme = str(getattr(info, "scheme", "http") or "http").lower()
        try:
            port = int(getattr(info, "port", DEFAULT_PORT) or DEFAULT_PORT)
        except (ValueError):
            port = DEFAULT_PORT
        reported_host = str(getattr(info, "reported_host", "") or "").strip()
        try:
            reported_port = int(getattr(info, "reported_port", 0) or 0)
        except (ValueError):
            reported_port = 0

        if reported_host and reported_port > 0:
            # Reverse proxies / relays can cause inbound peer requests to appear as loopback.
            # Prefer the peer's self-reported advertised endpoint in that case so:
            # - endpoint conflict detection remains meaningful, and
            # - eligible leaders don't get filtered out as "conflicted".
            if host in {"127.0.0.1", "localhost", "0.0.0.0", "::1"} or self._is_tailscale_host(reported_host):
                host, port = reported_host, reported_port
        return (scheme, host, port)

    def _endpoint_conflict_keys(self, peers: list[NodeInfo]) -> set[tuple[str, str, int]]:
        """Compute endpoint keys that are shared by >1 node (NAT/port collisions)."""
        counts: dict[tuple[str, str, int], int] = {}
        for p in peers:
            # Ignore dead peers: stale node_ids can linger after restarts and would
            # otherwise permanently mark the live node as "conflicted".
            if not p.is_alive():
                continue
            key = self._endpoint_key(p)
            if not key:
                continue
            counts[key] = counts.get(key, 0) + 1
        return {k for k, v in counts.items() if v > 1}

    async def _probe_nat_blocked_peer(self, peer: NodeInfo) -> bool:
        """Probe a NAT-blocked peer to check if it's now directly reachable.

        Jan 2026: Delegated to RecoveryManager (Phase 12 decomposition).
        """
        return await self.recovery_manager.probe_nat_blocked_peer(peer)

    async def _sweep_nat_recovery(self) -> int:
        """Periodically probe NAT-blocked peers to check if they've become reachable.

        Jan 2026: Delegated to RecoveryManager (Phase 12 decomposition).
        """
        return await self.recovery_manager.sweep_nat_recovery()

    # NOTE: _compute_connectivity_score() inlined at call site (Jan 2026 Phase 2)

    def _is_leader_eligible(
        self,
        peer: NodeInfo,
        conflict_keys: set[tuple[str, str, int]],
        *,
        require_alive: bool = True,
    ) -> bool:
        """Heuristic: leaders must be directly reachable and uniquely addressable.

        Jan 2, 2026: Enhanced to reject NAT-blocked and force_relay_mode nodes,
        and require minimum connectivity score of 0.3.
        """
        if require_alive and not peer.is_alive():
            return False
        voters = list(getattr(self, "voter_node_ids", []) or [])
        if voters and peer.node_id not in voters:
            return False
        if int(getattr(peer, "consecutive_failures", 0) or 0) >= MAX_CONSECUTIVE_FAILURES:
            return False
        # Jan 2, 2026: Reject NAT-blocked nodes
        if getattr(peer, "nat_blocked", False):
            return False
        # Jan 2, 2026: Reject force_relay_mode nodes (can't serve peers directly)
        if getattr(peer, "force_relay_mode", False):
            return False
        # Jan 13, 2026: Reject proxy_only nodes (cannot coordinate cluster operations)
        # These nodes are SSH jump hosts or API proxies with no AI/training capability
        # Check both NodeInfo.status and config (config is authoritative)
        node_status = getattr(peer, "status", "")
        if node_status == "proxy_only" or self._is_node_proxy_only(peer.node_id):
            return False
        # Jan 2, 2026: Require minimum connectivity score
        # Inline: was _compute_connectivity_score()
        if self.recovery_manager.compute_connectivity_score(peer) < 0.3:
            return False
        key = self._endpoint_key(peer)
        return not (key and key in conflict_keys)

    # Jan 27, 2026: Phase 17B - Removed _register_peer_with_dedup (never called)

    def _deduplicate_peers(self) -> int:
        """Periodic deduplication of peers dict.

        Dec 29, 2025: Scans for any duplicate entries that may have been
        created before deduplication was implemented, and merges them.

        Jan 13, 2026: Now actually removes duplicates instead of just logging.
        Keeps the most recently active node per IP (by last_heartbeat).

        Returns:
            Number of duplicates removed
        """
        removed = 0
        # This method is called periodically to clean up any legacy duplicates
        # Since we now key by node_id, duplicates shouldn't occur, but this
        # handles edge cases from state file loading and hostname changes
        with self.peers_lock:
            # Build IP -> node_id mapping to detect duplicates
            # Use reported_host (Tailscale) as primary dedup key since it's most stable
            ip_to_nodes: dict[str, list[str]] = {}
            for node_id, peer in self.peers.items():
                # Skip self
                if node_id == self.node_id:
                    continue
                # Prefer reported_host (Tailscale IPv6) for dedup - most stable identifier
                dedup_key = peer.reported_host or peer.effective_host or peer.host
                if dedup_key and dedup_key not in ("127.0.0.1", ""):
                    ip_to_nodes.setdefault(dedup_key, []).append(node_id)

            # Find IPs shared by multiple node_ids (duplicates)
            # Keep the most recently active one, remove the rest
            nodes_to_remove: set[str] = set()
            for ip, node_ids in ip_to_nodes.items():
                if len(node_ids) <= 1:
                    continue

                # Multiple nodes claim the same IP - keep the freshest one
                # Sort by last_heartbeat descending (most recent first)
                node_freshness = []
                for nid in node_ids:
                    peer = self.peers.get(nid)
                    if peer:
                        # Use last_heartbeat, fall back to last_seen
                        freshness = getattr(peer, "last_heartbeat", 0) or getattr(peer, "last_seen", 0) or 0
                        node_freshness.append((nid, freshness))

                if not node_freshness:
                    continue

                # Sort by freshness (most recent first)
                node_freshness.sort(key=lambda x: x[1], reverse=True)
                keeper = node_freshness[0][0]
                stale = [nid for nid, _ in node_freshness[1:]]

                if stale:
                    logger.info(
                        f"[Dedup] IP {ip}: keeping {keeper} (freshest), "
                        f"removing {len(stale)} stale entries: {stale}"
                    )
                    nodes_to_remove.update(stale)

            # Actually remove the duplicate nodes
            for node_id in nodes_to_remove:
                if node_id in self.peers:
                    del self.peers[node_id]
                    removed += 1
                    logger.info(f"[Dedup] Removed duplicate peer: {node_id}")

        if removed > 0:
            logger.info(f"[Dedup] Removed {removed} duplicate peer entries")

        return removed

    def _maybe_adopt_leader_from_peers(self) -> bool:
        """If we can already see a healthy leader, adopt it and avoid elections."""
        if self.role == NodeRole.LEADER:
            return False

        # Jan 12, 2026: Use lock-free PeerSnapshot for read-only access
        peers = [p for p in self._peer_snapshot.get_snapshot().values() if p.node_id != self.node_id]

        conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers])
        leaders = [
            p for p in peers
            if p.role == NodeRole.LEADER and self._is_leader_eligible(p, conflict_keys)
        ]

        voter_ids = list(getattr(self, "voter_node_ids", []) or [])
        if voter_ids:
            leaders = [p for p in leaders if p.node_id in voter_ids]

        if not leaders:
            return False

        # If multiple leaders exist (split brain), pick the lexicographically highest
        # ID (matches bully ordering) to converge.
        leader = sorted(leaders, key=lambda p: p.node_id)[-1]

        if self.leader_id != leader.node_id:
            logger.info(f"Adopted existing leader from peers: {leader.node_id}")
            # Jan 3, 2026 Sprint 13.3: Record election latency for "adopted" outcome
            # Only record if we were in an election (started_at > 0)
            if getattr(self, "_election_started_at", 0) > 0:
                self._record_election_latency("adopted")
        # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
        self._set_leader(leader.node_id, reason="join_existing_leader", save_state=True)
        self.last_leader_seen = time.time()  # Track when we last had a functioning leader
        return True

    async def _check_dead_peers_async(self):
        """Check for peers that have stopped responding (async version).

        This version uses AsyncLockWrapper to avoid blocking the event loop
        when acquiring the peers_lock.

        January 12, 2026: Refactored to move event emissions outside the lock
        to prevent deadlock risk when event handlers need the same lock.

        January 19, 2026: Added rate limiting (PEER_DEATH_RATE_LIMIT) to prevent
        cascade failures. When 5+ nodes are busy, ALL nodes would mark ALL of them
        dead simultaneously, causing gossip storms and further instability.
        Now max PEER_DEATH_RATE_LIMIT peers can be retired per check cycle.
        """
        now = time.time()
        dead_peers = []
        peers_to_purge = []

        # Jan 12, 2026: Collect event data inside lock, emit outside
        # This prevents deadlocks when event handlers need peers_lock
        retired_peers: list[tuple[str, float | None, float]] = []  # (node_id, last_hb, dead_for)
        recovered_peers: list[tuple[str, list[str]]] = []  # (node_id, capabilities)

        # Jan 19, 2026: Track candidates for retirement with their dead_for time
        # so we can sort by longest-dead and rate-limit retirements
        retirement_candidates: list[tuple[str, float, float | None]] = []  # (node_id, dead_for, last_hb)

        async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
            for node_id, info in self.peers.items():
                if not info.is_alive() and node_id != self.node_id:
                    dead_peers.append(node_id)
                    # Check if peer should be retired (long-dead)
                    try:
                        dead_for = now - float(getattr(info, "last_heartbeat", 0.0) or 0.0)
                    except (ValueError, AttributeError):
                        dead_for = float("inf")
                    if not getattr(info, "retired", False) and dead_for >= PEER_RETIRE_AFTER_SECONDS:
                        # Jan 19, 2026: Collect candidates instead of immediately retiring
                        last_hb = getattr(info, "last_heartbeat", None)
                        retirement_candidates.append((node_id, dead_for, last_hb))
                elif info.is_alive() and getattr(info, "retired", False):
                    # Peer came back: clear retirement and remove from dead tracking.
                    info.retired = False
                    info.retired_at = 0.0
                    # Jan 20, 2026: Clear cooldown on recovery
                    if self._cooldown_manager:
                        self._cooldown_manager.clear_cooldown(node_id)
                    else:
                        self._dead_peer_timestamps.pop(node_id, None)
                    # Collect data for event emission outside lock
                    caps = []
                    if hasattr(info, "gpu_type") and info.gpu_type:
                        caps.append(f"gpu:{info.gpu_type}")
                    recovered_peers.append((node_id, caps))
                    logger.info(f"Peer {node_id} recovered from retirement")

                    # Jan 21, 2026: Track state transition for diagnostics
                    # Jan 22, 2026: Use record_recovery() to clear SUSPECTED state
                    if self._peer_state_tracker:
                        try:
                            self._peer_state_tracker.record_recovery(
                                node_id=node_id,
                                details={"recovery_type": "retirement_cleared"},
                            )
                        except Exception:
                            pass  # Graceful degradation

            for node_id in dead_peers:
                info = self.peers.get(node_id)
                if info and getattr(info, "retired", False):
                    continue
                logger.info(f"Peer {node_id} is dead (no heartbeat for {PEER_TIMEOUT}s)")

            # Auto-purge very old retired peers
            for node_id, info in self.peers.items():
                if getattr(info, "retired", False):
                    retired_at = getattr(info, "retired_at", 0.0) or 0.0
                    if retired_at > 0 and (now - retired_at) >= PEER_PURGE_AFTER_SECONDS:
                        peers_to_purge.append(node_id)

            for node_id in peers_to_purge:
                del self.peers[node_id]
                logger.info(f"Auto-purged stale peer: {node_id} (retired for >{PEER_PURGE_AFTER_SECONDS}s)")

            # December 2025: Emit cluster health event if state changed
            # This enables pipeline coordination to pause/resume based on cluster health
            final_alive_count = sum(
                1 for p in self.peers.values()
                if p.is_alive() and not getattr(p, "retired", False)
            )
            final_node_count = len([
                p for p in self.peers.values()
                if not getattr(p, "retired", False)
            ])
            # Add self if not in peers
            if self.node_id not in self.peers:
                final_alive_count += 1
                final_node_count += 1
            # Dec 28, 2025: Fixed signature mismatch - pass correct parameters
            is_healthy = final_alive_count > 0
            quorum_met = self._has_voter_quorum() if hasattr(self, '_has_voter_quorum') else True
            self._emit_cluster_health_event_sync(is_healthy, final_alive_count, quorum_met)

            # Jan 12, 2026: Sync to lock-free snapshot after peer retirement/purge
            self._sync_peer_snapshot()

            # Capture final counts for capacity events (inside lock for consistency)
            # Jan 27, 2026: Using PeerQueryBuilder for consistent counting (Phase 3.2)
            capacity_total = len(self.peers)
            capacity_alive = self._peer_query.alive_non_retired_count(exclude_self=False).unwrap_or(0)
            capacity_gpu = self._peer_query.alive_gpu_count(exclude_self=False).unwrap_or(0)
            capacity_training = self._peer_query.training_enabled_count(exclude_self=False).unwrap_or(0)

        # Jan 19, 2026: Apply rate limiting to peer retirements
        # Sort by longest-dead first, then only retire up to PEER_DEATH_RATE_LIMIT peers
        if retirement_candidates:
            # Sort by dead_for descending (longest-dead first)
            retirement_candidates.sort(key=lambda x: x[1], reverse=True)

            # Rate-limit: only retire the top N peers this cycle
            to_retire = retirement_candidates[:PEER_DEATH_RATE_LIMIT]
            skipped = len(retirement_candidates) - len(to_retire)

            if skipped > 0:
                logger.warning(
                    f"Rate-limiting peer retirement: retiring {len(to_retire)}/{len(retirement_candidates)} "
                    f"candidates this cycle (skipped {skipped} to prevent cascade)"
                )

            # Now actually retire these peers (need brief lock acquisition)
            async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                for node_id, dead_for, last_hb in to_retire:
                    info = self.peers.get(node_id)
                    if info and not getattr(info, "retired", False):
                        info.retired = True
                        info.retired_at = now
                        # Jan 20, 2026: Use adaptive cooldown manager
                        if self._cooldown_manager:
                            self._cooldown_manager.record_death(node_id)
                        else:
                            self._dead_peer_timestamps[node_id] = now
                        retired_peers.append((node_id, last_hb, dead_for))
                        logger.info(f"Retired peer {node_id} (dead for {dead_for:.0f}s)")

                        # Jan 25, 2026: Log timeout disagreement warning for diagnostics
                        my_timeout = getattr(self.self_info, "effective_timeout", 0.0) or 180.0
                        peer_timeout = getattr(info, "effective_timeout", 0.0)
                        if peer_timeout > 0:
                            ratio = peer_timeout / my_timeout if my_timeout > 0 else 1.0
                            if ratio > 1.3 or ratio < 0.7:
                                logger.warning(
                                    f"[P2P] Timeout disagreement retiring {node_id}: "
                                    f"our_timeout={my_timeout:.1f}s, their_timeout={peer_timeout:.1f}s, "
                                    f"ratio={ratio:.2f}"
                                )

                        # Jan 21, 2026: Track state transition for diagnostics
                        # Jan 22, 2026: Use record_probe_failure() for SUSPECTED grace period
                        if self._peer_state_tracker:
                            try:
                                from scripts.p2p.diagnostics import DeathReason
                                self._peer_state_tracker.record_probe_failure(
                                    node_id=node_id,
                                    reason=DeathReason.HEARTBEAT_TIMEOUT,
                                    details={"dead_for": dead_for, "last_heartbeat": last_hb},
                                )
                            except Exception:
                                pass  # Graceful degradation

        # Jan 12, 2026: Emit events OUTSIDE the lock to prevent deadlocks
        # Event handlers may need to acquire peers_lock themselves
        for node_id, last_hb, dead_for in retired_peers:
            asyncio.create_task(self._emit_host_offline(node_id, "retired", last_hb))
            asyncio.create_task(self._emit_node_dead(node_id, "retired", last_hb, dead_for))
            asyncio.create_task(self._emit_cluster_capacity_changed(
                total_nodes=capacity_total,
                alive_nodes=capacity_alive,
                gpu_nodes=capacity_gpu,
                training_nodes=capacity_training,
                change_type="node_removed",
                change_details={"node_id": node_id, "reason": "peer_timeout"},
            ))

        for node_id, caps in recovered_peers:
            asyncio.create_task(self._emit_host_online(node_id, caps))
            asyncio.create_task(self._emit_cluster_capacity_changed(
                total_nodes=capacity_total,
                alive_nodes=capacity_alive,
                gpu_nodes=capacity_gpu,
                training_nodes=capacity_training,
                change_type="node_added",
                change_details={"node_id": node_id, "reason": "peer_recovered"},
            ))

        # Clear stale leader IDs after restarts/partitions
        if self.leader_id and not self._is_leader_lease_valid():
            old_leader_id = self.leader_id
            is_self_leader = (old_leader_id == self.node_id)
            logger.info(f"Clearing stale/expired leader lease: leader_id={old_leader_id}, is_self={is_self_leader}")

            if is_self_leader:
                # Jan 2026: Our own lease expired - use ULSM for broadcast-before-mutation
                await self._complete_step_down_async(TransitionReason.LEASE_EXPIRED)
            else:
                # Another node's stale lease - just clear locally, no broadcast needed
                # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                self._set_leader(None, reason="stale_remote_lease", save_state=False)
                self.leader_lease_id = ""
                self.leader_lease_expires = 0.0
                self.last_lease_renewal = 0.0
                # Dec 31, 2025: Set invalidation window to prevent gossip from re-setting stale leader
                # Window = 60 seconds - enough time for election to complete
                self._leader_invalidation_until = time.time() + 60.0
                # Emit LEADER_LOST before starting election (Dec 2025 fix)
                await self._emit_leader_lost(old_leader_id, "lease_expired")
                # CRITICAL: Check quorum before starting election to prevent quorum bypass
                if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                    logger.warning("Skipping election after stale lease clear: no voter quorum available")
                else:
                    asyncio.create_task(self._start_election())

        # If leader is dead, start election
        if self.leader_id and self.leader_id != self.node_id:
            async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                leader = self.peers.get(self.leader_id)
                peers_snapshot = [p for p in self.peers.values() if p.node_id != self.node_id]
            if leader:
                conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])
                if not self._is_leader_eligible(leader, conflict_keys):
                    reason = "dead" if not leader.is_alive() else "ineligible"
                    logger.info(f"Leader {self.leader_id} is {reason}, starting election")
                    # Dec 2025: Emit LEADER_LOST before clearing leader_id
                    old_leader_id = self.leader_id
                    # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                    self._set_leader(None, reason=f"leader_{reason}", save_state=False)
                    self.leader_lease_id = ""
                    self.leader_lease_expires = 0.0
                    self.last_lease_renewal = 0.0
                    # Dec 31, 2025: Set invalidation window to prevent gossip from re-setting dead leader
                    self._leader_invalidation_until = time.time() + 60.0
                    asyncio.create_task(self._emit_leader_lost(old_leader_id, reason))
                    # CRITICAL: Check quorum before starting election to prevent quorum bypass
                    if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                        logger.warning(f"Skipping election after leader {reason}: no voter quorum available")
                    else:
                        asyncio.create_task(self._start_election())
            else:
                # Dec 31, 2025: Leader not in peers dict - unknown/unreachable leader
                # This can happen if leader gossip propagated but peer discovery hasn't caught up
                # Clear the leader and allow election
                logger.warning(f"Leader {self.leader_id} not found in peers dict, clearing stale leader")
                old_leader_id = self.leader_id
                # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                self._set_leader(None, reason="leader_unknown_peer", save_state=False)
                self.leader_lease_id = ""
                self.leader_lease_expires = 0.0
                self.last_lease_renewal = 0.0
                self._leader_invalidation_until = time.time() + 60.0
                asyncio.create_task(self._emit_leader_lost(old_leader_id, "unknown_peer"))

        # If we're leaderless, periodically retry elections with adaptive backoff
        # December 29, 2025: Improved backoff to start faster then slow down
        if not self.leader_id and not self.election_in_progress:
            now = time.time()
            retry_count = int(getattr(self, "_election_retry_count", 0) or 0)
            # Adaptive backoff: 15s, 30s, 60s, 90s (capped)
            backoff_intervals = [15, 30, 60, 90]
            backoff_seconds = backoff_intervals[min(retry_count, len(backoff_intervals) - 1)]
            last_attempt = float(getattr(self, "last_election_attempt", 0.0) or 0.0)
            if now - last_attempt >= backoff_seconds:
                self.last_election_attempt = now
                self._election_retry_count = retry_count + 1
                # CRITICAL: Check quorum before starting election to prevent quorum bypass
                if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                    logger.warning(f"Skipping periodic election retry {retry_count + 1}: no voter quorum available")
                    # Jan 1, 2026: Check probabilistic fallback leadership when quorum unavailable
                    # This prevents indefinite cluster stalls when voters are unreachable
                    asyncio.create_task(self._check_probabilistic_leadership(now))
                else:
                    logger.info(f"Triggering election retry {retry_count + 1} after {backoff_seconds}s leaderless")
                    asyncio.create_task(self._start_election())
        elif self.leader_id:
            # Reset retry count when we have a leader
            self._election_retry_count = 0
            # Jan 1, 2026: Reset probabilistic claim probability when we have a leader
            self._provisional_claim_probability = PROVISIONAL_LEADER_INITIAL_PROBABILITY

    async def _probe_retired_peers_async(self) -> None:
        """Actively probe retired peers to detect recovery.

        Dec 30, 2025: Added to fix cluster connectivity after restart.
        Retired nodes don't send heartbeats, so we must probe them.
        This runs periodically (every PEER_RECOVERY_RETRY_INTERVAL) to
        detect nodes that have come back online after being retired.
        """
        # Collect retired peers (excluding self)
        async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
            retired = [
                p for p in self.peers.values()
                if getattr(p, "retired", False) and p.node_id != self.node_id
            ]

        if not retired:
            return

        logger.debug(f"Probing {len(retired)} retired peers for recovery")

        # Use a short timeout for health checks
        timeout = aiohttp.ClientTimeout(total=5)

        for peer in retired:
            try:
                # Try to reach the peer's health endpoint
                url = f"http://{peer.host}:{peer.port}/health"
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    async with session.get(url) as resp:
                        if resp.status == 200:
                            # Node is alive - un-retire it
                            async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                                if peer.node_id in self.peers:
                                    self.peers[peer.node_id].retired = False
                                    self.peers[peer.node_id].retired_at = 0.0
                                    self.peers[peer.node_id].last_heartbeat = time.time()

                            logger.info(f"Recovered retired peer via probe: {peer.node_id}")

                            # Jan 21, 2026: Track successful probe for diagnostics
                            if self._probe_tracker:
                                try:
                                    self._probe_tracker.record_probe(
                                        node_id=peer.node_id,
                                        success=True,
                                        latency_ms=None,  # Could measure this
                                        transport="http",
                                    )
                                except Exception:
                                    pass

                            # Jan 21, 2026: Track state transition for diagnostics
                            if self._peer_state_tracker:
                                try:
                                    from scripts.p2p.diagnostics import PeerState
                                    self._peer_state_tracker.record_transition(
                                        node_id=peer.node_id,
                                        to_state=PeerState.ALIVE,
                                        reason=None,
                                        details={"recovery_type": "probe_success"},
                                    )
                                except Exception:
                                    pass

                            # Emit HOST_ONLINE event
                            caps = []
                            if hasattr(peer, "gpu_type") and peer.gpu_type:
                                caps.append(f"gpu:{peer.gpu_type}")
                            await self._emit_host_online(peer.node_id, caps)

                            # Emit CLUSTER_CAPACITY_CHANGED
                            async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                                alive_count = sum(
                                    1 for p in self.peers.values()
                                    if p.is_alive() and not getattr(p, "retired", False)
                                )
                                gpu_count = sum(
                                    1 for p in self.peers.values()
                                    if p.is_alive() and not getattr(p, "retired", False)
                                    and getattr(p, "gpu_type", None)
                                )
                                training_count = sum(
                                    1 for p in self.peers.values()
                                    if p.is_alive() and not getattr(p, "retired", False)
                                    and getattr(p, "training_enabled", False)
                                )
                            asyncio.create_task(self._emit_cluster_capacity_changed(
                                total_nodes=len(self.peers),
                                alive_nodes=alive_count,
                                gpu_nodes=gpu_count,
                                training_nodes=training_count,
                                change_type="node_added",
                                change_details={"node_id": peer.node_id, "reason": "peer_recovered_via_probe"},
                            ))

            except (aiohttp.ClientError, asyncio.TimeoutError, OSError) as e:
                # Still unreachable - remain retired
                logger.debug(f"Retired peer {peer.node_id} still unreachable: {e}")

                # Jan 21, 2026: Track failed probe for diagnostics
                if self._probe_tracker:
                    try:
                        self._probe_tracker.record_probe(
                            node_id=peer.node_id,
                            success=False,
                            latency_ms=None,
                            transport="http",
                        )
                    except Exception:
                        pass

                # Jan 21, 2026: Track connection failure for diagnostics
                if self._conn_failure_tracker:
                    try:
                        self._conn_failure_tracker.record_failure(
                            node_id=peer.node_id,
                            error=e,
                            transport="http",
                            host=peer.host,
                            port=peer.port,
                        )
                    except Exception:
                        pass
                continue
            except Exception as e:
                # Unexpected error - log but don't crash
                logger.warning(f"Error probing retired peer {peer.node_id}: {e}")
                continue

    # NOTE: _check_dead_peers() sync method removed Jan 27, 2026 - Phase 3.2 cleanup
    # The async version _check_dead_peers_async() is the only active implementation.
    # Removed 206 lines of dead code (0 call sites confirmed via grep).

    async def _start_election(self):
        """Start leader election using Bully algorithm."""
        # Jan 3, 2026 Sprint 13.3: Track election start time for latency metrics
        self._start_election_timing()

        # Jan 19, 2026: Don't participate in elections until state loaded
        # CRITICAL FIX: Nodes were voting at 5s but state loads in 30-50s,
        # causing elections with incomplete cluster view (split-brain, thrashing)
        elapsed = time.time() - getattr(self, "_startup_time", 0)
        if elapsed < ELECTION_PARTICIPATION_DELAY:
            logger.info(
                f"[Election] Skipping: still in startup grace "
                f"({elapsed:.0f}s < {ELECTION_PARTICIPATION_DELAY}s)"
            )
            return

        # Jan 5, 2026: Global election cooldown to prevent rapid election storms
        # This complements the per-loop backoff in _maybe_trigger_election()
        # Jan 24, 2026: Increased from 15s to 30s to prevent rapid re-elections
        # With LEADER_LEASE_DURATION at 300s, 30s cooldown is safe and prevents churn
        ELECTION_GLOBAL_COOLDOWN = 30.0  # seconds
        now = time.time()
        last_election = getattr(self, "_last_election_completed", 0.0)
        if now - last_election < ELECTION_GLOBAL_COOLDOWN:
            logger.debug(
                f"[Election] Skipping: global cooldown ({now - last_election:.1f}s < {ELECTION_GLOBAL_COOLDOWN}s)"
            )
            return

        # Jan 22, 2026: Add random jitter to prevent election cascade
        # When leader dies, all nodes detect it simultaneously and try to start elections.
        # Without jitter, this causes election floods that destabilize the cluster.
        import random
        jitter = random.uniform(0.5, 3.0)  # 500ms to 3s
        await asyncio.sleep(jitter)

        # Re-check if election still needed after jitter (leader may have emerged)
        if self.leader_id and self.leader_id != self.node_id:
            logger.debug(f"[Election] Skipping after jitter: leader {self.leader_id} emerged")
            return

        self._update_self_info()

        # NAT-blocked nodes cannot act as a leader because peers can't reach them.
        if getattr(self.self_info, "nat_blocked", False):
            return

        # Jan 13, 2026: Strict quorum enforcement (P2P Cluster Stability Plan Phase 2)
        # Check quorum BEFORE proceeding with election
        voter_node_ids = list(getattr(self, "voter_node_ids", []) or [])
        if voter_node_ids:
            if self.node_id not in voter_node_ids:
                # December 29, 2025: Non-voters can request elections from voters
                # instead of just returning silently
                await self._request_election_from_voters("non_voter_detected_leaderless")
                return

            # Jan 13, 2026: Use strict quorum check when available
            try:
                from scripts.p2p.leader_election import should_block_election
                # Jan 12, 2026: Use lock-free PeerSnapshot for read-only access
                snapshot = self._peer_snapshot.get_snapshot()
                should_block, reason = should_block_election(
                    voter_node_ids,
                    snapshot,
                    self.node_id,
                )
                if should_block:
                    logger.warning(f"[Election] Blocked: {reason}")
                    self._safe_emit_event("ELECTION_BLOCKED", {
                        "node_id": self.node_id,
                        "reason": reason,
                        "voter_count": len(voter_node_ids),
                        "timestamp": time.time(),
                    })
                    return
            except ImportError:
                # Fall back to legacy quorum check
                if not self._has_voter_quorum():
                    return

        # Jan 12, 2026: Use lock-free PeerSnapshot for read-only access
        snapshot = self._peer_snapshot.get_snapshot()
        peers_snapshot = [p for p in snapshot.values() if p.node_id != self.node_id]

        conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])

        if self.leader_id and self.leader_id != self.node_id:
            # Jan 12, 2026: Use lock-free PeerSnapshot for read-only access
            leader = snapshot.get(self.leader_id)
            leader_ok = (
                leader is not None
                and leader.is_alive()
                and leader.role == NodeRole.LEADER
                and self._is_leader_eligible(leader, conflict_keys)
                and self._is_leader_lease_valid()
            )
            if leader_ok:
                return
            # Drop stale/ineligible leader so we don't keep advertising it.
            # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
            self._set_leader(None, reason="stale_ineligible_leader", save_state=False)
            self.leader_lease_id = ""
            self.leader_lease_expires = 0.0
        if self._maybe_adopt_leader_from_peers():
            return

        # Jan 22, 2026: Atomic election guard using asyncio.Lock
        # Prevents race condition where multiple coroutines pass the check
        # before any sets the flag, causing multiple simultaneous elections.
        async with self._election_lock:
            if self.election_in_progress:
                logger.debug("[Election] Already in progress, skipping")
                return
            # Atomically set the flag while holding the lock
            self.election_in_progress = True

        # =========================================================================
        # Jan 2, 2026: Leader Stickiness
        # If we recently were the leader, try to reclaim immediately.
        # If we're not the recent leader, check if any peer was and give them priority.
        # =========================================================================
        if self._was_recently_leader() and self._in_incumbent_grace_period():
            # We recently stepped down - try to reclaim leadership immediately
            logger.info(
                f"Incumbent advantage: attempting immediate leadership reclaim "
                f"(stepped down {time.time() - self._last_step_down_time:.1f}s ago)"
            )
            # C1 fix: Use leader_state_lock for role changes
            with self.leader_state_lock:
                self.role = NodeRole.CANDIDATE
            try:
                # Skip bully algorithm - try to become leader directly
                conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])
                if self._is_leader_eligible(self.self_info, conflict_keys):
                    await self._become_leader()
                    if self.role == NodeRole.LEADER:
                        logger.info("Incumbent reclaimed leadership successfully")
                        return
            finally:
                # C1 fix: Use leader_state_lock for role changes
                with self.leader_state_lock:
                    if self.role == NodeRole.CANDIDATE:
                        self.role = NodeRole.FOLLOWER
                # Don't clear election_in_progress here - fall through to normal election
            # If we failed to reclaim, fall through to normal election
            logger.info("Incumbent reclaim failed, falling back to normal election")

        # election_in_progress is already True (set atomically above)
        # C1 fix: Use leader_state_lock for role changes
        with self.leader_state_lock:
            self.role = NodeRole.CANDIDATE
        logger.info(f"Starting election, my ID: {self.node_id}")

        try:
            # Send election message to all nodes with higher IDs
            # Jan 12, 2026: Use lock-free PeerSnapshot for read-only access
            election_snapshot = self._peer_snapshot.get_snapshot()
            higher_nodes = [
                p for p in election_snapshot.values()
                if (
                    p.node_id > self.node_id
                    and self._is_leader_eligible(p, conflict_keys)
                )
            ]
            voter_node_ids = list(getattr(self, "voter_node_ids", []) or [])
            if voter_node_ids:
                higher_nodes = [p for p in higher_nodes if p.node_id in voter_node_ids]

            got_response = False

            timeout = ClientTimeout(total=ELECTION_TIMEOUT)
            async with get_client_session(timeout) as session:
                for peer in higher_nodes:
                    try:
                        url = self._url_for_peer(peer, "/election")
                        async with session.post(url, json={"candidate_id": self.node_id}, headers=self._auth_headers()) as resp:
                            if resp.status == 200:
                                data = await resp.json()
                                if data.get("response") == "ALIVE":
                                    got_response = True
                                    logger.info(f"Higher node {peer.node_id} responded")
                    except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
                        pass  # Network errors expected during elections

            # If no higher node responded, we become leader
            if not got_response:
                # Only become leader if we're eligible (unique + directly reachable).
                if self._is_leader_eligible(self.self_info, conflict_keys):
                    await self._become_leader()
            else:
                # Wait for coordinator message
                await asyncio.sleep(ELECTION_TIMEOUT * 2)
                # If no coordinator arrives, fall back to adopting any eligible leader we can see.
                self._maybe_adopt_leader_from_peers()

        finally:
            self.election_in_progress = False
            # C1 fix: Use leader_state_lock for role changes
            with self.leader_state_lock:
                if self.role == NodeRole.CANDIDATE:
                    # Jan 3, 2026 Sprint 13.3: Record election latency for "timeout" outcome
                    # Election ended without becoming leader or adopting one
                    if getattr(self, "_election_started_at", 0) > 0:
                        self._record_election_latency("timeout")
                    self.role = NodeRole.FOLLOWER

    async def _become_leader(self):
        """Become the cluster leader with lease-based leadership."""
        self._update_self_info()
        if getattr(self.self_info, "nat_blocked", False):
            logger.info(f"Refusing leadership while NAT-blocked: {self.node_id}")
            return
        if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
            logger.info(f"Refusing leadership without voter quorum: {self.node_id}")
            return
        import uuid
        lease_id = f"{self.node_id}_{int(time.time())}_{uuid.uuid4().hex[:8]}"
        lease_expires = await self._acquire_voter_lease_quorum(lease_id, int(LEADER_LEASE_DURATION))
        if getattr(self, "voter_node_ids", []) and not lease_expires:
            logger.error(f"Failed to obtain voter lease quorum; refusing leadership: {self.node_id}")
            # Jan 3, 2026 Sprint 13.3: Record election latency for "lost" outcome (no quorum)
            self._record_election_latency("lost")
            # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
            self._set_leader(None, reason="election_failed_no_quorum", save_state=False)
            self.leader_lease_id = ""
            self.leader_lease_expires = 0.0
            self.last_lease_renewal = 0.0
            self._release_voter_grant_if_self()
            # Jan 5, 2026: Mark election completed (even on failure) for cooldown tracking
            self._last_election_completed = time.time()
            self._save_state()
            return

        logger.info(f"I am now the leader: {self.node_id}")
        # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
        self._set_leader(self.node_id, reason="become_leader", save_state=False)
        self.last_leader_seen = time.time()  # Track when we last had a functioning leader

        # Jan 5, 2026: Register self in peers dict when becoming leader
        # This ensures the leader is visible in peers iteration and quorum checks
        self._register_self_in_peers()

        # Jan 3, 2026 Sprint 13.3: Record election latency for "won" outcome
        self._record_election_latency("won")
        # Dec 31, 2025: Track leadership acquisition time and reset quorum fail counters
        self._last_become_leader_time = time.time()
        # Jan 23, 2026: Reset ULSM QuorumHealth (unified tracker)
        if hasattr(self, "_leadership_sm") and self._leadership_sm:
            self._leadership_sm.quorum_health.reset()

        # Phase 29: Increment cluster epoch on leadership change
        # This helps resolve split-brain when partitions merge
        self._increment_cluster_epoch()

        # Phase 15.1.1: Increment lease epoch and create fence token
        # This provides split-brain protection by ensuring each leadership
        # term has a unique, monotonically increasing epoch
        self._lease_epoch += 1
        self._fence_token = f"{self.node_id}:{self._lease_epoch}:{time.time()}"
        logger.info(f"Leader lease fencing: epoch={self._lease_epoch}, token={self._fence_token}")

        # CRITICAL: Emit LEADER_ELECTED event (Dec 2025 fix)
        # This enables LeadershipCoordinator and other components to track leadership changes
        asyncio.create_task(self._emit_leader_elected(self.node_id, getattr(self, "cluster_epoch", 0)))

        # Lease-based leadership (voter-backed when enabled).
        self.leader_lease_id = lease_id
        self.leader_lease_expires = float(lease_expires or (time.time() + LEADER_LEASE_DURATION))
        self.last_lease_renewal = time.time()

        # Announce to all peers with lease information
        with self.peers_lock:
            peers = list(self.peers.values())

        timeout = ClientTimeout(total=5)
        async with get_client_session(timeout) as session:
            for peer in peers:
                if peer.node_id != self.node_id:
                    try:
                        url = self._url_for_peer(peer, "/coordinator")
                        await session.post(url, json={
                            "leader_id": self.node_id,
                            "lease_id": self.leader_lease_id,
                            "lease_expires": self.leader_lease_expires,
                            "voter_node_ids": list(getattr(self, "voter_node_ids", []) or []),
                            # Phase 15.1.1: Include epoch and fence token for split-brain protection
                            "lease_epoch": self._lease_epoch,
                            "fence_token": self._fence_token,
                        }, headers=self._auth_headers())
                    except (aiohttp.ClientError, asyncio.TimeoutError, KeyError, IndexError, AttributeError):
                        pass  # Network errors expected during leader announcements

        # Jan 5, 2026: Mark election completed for cooldown tracking
        self._last_election_completed = time.time()

        self._save_state()

        # Start monitoring services when becoming leader
        await self._start_monitoring_if_leader()

        # Start P2P auto-deployer when becoming leader
        await self._start_p2p_auto_deployer()

    # ============================================
    # Probabilistic Fallback Leadership (Jan 1, 2026)
    # ============================================

    async def _check_probabilistic_leadership(self, now: float) -> None:
        """Check if we should claim provisional leadership using probabilistic fallback.

        Jan 1, 2026: When normal elections repeatedly fail (e.g., voter quorum unavailable),
        nodes can claim provisional leadership with increasing probability. This prevents
        indefinite cluster stalls while still preferring proper elections.

        Design:
        1. Only activate after PROVISIONAL_LEADER_MIN_LEADERLESS_TIME (5 min)
        2. Probability starts low, grows exponentially over time
        3. If random check passes, claim PROVISIONAL_LEADER state
        4. Announce to peers and collect acknowledgments
        5. Promote to full LEADER after quorum ack or timeout with no challengers
        """
        import random

        # Skip if we already have a leader or are claiming
        if self.leader_id or self.role in (NodeRole.LEADER, NodeRole.PROVISIONAL_LEADER, NodeRole.CANDIDATE):
            return

        # Rate limit checks
        if now - self._last_provisional_check < PROVISIONAL_LEADER_CHECK_INTERVAL:
            return
        self._last_provisional_check = now

        # Check if we've been leaderless long enough
        leaderless_duration = now - self.last_leader_seen
        if leaderless_duration < PROVISIONAL_LEADER_MIN_LEADERLESS_TIME:
            return

        # Check if we're eligible (not NAT-blocked, preferably GPU node)
        self._update_self_info()
        if getattr(self.self_info, "nat_blocked", False):
            logger.debug("Skipping probabilistic leadership: NAT-blocked")
            return

        # Calculate current probability based on leaderless duration
        # Probability grows exponentially beyond minimum threshold
        minutes_beyond_minimum = (leaderless_duration - PROVISIONAL_LEADER_MIN_LEADERLESS_TIME) / 60.0
        current_prob = min(
            PROVISIONAL_LEADER_MAX_PROBABILITY,
            PROVISIONAL_LEADER_INITIAL_PROBABILITY * (PROVISIONAL_LEADER_PROBABILITY_GROWTH_RATE ** minutes_beyond_minimum)
        )
        self._provisional_claim_probability = current_prob

        # Roll the dice
        roll = random.random()
        logger.debug(f"Probabilistic leadership check: roll={roll:.3f}, threshold={current_prob:.3f}, "
                    f"leaderless={int(leaderless_duration)}s")

        if roll >= current_prob:
            return  # Not claiming this time

        # We're claiming provisional leadership
        logger.info(f"Claiming provisional leadership after {int(leaderless_duration)}s leaderless "
                   f"(prob={current_prob:.2%}, roll={roll:.3f})")

        await self._claim_provisional_leadership()

    async def _claim_provisional_leadership(self) -> None:
        """Claim provisional leadership and announce to peers.

        Provisional leaders can dispatch work but must be confirmed by:
        - Quorum acknowledgment from peers, OR
        - No challengers after timeout period (with node_id tiebreaker if contested)
        """
        import uuid

        now = time.time()

        # Set provisional state
        # C1 fix: Use leader_state_lock for role/leader_id changes
        with self.leader_state_lock:
            self.role = NodeRole.PROVISIONAL_LEADER
            self._provisional_leader_claimed_at = now
            self._provisional_leader_acks = {self.node_id}  # Self-ack
            self._provisional_leader_challengers = {}

            # Create a provisional lease (shorter than normal)
            provisional_lease_id = f"PROVISIONAL_{self.node_id}_{uuid.uuid4().hex[:8]}"
            self.leader_lease_id = provisional_lease_id
            self.leader_lease_expires = now + PROVISIONAL_LEADER_QUORUM_TIMEOUT
            self.last_lease_renewal = now

            # Set ourselves as leader (provisional) so peers know who's claiming
            self.leader_id = self.node_id

        logger.info(f"Provisional leadership claimed: lease={provisional_lease_id}")

        # Announce provisional claim to all peers
        async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
            peers = [p for p in self.peers.values() if p.node_id != self.node_id and p.is_alive()]

        if not peers:
            # No peers to acknowledge, promote immediately
            logger.info("No alive peers to acknowledge, promoting immediately to full leader")
            await self._promote_provisional_to_leader("no_peers")
            return

        # Send provisional leadership claim to all peers
        timeout = aiohttp.ClientTimeout(total=5)
        acks_received = 0
        challengers = []

        async with get_client_session(timeout) as session:
            for peer in peers:
                try:
                    url = self._url_for_peer(peer, "/provisional-leader/claim")
                    async with session.post(
                        url,
                        json={
                            "claimant_id": self.node_id,
                            "lease_id": provisional_lease_id,
                            "claimed_at": now,
                        },
                        headers=self._auth_headers(),
                    ) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            if data.get("ack"):
                                self._provisional_leader_acks.add(peer.node_id)
                                acks_received += 1
                                logger.debug(f"Provisional ack from {peer.node_id}")
                            elif data.get("challenge"):
                                challenger_id = data.get("challenger_id", peer.node_id)
                                self._provisional_leader_challengers[challenger_id] = now
                                challengers.append(challenger_id)
                                logger.info(f"Provisional challenge from {challenger_id}")
                except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
                    pass  # Network errors expected

        logger.info(f"Provisional claim results: {acks_received} acks, {len(challengers)} challengers")

        # Handle challenges using node_id tiebreaker
        if challengers:
            # Find highest challenger
            all_claimants = [self.node_id] + challengers
            all_claimants.sort(reverse=True)
            winner = all_claimants[0]

            if winner != self.node_id:
                logger.info(f"Stepping down from provisional: {winner} > {self.node_id}")
                self._step_down_from_provisional()
                return

            logger.info(f"Won provisional tiebreaker against {challengers}")

        # Check if we have quorum
        total_peers = len(peers) + 1  # Include self
        quorum_size = (total_peers // 2) + 1
        current_acks = len(self._provisional_leader_acks)

        if current_acks >= quorum_size:
            logger.info(f"Quorum achieved ({current_acks}/{quorum_size}), promoting to full leader")
            await self._promote_provisional_to_leader("quorum_achieved")
        else:
            # Schedule a follow-up check after timeout period
            logger.info(f"Quorum not yet achieved ({current_acks}/{quorum_size}), waiting for timeout")
            asyncio.get_event_loop().call_later(
                PROVISIONAL_LEADER_QUORUM_TIMEOUT,
                lambda: asyncio.create_task(self._check_provisional_promotion())
            )

    async def _check_provisional_promotion(self) -> None:
        """Check if provisional leader should be promoted after timeout period."""
        if self.role != NodeRole.PROVISIONAL_LEADER:
            return  # Already promoted or stepped down

        now = time.time()
        claim_duration = now - self._provisional_leader_claimed_at

        if claim_duration < PROVISIONAL_LEADER_QUORUM_TIMEOUT:
            return  # Not time yet

        # Check for any challengers that won during the timeout
        if self._provisional_leader_challengers:
            all_claimants = [self.node_id] + list(self._provisional_leader_challengers.keys())
            all_claimants.sort(reverse=True)
            winner = all_claimants[0]

            if winner != self.node_id:
                logger.info(f"Challenger {winner} won during timeout period")
                self._step_down_from_provisional()
                return

        # No successful challengers, promote to full leader
        logger.info(f"Provisional timeout elapsed with no successful challengers, promoting to full leader")
        await self._promote_provisional_to_leader("timeout_no_challengers")

    async def _promote_provisional_to_leader(self, reason: str) -> None:
        """Promote from provisional to full leader.

        Args:
            reason: Why we're promoting (quorum_achieved, timeout_no_challengers, no_peers)
        """
        if self.role == NodeRole.LEADER:
            return  # Already promoted

        logger.info(f"Promoting from provisional to full leader: {reason}")

        # Clear provisional state
        self._provisional_leader_claimed_at = 0.0
        self._provisional_leader_acks.clear()
        self._provisional_leader_challengers.clear()

        # Full leader transition (subset of _become_leader, but without voter lease)
        import uuid
        now = time.time()

        # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
        self._set_leader(self.node_id, reason=f"promote_provisional_{reason}", save_state=False)
        self.last_leader_seen = now

        # Jan 5, 2026: Register self in peers dict when promoted to leader
        self._register_self_in_peers()

        # Create a new lease ID to mark full leadership
        lease_id = f"FALLBACK_{self.node_id}_{int(now)}_{uuid.uuid4().hex[:8]}"
        self.leader_lease_id = lease_id
        self.leader_lease_expires = now + LEADER_LEASE_DURATION
        self.last_lease_renewal = now

        # Track that this is fallback leadership (for monitoring)
        self._fallback_leader_since = now
        self._fallback_leader_reason = reason

        # Increment epochs
        self._increment_cluster_epoch()
        self._lease_epoch += 1
        self._fence_token = f"{self.node_id}:{self._lease_epoch}:{now}"

        # Emit LEADER_ELECTED event
        asyncio.create_task(self._emit_leader_elected(self.node_id, getattr(self, "cluster_epoch", 0)))

        # Announce to all peers
        async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
            peers = list(self.peers.values())

        timeout = aiohttp.ClientTimeout(total=5)
        async with get_client_session(timeout) as session:
            for peer in peers:
                if peer.node_id != self.node_id:
                    try:
                        url = self._url_for_peer(peer, "/coordinator")
                        await session.post(
                            url,
                            json={
                                "leader_id": self.node_id,
                                "lease_id": self.leader_lease_id,
                                "lease_expires": self.leader_lease_expires,
                                "fallback_leadership": True,
                                "lease_epoch": self._lease_epoch,
                                "fence_token": self._fence_token,
                            },
                            headers=self._auth_headers(),
                        )
                    except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
                        pass

        self._save_state()

        # Start monitoring services
        await self._start_monitoring_if_leader()
        await self._start_p2p_auto_deployer()

        logger.info(f"Full fallback leadership established: lease={lease_id}")

    def _step_down_from_provisional(self) -> None:
        """Step down from provisional leadership (lost to challenger).

        Jan 2026: Uses ULSM for broadcast-before-mutation pattern.
        """
        logger.info("Stepping down from provisional leadership via ULSM")

        # Clear provisional-specific state first (ULSM doesn't know about these)
        self._provisional_leader_claimed_at = 0.0
        self._provisional_leader_acks.clear()
        self._provisional_leader_challengers.clear()

        # Use ULSM step-down (broadcasts to peers, then clears leader state)
        self._schedule_step_down_sync(TransitionReason.ARBITER_OVERRIDE)

        # Notify voters of lease revocation
        try:
            asyncio.create_task(self._notify_voters_lease_revoked())
        except RuntimeError:
            # Not in async context, schedule on event loop if available
            loop = asyncio.get_event_loop()
            if loop.is_running():
                asyncio.run_coroutine_threadsafe(self._notify_voters_lease_revoked(), loop)

    async def _request_election_from_voters(self, reason: str = "non_voter_request") -> bool:
        """December 29, 2025: Non-voters can request that voters start an election.

        Instead of silently returning when a non-voter tries to start an election,
        this method sends requests to known voters to have them start one.

        Args:
            reason: Why the election is being requested

        Returns:
            True if at least one voter accepted the request
        """
        voter_node_ids = list(getattr(self, "voter_node_ids", []) or [])
        if not voter_node_ids:
            return False

        logger.info(f"Non-voter {self.node_id} requesting election from voters: {reason}")

        # Rate limit election requests to avoid spamming
        now = time.time()
        last_request = getattr(self, "_last_election_request", 0.0)
        if now - last_request < 30:  # At most once per 30 seconds
            logger.debug("Skipping election request: rate limited")
            return False
        self._last_election_request = now

        accepted = False
        # Jan 12, 2026: Copy-on-write - single snapshot instead of lock-per-voter
        with self.peers_lock:
            peers_snapshot = dict(self.peers)

        async with aiohttp.ClientSession() as session:
            for voter_id in voter_node_ids[:3]:  # Limit to 3 voters
                voter = peers_snapshot.get(voter_id)
                if not voter or not voter.is_alive():
                    continue

                try:
                    url = self._url_for_peer(voter, "/election/request")
                    async with session.post(
                        url,
                        json={"requester_id": self.node_id, "reason": reason},
                        headers=self._auth_headers(),
                        timeout=aiohttp.ClientTimeout(total=5.0),
                    ) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            if data.get("accepted"):
                                logger.info(
                                    f"Voter {voter_id} accepted election request: {data.get('action')}"
                                )
                                accepted = True
                                break  # One voter accepting is enough
                except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                    logger.debug(f"Failed to request election from {voter_id}: {e}")
                    continue

        if not accepted:
            logger.warning(f"No voters accepted election request from {self.node_id}")
        return accepted

    async def _check_emergency_coordinator_fallback(self):
        """DECENTRALIZED: When voter quorum is unreachable for >5 min, any GPU node can coordinate.

        EMERGENCY COORDINATOR: This is a last-resort fallback when the normal voter-based
        leadership cannot be established due to:
        - Too many voters being offline
        - Network partition isolating voters
        - Cluster-wide issues

        In this mode, the node acts as a temporary coordinator WITHOUT voter consensus.
        It will relinquish control once voter quorum is restored.
        """
        now = time.time()

        # Only check every 60 seconds
        last_check = getattr(self, "_last_emergency_coord_check", 0)
        if now - last_check < 60:
            return
        self._last_emergency_coord_check = now

        # Skip if we already are a leader
        if self.role == NodeRole.LEADER:
            return

        # Skip if we have a known leader
        if self.leader_id:
            self._emergency_coordinator_since = 0
            return

        # Check if we have voter quorum
        if self._has_voter_quorum():
            self._emergency_coordinator_since = 0
            return  # Normal election should work

        # Track how long we've been without voter quorum
        quorum_missing_since = getattr(self, "_quorum_missing_since", 0)
        if quorum_missing_since == 0:
            self._quorum_missing_since = now
            return

        EMERGENCY_THRESHOLD = 300  # 5 minutes without quorum triggers emergency
        quorum_missing_duration = now - quorum_missing_since

        if quorum_missing_duration < EMERGENCY_THRESHOLD:
            return

        # Check if we're eligible (must be GPU node, not NAT-blocked)
        self._update_self_info()
        if not getattr(self.self_info, "has_gpu", False):
            return
        if getattr(self.self_info, "nat_blocked", False):
            return

        # Use consistent hashing to determine which node should be emergency coordinator
        # This prevents multiple nodes from declaring themselves coordinator
        with self.peers_lock:
            candidates = [self.node_id]
            for peer in self.peers.values():
                if not peer.is_alive():
                    continue
                if not getattr(peer, "has_gpu", False):
                    continue
                if getattr(peer, "nat_blocked", False):
                    continue
                candidates.append(peer.node_id)

        if not candidates:
            return

        # Deterministic selection: highest node_id wins (simple, consistent)
        candidates.sort(reverse=True)
        designated_coordinator = candidates[0]

        if designated_coordinator != self.node_id:
            return  # Another node should be coordinator

        # Become emergency coordinator (without voter lease)
        logger.info(f"EMERGENCY COORDINATOR: Taking leadership without voter quorum "
              f"(quorum missing for {int(quorum_missing_duration)}s, {len(candidates)} candidates)")

        # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
        self._set_leader(self.node_id, reason="emergency_coordinator", save_state=False)
        self.last_leader_seen = now
        self._emergency_coordinator_since = now

        # Use a special lease ID to mark emergency mode
        import uuid
        self.leader_lease_id = f"EMERGENCY_{self.node_id}_{uuid.uuid4().hex[:8]}"
        self.leader_lease_expires = now + 120  # Short lease - needs frequent renewal
        self.last_lease_renewal = now

        # Announce emergency leadership
        with self.peers_lock:
            peers = list(self.peers.values())

        timeout = ClientTimeout(total=5)
        async with get_client_session(timeout) as session:
            for peer in peers:
                if peer.node_id != self.node_id:
                    try:
                        url = self._url_for_peer(peer, "/coordinator")
                        await session.post(url, json={
                            "leader_id": self.node_id,
                            "lease_id": self.leader_lease_id,
                            "lease_expires": self.leader_lease_expires,
                            "emergency": True,
                        }, headers=self._auth_headers())
                    except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
                        pass  # Network errors expected during emergency coordination

        self._save_state()
        logger.info(f"EMERGENCY COORDINATOR: {self.node_id} is now emergency leader")

    def _get_peer_health_score(self, peer_id: str) -> float:
        """Calculate health score for a peer (0-100, higher is healthier).

        Jan 2026: Delegated to HealthMetricsManager (Phase 9 decomposition).
        """
        return self.health_metrics_manager.get_peer_health_score(peer_id)

    def _record_p2p_sync_result(self, peer_id: str, success: bool, latency_ms: float = 0.0):
        """Record P2P sync result for circuit breaker, metrics, and reputation.

        Jan 2026: Delegated to HealthMetricsManager (Phase 9 decomposition).
        """
        self.health_metrics_manager.record_p2p_sync_result(peer_id, success, latency_ms)

    async def _p2p_data_sync(self):
        """DECENTRALIZED: Nodes sync data directly with peers without leader coordination.

        P2P DATA SYNC with enhancements:
        - Health-based peer selection (avoids overloaded nodes)
        - Circuit breaker (skips unreliable peers)
        - Delta sync (only syncs files newer than last sync)
        - Model file prioritization (syncs models first)
        - ADAPTIVE INTERVALS: adjusts based on cluster activity and success rate
        """
        now = time.time()

        # ADAPTIVE INTERVAL: Uses activity-aware interval instead of fixed 5 min
        interval = self._get_adaptive_sync_interval("data")
        last_check = getattr(self, "_last_p2p_sync_check", 0)
        if now - last_check < interval:
            return
        self._last_p2p_sync_check = now

        # Skip if leader is actively managing sync (avoid conflicts)
        if self.role == NodeRole.LEADER:
            return  # Leader uses centralized sync

        # Skip if a sync is already in progress
        if getattr(self, "sync_in_progress", False):
            return

        # Skip if under disk pressure
        if getattr(self.self_info, "disk_percent", 0) > 85:
            return

        # Get our local manifest (use cache for speed)
        local_manifest = getattr(self, "local_data_manifest", None)
        if not local_manifest:
            try:
                # Jan 23, 2026: Wrap in asyncio.to_thread() to prevent event loop blocking
                # collect_local_manifest_cached() does file I/O and SQLite operations
                local_manifest = await asyncio.to_thread(
                    self.sync_planner.collect_local_manifest_cached, max_cache_age=600
                )
                with self.manifest_lock:
                    self.local_data_manifest = local_manifest
            except (AttributeError):
                return

        # Get local file set with timestamps for delta sync
        local_files = {}
        for file_info in getattr(local_manifest, "files", []) or []:
            rel_path = getattr(file_info, "relative_path", "")
            if rel_path:
                local_files[rel_path] = getattr(file_info, "modified_at", 0)

        # Check peer manifests from gossip cache
        peer_manifests = getattr(self, "_gossip_peer_manifests", {})
        if not peer_manifests:
            return

        # Find files we're missing that peers have (with prioritization)
        files_to_sync: dict[str, list[tuple]] = {}  # peer_id -> [(file, priority)]
        file_hashes: dict[str, str] = {}  # file_path -> hash (for dedup tracking)
        last_sync_time = getattr(self, "_last_successful_p2p_sync", 0)

        for peer_id, peer_manifest in peer_manifests.items():
            if peer_id == self.node_id:
                continue

            # Check circuit breaker
            health = self._get_peer_health_score(peer_id)
            if health <= 0:
                continue

            peer_files = getattr(peer_manifest, "files", []) or []
            for file_info in peer_files:
                rel_path = getattr(file_info, "relative_path", "")
                modified_at = getattr(file_info, "modified_at", 0)
                file_hash = getattr(file_info, "file_hash", "")
                file_size = getattr(file_info, "size_bytes", 0)

                if not rel_path:
                    continue

                # Skip if we have this file with same or newer timestamp
                if rel_path in local_files and local_files[rel_path] >= modified_at:
                    continue

                # Skip if file is older than last sync (delta optimization)
                if modified_at < last_sync_time and rel_path in local_files:
                    continue

                # DATA DEDUPLICATION: Skip if we already synced this file (by hash)
                if file_hash and self._is_file_already_synced(file_hash):
                    self._record_dedup_skip(file_count=1, bytes_saved=file_size)
                    continue

                # Calculate priority (models > ELO/training DBs > training data > selfplay)
                priority = 0
                if "models/" in rel_path or rel_path.endswith(".pt") or rel_path.endswith(".onnx"):
                    priority = 100  # Highest priority for models
                elif rel_path.endswith(".db") and ("unified_elo" in rel_path or "elo_ratings" in rel_path):
                    priority = 90  # Very high priority for ELO database
                elif rel_path.endswith(".db") and ("canonical_" in rel_path or "consolidated_training" in rel_path or "training_pool" in rel_path):
                    priority = 80  # High priority for training databases
                elif "training/" in rel_path:
                    priority = 50
                elif rel_path.endswith(".db"):
                    priority = 30  # Medium priority for other databases
                else:
                    priority = 10

                if peer_id not in files_to_sync:
                    files_to_sync[peer_id] = []
                files_to_sync[peer_id].append((rel_path, priority, health))

                # Track hash for dedup recording after sync
                if file_hash:
                    file_hashes[rel_path] = file_hash

        if not files_to_sync:
            return

        # Select best peer using health score AND file count
        def peer_score(peer_id):
            files = files_to_sync[peer_id]
            health = self._get_peer_health_score(peer_id)
            file_score = sum(f[1] for f in files)  # Sum of priorities
            return health * 0.4 + file_score * 0.6

        best_peer = max(files_to_sync.keys(), key=peer_score)
        files_with_priority = files_to_sync[best_peer]

        # Sort by priority (highest first) and take top 10
        files_with_priority.sort(key=lambda x: x[1], reverse=True)
        files_to_request = [f[0] for f in files_with_priority[:10]]

        # Check if peer is alive
        with self.peers_lock:
            peer = self.peers.get(best_peer)

        if not peer or not peer.is_alive():
            return

        # Log and initiate sync
        total_missing = sum(len(f) for f in files_to_sync.values())
        model_files = sum(1 for f in files_to_request if "models/" in f or f.endswith(".pt"))
        logger.info(f"P2P SYNC: Missing {total_missing} files, requesting {len(files_to_request)} "
              f"({model_files} models) from {best_peer} (health={self._get_peer_health_score(best_peer):.0f})")

        try:
            import uuid
            job = DataSyncJob(
                job_id=f"p2p_{uuid.uuid4().hex[:8]}",
                source_node=best_peer,
                target_node=self.node_id,
                files=files_to_request,
            )

            self.sync_in_progress = True
            try:
                success = await self._request_node_sync(job)
                self._record_p2p_sync_result(best_peer, success)
                self._record_sync_result_for_adaptive("data", success)  # ADAPTIVE INTERVAL

                if success:
                    logger.info(f"P2P SYNC: Completed {len(files_to_request)} files from {best_peer}")
                    self._last_successful_p2p_sync = now
                    # Invalidate manifest cache
                    cache_path = self.sync_planner.get_manifest_cache_path()
                    if cache_path.exists():
                        cache_path.unlink()
                    # Update metrics
                    if hasattr(self, "_p2p_sync_metrics"):
                        self._p2p_sync_metrics["bytes"] += job.bytes_transferred
                    # DATA DEDUPLICATION: Record synced file hashes
                    for fpath in files_to_request:
                        if fpath in file_hashes:
                            self._record_synced_file(file_hashes[fpath], 0)
                else:
                    logger.info(f"P2P SYNC: Failed from {best_peer}: {job.error_message}")
            finally:
                self.sync_in_progress = False

        except Exception as e:  # noqa: BLE001
            logger.info(f"P2P SYNC: Error: {e}")
            self._record_sync_result_for_adaptive("data", False)  # ADAPTIVE: record failure
            self._record_p2p_sync_result(best_peer, False)
            self.sync_in_progress = False

    async def _p2p_model_sync(self):
        """DECENTRALIZED: Sync model files via P2P for faster model distribution.

        MODEL P2P SYNC: Ensures all nodes have access to latest trained models
        without relying on leader-coordinated sync. Prioritizes:
        - Newer models (by timestamp)
        - Models for active board configurations
        - NNUE models (smaller, faster to sync)
        - ADAPTIVE INTERVALS: faster during training, slower when idle
        """
        now = time.time()

        # ADAPTIVE INTERVAL: Uses activity-aware interval (faster during training)
        interval = self._get_adaptive_sync_interval("model")
        last_check = getattr(self, "_last_p2p_model_sync", 0)
        if now - last_check < interval:
            return
        self._last_p2p_model_sync = now

        # Skip if sync in progress
        if getattr(self, "sync_in_progress", False):
            return

        # Get model files from local manifest
        local_manifest = getattr(self, "local_data_manifest", None)
        if not local_manifest:
            return

        local_models = set()
        for file_info in getattr(local_manifest, "files", []) or []:
            rel_path = getattr(file_info, "relative_path", "")
            if rel_path and ("models/" in rel_path or rel_path.endswith((".pt", ".onnx", ".bin"))):
                local_models.add(rel_path)

        # Check peer manifests for models we're missing
        peer_manifests = getattr(self, "_gossip_peer_manifests", {})
        missing_models: dict[str, list[str]] = {}

        for peer_id, peer_manifest in peer_manifests.items():
            if peer_id == self.node_id:
                continue

            health = self._get_peer_health_score(peer_id)
            if health <= 0:
                continue

            peer_files = getattr(peer_manifest, "files", []) or []
            for file_info in peer_files:
                rel_path = getattr(file_info, "relative_path", "")
                if not rel_path:
                    continue
                if not ("models/" in rel_path or rel_path.endswith((".pt", ".onnx", ".bin"))):
                    continue
                if rel_path in local_models:
                    continue

                if peer_id not in missing_models:
                    missing_models[peer_id] = []
                missing_models[peer_id].append(rel_path)

        if not missing_models:
            return

        # Pick healthiest peer with models
        best_peer = max(missing_models.keys(), key=lambda p: self._get_peer_health_score(p))
        models_to_sync = missing_models[best_peer][:5]  # Max 5 models per cycle

        with self.peers_lock:
            peer = self.peers.get(best_peer)
        if not peer or not peer.is_alive():
            return

        logger.info(f"MODEL SYNC: Requesting {len(models_to_sync)} models from {best_peer}")

        try:
            import uuid
            job = DataSyncJob(
                job_id=f"model_{uuid.uuid4().hex[:8]}",
                source_node=best_peer,
                target_node=self.node_id,
                files=models_to_sync,
            )

            self.sync_in_progress = True
            try:
                success = await self._request_node_sync(job)
                self._record_p2p_sync_result(best_peer, success)
                self._record_sync_result_for_adaptive("model", success)  # ADAPTIVE INTERVAL
                if success:
                    logger.info(f"MODEL SYNC: Got {len(models_to_sync)} models from {best_peer}")
            finally:
                self.sync_in_progress = False
        except Exception as e:  # noqa: BLE001
            logger.info(f"MODEL SYNC: Error: {e}")
            self._record_sync_result_for_adaptive("model", False)  # ADAPTIVE: record failure
            self.sync_in_progress = False

    async def _p2p_training_db_sync(self):
        """DECENTRALIZED: Sync training databases via P2P for improved training diversity.

        TRAINING DB P2P SYNC: Ensures all nodes have access to consolidated training
        data without relying on leader-coordinated sync. Prioritizes:
        - canonical_*.db (canonical training data)
        - consolidated_training*.db (merged training data)
        - training_pool*.db (training pool databases)

        This improves training diversity by ensuring all nodes can train on
        cluster-wide data, not just their local selfplay games.

        ADAPTIVE INTERVALS: faster during training, slower when idle.
        """
        now = time.time()

        # ADAPTIVE INTERVAL: Uses activity-aware interval (faster during training)
        interval = self._get_adaptive_sync_interval("training_db")
        last_check = getattr(self, "_last_p2p_training_db_sync", 0)
        if now - last_check < interval:
            return
        self._last_p2p_training_db_sync = now

        # Skip if sync is in progress
        if getattr(self, "sync_in_progress", False):
            return

        # Skip if under disk pressure
        if getattr(self.self_info, "disk_percent", 0) > 80:
            return

        # Get our local files
        local_manifest = getattr(self, "local_data_manifest", None)
        if not local_manifest:
            return

        local_dbs = set()
        local_db_sizes = {}
        for file_info in getattr(local_manifest, "files", []) or []:
            rel_path = getattr(file_info, "relative_path", "")
            if rel_path.endswith(".db"):
                local_dbs.add(rel_path)
                local_db_sizes[rel_path] = getattr(file_info, "size_bytes", 0)

        # Check peer manifests for training databases
        peer_manifests = getattr(self, "_gossip_peer_manifests", {})
        if not peer_manifests:
            return

        # Find training databases we're missing or have smaller versions of
        missing_dbs: dict[str, list[tuple]] = {}  # peer_id -> [(db_path, size)]

        for peer_id, peer_manifest in peer_manifests.items():
            if peer_id == self.node_id:
                continue

            # Check circuit breaker
            health = self._get_peer_health_score(peer_id)
            if health <= 0:
                continue

            peer_files = getattr(peer_manifest, "files", []) or []
            for file_info in peer_files:
                rel_path = getattr(file_info, "relative_path", "")
                size = getattr(file_info, "size_bytes", 0)

                # Only sync training-related databases and ELO database
                if not rel_path.endswith(".db"):
                    continue
                if not ("canonical_" in rel_path or "consolidated_training" in rel_path or
                        "training_pool" in rel_path or "unified_elo" in rel_path or
                        "elo_ratings" in rel_path):
                    continue

                # Skip empty databases
                if size < 1024:
                    continue

                # Check if we don't have it or have a smaller version
                local_size = local_db_sizes.get(rel_path, 0)
                if local_size >= size:
                    continue

                if peer_id not in missing_dbs:
                    missing_dbs[peer_id] = []
                missing_dbs[peer_id].append((rel_path, size, health))

        if not missing_dbs:
            return

        # Pick healthiest peer with training DBs
        best_peer = max(missing_dbs.keys(), key=lambda p: self._get_peer_health_score(p))
        dbs_to_sync = [db[0] for db in missing_dbs[best_peer][:3]]  # Max 3 DBs per cycle

        # Check if peer is alive
        with self.peers_lock:
            peer = self.peers.get(best_peer)
        if not peer or not peer.is_alive():
            return

        logger.info(f"TRAINING DB SYNC: Requesting {len(dbs_to_sync)} training DBs from {best_peer}")

        try:
            import uuid
            job = DataSyncJob(
                job_id=f"traindb_{uuid.uuid4().hex[:8]}",
                source_node=best_peer,
                target_node=self.node_id,
                files=dbs_to_sync,
            )

            self.sync_in_progress = True
            try:
                success = await self._request_node_sync(job)
                self._record_p2p_sync_result(best_peer, success)
                self._record_sync_result_for_adaptive("training_db", success)  # ADAPTIVE INTERVAL
                if success:
                    logger.info(f"TRAINING DB SYNC: Got {len(dbs_to_sync)} training DBs from {best_peer}")
            finally:
                self.sync_in_progress = False
        except Exception as e:  # noqa: BLE001
            logger.info(f"TRAINING DB SYNC: Error: {e}")
            self._record_sync_result_for_adaptive("training_db", False)  # ADAPTIVE: record failure
            self.sync_in_progress = False

    # Jan 27, 2026: Gossip methods delegated to GossipProtocolMixin (Phase 13 decomposition)
    # _gossip_state_to_peers(), _get_gossip_known_states() are inherited from mixin

    def _get_peer_endpoints_for_gossip(self) -> list[dict[str, Any]]:
        """Phase 28: Get peer endpoints to share via gossip for peer-of-peer discovery.

        Returns a list of alive peer endpoints with connection info.
        This enables nodes to discover peers they can't reach directly.

        Jan 27, 2026: Migrated to PeerQueryBuilder (Phase 3.2).
        """
        return self._peer_query.to_endpoint_dicts(limit=GOSSIP_MAX_PEER_ENDPOINTS).unwrap_or([])

    # Jan 27, 2026: Phase 13 Gossip Protocol Cleanup - Inherited from GossipProtocolMixin:
    # - _process_gossip_response, _process_gossip_peer_endpoints, _try_connect_gossip_peer
    # - _handle_incoming_cluster_epoch, _gossip_anti_entropy_repair
    # - _tcp_probe_peer, _wire_cooldown_manager_probe, _wire_connection_pool_dynamic_sizing
    # - _get_cluster_size_for_pool, _is_peer_alive_for_circuit_breaker

    # =========================================================================
    # DISTRIBUTED TRAINING COORDINATION
    # =========================================================================
    # These functions enable nodes to coordinate training decisions without
    # relying on a leader, using gossip to share training state cluster-wide.
    # =========================================================================

    def _get_local_active_training_configs(self) -> list[dict]:
        """Get list of training configs currently running on this node.

        DISTRIBUTED TRAINING: Share what training this node is doing so other
        nodes can avoid duplicate training for the same configuration.

        Returns list of dicts with:
        - config_key: e.g. "square8_2p"
        - job_type: "nnue", "cmaes", etc.
        - started_at: timestamp when training started
        """
        active_configs = []
        with self.jobs_lock:
            for _job_id, job in self.local_jobs.items():
                job_type = getattr(job, "job_type", "")
                # Only include training-type jobs
                if job_type in ("nnue", "nnue_training", "training", "cmaes"):
                    board_type = getattr(job, "board_type", "")
                    num_players = getattr(job, "num_players", 2)
                    if board_type:
                        config_key = f"{board_type}_{num_players}p"
                        started_at = getattr(job, "started_at", time.time())
                        active_configs.append({
                            "config_key": config_key,
                            "job_type": job_type,
                            "started_at": started_at,
                        })
        return active_configs

    def _get_cluster_active_training_configs(self) -> dict[str, list[str]]:
        """Get all active training configs across the cluster via gossip.

        DISTRIBUTED TRAINING COORDINATION: Query gossip state to see what
        training is running cluster-wide. This enables nodes to avoid
        duplicate training without leader coordination.

        Returns: { config_key -> [list of node_ids training that config] }
        """
        cluster_configs: dict[str, list[str]] = {}

        # Include our own training
        for config in self._get_local_active_training_configs():
            config_key = config["config_key"]
            if config_key not in cluster_configs:
                cluster_configs[config_key] = []
            cluster_configs[config_key].append(self.node_id)

        # Include training from gossip state
        gossip_states = getattr(self, "_gossip_peer_states", {})
        now = time.time()
        for node_id, state in gossip_states.items():
            # Skip stale states (older than 2 minutes)
            if state.get("timestamp", 0) < now - 120:
                continue
            # Skip our own state
            if node_id == self.node_id:
                continue

            active_training = state.get("active_training_configs", [])
            for config in active_training:
                config_key = config.get("config_key", "")
                if config_key:
                    if config_key not in cluster_configs:
                        cluster_configs[config_key] = []
                    if node_id not in cluster_configs[config_key]:
                        cluster_configs[config_key].append(node_id)

        return cluster_configs

    def _is_config_being_trained_cluster_wide(self, config_key: str) -> tuple[bool, list[str]]:
        """Check if a config is already being trained somewhere in the cluster.

        DISTRIBUTED TRAINING: Before starting training for a config, check if
        another node is already training it. This avoids wasted resources.

        Returns: (is_being_trained, list_of_nodes_training_it)
        """
        cluster_configs = self._get_cluster_active_training_configs()
        training_nodes = cluster_configs.get(config_key, [])
        return (len(training_nodes) > 0, training_nodes)

    def _should_claim_training_slot(self, config_key: str) -> bool:
        """Decide if this node should claim a training slot for a config.

        DISTRIBUTED TRAINING COORDINATION: Use a deterministic algorithm to
        decide which node gets to train a config when multiple nodes want to.

        Algorithm:
        - If no one is training this config, the node with lowest ID claims it
        - If already training, don't start a duplicate
        - Include jitter to handle race conditions
        """
        is_training, _training_nodes = self._is_config_being_trained_cluster_wide(config_key)

        if is_training:
            # Config is already being trained somewhere
            return False

        # Get all nodes that might want to train (GPU nodes with data)
        candidate_nodes = [self.node_id]
        gossip_states = getattr(self, "_gossip_peer_states", {})
        now = time.time()
        for node_id, state in gossip_states.items():
            if state.get("timestamp", 0) < now - 120:
                continue
            if state.get("has_gpu", False):
                training_jobs = state.get("training_jobs", 0)
                # Only consider nodes with capacity (< 3 training jobs)
                if training_jobs < 3:
                    candidate_nodes.append(node_id)

        # Sort deterministically
        candidate_nodes = sorted(set(candidate_nodes))

        # The node with lowest ID that has capacity claims the slot
        # Add position-based jitter: higher position = less likely to claim
        import random
        my_position = candidate_nodes.index(self.node_id) if self.node_id in candidate_nodes else len(candidate_nodes)

        # First candidate always claims, others have decreasing probability
        claim_probability = max(0.1, 1.0 - (my_position * 0.3))

        return random.random() < claim_probability

    # =========================================================================
    # TRAINING TRIGGER IDEMPOTENCY (Phase 4 - Dec 2025)
    # =========================================================================
    # Hash-based deduplication to prevent duplicate training during leader
    # transitions. Each training trigger is hashed and stored; subsequent
    # triggers with the same hash within the TTL are rejected.
    # =========================================================================

    def _compute_training_trigger_hash(self, config_key: str, game_count: int) -> str:
        """Compute a hash for training trigger deduplication.

        IDEMPOTENCY: Hash is based on:
        - config_key (board_type + num_players)
        - game_count bucket (rounded to 1000 to allow minor variations)
        - time bucket (15-minute windows)

        This allows the same trigger to be rejected if attempted multiple times
        within a 15-minute window for the same approximate data state.
        """
        import hashlib

        # Round game count to nearest 1000 to tolerate minor variations
        game_bucket = (game_count // 1000) * 1000

        # Use 15-minute time buckets
        time_bucket = int(time.time() // 900) * 900

        hash_input = f"{config_key}:{game_bucket}:{time_bucket}"
        return hashlib.sha256(hash_input.encode()).hexdigest()[:16]

    def _is_training_trigger_duplicate(self, trigger_hash: str) -> bool:
        """Check if a training trigger is a duplicate.

        IDEMPOTENCY: Returns True if this trigger hash was seen recently.
        """
        if not hasattr(self, "_training_trigger_cache"):
            self._training_trigger_cache: dict[str, float] = {}

        now = time.time()
        ttl = 900  # 15-minute TTL for trigger cache

        # Cleanup old entries
        expired = [h for h, ts in self._training_trigger_cache.items() if now - ts > ttl]
        for h in expired:
            del self._training_trigger_cache[h]

        # Check if duplicate
        return trigger_hash in self._training_trigger_cache

    def _record_training_trigger(self, trigger_hash: str) -> None:
        """Record a training trigger for deduplication."""
        if not hasattr(self, "_training_trigger_cache"):
            self._training_trigger_cache = {}

        self._training_trigger_cache[trigger_hash] = time.time()

    def _check_training_idempotency(self, config_key: str, game_count: int) -> tuple[bool, str]:
        """Check if training can proceed (idempotency check).

        Returns:
            (can_proceed, trigger_hash) - can_proceed is False if duplicate
        """
        trigger_hash = self._compute_training_trigger_hash(config_key, game_count)

        if self._is_training_trigger_duplicate(trigger_hash):
            logger.info(f"IDEMPOTENT: Training trigger {trigger_hash[:8]} for {config_key} is duplicate, skipping")
            return False, trigger_hash

        return True, trigger_hash

    def _get_distributed_training_summary(self) -> dict:
        """Get summary of distributed training state for /status endpoint."""
        cluster_configs = self._get_cluster_active_training_configs()
        return {
            "active_configs": list(cluster_configs.keys()),
            "total_training_jobs": sum(len(nodes) for nodes in cluster_configs.values()),
            "configs_by_node_count": {k: len(v) for k, v in cluster_configs.items()},
        }

    # =========================================================================
    # DISTRIBUTED ELO
    # =========================================================================
    # Share ELO ratings via gossip for cluster-wide visibility without
    # requiring every node to query the ELO database directly.
    # =========================================================================

    def _get_local_elo_summary(self) -> dict:
        """Get summary of local ELO ratings for gossip propagation.

        DISTRIBUTED ELO: Share top models and their ratings via gossip so all
        nodes have visibility into model performance without querying the DB.

        LAZY LOADING: Defers ELO query until after startup (60s) to avoid
        slowing node initialization. Uses 10-minute cache to reduce DB load.

        Returns dict with:
        - top_models: List of top 5 models with ratings
        - total_models: Total number of rated models
        - last_update: Timestamp of last ELO update
        """
        now = time.time()
        cache_key = "_elo_summary_cache"
        cache_time_key = "_elo_summary_cache_time"
        startup_key = "_elo_startup_time"
        cached = getattr(self, cache_key, None)
        cached_time = getattr(self, cache_time_key, 0)

        # Track startup time for lazy loading
        if not hasattr(self, startup_key):
            setattr(self, startup_key, now)

        startup_time = getattr(self, startup_key, now)

        # LAZY LOADING: Don't query ELO during first 60s of startup
        if now - startup_time < 60:
            return {"top_models": [], "total_models": 0, "last_update": 0, "deferred": True}

        # Use 10-minute cache (was 5 min) to reduce DB load
        if cached and now - cached_time < 600:
            return cached

        summary = {
            "top_models": [],
            "total_models": 0,
            "last_update": 0,
        }

        try:
            from app.tournament import get_elo_database
            db = get_elo_database()

            # Get top 5 models by ELO (single optimized query)
            with db._get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT participant_id, rating, games_played, last_update,
                           (SELECT COUNT(*) FROM elo_ratings) as total,
                           (SELECT MAX(last_update) FROM elo_ratings) as max_updated
                    FROM elo_ratings
                    ORDER BY rating DESC
                    LIMIT 5
                """)
                rows = cursor.fetchall()

                if rows:
                    summary["total_models"] = rows[0][4] if rows[0][4] else 0
                    summary["last_update"] = rows[0][5] if rows[0][5] else 0

                for row in rows:
                    summary["top_models"].append({
                        "model": row[0],
                        "elo": round(row[1]),
                        "games": row[2],
                    })

        except (KeyError, IndexError, AttributeError, ImportError, sqlite3.OperationalError):
            # Silently fail - ELO summary is optional
            # Dec 2025: Added sqlite3.OperationalError to handle corrupted databases gracefully
            pass

        # Cache the result
        setattr(self, cache_key, summary)
        setattr(self, cache_time_key, now)

        return summary

    def _get_cluster_elo_summary(self) -> dict:
        """Get cluster-wide ELO summary from gossip state.

        DISTRIBUTED ELO: Aggregate ELO info from all nodes via gossip to get
        a cluster-wide view of model performance.
        """
        all_models = {}
        gossip_states = getattr(self, "_gossip_peer_states", {})
        now = time.time()

        # Include our own ELO summary
        local_summary = self._get_local_elo_summary()
        for model_info in local_summary.get("top_models", []):
            model_name = model_info.get("model", "")
            if model_name:
                all_models[model_name] = model_info

        # Include ELO summaries from gossip
        for _node_id, state in gossip_states.items():
            if state.get("timestamp", 0) < now - 300:  # Skip stale states
                continue

            elo_summary = state.get("elo_summary", {})
            for model_info in elo_summary.get("top_models", []):
                model_name = model_info.get("model", "")
                if model_name:
                    # Keep highest ELO seen for each model
                    existing = all_models.get(model_name, {})
                    if model_info.get("elo", 0) > existing.get("elo", 0):
                        all_models[model_name] = model_info

        # Sort by ELO and return top 10
        sorted_models = sorted(all_models.values(), key=lambda x: x.get("elo", 0), reverse=True)
        return {
            "top_models": sorted_models[:10],
            "total_unique_models": len(all_models),
        }

    def _load_curriculum_weights(self) -> dict[str, float]:
        """Load curriculum weights for selfplay prioritization."""
        if not HAS_CURRICULUM_WEIGHTS or load_curriculum_weights is None:
            return {}
        try:
            return load_curriculum_weights()
        except Exception as e:  # noqa: BLE001
            logger.debug(f"[P2P] Failed to load curriculum weights: {e}")
            return {}

    # =========================================================================
    # AUTOMATIC NODE RECOVERY
    # =========================================================================
    # Detect stuck/unhealthy nodes via gossip and trigger automatic recovery
    # (service restart) to maintain cluster health without manual intervention.
    # =========================================================================

    async def _check_node_recovery(self):
        """Jan 2026: Delegated to RecoveryManager (Phase 12 decomposition)."""
        await self.recovery_manager.check_node_recovery()

    async def _attempt_node_recovery(self, node_id: str, peer) -> bool:
        """Jan 2026: Delegated to RecoveryManager (Phase 12 decomposition)."""
        return await self.recovery_manager.attempt_node_recovery(node_id, peer)

    def _get_node_recovery_metrics(self) -> dict:
        """Jan 2026: Delegated to RecoveryManager (Phase 12 decomposition)."""
        from dataclasses import asdict
        metrics = self.recovery_manager.get_node_recovery_metrics()
        return asdict(metrics)

    # =========================================================================
    # STABILITY CONTROLLER CALLBACKS (Jan 2026 - Self-Healing Architecture)
    # =========================================================================
    # Recovery action callbacks triggered by StabilityController when symptoms
    # are detected. Each callback records effectiveness for feedback loop.
    # =========================================================================

    def _get_stability_metrics(self) -> dict:
        """Get current stability metrics for effectiveness tracking.

        Returns metrics used to evaluate whether recovery actions helped.
        """
        alive_count = self._peer_query.alive_count(exclude_self=False).unwrap_or(0)
        total_count = len(self.peers) + 1  # Include self

        # Calculate stability score (0-100)
        stability_score = 0.0
        if total_count > 0:
            alive_ratio = alive_count / total_count
            stability_score = alive_ratio * 100

            # Bonus for having a leader
            if self.leader_id:
                stability_score += 10

            # Penalty for flapping peers
            if self._peer_state_tracker:
                try:
                    diag = self._peer_state_tracker.get_diagnostics()
                    flapping_count = len(diag.get("flapping_peers", []))
                    stability_score -= flapping_count * 5
                except Exception:
                    pass

        return {
            "alive_count": alive_count,
            "total_count": total_count,
            "stability_score": max(0, min(100, stability_score)),
        }

    async def _action_increase_timeout(
        self, nodes: list[str], symptom: Any
    ) -> None:
        """Increase timeout for affected nodes."""
        if not self._adaptive_timeouts:
            return

        for node_id in nodes:
            self._adaptive_timeouts.increase_timeout(node_id)

        if self._effectiveness_tracker:
            self._effectiveness_tracker.record_action(
                "increase_timeout",
                nodes,
                {"symptom": symptom.symptom.value if hasattr(symptom, "symptom") else str(symptom)},
            )
        logger.info(f"[Stability] Increased timeout for {len(nodes)} nodes")

    async def _action_decrease_timeout(
        self, nodes: list[str], symptom: Any
    ) -> None:
        """Decrease timeout for affected nodes."""
        if not self._adaptive_timeouts:
            return

        for node_id in nodes:
            self._adaptive_timeouts.decrease_timeout(node_id)

        if self._effectiveness_tracker:
            self._effectiveness_tracker.record_action(
                "decrease_timeout",
                nodes,
                {"symptom": symptom.symptom.value if hasattr(symptom, "symptom") else str(symptom)},
            )
        logger.info(f"[Stability] Decreased timeout for {len(nodes)} nodes")

    async def _action_scale_pool(
        self, nodes: list[str], symptom: Any
    ) -> None:
        """Scale up connection pool size."""
        if self._effectiveness_tracker:
            self._effectiveness_tracker.record_action(
                "scale_pool_up",
                [],
                {"symptom": symptom.symptom.value if hasattr(symptom, "symptom") else str(symptom)},
            )
        logger.info("[Stability] Would scale connection pool (not implemented)")

    async def _action_reset_circuits(
        self, nodes: list[str], symptom: Any
    ) -> None:
        """Reset circuit breakers for affected nodes.

        January 22, 2026 - P2P Self-Healing Architecture:
        Now resets both node-level and per-transport circuit breakers.
        This enables transport fallover when one transport (e.g., Tailscale) fails.
        """
        reset_count = 0
        transport_reset_count = 0

        # Reset node-level circuit breakers
        try:
            from app.distributed.circuit_breaker import reset_circuit_breaker
            for node_id in nodes:
                try:
                    reset_circuit_breaker(node_id)
                    reset_count += 1
                except Exception as e:
                    logger.debug(f"Failed to reset node circuit for {node_id}: {e}")
        except ImportError:
            logger.debug("Circuit breaker module not available")

        # Reset per-transport circuit breakers for transport fallover
        try:
            from app.distributed.circuit_breaker import reset_transport_breakers_for_host
            for node_id in nodes:
                try:
                    # Get the host/IP for this node
                    peer = self.peers.get(node_id)
                    if peer:
                        host = getattr(peer, "ip", None) or getattr(peer, "host", None) or node_id
                        count = reset_transport_breakers_for_host(host)
                        transport_reset_count += count
                        if count > 0:
                            logger.debug(
                                f"[Stability] Reset {count} transport circuits for {node_id}"
                            )
                except Exception as e:
                    logger.debug(f"Failed to reset transport circuits for {node_id}: {e}")
        except ImportError:
            logger.debug("Transport circuit breaker module not available")

        if self._effectiveness_tracker:
            self._effectiveness_tracker.record_action(
                "reset_circuit",
                nodes,
                {
                    "symptom": symptom.symptom.value if hasattr(symptom, "symptom") else str(symptom),
                    "node_circuits_reset": reset_count,
                    "transport_circuits_reset": transport_reset_count,
                },
            )
        logger.info(
            f"[Stability] Reset circuits: {reset_count} node, {transport_reset_count} transport"
        )

    async def _action_increase_cooldown(
        self, nodes: list[str], symptom: Any
    ) -> None:
        """Increase cooldown period for recovery actions."""
        if self._stability_controller:
            old_cooldown = self._stability_controller._action_cooldown
            self._stability_controller._action_cooldown = min(old_cooldown * 1.5, 600.0)
            logger.info(
                f"[Stability] Increased action cooldown: {old_cooldown:.0f}s -> "
                f"{self._stability_controller._action_cooldown:.0f}s"
            )

        if self._effectiveness_tracker:
            self._effectiveness_tracker.record_action(
                "increase_cooldown",
                [],
                {"symptom": symptom.symptom.value if hasattr(symptom, "symptom") else str(symptom)},
            )

    async def _action_reinject_peer(
        self, nodes: list[str], symptom: Any
    ) -> None:
        """Reinject dead peers back into alive state for retry."""
        reinjected = 0
        for node_id in nodes:
            if node_id in self.peers:
                peer = self.peers[node_id]
                if not peer.is_alive():
                    peer.last_seen = time.time()
                    peer.status = "alive"
                    reinjected += 1
                    logger.info(f"[Stability] Reinjected peer {node_id}")

        if self._effectiveness_tracker:
            self._effectiveness_tracker.record_action(
                "reinject_peer",
                nodes,
                {"symptom": symptom.symptom.value if hasattr(symptom, "symptom") else str(symptom)},
            )
        logger.info(f"[Stability] Reinjected {reinjected}/{len(nodes)} peers")

    async def _action_emit_alert(
        self, nodes: list[str], symptom: Any
    ) -> None:
        """Emit alert for manual intervention."""
        symptom_str = symptom.symptom.value if hasattr(symptom, "symptom") else str(symptom)
        confidence = symptom.confidence if hasattr(symptom, "confidence") else 0.0
        root_cause = symptom.root_cause if hasattr(symptom, "root_cause") else "unknown"

        logger.warning(
            f"[Stability ALERT] {symptom_str} detected "
            f"(confidence={confidence:.2f}, cause={root_cause}, nodes={len(nodes)})"
        )

        try:
            from app.coordination.data_events import DataEventType
            from app.coordination.event_router import emit_event

            emit_event(
                DataEventType.STABILITY_ALERT,
                {
                    "symptom": symptom_str,
                    "confidence": confidence,
                    "root_cause": root_cause,
                    "affected_nodes": nodes[:10],
                    "timestamp": time.time(),
                },
            )
        except Exception:
            pass

        if self._effectiveness_tracker:
            self._effectiveness_tracker.record_action(
                "emit_alert",
                nodes,
                {"symptom": symptom_str},
            )

    # =========================================================================
    # GOSSIP-BASED LEADER HINTS
    # =========================================================================
    # Share leader preferences via gossip to enable faster leader elections.
    # When current leader fails, nodes can quickly converge on a new leader
    # based on hints from peers rather than running full election.
    # =========================================================================

    def _get_leader_hint(self) -> dict:
        """Get this node's leader hint for gossip propagation.

        LEADER HINTS: Share information about preferred leader candidates to
        enable faster convergence during elections. Hints include:
        - Current known leader and lease expiry
        - Preferred successor (highest-priority eligible node)
        - This node's priority rank
        """
        hint = {
            "current_leader": self.leader_id,
            "lease_expires": getattr(self, "leader_lease_expires", 0),
            "preferred_successor": None,
            "my_priority": 0,
        }

        # Calculate this node's priority (lower is better for Bully algorithm)
        # But we want to express it as a score (higher is better)
        with self.peers_lock:
            all_nodes = [self.node_id] + [p.node_id for p in self.peers.values() if p.is_alive()]

        all_nodes_sorted = sorted(all_nodes, reverse=True)  # Bully: higher ID wins
        if self.node_id in all_nodes_sorted:
            hint["my_priority"] = len(all_nodes_sorted) - all_nodes_sorted.index(self.node_id)

        # Find preferred successor (highest priority eligible node that's not current leader)
        voter_ids = list(getattr(self, "voter_node_ids", []) or [])
        for node_id in all_nodes_sorted:
            if node_id == self.leader_id:
                continue
            if voter_ids and node_id not in voter_ids:
                continue
            hint["preferred_successor"] = node_id
            break

        return hint

    def _get_cluster_leader_consensus(self) -> dict:
        """Get cluster consensus on leader from gossip hints.

        LEADER CONSENSUS: Aggregate leader hints from all nodes to determine
        if there's agreement on who the leader is/should be.
        """
        leader_votes = {}
        successor_votes = {}
        gossip_states = getattr(self, "_gossip_peer_states", {})
        now = time.time()

        # Count our vote
        our_hint = self._get_leader_hint()
        if our_hint["current_leader"]:
            leader_votes[our_hint["current_leader"]] = leader_votes.get(our_hint["current_leader"], 0) + 1
        if our_hint["preferred_successor"]:
            successor_votes[our_hint["preferred_successor"]] = successor_votes.get(our_hint["preferred_successor"], 0) + 1

        # Count votes from gossip
        for _node_id, state in gossip_states.items():
            if state.get("timestamp", 0) < now - 120:  # Skip stale states
                continue

            hint = state.get("leader_hint", {})
            leader = hint.get("current_leader")
            successor = hint.get("preferred_successor")

            if leader:
                leader_votes[leader] = leader_votes.get(leader, 0) + 1
            if successor:
                successor_votes[successor] = successor_votes.get(successor, 0) + 1

        # Find consensus leader and successor
        consensus_leader = max(leader_votes.items(), key=lambda x: x[1])[0] if leader_votes else None
        consensus_successor = max(successor_votes.items(), key=lambda x: x[1])[0] if successor_votes else None

        result = {
            "consensus_leader": consensus_leader,
            "leader_agreement": leader_votes.get(consensus_leader, 0) if consensus_leader else 0,
            "consensus_successor": consensus_successor,
            "successor_agreement": successor_votes.get(consensus_successor, 0) if consensus_successor else 0,
            "total_voters": len(gossip_states) + 1,
        }

        # ALERTING: Check for low leader consensus (only leader alerts, rate limited)
        if self.role == NodeRole.LEADER and result["total_voters"] >= 3:
            agreement_ratio = result["leader_agreement"] / result["total_voters"]
            last_low_consensus_alert = getattr(self, "_last_low_consensus_alert", 0)
            if agreement_ratio < 0.5 and now - last_low_consensus_alert > 3600:  # Alert once per hour max
                self._last_low_consensus_alert = now
                asyncio.create_task(self.notifier.send(
                    title="Low Leader Consensus",
                    message=f"Only {result['leader_agreement']}/{result['total_voters']} nodes agree on leader",
                    level="warning",
                    fields={
                        "Agreement": f"{agreement_ratio*100:.0f}%",
                        "Consensus Leader": str(consensus_leader),
                        "Total Voters": str(result["total_voters"]),
                        "Action": "Check for network partitions or stale nodes",
                    },
                    node_id=self.node_id,
                ))

        return result

    # =========================================================================
    # PEER REPUTATION TRACKING
    # =========================================================================
    # Track peer reliability over time for better peer selection in P2P sync,
    # gossip, and other distributed operations.
    # =========================================================================

    def _record_peer_interaction(self, peer_id: str, success: bool, interaction_type: str = "general"):
        """Record a peer interaction for reputation tracking.

        PEER REPUTATION: Track success/failure rates for different interaction types:
        - sync: File sync operations
        - gossip: Gossip message exchanges
        - heartbeat: Heartbeat responses
        - command: Remote command executions
        """
        if not hasattr(self, "_peer_reputation"):
            self._peer_reputation = {}

        if peer_id not in self._peer_reputation:
            self._peer_reputation[peer_id] = {
                "total_success": 0,
                "total_failure": 0,
                "recent_success": 0,
                "recent_failure": 0,
                "last_success": 0,
                "last_failure": 0,
                "last_reset": time.time(),
                "by_type": {},
            }

        rep = self._peer_reputation[peer_id]
        now = time.time()

        # Reset recent counters every hour
        if now - rep["last_reset"] > 3600:
            rep["recent_success"] = 0
            rep["recent_failure"] = 0
            rep["last_reset"] = now

        if success:
            rep["total_success"] += 1
            rep["recent_success"] += 1
            rep["last_success"] = now
        else:
            rep["total_failure"] += 1
            rep["recent_failure"] += 1
            rep["last_failure"] = now

        # Track by type
        if interaction_type not in rep["by_type"]:
            rep["by_type"][interaction_type] = {"success": 0, "failure": 0}
        if success:
            rep["by_type"][interaction_type]["success"] += 1
        else:
            rep["by_type"][interaction_type]["failure"] += 1

    def _get_peer_reputation_score(self, peer_id: str) -> float:
        """Get reputation score for a peer (0-100, higher is better).

        PEER REPUTATION SCORE: Combines multiple factors:
        - Recent success rate (70% weight) - last hour
        - Historical success rate (20% weight) - all time
        - Recency bonus (10% weight) - recent activity
        """
        if not hasattr(self, "_peer_reputation"):
            return 50.0  # Default neutral score

        rep = self._peer_reputation.get(peer_id)
        if not rep:
            return 50.0

        now = time.time()

        # Recent success rate (last hour)
        recent_total = rep["recent_success"] + rep["recent_failure"]
        recent_rate = rep["recent_success"] / max(1, recent_total)

        # Historical success rate
        total = rep["total_success"] + rep["total_failure"]
        historical_rate = rep["total_success"] / max(1, total)

        # Recency bonus (active peers get a boost)
        last_interaction = max(rep["last_success"], rep["last_failure"])
        recency_hours = (now - last_interaction) / 3600 if last_interaction > 0 else 24
        recency_score = max(0, 1.0 - (recency_hours / 24))  # Decays over 24 hours

        # Weighted score
        score = (recent_rate * 70) + (historical_rate * 20) + (recency_score * 10)

        return min(100.0, max(0.0, score))

    def _get_peer_reputation_summary(self) -> dict:
        """Get summary of peer reputation for gossip propagation.

        Share top/bottom peers by reputation to help cluster converge on
        reliable peer selection.
        """
        if not hasattr(self, "_peer_reputation"):
            return {"reliable_peers": [], "unreliable_peers": []}

        scores = []
        for peer_id in self._peer_reputation:
            score = self._get_peer_reputation_score(peer_id)
            scores.append((peer_id, score))

        scores.sort(key=lambda x: x[1], reverse=True)

        return {
            "reliable_peers": [{"peer": p, "score": round(s)} for p, s in scores[:5] if s >= 70],
            "unreliable_peers": [{"peer": p, "score": round(s)} for p, s in scores[-3:] if s < 30],
        }

    def _get_cluster_peer_reputation(self) -> dict:
        """Aggregate peer reputation from gossip for cluster-wide view."""
        all_scores = {}
        gossip_states = getattr(self, "_gossip_peer_states", {})
        now = time.time()

        # Include our own reputation data
        local_summary = self._get_peer_reputation_summary()
        for peer_info in local_summary.get("reliable_peers", []):
            peer_id = peer_info["peer"]
            if peer_id not in all_scores:
                all_scores[peer_id] = []
            all_scores[peer_id].append(peer_info["score"])

        # Include reputation from gossip
        for _node_id, state in gossip_states.items():
            if state.get("timestamp", 0) < now - 300:
                continue

            rep_summary = state.get("peer_reputation", {})
            for peer_info in rep_summary.get("reliable_peers", []):
                peer_id = peer_info["peer"]
                if peer_id not in all_scores:
                    all_scores[peer_id] = []
                all_scores[peer_id].append(peer_info["score"])

        # Calculate average scores
        avg_scores = {peer: sum(scores) / len(scores) for peer, scores in all_scores.items() if scores}
        sorted_peers = sorted(avg_scores.items(), key=lambda x: x[1], reverse=True)

        return {
            "most_reliable": [{"peer": p, "avg_score": round(s)} for p, s in sorted_peers[:10]],
            "peers_tracked": len(all_scores),
        }

    # ============================================================================
    # ADAPTIVE SYNC INTERVAL MANAGEMENT
    # ============================================================================
    # Dynamically adjusts P2P sync intervals based on:
    # - Cluster activity (training happening = more frequent sync)
    # - Success rate (failures = back off, successes = speed up)
    # - Data freshness (new data in cluster = more frequent sync)
    # ============================================================================

    def _init_adaptive_sync_intervals(self):
        """Initialize adaptive sync interval tracking."""
        self._adaptive_intervals = {
            "data": P2P_DATA_SYNC_BASE,
            "model": P2P_MODEL_SYNC_BASE,
            "training_db": P2P_TRAINING_DB_SYNC_BASE,
        }
        self._sync_success_streak = {
            "data": 0,
            "model": 0,
            "training_db": 0,
        }
        self._sync_failure_streak = {
            "data": 0,
            "model": 0,
            "training_db": 0,
        }
        self._last_interval_adjustment = 0

    def _get_adaptive_sync_interval(self, sync_type: str) -> float:
        """Get the current adaptive interval for a sync type.

        ADAPTIVE SYNC INTERVALS: Intervals adjust based on:
        1. Cluster activity (training = faster sync for models)
        2. Success rate (failures = back off)
        3. Base/min/max bounds per sync type

        Args:
            sync_type: One of "data", "model", "training_db"

        Returns:
            Current interval in seconds
        """
        if not hasattr(self, "_adaptive_intervals"):
            self._init_adaptive_sync_intervals()

        # Get current interval
        current = self._adaptive_intervals.get(sync_type, P2P_DATA_SYNC_BASE)

        # Apply activity-based adjustment
        activity_factor = self._calculate_cluster_activity_factor()

        # Get bounds for this sync type
        if sync_type == "data":
            min_interval = P2P_DATA_SYNC_MIN
            max_interval = P2P_DATA_SYNC_MAX
        elif sync_type == "model":
            min_interval = P2P_MODEL_SYNC_MIN
            max_interval = P2P_MODEL_SYNC_MAX
        elif sync_type == "training_db":
            min_interval = P2P_TRAINING_DB_SYNC_MIN
            max_interval = P2P_TRAINING_DB_SYNC_MAX
        else:
            min_interval = 120
            max_interval = 600

        # Apply activity factor (0.5-1.0 = active cluster, 1.0-2.0 = idle cluster)
        adjusted = current * activity_factor

        # Clamp to bounds
        return max(min_interval, min(max_interval, adjusted))

    def _calculate_cluster_activity_factor(self) -> float:
        """Calculate cluster activity factor for sync interval adjustment.

        CLUSTER ACTIVITY FACTOR:
        - < 1.0: Active cluster (training, selfplay) = faster sync
        - 1.0: Normal activity
        - > 1.0: Idle cluster = slower sync

        Returns:
            Activity factor (0.5 to 2.0)
        """
        now = time.time()

        # Check training activity (with defensive checks)
        training_active = False
        training_lock = getattr(self, "training_lock", None)
        training_jobs = getattr(self, "training_jobs", {})
        if training_lock and training_jobs:
            try:
                with training_lock:
                    for job in training_jobs.values():
                        if getattr(job, "status", None) == "running":
                            training_active = True
                            break
            except (AttributeError):
                pass

        # Check selfplay activity (count active jobs)
        selfplay_count = 0
        jobs_lock = getattr(self, "jobs_lock", None)
        selfplay_jobs = getattr(self, "selfplay_jobs", {})
        if jobs_lock and selfplay_jobs:
            try:
                with jobs_lock:
                    for job in selfplay_jobs.values():
                        if getattr(job, "status", None) == "running":
                            selfplay_count += 1
            except (AttributeError):
                pass

        # Check recent data generation from gossip
        recent_data = False
        gossip_states = getattr(self, "_gossip_node_states", {}) or {}
        for _node_id, state in gossip_states.items():
            if not isinstance(state, dict):
                continue
            last_game = state.get("last_game_time", 0)
            if now - last_game < 300:  # Game in last 5 min
                recent_data = True
                break

        # Calculate factor
        factor = 1.0

        if training_active:
            factor *= 0.5  # Much faster sync during training
        elif selfplay_count >= 5:
            factor *= 0.7  # Faster sync with active selfplay
        elif selfplay_count > 0:
            factor *= 0.85  # Slightly faster with some activity

        if recent_data:
            factor *= 0.9  # Faster sync when new data available

        # If completely idle (no jobs, stale data), slow down
        if selfplay_count == 0 and not training_active and not recent_data:
            factor *= 1.5

        return max(0.5, min(2.0, factor))

    def _record_sync_result_for_adaptive(self, sync_type: str, success: bool):
        """Record sync result to adjust adaptive intervals.

        ADAPTIVE INTERVAL ADJUSTMENT:
        - On success: reduce interval (speed up) up to min
        - On failure: increase interval (back off) up to max

        Args:
            sync_type: One of "data", "model", "training_db"
            success: Whether sync succeeded
        """
        if not hasattr(self, "_adaptive_intervals"):
            self._init_adaptive_sync_intervals()

        if success:
            self._sync_success_streak[sync_type] = self._sync_success_streak.get(sync_type, 0) + 1
            self._sync_failure_streak[sync_type] = 0

            # After 3 consecutive successes, speed up
            if self._sync_success_streak[sync_type] >= 3:
                current = self._adaptive_intervals[sync_type]
                new_interval = current * P2P_SYNC_SPEEDUP_FACTOR

                # Get min bound
                if sync_type == "data":
                    min_interval = P2P_DATA_SYNC_MIN
                elif sync_type == "model":
                    min_interval = P2P_MODEL_SYNC_MIN
                else:
                    min_interval = P2P_TRAINING_DB_SYNC_MIN

                self._adaptive_intervals[sync_type] = max(min_interval, new_interval)
                self._sync_success_streak[sync_type] = 0  # Reset streak
        else:
            self._sync_failure_streak[sync_type] = self._sync_failure_streak.get(sync_type, 0) + 1
            self._sync_success_streak[sync_type] = 0

            # On any failure, back off
            current = self._adaptive_intervals[sync_type]
            new_interval = current * P2P_SYNC_BACKOFF_FACTOR

            # Get max bound
            if sync_type == "data":
                max_interval = P2P_DATA_SYNC_MAX
            elif sync_type == "model":
                max_interval = P2P_MODEL_SYNC_MAX
            else:
                max_interval = P2P_TRAINING_DB_SYNC_MAX

            self._adaptive_intervals[sync_type] = min(max_interval, new_interval)

    def _get_sync_interval_summary(self) -> dict:
        """Get summary of current adaptive sync intervals for monitoring."""
        if not hasattr(self, "_adaptive_intervals"):
            self._init_adaptive_sync_intervals()

        return {
            "data_interval": round(self._get_adaptive_sync_interval("data")),
            "model_interval": round(self._get_adaptive_sync_interval("model")),
            "training_db_interval": round(self._get_adaptive_sync_interval("training_db")),
            "activity_factor": round(self._calculate_cluster_activity_factor(), 2),
            "data_streak": {
                "success": self._sync_success_streak.get("data", 0),
                "failure": self._sync_failure_streak.get("data", 0),
            },
            "model_streak": {
                "success": self._sync_success_streak.get("model", 0),
                "failure": self._sync_failure_streak.get("model", 0),
            },
        }

    # ============================================================================
    # SELFPLAY DATA DEDUPLICATION
    # ============================================================================
    # Tracks synced files and game IDs to avoid redundant transfers during P2P sync.
    # Uses bloom filter for efficient game ID tracking and file hash caching.
    # ============================================================================

    def _init_data_deduplication(self):
        """Initialize data deduplication tracking."""
        self._synced_file_hashes: set[str] = set()  # Hash -> synced
        self._known_game_ids: set[str] = set()  # Game IDs we have
        self._dedup_stats = {
            "files_skipped": 0,
            "games_skipped": 0,
            "bytes_saved": 0,
            "last_cleanup": time.time(),
        }
        self._dedup_lock = threading.Lock()

    def _record_synced_file(self, file_hash: str, file_size: int):
        """Record a file as synced for deduplication.

        DATA DEDUPLICATION: Track file hashes we've synced to avoid
        re-syncing the same file from different peers.

        Args:
            file_hash: Hash of the synced file
            file_size: Size in bytes (for metrics)
        """
        if not hasattr(self, "_synced_file_hashes"):
            self._init_data_deduplication()

        with self._dedup_lock:
            self._synced_file_hashes.add(file_hash)

    def _is_file_already_synced(self, file_hash: str) -> bool:
        """Check if file was already synced based on hash.

        Args:
            file_hash: Hash to check

        Returns:
            True if file was already synced
        """
        if not hasattr(self, "_synced_file_hashes"):
            self._init_data_deduplication()

        if not file_hash:
            return False

        with self._dedup_lock:
            return file_hash in self._synced_file_hashes

    # Jan 27, 2026: Phase 17B - Removed _record_game_ids (never called)
    # NOTE: _filter_unknown_games removed Dec 27, 2025 (dead code, never called)

    def _record_dedup_skip(self, file_count: int = 0, game_count: int = 0, bytes_saved: int = 0):
        """Record deduplication skip for metrics.

        Args:
            file_count: Number of files skipped
            game_count: Number of games skipped
            bytes_saved: Bytes saved by skipping
        """
        if not hasattr(self, "_dedup_stats"):
            self._init_data_deduplication()

        with self._dedup_lock:
            self._dedup_stats["files_skipped"] += file_count
            self._dedup_stats["games_skipped"] += game_count
            self._dedup_stats["bytes_saved"] += bytes_saved

    # NOTE: _cleanup_dedup_cache removed Dec 27, 2025 (dead code, never called)

    def _get_dedup_summary(self) -> dict:
        """Get deduplication metrics summary."""
        if not hasattr(self, "_dedup_stats"):
            self._init_data_deduplication()

        with self._dedup_lock:
            return {
                "files_skipped": self._dedup_stats.get("files_skipped", 0),
                "games_skipped": self._dedup_stats.get("games_skipped", 0),
                "bytes_saved_mb": round(self._dedup_stats.get("bytes_saved", 0) / (1024 * 1024), 2),
                "known_file_hashes": len(self._synced_file_hashes),
                "known_game_ids": len(self._known_game_ids),
            }

    def _get_swim_raft_status(self) -> dict[str, Any]:
        """Get SWIM/Raft protocol status summary.

        Returns status of the new P2P protocols (Phase 5, Dec 26, 2025):
        - SWIM: Gossip-based membership with 5s failure detection
        - Raft: Replicated work queue with sub-second failover

        These protocols are enabled via feature flags and provide
        gradual migration from HTTP/Bully to SWIM/Raft.
        """
        try:
            from scripts.p2p.constants import (  # noqa: I001
                SWIM_ENABLED, RAFT_ENABLED, MEMBERSHIP_MODE, CONSENSUS_MODE
            )
        except ImportError:
            SWIM_ENABLED = False
            RAFT_ENABLED = False
            MEMBERSHIP_MODE = "http"
            CONSENSUS_MODE = "bully"

        # Check SWIM status
        swim_status = {
            "enabled": SWIM_ENABLED,
            "available": False,
            "started": getattr(self, "_swim_started", False),
            "alive_count": 0,
        }
        try:
            from app.p2p.swim_adapter import SWIM_AVAILABLE
            swim_status["available"] = SWIM_AVAILABLE

            # Get membership summary if SWIM is active
            if hasattr(self, "get_swim_membership_summary"):
                summary = self.get_swim_membership_summary()
                swim_status.update({
                    "started": summary.get("swim_started", False),
                    "alive_count": summary.get("swim", {}).get("alive", 0),
                    "suspected_count": summary.get("swim", {}).get("suspected", 0),
                    "failed_count": summary.get("swim", {}).get("failed", 0),
                })
        except ImportError:
            pass
        except Exception as e:  # noqa: BLE001
            swim_status["error"] = str(e)

        # Check Raft status
        raft_status = {
            "enabled": RAFT_ENABLED,
            "available": False,
            "initialized": getattr(self, "_raft_initialized", False),
            "is_leader": False,
        }
        try:
            from scripts.p2p.consensus_mixin import PYSYNCOBJ_AVAILABLE
            raft_status["available"] = PYSYNCOBJ_AVAILABLE

            # Get Raft consensus status if available
            if hasattr(self, "get_raft_status"):
                raft_info = self.get_raft_status()
                raft_status.update({
                    "initialized": raft_info.get("raft_initialized", False),
                    "is_leader": raft_info.get("is_raft_leader", False),
                    "leader_address": raft_info.get("raft_leader", ""),
                    "should_use_raft": raft_info.get("should_use_raft", False),
                })
                if "work_queue_status" in raft_info:
                    wq = raft_info["work_queue_status"]
                    raft_status["work_queue"] = {
                        "pending": wq.get("by_status", {}).get("pending", 0),
                        "claimed": wq.get("by_status", {}).get("claimed", 0),
                        "completed": wq.get("by_status", {}).get("completed", 0),
                    }
        except ImportError:
            pass
        except Exception as e:  # noqa: BLE001
            raft_status["error"] = str(e)

        return {
            "membership_mode": MEMBERSHIP_MODE,
            "consensus_mode": CONSENSUS_MODE,
            "swim": swim_status,
            "raft": raft_status,
            "hybrid_status": {
                "swim_fallback_active": not swim_status.get("started", False) and MEMBERSHIP_MODE != "http",
                "raft_fallback_active": not raft_status.get("initialized", False) and CONSENSUS_MODE != "bully",
            },
        }

    def _get_cluster_observability(self) -> dict[str, Any]:
        """Get cluster observability metrics for debugging.

        December 30, 2025: Added to help diagnose idle GPU nodes and
        peer visibility discrepancies across the cluster.

        Returns:
            Dict with:
            - unhealthy_nodes: Nodes in unhealthy set (excluded from work)
            - gossip_discovered_peers: Count of peers found via gossip
            - cluster_job_distribution: Jobs per node for balance analysis
        """
        result: dict[str, Any] = {}

        # 1. Unhealthy nodes from node_selector
        try:
            if hasattr(self, "node_selector") and self.node_selector:
                unhealthy_set = getattr(self.node_selector, "_unhealthy_nodes", set())
                unhealthy_reasons = getattr(self.node_selector, "_unhealthy_reasons", {})
                result["unhealthy_nodes"] = {
                    "count": len(unhealthy_set),
                    "node_ids": list(unhealthy_set),
                    "reasons": dict(unhealthy_reasons),
                }
            else:
                result["unhealthy_nodes"] = {"error": "node_selector not available"}
        except Exception as e:  # noqa: BLE001
            result["unhealthy_nodes"] = {"error": str(e)}

        # 2. Gossip-discovered peers
        try:
            gossip_endpoints = getattr(self, "_gossip_learned_endpoints", {})
            result["gossip_discovered_peers"] = {
                "count": len(gossip_endpoints),
                "node_ids": list(gossip_endpoints.keys()),
            }
        except Exception as e:  # noqa: BLE001
            result["gossip_discovered_peers"] = {"error": str(e)}

        # 3. Cluster job distribution (for balance analysis)
        try:
            with self.peers_lock:
                job_distribution = {}
                for node_id, peer in self.peers.items():
                    if peer.is_alive():
                        job_distribution[node_id] = {
                            "selfplay_jobs": int(getattr(peer, "selfplay_jobs", 0) or 0),
                            "training_jobs": int(getattr(peer, "training_jobs", 0) or 0),
                            "gpu_percent": float(getattr(peer, "gpu_percent", 0) or 0),
                        }
                # Add self
                job_distribution[self.node_id] = {
                    "selfplay_jobs": int(getattr(self.self_info, "selfplay_jobs", 0) or 0),
                    "training_jobs": int(getattr(self.self_info, "training_jobs", 0) or 0),
                    "gpu_percent": float(getattr(self.self_info, "gpu_percent", 0) or 0),
                }

            # Compute summary stats
            if job_distribution:
                all_jobs = [d["selfplay_jobs"] for d in job_distribution.values()]
                avg_jobs = sum(all_jobs) / len(all_jobs) if all_jobs else 0
                max_jobs = max(all_jobs) if all_jobs else 0
                min_jobs = min(all_jobs) if all_jobs else 0
                idle_count = sum(1 for j in all_jobs if j == 0)
                result["cluster_job_distribution"] = {
                    "node_count": len(job_distribution),
                    "avg_selfplay_jobs": round(avg_jobs, 1),
                    "max_selfplay_jobs": max_jobs,
                    "min_selfplay_jobs": min_jobs,
                    "idle_nodes": idle_count,
                    "per_node": job_distribution,
                }
            else:
                result["cluster_job_distribution"] = {"error": "no peers available"}
        except Exception as e:  # noqa: BLE001
            result["cluster_job_distribution"] = {"error": str(e)}

        return result

    def _get_data_summary_cached(self) -> dict[str, Any]:
        """Get cached data summary for /status endpoint.

        January 13, 2026: Added as part of unified data discovery infrastructure.
        Returns game counts from local canonical databases for quick access.
        For full multi-source data, use /data/summary endpoint.

        January 23, 2026: FIXED - Removed blocking SQLite fallback that was
        causing event loop blocks. Now only returns cached data or empty dict.
        The fallback to canonical DB scan should be done via async methods.

        Returns:
            Dict with total game counts per config from local canonical DBs
        """
        try:
            # Use cached game counts from selfplay scheduler if available
            if hasattr(self, "selfplay_scheduler") and self.selfplay_scheduler:
                counts = getattr(self.selfplay_scheduler, "_p2p_game_counts", None)
                if counts:
                    total = sum(counts.values())
                    return {
                        "total_games": total,
                        "by_config": dict(counts),
                        "source": "selfplay_scheduler_cache",
                        "config_count": len(counts),
                    }

            # FIXED Jan 23, 2026: Do NOT fall back to blocking SQLite scan here.
            # The sync method _seed_selfplay_scheduler_game_counts_sync() was
            # blocking the event loop for seconds. Return empty dict instead.
            # Game counts will be populated async via selfplay_scheduler.
            return {
                "total_games": 0,
                "by_config": {},
                "source": "none",
                "error": "No cached data - scheduler not initialized yet",
            }

        except Exception as e:  # noqa: BLE001
            return {
                "total_games": 0,
                "by_config": {},
                "source": "error",
                "error": str(e),
            }

    def _get_cooldown_stats(self) -> dict[str, Any]:
        """Get adaptive dead peer cooldown statistics for monitoring.

        January 20, 2026: Added to expose cooldown manager metrics in /status.
        Helps diagnose peer recovery issues and verify adaptive cooldown is working.

        Returns:
            Dict with:
            - enabled: Whether the adaptive cooldown manager is active
            - nodes_in_cooldown: Number of nodes currently in cooldown
            - stats: Cooldown manager statistics (probes, recoveries, etc.)
            - in_cooldown: List of nodes currently in cooldown with their tier/remaining time
        """
        if not self._cooldown_manager:
            # Fallback to legacy dict tracking
            return {
                "enabled": False,
                "nodes_in_cooldown": len(self._dead_peer_timestamps),
                "fallback_mode": True,
                "dead_peer_timestamps": {
                    node_id: {"dead_since": ts, "age_seconds": time.time() - ts}
                    for node_id, ts in self._dead_peer_timestamps.items()
                },
            }

        try:
            stats = self._cooldown_manager.get_stats()
            in_cooldown = self._cooldown_manager.get_all_in_cooldown()
            return {
                "enabled": True,
                "nodes_in_cooldown": stats.get("nodes_in_cooldown", 0),
                "stats": stats,
                "in_cooldown": in_cooldown,
                "fallback_mode": False,
            }
        except Exception as e:  # noqa: BLE001
            return {
                "enabled": True,
                "error": str(e),
            }

    # NOTE: _get_peer_health_summary() inlined at call site (Jan 2026 Phase 2)

    def _get_fallback_status(self) -> dict[str, Any]:
        """Get fallback mechanism status for debugging partition issues.

        Session 17.41 (Jan 6, 2026): Exposes visibility into why fallback mechanisms
        aren't activating during network partitions. This helps diagnose issues where
        the work queue has items but workers can't claim jobs because the leader is
        unreachable and fallbacks haven't kicked in.

        Returns:
            Dict with:
            - autonomous_queue: Whether local queue fallback is active
            - work_discovery: Multi-channel work discovery status
            - leader_status: Leader contact timing
            - partition_healer: Partition healing escalation state
        """
        result: dict[str, Any] = {}
        now = time.time()

        # 1. Autonomous queue status
        try:
            loop = getattr(self, "_autonomous_queue_loop", None)
            if loop is not None:
                loop_status = loop.get_status() if hasattr(loop, "get_status") else {}
                result["autonomous_queue"] = {
                    "active": loop_status.get("activated", False),
                    "enabled": loop_status.get("enabled", False),
                    "running": loop_status.get("running", False),
                    "activation_reason": loop_status.get("activation_reason", ""),
                    "no_leader_duration": loop_status.get("no_leader_duration", 0.0),
                    "queue_depth": loop_status.get("queue_depth", 0),
                }
            else:
                result["autonomous_queue"] = {"error": "loop_not_initialized"}
        except Exception as e:  # noqa: BLE001
            result["autonomous_queue"] = {"error": str(e)}

        # 2. Work discovery manager status
        try:
            from scripts.p2p.loops.job_loops import get_work_discovery_manager
            manager = get_work_discovery_manager()
            if manager is not None:
                mgr_status = manager.get_status() if hasattr(manager, "get_status") else {}
                result["work_discovery"] = {
                    "enabled": mgr_status.get("enabled", False),
                    "active_channels": mgr_status.get("active_channels", []),
                    "last_work_time": mgr_status.get("last_work_time", 0.0),
                    "claims_via_leader": mgr_status.get("claims_via_leader", 0),
                    "claims_via_peer": mgr_status.get("claims_via_peer", 0),
                    "claims_via_local": mgr_status.get("claims_via_local", 0),
                }
            else:
                result["work_discovery"] = {"error": "manager_not_initialized"}
        except ImportError:
            result["work_discovery"] = {"error": "import_failed"}
        except Exception as e:  # noqa: BLE001
            result["work_discovery"] = {"error": str(e)}

        # 3. Leader contact status
        try:
            last_leader_seen = getattr(self, "last_leader_seen", now)
            leader_unreachable_duration = now - last_leader_seen
            result["leader_status"] = {
                "last_leader_seen": last_leader_seen,
                "leader_unreachable_duration": round(leader_unreachable_duration, 1),
                "is_leaderless": self.leader_id is None or self.leader_id == "",
                "current_leader_id": self.leader_id,
                "is_self_leader": self._is_leader(),
            }
        except Exception as e:  # noqa: BLE001
            result["leader_status"] = {"error": str(e)}

        # 4. Partition healer status (if available)
        try:
            from scripts.p2p.partition_healer import get_partition_healer
            healer = get_partition_healer()
            healer_status = healer.get_status()
            result["partition_healer"] = {
                "escalation_level": healer_status.get("escalation_level", 0),
                "last_healing_attempt": healer_status.get("last_healing_attempt", 0.0),
                "healing_in_progress": healer_status.get("healing_in_progress", False),
                "has_orchestrator": healer_status.get("has_orchestrator", False),
                "election_ready": healer_status.get("election_ready", True),
            }
        except ImportError:
            result["partition_healer"] = {"error": "import_failed"}
        except Exception as e:  # noqa: BLE001
            result["partition_healer"] = {"error": str(e)}

        return result

    # ============================================================================
    # DISTRIBUTED TOURNAMENT SCHEDULING
    # ============================================================================
    # Allows tournaments to be scheduled and coordinated via gossip protocol
    # without requiring a leader. Uses consensus to elect tournament coordinator.
    # Jan 2026: Delegated to TournamentManager (Phase 11 decomposition).
    # ============================================================================

    # Jan 27, 2026: Phase 17B - Removed _init_distributed_tournament_scheduling (unused delegation wrapper)

    def _get_tournament_gossip_state(self) -> dict:
        """Get tournament state for gossip propagation.

        Jan 2026: Delegated to TournamentManager (Phase 11 decomposition).
        """
        return self.tournament_manager.get_tournament_gossip_state()

    def _process_tournament_gossip(self, node_id: str, tournament_state: dict):
        """Process tournament info received via gossip.

        Jan 2026: Delegated to TournamentManager (Phase 11 decomposition).
        """
        self.tournament_manager.process_tournament_gossip(node_id, tournament_state)

    def _check_tournament_consensus(self):
        """Check if any tournament proposals have reached consensus.

        Jan 2026: Delegated to TournamentManager (Phase 11 decomposition).
        """
        self.tournament_manager.check_tournament_consensus()

    # NOTE: _start_tournament_from_proposal removed Jan 28, 2026 (dead code)
    # Use self.tournament_manager.start_tournament_from_proposal() directly.

    def _get_distributed_tournament_summary(self) -> dict:
        """Get summary of distributed tournament scheduling for status endpoint.

        Jan 2026: Delegated to TournamentManager (Phase 11 decomposition).
        """
        return self.tournament_manager.get_distributed_tournament_summary()

    async def _start_monitoring_if_leader(self):
        """Start Prometheus/Grafana when we become leader (P2P monitoring resilience)."""
        if not self.monitoring_manager:
            return
        if self.role != NodeRole.LEADER:
            return
        if self._monitoring_was_leader:
            return  # Already started

        try:
            # Update peer list for Prometheus config
            with self.peers_lock:
                peer_list = [
                    {"node_id": p.node_id, "host": p.host, "port": getattr(p, "metrics_port", 9091)}
                    for p in self.peers.values()
                    if p.node_id != self.node_id and p.is_healthy()
                ]
            self.monitoring_manager.update_peers(peer_list)

            # Start monitoring services
            success = await self.monitoring_manager.start_as_leader()
            if success:
                logger.info("Monitoring services started on leader node")
                self._monitoring_was_leader = True
            else:
                logger.error("Failed to start monitoring services")
        except Exception as e:  # noqa: BLE001
            logger.error(f"starting monitoring services: {e}")

    async def _stop_monitoring_if_not_leader(self):
        """Stop Prometheus/Grafana when we step down from leadership."""
        if not self.monitoring_manager:
            return
        if not self._monitoring_was_leader:
            return  # Never started

        if self.role != NodeRole.LEADER:
            try:
                await self.monitoring_manager.stop()
                logger.info("Monitoring services stopped (no longer leader)")
                self._monitoring_was_leader = False
            except Exception as e:  # noqa: BLE001
                logger.error(f"stopping monitoring services: {e}")

    async def _start_p2p_auto_deployer(self):
        """Start P2P auto-deployer when we become leader.

        The auto-deployer ensures P2P orchestrator is running on all cluster nodes.
        This solves the fundamental gap where P2P deployment was manual-only.
        """
        if self.role != NodeRole.LEADER:
            return
        if self._auto_deployer_task is not None:
            return  # Already running

        try:
            from app.coordination.p2p_auto_deployer import P2PAutoDeployer, P2PDeploymentConfig

            config = P2PDeploymentConfig(
                check_interval_seconds=300.0,  # Check every 5 minutes
                min_coverage_percent=90.0,
            )
            self.p2p_auto_deployer = P2PAutoDeployer(config=config)

            # Run as background task
            self._auto_deployer_task = asyncio.create_task(
                self.p2p_auto_deployer.run_daemon(),
                name="p2p_auto_deployer"
            )
            logger.info("P2P Auto-Deployer started (leader responsibility)")
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to start P2P auto-deployer: {e}")

    async def _stop_p2p_auto_deployer(self):
        """Stop P2P auto-deployer when we step down from leadership."""
        if self.p2p_auto_deployer:
            self.p2p_auto_deployer.stop()
        if self._auto_deployer_task:
            self._auto_deployer_task.cancel()
            with contextlib.suppress(asyncio.CancelledError):
                await self._auto_deployer_task
            self._auto_deployer_task = None
        self.p2p_auto_deployer = None
        logger.info("P2P Auto-Deployer stopped")

    async def _renew_leader_lease(self):
        """Renew our leadership lease and broadcast to peers."""
        if self.role != NodeRole.LEADER:
            return
        # Jan 23, 2026: Use ULSM QuorumHealth for unified quorum tracking
        # (Phase 1 fix: previously used separate _quorum_fail_count which diverged from _is_leader())
        if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
            # Jan 2, 2026: Use _count_alive_voters() to check IP:port matches
            voters_alive = self._count_alive_voters()
            quorum_size = getattr(self, "voter_quorum_size", 0)
            # Use ULSM QuorumHealth - same tracker as _is_leader() uses
            threshold_exceeded = self._leadership_sm.quorum_health.record_failure(voters_alive)
            fail_count = self._leadership_sm.quorum_health.consecutive_failures
            threshold = self._leadership_sm.quorum_health.failure_threshold
            logger.warning(
                f"[LeaseRenewal] Voter quorum check failed ({fail_count}/{threshold}): "
                f"voters_alive={voters_alive}, quorum_size={quorum_size}"
            )
            if threshold_exceeded:
                logger.info(f"Lost voter quorum ({threshold} consecutive failures via ULSM); stepping down: {self.node_id}")
                # Jan 2026: Use ULSM step-down which broadcasts to peers BEFORE local mutation
                self._schedule_step_down_sync(TransitionReason.QUORUM_LOST)
                self._release_voter_grant_if_self()
            return
        else:
            # Reset ULSM quorum health counter on success
            voters_alive = self._count_alive_voters()
            self._leadership_sm.quorum_health.record_success(voters_alive)

        now = time.time()
        if now - self.last_lease_renewal < LEADER_LEASE_RENEW_INTERVAL:
            return  # Too soon to renew

        lease_id = str(self.leader_lease_id or "")
        if not lease_id:
            lease_id = f"{self.node_id}_{int(time.time())}_{uuid.uuid4().hex[:8]}"
        lease_expires = await self._acquire_voter_lease_quorum(lease_id, int(LEADER_LEASE_DURATION))
        if getattr(self, "voter_node_ids", []) and not lease_expires:
            # Voter quorum failed - try arbiter fallback before stepping down
            logger.info("Voter lease quorum failed; checking arbiter...")
            arbiter_leader = await self._query_arbiter_for_leader()
            if arbiter_leader == self.node_id:
                # Arbiter still recognizes us as leader - extend lease provisionally
                logger.info("Arbiter confirms us as leader despite quorum failure; continuing with provisional lease")
                lease_expires = now + LEADER_LEASE_DURATION / 2  # Shorter lease until quorum recovers
            elif arbiter_leader:
                # Arbiter says someone else is leader - defer to arbiter
                logger.info(f"Arbiter reports different leader ({arbiter_leader}); stepping down")
                # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                self._set_leader(arbiter_leader, reason="arbiter_override", save_state=False)
                self.leader_lease_id = ""
                self.leader_lease_expires = 0.0
                self.last_lease_renewal = 0.0
                self._release_voter_grant_if_self()
                self._save_state()
                return
            else:
                # Arbiter also unreachable - step down to be safe
                logger.error(f"Failed to renew voter lease quorum and arbiter unreachable; stepping down: {self.node_id}")
                # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                self._set_leader(None, reason="arbiter_unreachable", save_state=False)
                self.leader_lease_id = ""
                self.leader_lease_expires = 0.0
                self.last_lease_renewal = 0.0
                self._release_voter_grant_if_self()
                self._save_state()
                return

        self.leader_lease_id = lease_id
        self.leader_lease_expires = float(lease_expires or (now + LEADER_LEASE_DURATION))
        self.last_lease_renewal = now

        # Jan 5, 2026: Renew self-leadership in state machine during lease renewal
        # Prevents the leader self-acknowledgment bug where leader_id is only set
        # during state transitions, causing the cluster to appear leaderless
        if hasattr(self, "_leadership_sm") and self._leadership_sm:
            try:
                self._leadership_sm.renew_self_leadership()
            except Exception as e:
                logger.debug(f"[LeaseRenewal] Failed to renew self-leadership: {e}")

        # Broadcast lease renewal to all peers
        with self.peers_lock:
            peers = list(self.peers.values())

        timeout = ClientTimeout(total=3)
        try:
            async with get_client_session(timeout) as session:
                for peer in peers:
                    if peer.node_id != self.node_id and peer.is_alive():
                        try:
                            url = self._url_for_peer(peer, "/coordinator")
                            await session.post(url, json={
                                "leader_id": self.node_id,
                                "lease_id": self.leader_lease_id,
                                "lease_expires": self.leader_lease_expires,
                                "lease_renewal": True,
                                "voter_node_ids": list(getattr(self, "voter_node_ids", []) or []),
                            }, headers=self._auth_headers())
                        except (aiohttp.ClientError, asyncio.TimeoutError, KeyError, IndexError, AttributeError):
                            pass  # Network errors expected during lease renewal
        except Exception as e:  # noqa: BLE001
            logger.info(f"Lease renewal error: {e}")

    def _is_leader_lease_valid(self) -> bool:
        """Check if the current leader's lease is still valid."""
        if not self.leader_id:
            return False

        # Jan 13, 2026: Reject proxy_only leaders - they should never have been elected
        # This forces a re-election if a proxy_only node somehow became leader
        if self._is_node_proxy_only(self.leader_id):
            logger.warning(
                f"[LeaderValidation] Current leader {self.leader_id} is proxy_only - invalidating lease"
            )
            return False

        # Jan 19, 2026: Use symmetric grace period for both leader and followers
        # CRITICAL FIX: Previous code used strict check for leader (no grace) but
        # lenient check for followers (+90s grace). This caused 90s split-brain window:
        # - Leader steps down at t=180
        # - Followers still see leader valid until t=270
        # - During t=180-270: leader="none" on leader, leader=old on followers
        # Now using consistent 30s grace for everyone to prevent split-brain.
        grace = 30  # Same grace for leader and followers
        return time.time() < self.leader_lease_expires + grace

    async def _check_and_resolve_split_brain(self) -> bool:
        """Check for split-brain (multiple leaders) and resolve by stepping down if needed.

        Jan 28, 2026: Phase 18C - Thin wrapper delegating to QuorumManager.
        """
        if self.quorum_manager:
            # Ensure orchestrator reference is set (for late binding)
            if not getattr(self.quorum_manager, "_orchestrator", None):
                self.quorum_manager.set_orchestrator(self)
            return await self.quorum_manager.check_and_resolve_split_brain()
        return False

    async def _job_management_loop(self):
        """Manage jobs - leader coordinates cluster, all nodes handle local operations."""
        while self.running:
            try:
                # ==== DECENTRALIZED OPERATIONS (all nodes) ====
                # These run on every node to ensure cluster health even without a leader
                # Makes the cluster resilient to leader instability

                # Local data consolidation: merge siloed job DBs to main selfplay.db
                await self._consolidate_selfplay_data()

                # Local stuck job detection: each node monitors its own processes
                await self._check_local_stuck_jobs()

                # Local resource cleanup: handle disk/memory pressure independently
                await self._local_resource_cleanup()

                # Local job management: start/stop jobs based on node capacity
                await self._manage_local_jobs_decentralized()

                # Local GPU auto-scaling: optimize GPU utilization independently
                await self._local_gpu_auto_scale()

                # Leaderless training fallback: trigger local training if no leader for too long
                await self._check_local_training_fallback()

                # Emergency coordinator: if voter quorum unavailable for >5min, take leadership
                await self._check_emergency_coordinator_fallback()

                # P2P data sync: nodes can sync data directly without leader
                await self._p2p_data_sync()

                # P2P model sync: dedicated model distribution (more frequent)
                await self._p2p_model_sync()

                # P2P training DB sync: sync training databases for diversity
                await self._p2p_training_db_sync()

                # Gossip protocol: share state with random peers
                await self._gossip_state_to_peers()

                # Anti-entropy repair: periodic full state reconciliation
                await self._gossip_anti_entropy_repair()

                # ==== LEADER-ONLY OPERATIONS ====
                if self.role == NodeRole.LEADER:
                    # LEARNED LESSONS - Check for split-brain before acting as leader
                    if await self._check_and_resolve_split_brain():
                        # We stepped down, skip this cycle
                        await asyncio.sleep(JOB_CHECK_INTERVAL)
                        continue

                    await self._manage_cluster_jobs()
                    # Cluster rebalancing: migrate jobs from weak to powerful nodes
                    await self._check_cluster_balance()
                    # Phase 3: Check if training should be triggered automatically
                    await self._check_and_trigger_training()
                    # Phase 5: Check improvement cycles for automated training
                    await self._check_improvement_cycles()
                    # Cluster-wide stuck job detection (remote nodes)
                    await self._check_and_kill_stuck_jobs()
                    # Work queue rebalancing: assign queued work to idle nodes
                    await self._auto_rebalance_from_work_queue()
                    # Self-healing: auto-scale GPU utilization toward 60-80% target
                    await self._auto_scale_gpu_utilization()
                    # Self-healing: probe NAT-blocked peers to check if they've become reachable
                    await self._sweep_nat_recovery()
                    # Self-healing: detect and recover stuck nodes via SSH restart
                    await self._check_node_recovery()
            except Exception as e:  # noqa: BLE001
                logger.info(f"Job management error: {e}")

            await asyncio.sleep(JOB_CHECK_INTERVAL)

    async def _check_and_kill_stuck_jobs(self) -> int:
        """Detect and terminate stuck training/selfplay jobs.

        Jan 2026: Delegated to JobLifecycleManager.
        """
        return await self.job_lifecycle_manager.check_and_kill_stuck_jobs()

    async def _check_local_stuck_jobs(self) -> int:
        """DECENTRALIZED: Detect and kill stuck processes on THIS node only.

        Jan 2026: Delegated to JobLifecycleManager.
        """
        return await self.job_lifecycle_manager.check_local_stuck_jobs()

    async def _remote_kill_stuck_job(self, target_node: str, job_id: str, job_type: str) -> bool:
        """Send kill command to remote node for stuck job.

        Jan 2026: Delegated to JobLifecycleManager.
        """
        return await self.job_lifecycle_manager.remote_kill_stuck_job(
            target_node, job_id, job_type
        )

    async def _manage_local_jobs_decentralized(self) -> int:
        """DECENTRALIZED: Each node manages its own job count based on gossip state.

        Runs on ALL nodes to ensure selfplay continues even during leader elections.
        Each node autonomously:
        1. Checks its own resource pressure (disk, memory, CPU)
        2. Uses gossip state to calculate proportional job count
        3. Starts or stops local jobs as needed

        PHASE 3 DECENTRALIZATION (Dec 2025):
        - With Serf providing reliable failure detection, we can act quickly
        - Proportional allocation based on gossip cluster capacity
        - 30-second timeout for faster leader-failure recovery

        Returns:
            Number of jobs started/stopped
        """
        changes = 0
        now = time.time()

        # Rate limit: check every 30 seconds (reduced from 60s for faster response)
        last_check = getattr(self, "_last_local_job_manage", 0)
        if now - last_check < 30:
            return 0
        self._last_local_job_manage = now

        # Skip if leader is managing (avoid conflicts)
        # But continue if leaderless for > 30 seconds (reduced from 60s for Serf reliability)
        # Dec 30, 2025: Also allow self-assignment if leader exists but isn't dispatching work
        if self.role == NodeRole.LEADER:
            return 0  # Leader uses centralized management
        if self.leader_id:
            leaderless_duration = now - getattr(self, "last_leader_seen", now)
            work_dispatch_gap = now - getattr(self, "last_work_from_leader", now)

            # Defer to leader only if BOTH conditions are met:
            # 1. Leader was seen recently (alive)
            # 2. Leader has been dispatching work recently (active)
            if leaderless_duration < LEADERLESS_TRAINING_TIMEOUT:
                if work_dispatch_gap < LEADER_WORK_DISPATCH_TIMEOUT:
                    return 0  # Have a functioning leader that's actively dispatching
                else:
                    # Leader is present but not dispatching work - allow self-assignment
                    logger.info(
                        f"LOCAL: Leader present but no work dispatched in {work_dispatch_gap:.0f}s "
                        f"(timeout={LEADER_WORK_DISPATCH_TIMEOUT}s) - self-assigning"
                    )

        # Update self info
        self._update_self_info()
        node = self.self_info

        # Check resource pressure - don't start jobs if under pressure
        if node.disk_percent >= DISK_WARNING_THRESHOLD:
            logger.info(f"LOCAL: Disk at {node.disk_percent:.0f}% - skipping job starts")
            await self._cleanup_local_disk()
            return 0

        if node.memory_percent >= MEMORY_WARNING_THRESHOLD:
            logger.info(f"LOCAL: Memory at {node.memory_percent:.0f}% - skipping job starts")
            return 0

        # Calculate target jobs for this node (delegated to SelfplayScheduler Dec 2025)
        target_selfplay = self.selfplay_scheduler.get_target_jobs_for_node(node)
        current_jobs = int(getattr(node, "selfplay_jobs", 0) or 0)

        # Dec 30, 2025: Cluster-aware job balancing
        # Prevent job concentration on a few nodes by checking cluster average
        # Skip spawning if we're already above average + 2 to give idle nodes a chance
        try:
            # Jan 10, 2026: Copy-on-read pattern - minimize lock hold time
            with self.peers_lock:
                peers_snapshot = list(self.peers.values())
            # Compute job counts outside lock (is_alive() can be slow)
            peer_job_counts = [
                int(getattr(p, "selfplay_jobs", 0) or 0)
                for p in peers_snapshot
                if p.is_alive() and hasattr(p, "selfplay_jobs")
            ]
            if peer_job_counts:
                avg_jobs = sum(peer_job_counts) / len(peer_job_counts)
                if current_jobs > avg_jobs + 2:
                    logger.info(
                        f"LOCAL: Skipping job spawn - {current_jobs} jobs > cluster avg {avg_jobs:.1f}+2 "
                        f"(letting underutilized nodes catch up)"
                    )
                    return 0
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Cluster balance check error: {e}")

        # Start jobs if below target
        if current_jobs < target_selfplay:
            needed = min(target_selfplay - current_jobs, 3)  # Max 3 per cycle
            logger.info(f"LOCAL: Starting {needed} selfplay job(s) ({current_jobs}/{target_selfplay})")

            # Dec 27, 2025: Generate batch ID and emit BATCH_SCHEDULED
            batch_id = f"selfplay_{self.node_id}_{int(time.time())}"
            first_config = self.selfplay_scheduler.pick_weighted_config(node)
            config_key = f"{first_config['board_type']}_{first_config['num_players']}p" if first_config else "mixed"
            await self._emit_batch_scheduled(
                batch_id=batch_id,
                batch_type="selfplay",
                config_key=config_key,
                job_count=needed,
                target_nodes=[self.node_id],
                reason="local_job_management",
            )

            jobs_dispatched = 0
            jobs_failed = 0

            # Jan 2026 fix: Detect GPU tier for proper job type selection
            # High-end GPUs (GH200, H100, H200, A100, 5090, 4090) get GPU_SELFPLAY/GUMBEL_SELFPLAY
            # Mid-tier GPUs get HYBRID_SELFPLAY (CPU rules + GPU eval)
            gpu_name_raw = getattr(node, "gpu_name", "") or ""
            gpu_name = gpu_name_raw.upper()
            has_gpu = bool(getattr(node, "has_gpu", False))

            # Session 17.50: YAML fallback when runtime GPU detection fails
            # Runtime detection can fail on some nodes (vGPU, containers, driver issues)
            if not has_gpu or not gpu_name:
                yaml_has_gpu, yaml_gpu_name, yaml_vram = self._check_yaml_gpu_config()
                if yaml_has_gpu:
                    logger.info(
                        f"LOCAL: Runtime GPU detection failed but YAML shows GPU "
                        f"({yaml_gpu_name}, {yaml_vram}GB). Using YAML config."
                    )
                    has_gpu = True
                    gpu_name_raw = yaml_gpu_name
                    gpu_name = yaml_gpu_name.upper()

            is_high_end_gpu = any(tag in gpu_name for tag in ("H100", "H200", "GH200", "A100", "5090", "4090"))
            is_apple_gpu = "MPS" in gpu_name or "APPLE" in gpu_name

            # Log GPU tier detection for visibility
            if has_gpu and is_high_end_gpu:
                logger.info(f"LOCAL: High-end GPU detected ({gpu_name_raw}) - using GPU/Gumbel selfplay")
            elif has_gpu and not is_apple_gpu:
                logger.info(f"LOCAL: Mid-tier GPU detected ({gpu_name_raw}) - using hybrid selfplay")

            import random  # Import once outside loop

            for _ in range(needed):
                try:
                    # Pick a config weighted by priority (using SelfplayScheduler manager)
                    config = self.selfplay_scheduler.pick_weighted_config(node)
                    if config:
                        # Jan 2026: Select job type based on GPU tier (consistent with cluster dispatch)
                        if has_gpu and is_high_end_gpu and not is_apple_gpu:
                            # High-end GPUs: 50% GUMBEL (quality) / 50% GPU_SELFPLAY (volume)
                            if random.random() < 0.5:
                                job_type = JobType.GUMBEL_SELFPLAY
                                engine_mode = "gumbel-mcts"
                            else:
                                job_type = JobType.GPU_SELFPLAY
                                engine_mode = "gpu"
                        elif has_gpu and not is_apple_gpu:
                            # Mid-tier GPUs: HYBRID mode for rule fidelity
                            job_type = JobType.HYBRID_SELFPLAY
                            engine_mode = "mixed"
                        else:
                            # CPU-only or Apple MPS: CPU selfplay
                            job_type = JobType.SELFPLAY
                            engine_mode = config.get("engine_mode", "gumbel-mcts")

                        job = await self._start_local_job(
                            job_type,
                            board_type=config["board_type"],
                            num_players=config["num_players"],
                            engine_mode=engine_mode,
                        )
                        if job:
                            changes += 1
                            jobs_dispatched += 1
                        else:
                            jobs_failed += 1
                except Exception as e:  # noqa: BLE001
                    logger.info(f"LOCAL: Failed to start selfplay: {e}")
                    jobs_failed += 1
                    break

            # Dec 27, 2025: Emit BATCH_DISPATCHED after loop completes
            await self._emit_batch_dispatched(
                batch_id=batch_id,
                batch_type="selfplay",
                config_key=config_key,
                jobs_dispatched=jobs_dispatched,
                jobs_failed=jobs_failed,
                target_nodes=[self.node_id],
            )

        # Stop jobs if way over target (2x or more)
        elif current_jobs > target_selfplay * 2:
            excess = current_jobs - target_selfplay
            logger.info(f"LOCAL: Reducing selfplay jobs by {excess} ({current_jobs}/{target_selfplay})")
            await self._reduce_local_selfplay_jobs(target_selfplay, reason="over_target")
            changes += excess

        if changes > 0:
            logger.info(f"LOCAL job management: {changes} change(s)")
        return changes

    async def _local_gpu_auto_scale(self) -> int:
        """DECENTRALIZED: Each GPU node manages its own GPU utilization.

        Jan 28, 2026: Phase 18A - Delegates to JobCoordinationManager.
        """
        return await self.job_coordination_manager.local_gpu_auto_scale()

    async def _local_resource_cleanup(self):
        """DECENTRALIZED: Each node handles its own resource pressure.

        Jan 27, 2026: Phase 16B - Delegates to JobCoordinationManager.
        """
        await self.job_coordination_manager.local_resource_cleanup()

    # Dec 2025: Job/selfplay methods delegated to job_manager and selfplay_scheduler

    async def _auto_scale_gpu_utilization(self) -> int:
        """Auto-scale selfplay jobs to reach 60-80% GPU utilization.

        Detects underutilized GPU nodes and starts selfplay jobs to improve
        cluster throughput while maintaining game quality and rule fidelity.

        Dec 2025 fix: Job type is selected based on GPU capabilities:
        - High-end GPUs (GH200, H100, A100, 5090, 4090): 50% GUMBEL / 50% GPU_SELFPLAY
        - Mid-tier GPUs: HYBRID mode (CPU rules + GPU eval) for rule fidelity

        Returns:
            Number of new selfplay jobs started
        """
        TARGET_GPU_MIN = 60.0  # Target minimum GPU utilization
        TARGET_GPU_MAX = 80.0  # Target maximum GPU utilization
        MIN_IDLE_TIME = 120    # Seconds of low GPU before scaling up

        started = 0
        now = time.time()

        # Rate limit auto-scaling (once per 2 minutes)
        last_scale = getattr(self, "_last_gpu_auto_scale", 0)
        if now - last_scale < 120:
            return 0

        with self.peers_lock:
            peers_snapshot = list(self.peers.values())

        underutilized_gpu_nodes = []

        # Load policy manager for filtering
        policy_manager = None
        try:
            from app.coordination.node_policies import get_policy_manager
            policy_manager = get_policy_manager()
        except ImportError:
            pass

        for peer in peers_snapshot:
            if not peer.is_alive():
                continue
            has_gpu = bool(getattr(peer, "has_gpu", False))
            if not has_gpu:
                continue

            # Policy check: skip nodes that don't allow selfplay
            if policy_manager and not policy_manager.is_work_allowed(peer.node_id, "selfplay"):
                continue

            gpu_percent = float(getattr(peer, "gpu_percent", 0) or 0)
            gpu_name = (getattr(peer, "gpu_name", "") or "").lower()
            selfplay_jobs = int(getattr(peer, "selfplay_jobs", 0) or 0)
            training_jobs = int(getattr(peer, "training_jobs", 0) or 0)

            # Skip if already training
            if training_jobs > 0:
                continue

            # Check if underutilized
            if gpu_percent < TARGET_GPU_MIN:
                # Track how long it's been underutilized
                idle_key = f"_gpu_idle_since_{peer.node_id}"
                idle_since = getattr(self, idle_key, 0)
                if idle_since == 0:
                    setattr(self, idle_key, now)
                elif now - idle_since > MIN_IDLE_TIME:
                    # Calculate how many more jobs to add
                    gpu_headroom = TARGET_GPU_MAX - gpu_percent
                    # Estimate jobs based on GPU tier
                    if any(tag in gpu_name for tag in ("h100", "h200", "gh200", "5090")):
                        jobs_per_10_percent = 2
                    elif any(tag in gpu_name for tag in ("a100", "4090", "3090")):
                        jobs_per_10_percent = 1.5
                    else:
                        jobs_per_10_percent = 1

                    new_jobs = max(1, int(gpu_headroom / 10 * jobs_per_10_percent))
                    new_jobs = min(new_jobs, 4)  # Cap at 4 new jobs per cycle

                    underutilized_gpu_nodes.append({
                        "node_id": peer.node_id,
                        "gpu_percent": gpu_percent,
                        "gpu_name": gpu_name,
                        "current_jobs": selfplay_jobs,
                        "new_jobs": new_jobs,
                    })
            else:
                # GPU is utilized, reset idle timer
                idle_key = f"_gpu_idle_since_{peer.node_id}"
                setattr(self, idle_key, 0)

        # Start GPU selfplay on underutilized nodes
        for node_info in underutilized_gpu_nodes[:3]:  # Max 3 nodes per cycle
            node_id = node_info["node_id"]
            new_jobs = node_info["new_jobs"]

            gpu_name = (node_info.get("gpu_name", "") or "").upper()
            is_high_end = any(tag in gpu_name for tag in ("H100", "H200", "GH200", "A100", "5090", "4090"))
            job_type_str = "GUMBEL/GPU" if is_high_end else "diverse/hybrid"
            print(
                f"[P2P] Auto-scale: {node_id} at {node_info['gpu_percent']:.0f}% GPU, "
                f"starting {new_jobs} {job_type_str} selfplay job(s)"
            )

            for _ in range(new_jobs):
                try:
                    # Schedule selfplay job (type selected by _schedule_diverse_selfplay_on_node)
                    job = await self._schedule_diverse_selfplay_on_node(node_id)
                    if job:
                        started += 1
                except Exception as e:  # noqa: BLE001
                    logger.error(f"Failed to start diverse selfplay on {node_id}: {e}")
                    break

        if started > 0:
            self._last_gpu_auto_scale = now
            logger.info(f"Auto-scale: started {started} new diverse/hybrid selfplay job(s)")

        return started

    async def _auto_rebalance_from_work_queue(self) -> int:
        """Auto-rebalance: assign queued work to idle GPU nodes.

        When idle GPU-heavy nodes are detected, check the work queue for pending
        high-priority work and dispatch it. This ensures queued work gets done
        before falling back to selfplay auto-scaling.

        Returns:
            Number of work items dispatched
        """
        GPU_IDLE_THRESHOLD = 10.0  # Node is idle if GPU < 10%
        MIN_IDLE_TIME = 60  # Seconds of idle before assigning work
        GPU_HEAVY_TAGS = ['gh200', 'h100', 'h200', 'a100', '4090', '5090']

        dispatched = 0
        now = time.time()

        # Rate limit rebalancing (once per minute)
        last_rebalance = getattr(self, "_last_work_queue_rebalance", 0)
        if now - last_rebalance < 60:
            return 0

        # Check if work queue is available
        wq = get_work_queue()
        if wq is None:
            return 0

        # Get queue status
        queue_status = wq.get_queue_status()
        pending_count = queue_status.get("by_status", {}).get("pending", 0)
        if pending_count == 0:
            return 0  # No work to dispatch

        # Find idle GPU-heavy nodes
        idle_nodes = []

        with self.peers_lock:
            peers_snapshot = list(self.peers.values())

        for peer in peers_snapshot:
            if not peer.is_alive() or peer.retired:
                continue

            has_gpu = bool(getattr(peer, "has_gpu", False))
            if not has_gpu:
                continue

            gpu_name = (getattr(peer, "gpu_name", "") or "").upper()
            is_gpu_heavy = any(tag.upper() in gpu_name for tag in GPU_HEAVY_TAGS)
            if not is_gpu_heavy:
                continue

            gpu_percent = float(getattr(peer, "gpu_percent", 0) or 0)
            training_jobs = int(getattr(peer, "training_jobs", 0) or 0)
            selfplay_jobs = int(getattr(peer, "selfplay_jobs", 0) or 0)

            # Skip if already busy
            if training_jobs > 0:
                continue

            # Check if truly idle
            if gpu_percent < GPU_IDLE_THRESHOLD:
                # Track how long it's been idle
                idle_key = f"_wq_idle_since_{peer.node_id}"
                idle_since = getattr(self, idle_key, 0)
                if idle_since == 0:
                    setattr(self, idle_key, now)
                elif now - idle_since > MIN_IDLE_TIME:
                    # Get allowed work types for this node
                    try:
                        from app.coordination.node_policies import get_policy_manager
                        pm = get_policy_manager()
                        allowed = list(pm.get_allowed_work_types(peer.node_id))
                    except ImportError:
                        allowed = ["training", "gpu_cmaes", "tournament", "selfplay"]

                    idle_nodes.append({
                        "node_id": peer.node_id,
                        "peer": peer,
                        "gpu_percent": gpu_percent,
                        "gpu_name": gpu_name,
                        "allowed": allowed,
                        "selfplay_jobs": selfplay_jobs,
                    })
            else:
                # Not idle, reset timer
                idle_key = f"_wq_idle_since_{peer.node_id}"
                setattr(self, idle_key, 0)

        # Dispatch work to idle nodes
        for node_info in idle_nodes[:5]:  # Max 5 nodes per cycle
            node_id = node_info["node_id"]
            allowed = node_info["allowed"]

            # Try to claim work for this node using Raft or SQLite based on consensus mode
            # claim_work_distributed() returns a dict with work_id, work_type, config, etc.
            work_item = self.claim_work_distributed(node_id, allowed)
            if work_item is None:
                continue

            # Get work_type - may be string or WorkType enum
            work_type_str = work_item.get("work_type", "unknown")
            if hasattr(work_type_str, "value"):
                work_type_str = work_type_str.value
            work_id = work_item.get("work_id", "unknown")

            print(
                f"[P2P] Work queue rebalance: {node_id} idle at {node_info['gpu_percent']:.0f}% GPU, "
                f"assigning {work_type_str} work ({work_id})"
            )

            # Dispatch work to the node
            success = await self._dispatch_queued_work(node_info["peer"], work_item)
            if success:
                # Mark work as started using distributed method for Raft consistency
                self.start_work_distributed(work_id)
                dispatched += 1
                # Reset idle timer since we assigned work
                idle_key = f"_wq_idle_since_{node_id}"
                setattr(self, idle_key, 0)
            else:
                # Failed to dispatch, reset work status for retry
                self.fail_work_distributed(work_id, "dispatch_failed")

        if dispatched > 0:
            self._last_work_queue_rebalance = now
            logger.info(f"Work queue rebalance: dispatched {dispatched} work item(s) to idle nodes")

        return dispatched

    async def _dispatch_queued_work(self, peer: NodeInfo, work_item: dict) -> bool:
        """Dispatch a work queue item to a specific node.

        Jan 27, 2026: Phase 16B - Delegates to JobCoordinationManager.
        """
        return await self.job_coordination_manager.dispatch_queued_work(peer, work_item)

    async def _schedule_diverse_selfplay_on_node(self, node_id: str) -> dict | None:
        """Schedule a diverse selfplay job on a specific node.

        Jan 27, 2026: Phase 16C - Delegates to JobCoordinationManager.
        """
        return await self.job_coordination_manager.schedule_diverse_selfplay_on_node(node_id)

    # Backward compatibility alias (GPU selfplay now redirects to diverse/hybrid)
    _schedule_gpu_selfplay_on_node = _schedule_diverse_selfplay_on_node

    # Dec 2025: Selfplay target methods delegated to selfplay_scheduler

    async def _check_cluster_balance(self) -> dict[str, Any]:
        """Check and rebalance jobs across the cluster.

        This method identifies:
        1. Powerful nodes that are underutilized (high capacity, low jobs)
        2. Weak nodes that are overloaded (low capacity, high jobs)

        When imbalance is detected, it reduces jobs on weak nodes so the
        scheduler can assign them to more powerful nodes.

        Returns dict with rebalancing actions taken.
        """
        try:
            with self.peers_lock:
                alive_peers = [p for p in self.peers.values() if p.is_alive()]

            all_nodes = [*alive_peers, self.self_info]
            healthy_nodes = [n for n in all_nodes if n.is_healthy()]

            if len(healthy_nodes) < 2:
                return {"action": "none", "reason": "insufficient_nodes"}

            # Calculate capacity and utilization for each node
            node_stats = []
            for node in healthy_nodes:
                target = self.selfplay_scheduler.get_target_jobs_for_node(node)
                current = int(getattr(node, "selfplay_jobs", 0) or 0)
                utilization = current / max(1, target)  # How full is this node
                capacity_score = target  # Higher = more powerful

                node_stats.append({
                    "node": node,
                    "target": target,
                    "current": current,
                    "utilization": utilization,
                    "capacity": capacity_score,
                    "load_score": node.get_load_score(),
                })

            # Find underutilized powerful nodes (capacity > median, utilization < 50%)
            sorted_by_capacity = sorted(node_stats, key=lambda x: x["capacity"], reverse=True)
            median_capacity = sorted_by_capacity[len(sorted_by_capacity) // 2]["capacity"]

            underutilized_powerful = [
                n for n in node_stats
                if n["capacity"] > median_capacity and n["utilization"] < 0.5
            ]

            # Find overloaded weak nodes (capacity < median, utilization > 100%)
            overloaded_weak = [
                n for n in node_stats
                if n["capacity"] < median_capacity and n["utilization"] > 1.0
            ]

            if not underutilized_powerful or not overloaded_weak:
                return {"action": "none", "reason": "balanced"}

            # Calculate rebalancing opportunity
            spare_capacity = sum(
                max(0, n["target"] - n["current"]) for n in underutilized_powerful
            )
            excess_load = sum(
                max(0, n["current"] - n["target"]) for n in overloaded_weak
            )

            if spare_capacity < 2 or excess_load < 2:
                return {"action": "none", "reason": "minimal_imbalance"}

            # Migrate: reduce jobs on weak nodes
            rebalance_actions = []
            jobs_to_migrate = min(spare_capacity, excess_load)

            for weak_node in sorted(overloaded_weak, key=lambda x: x["utilization"], reverse=True):
                if jobs_to_migrate <= 0:
                    break

                node = weak_node["node"]
                reduce_by = min(
                    weak_node["current"] - weak_node["target"],
                    jobs_to_migrate
                )
                new_target = weak_node["current"] - reduce_by

                if reduce_by > 0:
                    print(
                        f"[P2P] Cluster rebalance: {node.node_id} overloaded "
                        f"({weak_node['current']}/{weak_node['target']} jobs, "
                        f"{weak_node['utilization']*100:.0f}% util) - reducing by {reduce_by}"
                    )

                    if node.node_id == self.node_id:
                        await self._reduce_local_selfplay_jobs(new_target, reason="cluster_rebalance")
                    else:
                        await self._request_reduce_selfplay(node, new_target, reason="cluster_rebalance")

                    rebalance_actions.append({
                        "node": node.node_id,
                        "reduced_by": reduce_by,
                        "new_target": new_target,
                    })
                    jobs_to_migrate -= reduce_by

            # Record rebalancing metric
            if rebalance_actions:
                self.record_metric(
                    "cluster_rebalance",
                    len(rebalance_actions),
                    metadata={
                        "spare_capacity": spare_capacity,
                        "excess_load": excess_load,
                        "actions": rebalance_actions,
                    },
                )

            return {
                "action": "rebalanced",
                "spare_capacity": spare_capacity,
                "excess_load": excess_load,
                "actions": rebalance_actions,
            }

        except Exception as e:  # noqa: BLE001
            logger.info(f"Cluster balance check error: {e}")
            return {"action": "error", "error": str(e)}

    async def _manage_cluster_jobs(self):
        """Manage jobs across the cluster (leader only).

        LEARNED LESSONS incorporated:
        - Check disk space BEFORE starting jobs (Vast.ai 91-93% disk issue)
        - Check memory to prevent OOM (AWS instance crashed at 31GB+)
        - Trigger cleanup when approaching limits
        - Use is_healthy() not just is_alive()
        """
        logger.info("Leader: Managing cluster jobs...")

        # Track cluster management run via JobOrchestrationManager (Jan 2026)
        if hasattr(self, "job_orchestration") and self.job_orchestration:
            self.job_orchestration.record_cluster_management_run()

        # Gather cluster state
        with self.peers_lock:
            alive_peers = [p for p in self.peers.values() if p.is_alive()]

        # Add self
        self._update_self_info()
        all_nodes = [*alive_peers, self.self_info]

        # Phase 1: Handle resource warnings and cleanup
        for node in all_nodes:
            # LEARNED LESSONS - Proactive disk cleanup before hitting critical
            if node.disk_percent >= DISK_CLEANUP_THRESHOLD:
                logger.info(f"{node.node_id}: Disk at {node.disk_percent:.0f}% - triggering cleanup")
                if node.node_id == self.node_id:
                    await self._cleanup_local_disk()
                else:
                    await self._request_remote_cleanup(node)
                continue  # Skip job creation this cycle

            # Load shedding: when a node is under memory/disk pressure, ask it to
            # stop excess selfplay jobs so it can recover (prevents OOM + disk-full).
            pressure_reasons: list[str] = []
            if node.memory_percent >= MEMORY_WARNING_THRESHOLD:
                pressure_reasons.append("memory")
            if node.disk_percent >= DISK_WARNING_THRESHOLD:
                pressure_reasons.append("disk")

            if pressure_reasons:
                desired = self.selfplay_scheduler.get_target_jobs_for_node(node)
                if node.memory_percent >= MEMORY_CRITICAL_THRESHOLD or node.disk_percent >= DISK_CRITICAL_THRESHOLD:
                    desired = 0

                if node.selfplay_jobs > desired:
                    reason = "+".join(pressure_reasons)
                    print(
                        f"[P2P] {node.node_id}: Load shedding (reason={reason}) "
                        f"{node.selfplay_jobs}->{desired} selfplay jobs"
                    )
                    if node.node_id == self.node_id:
                        await self._reduce_local_selfplay_jobs(desired, reason=reason)
                    else:
                        await self._request_reduce_selfplay(node, desired, reason=reason)

        # Phase 1.5: LEARNED LESSONS - Detect stuck jobs (GPU idle with running processes)
        # This addresses the vast-5090-quad issue where 582 processes ran at 0% GPU
        for node in all_nodes:
            if not node.has_gpu or node.selfplay_jobs <= 0:
                # No GPU or no jobs running - not stuck
                if node.node_id in self.gpu_idle_since:
                    del self.gpu_idle_since[node.node_id]
                continue

            # Check if GPU is idle (< threshold) with jobs running
            gpu_name = (node.gpu_name or "").upper()
            is_cuda_gpu = "MPS" not in gpu_name and "APPLE" not in gpu_name
            if not is_cuda_gpu:
                continue  # Skip Apple Silicon, doesn't have nvidia-smi

            if node.gpu_percent < GPU_IDLE_THRESHOLD:
                # GPU idle with jobs running - track or take action
                if node.node_id not in self.gpu_idle_since:
                    self.gpu_idle_since[node.node_id] = time.time()
                    logger.info(f"{node.node_id}: GPU idle ({node.gpu_percent:.0f}%) with {node.selfplay_jobs} jobs - monitoring")
                else:
                    idle_duration = time.time() - self.gpu_idle_since[node.node_id]
                    if idle_duration >= GPU_IDLE_RESTART_TIMEOUT:
                        logger.info(f"{node.node_id}: STUCK! GPU idle for {idle_duration:.0f}s with {node.selfplay_jobs} jobs")
                        logger.info(f"{node.node_id}: Requesting job restart...")
                        if node.node_id == self.node_id:
                            await self._restart_local_stuck_jobs()
                        else:
                            await self._request_job_restart(node)
                        del self.gpu_idle_since[node.node_id]
            else:
                # GPU is working - clear idle tracking
                if node.node_id in self.gpu_idle_since:
                    del self.gpu_idle_since[node.node_id]

        # Phase 1.6: Detect runaway selfplay processes (lost tracking / manual runs).
        # If a node reports an absurd number of selfplay processes, request a
        # restart sweep to kill untracked jobs and recover capacity.
        for node in all_nodes:
            try:
                target_selfplay = self.selfplay_scheduler.get_target_jobs_for_node(node)
                dynamic_threshold = max(16, target_selfplay * 3)
                runaway_threshold = (
                    int(RUNAWAY_SELFPLAY_PROCESS_THRESHOLD)
                    if int(RUNAWAY_SELFPLAY_PROCESS_THRESHOLD) > 0
                    else int(dynamic_threshold)
                )
                if int(getattr(node, "selfplay_jobs", 0) or 0) < runaway_threshold:
                    continue
            except (ValueError, AttributeError):
                continue

            print(
                f"[P2P] {node.node_id}: RUNAWAY selfplay count ({node.selfplay_jobs}) "
                f">= {runaway_threshold}  requesting restart sweep"
            )
            if node.node_id == self.node_id:
                await self._restart_local_stuck_jobs()
            else:
                await self._request_job_restart(node)

        # Phase 2: Calculate desired job distribution for healthy nodes
        # LEARNED LESSONS - Sort nodes by load score for load balancing
        # Least-loaded nodes get jobs first to ensure even distribution
        healthy_nodes = [n for n in all_nodes if n.is_healthy()]
        healthy_nodes.sort(key=lambda n: n.get_load_score())

        if healthy_nodes:
            load_summary = ", ".join(
                f"{n.node_id[:12]}={n.get_load_score():.0f}%"
                for n in healthy_nodes[:5]
            )
            logger.info(f"Load balancing: {load_summary}")

        for node in healthy_nodes:
            load_score = node.get_load_score()
            if load_score >= LOAD_MAX_FOR_NEW_JOBS:
                logger.info(f"{node.node_id}: Load {load_score:.0f}% - skipping new job starts")
                continue

            # LEARNED LESSONS - Reduce target when approaching limits
            # Base targets:
            # - GPU nodes: fixed concurrency tuned for GPU throughput.
            # - CPU-only nodes: scale with CPU cores (and cap by memory).
            # HYBRID MODE: Get separate GPU and CPU-only job targets (delegated)
            hybrid_targets = self.selfplay_scheduler.get_hybrid_job_targets(node)
            gpu_job_target = hybrid_targets.get("gpu_jobs", 0)
            cpu_only_target = hybrid_targets.get("cpu_only_jobs", 0)
            total_target = hybrid_targets.get("total_jobs", gpu_job_target)

            # Backward compat: use total_target like the old target_selfplay
            target_selfplay = total_target

            # Check if node needs more jobs
            if node.selfplay_jobs < target_selfplay:
                needed = target_selfplay - node.selfplay_jobs
                logger.info(f"{node.node_id} needs {needed} more selfplay jobs")

                # January 2026: Selfplay configs extracted to scripts/p2p/config/selfplay_job_configs.py
                # Use imported helper functions for filtering and weighting
                node_mem = int(getattr(node, "memory_gb", 0) or 0)
                filtered_configs = get_filtered_configs(node_memory_gb=node_mem)
                weighted_configs = get_weighted_configs(filtered_configs)
                unique_configs = get_unique_configs(filtered_configs)

                # FIXED: Start all needed jobs with fair config distribution
                # Instead of max 2, start up to 10 at a time to quickly fill all configs
                jobs_to_start = min(needed, 10)  # Start up to 10 jobs per iteration

                # HYBRID MODE: Calculate how many GPU vs CPU-only jobs to spawn
                # If node already has gpu_job_target GPU jobs, spawn CPU-only jobs instead
                current_gpu_jobs = min(node.selfplay_jobs, gpu_job_target)
                remaining_gpu_slots = max(0, gpu_job_target - current_gpu_jobs)
                remaining_cpu_slots = max(0, cpu_only_target)  # Can always spawn CPU-only if capacity
                should_use_cpu_only = self.selfplay_scheduler.should_spawn_cpu_only_jobs(node) and cpu_only_target > 0

                for i in range(jobs_to_start):
                    # LEARNED LESSONS - Smart CPU/GPU task routing:
                    # - High-end GPUs (H100/H200/A100/5090/4090) get GPU_SELFPLAY for max throughput
                    #   with automatic CPU validation to ensure data quality
                    # - Mid-tier GPUs get HYBRID_SELFPLAY (CPU rules + GPU eval)
                    # - CPU-only nodes or GPU-saturated nodes get CPU_SELFPLAY
                    # This ensures expensive GPU resources are utilized properly
                    # while CPU instances handle CPU-bound tasks efficiently
                    gpu_name = (node.gpu_name or "").upper()
                    node_has_gpu = bool(node.has_gpu)

                    # Session 17.50: YAML fallback when runtime GPU detection fails
                    if not node_has_gpu or not gpu_name:
                        yaml_has_gpu, yaml_gpu_name, _ = self._check_yaml_gpu_config(node.node_id)
                        if yaml_has_gpu:
                            node_has_gpu = True
                            if yaml_gpu_name:
                                gpu_name = yaml_gpu_name.upper()

                    is_high_end_gpu = any(tag in gpu_name for tag in ("H100", "H200", "GH200", "A100", "5090", "4090"))
                    is_apple_gpu = "MPS" in gpu_name or "APPLE" in gpu_name

                    # GPU utilization check: if GPU is at 0% with jobs running, those jobs
                    # are probably CPU-only. This is an opportunity to START GPU work, not avoid it.
                    # Only consider GPU "unavailable" if there are existing GPU jobs AND utilization is 0%
                    # (which would indicate driver/container issues)
                    # Jan 7, 2026: Fixed to use new gpu_job_count field (gpu_selfplay_jobs never existed)
                    gpu_percent = getattr(node, "gpu_percent", 0) or 0
                    gpu_job_count = getattr(node, "gpu_job_count", 0) or 0
                    gpu_failure_count = getattr(node, "gpu_failure_count", 0) or 0
                    last_gpu_failure = getattr(node, "last_gpu_job_failure", 0) or 0
                    gpu_seems_unavailable = (
                        node_has_gpu
                        and not is_apple_gpu
                        and (
                            # Driver issue: GPU jobs running but 0% utilization
                            (gpu_job_count >= 2 and gpu_percent < 1)
                            # Recent consecutive failures: 3+ failures in last 5 minutes
                            or (gpu_failure_count >= 3 and time.time() - last_gpu_failure < 300)
                        )
                    )
                    if gpu_seems_unavailable:
                        reason = "driver issue" if (gpu_job_count >= 2 and gpu_percent < 1) else f"{gpu_failure_count} recent failures"
                        logger.info(f"WARNING: {node.node_id} has GPU but appears unavailable ({reason})")
                    elif node_has_gpu and gpu_percent < 10 and node.selfplay_jobs > 0:
                        # GPU idle but has CPU jobs - this is normal, will prioritize GPU work
                        logger.debug(f"Node {node.node_id} has {node.selfplay_jobs} CPU jobs but GPU idle ({gpu_percent:.0f}%) - will add GPU work")

                    # Jan 7, 2026: Role-based job preference to ensure nodes run appropriate job types
                    job_preference = self._get_node_job_preference(node.node_id)

                    # Role enforcement: respect node's configured role
                    if job_preference == "training_only":
                        logger.debug(f"Skipping {node.node_id} for selfplay (training_only role)")
                        continue
                    elif job_preference == "cpu_only":
                        # Coordinator/CPU nodes: force CPU-only selfplay
                        spawn_cpu_only = True
                        if remaining_cpu_slots > 0:
                            remaining_cpu_slots -= 1
                        else:
                            logger.debug(f"Skipping {node.node_id} - cpu_only role but no CPU slots")
                            continue
                    elif job_preference == "gpu_only" and node_has_gpu and not gpu_seems_unavailable:
                        # GPU-only nodes: must use GPU, skip if no slots
                        if remaining_gpu_slots > 0:
                            remaining_gpu_slots -= 1
                            spawn_cpu_only = False
                        else:
                            logger.debug(f"Skipping {node.node_id} - gpu_only role but no GPU slots")
                            continue
                    else:
                        # "both" role: use original hybrid logic
                        # HYBRID MODE: Decide between GPU and CPU-only based on capacity
                        spawn_cpu_only = False
                        if remaining_gpu_slots > 0 and not gpu_seems_unavailable:
                            remaining_gpu_slots -= 1
                        elif should_use_cpu_only and remaining_cpu_slots > 0:
                            spawn_cpu_only = True
                            remaining_cpu_slots -= 1
                        elif gpu_seems_unavailable:
                            spawn_cpu_only = True

                    if spawn_cpu_only:
                        # Pure CPU selfplay to utilize excess CPU capacity
                        job_type = JobType.CPU_SELFPLAY
                        task_type_str = "CPU-only (hybrid mode)"
                    elif node.has_gpu and is_high_end_gpu and not is_apple_gpu and not gpu_seems_unavailable:
                        # High-end CUDA GPUs: Mix of GPU_SELFPLAY (volume) and GUMBEL_SELFPLAY (quality)
                        # GPU selfplay now has high parity with CPU rules (2025-12 upgrade)
                        # Use Gumbel MCTS ~50% of time for high-quality training data (self-improvement loop)
                        # Increased from 20% to close the training loop - AlphaZero needs NN+MCTS data
                        import random
                        if random.random() < 0.5:  # 50% chance for Gumbel MCTS (quality) - was 20%
                            job_type = JobType.GUMBEL_SELFPLAY
                            task_type_str = "GUMBEL (high-quality)"
                        else:  # 50% for GPU selfplay (volume)
                            job_type = JobType.GPU_SELFPLAY
                            task_type_str = "GPU (high-parity)"
                    elif node.has_gpu and not is_apple_gpu and not gpu_seems_unavailable:
                        # Mid-tier GPUs: Use hybrid (CPU rules + GPU eval)
                        job_type = JobType.HYBRID_SELFPLAY
                        task_type_str = "HYBRID (accel)"
                    else:
                        job_type = JobType.SELFPLAY
                        task_type_str = "CPU-only"

                    gpu_info = f"gpu={node.gpu_name or 'none'}, gpu%={getattr(node, 'gpu_percent', 0):.0f}" if node.has_gpu else "no-gpu"
                    logger.info(f"Assigning {task_type_str} task to {node.node_id} ({gpu_info}, load={node.get_load_score():.0f}%)")

                    # FIXED: Round-robin config selection to ensure all configs get coverage
                    # Use unique_configs list for fair distribution across all 9 board/player combos
                    if self.improvement_cycle_manager and hasattr(self.improvement_cycle_manager, 'get_next_selfplay_config_for_node'):
                        # Node-aware dynamic selection: routes hex/sq19/3p/4p to powerful nodes
                        node_gpu_power = node.gpu_power_score() if hasattr(node, 'gpu_power_score') else 0
                        node_memory = int(getattr(node, 'memory_gb', 0) or 0)
                        config = self.improvement_cycle_manager.get_next_selfplay_config_for_node(
                            node_gpu_power=node_gpu_power,
                            node_memory_gb=node_memory,
                            cluster_data=self.cluster_data_manifest
                        )
                    else:
                        # Round-robin across unique configs for fair coverage
                        # Each iteration picks the next config in the list
                        config_idx = i % len(unique_configs)
                        config = unique_configs[config_idx]

                    # Track diversity metrics for monitoring (delegated to SelfplayScheduler)
                    self.selfplay_scheduler.track_diversity(config)

                    if node.node_id == self.node_id:
                        await self._start_local_job(
                            job_type,
                            board_type=config["board_type"],
                            num_players=config["num_players"],
                            engine_mode=config["engine_mode"],
                        )
                    else:
                        await self._request_remote_job(
                            node, job_type,
                            board_type=config["board_type"],
                            num_players=config["num_players"],
                            engine_mode=config["engine_mode"],
                        )

    def _emergency_memory_cleanup(self) -> None:
        """Emergency memory cleanup when memory is critical.

        Jan 2026: Delegated to MemoryDiskManager (Phase 10 decomposition).
        """
        self.memory_disk_manager.emergency_memory_cleanup()

    async def _cleanup_local_disk(self):
        """Clean up disk space on local node.

        Jan 2026: Delegated to MemoryDiskManager (Phase 10 decomposition).
        """
        await self.memory_disk_manager.cleanup_local_disk()

    async def _request_remote_cleanup(self, node: NodeInfo):
        """Request a remote node to clean up disk space.

        Jan 2026: Delegated to MemoryDiskManager (Phase 10 decomposition).
        """
        await self.memory_disk_manager.request_remote_cleanup_via_orchestrator(node)

    async def _reduce_local_selfplay_jobs(self, target_selfplay_jobs: int, *, reason: str) -> dict[str, Any]:
        """Best-effort: stop excess selfplay jobs on this node (load shedding).

        Jan 2026: Delegated to MemoryDiskManager (Phase 10 decomposition).
        """
        return await self.memory_disk_manager.reduce_local_selfplay_jobs(target_selfplay_jobs, reason=reason)

    async def _request_reduce_selfplay(self, node: NodeInfo, target_selfplay_jobs: int, *, reason: str) -> None:
        """Ask a node to shed excess selfplay (used for memory/disk pressure).

        Jan 2026: Delegated to MemoryDiskManager (Phase 10 decomposition).
        """
        await self.memory_disk_manager.request_reduce_selfplay(node, target_selfplay_jobs, reason=reason)

    async def _restart_local_stuck_jobs(self):
        """Kill stuck selfplay processes and let job management restart them.

        LEARNED LESSONS - Addresses the issue where processes accumulate but GPU stays at 0%.
        """
        logger.info("Restarting stuck local selfplay jobs...")
        try:
            # Kill tracked selfplay jobs (avoid broad pkill patterns).
            jobs_to_clear: list[str] = []
            pids_to_kill: set[int] = set()
            with self.jobs_lock:
                for job_id, job in self.local_jobs.items():
                    if job.job_type not in (JobType.SELFPLAY, JobType.GPU_SELFPLAY, JobType.HYBRID_SELFPLAY, JobType.CPU_SELFPLAY, JobType.GUMBEL_SELFPLAY):
                        continue
                    jobs_to_clear.append(job_id)
                    if job.pid:
                        try:
                            pids_to_kill.add(int(job.pid))
                        except (ValueError, AttributeError):
                            continue

            # Sweep for untracked selfplay processes (e.g. lost local_jobs state) and kill them too.
            try:
                import shutil

                if shutil.which("pgrep"):
                    # December 2025: Added selfplay.py - unified entry point
                    for pattern in (
                        "selfplay.py",
                        "run_self_play_soak.py",
                        "run_gpu_selfplay.py",
                        "run_hybrid_selfplay.py",
                        "run_random_selfplay.py",
                    ):
                        out = subprocess.run(
                            ["pgrep", "-f", pattern],
                            capture_output=True,
                            text=True,
                            timeout=5,
                        )
                        if out.returncode == 0 and out.stdout.strip():
                            for token in out.stdout.strip().split():
                                try:
                                    pids_to_kill.add(int(token))
                                except (ValueError, AttributeError):
                                    continue
            except (ValueError, AttributeError):
                pass

            pids_to_kill.discard(int(os.getpid()))

            killed = 0
            for pid in sorted(pids_to_kill):
                try:
                    os.kill(pid, signal.SIGKILL)
                    killed += 1
                except (AttributeError):
                    continue

            # Clear our job tracking - they'll be restarted next cycle.
            with self.jobs_lock:
                for job_id in jobs_to_clear:
                    self.local_jobs.pop(job_id, None)

            logger.info(f"Killed {killed} processes, cleared {len(jobs_to_clear)} job records")
        except Exception as e:  # noqa: BLE001
            logger.error(f"killing stuck processes: {e}")

    async def _request_job_restart(self, node: NodeInfo):
        """Request a remote node to restart its stuck selfplay jobs."""
        try:
            if getattr(node, "nat_blocked", False):
                cmd_id = await self._enqueue_relay_command_for_peer(node, "restart_stuck_jobs", {})
                if cmd_id:
                    logger.info(f"Enqueued relay restart_stuck_jobs for {node.node_id}")
                else:
                    logger.info(f"Relay queue full for {node.node_id}; skipping restart enqueue")
                return
            timeout = ClientTimeout(total=HTTP_TOTAL_TIMEOUT)
            async with get_client_session(timeout) as session:
                last_err: str | None = None
                for url in self._urls_for_peer(node, "/restart_stuck_jobs"):
                    try:
                        async with session.post(url, json={}, headers=self._auth_headers()) as resp:
                            if resp.status != 200:
                                last_err = f"http_{resp.status}"
                                continue
                            data = await resp.json()
                            if data.get("success"):
                                logger.info(f"Job restart requested on {node.node_id}")
                                return
                            last_err = str(data.get("error") or "restart_failed")
                    except Exception as e:  # noqa: BLE001
                        last_err = str(e)
                        continue
                if last_err:
                    logger.info(f"Job restart request failed on {node.node_id}: {last_err}")
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to request job restart from {node.node_id}: {e}")

    async def _start_local_job(
        self,
        job_type: JobType,
        board_type: str = "square8",
        num_players: int = 2,
        engine_mode: str = "gumbel-mcts",  # GPU-accelerated Gumbel MCTS
        job_id: str | None = None,
        cuda_visible_devices: str | None = None,
        export_params: dict[str, Any] | None = None,
        simulation_budget: int | None = None,  # Gumbel MCTS budget (None = use tier default)
    ) -> ClusterJob | None:
        """Start a job on the local node.

        SAFEGUARD: Checks coordination safeguards before spawning.
        """
        try:
            # SAFEGUARD: Check safeguards before spawning
            if HAS_SAFEGUARDS and _safeguards:
                task_type_str = job_type.value if hasattr(job_type, 'value') else str(job_type)
                allowed, reason = check_before_spawn(task_type_str, self.node_id)
                if not allowed:
                    logger.info(f"SAFEGUARD blocked {task_type_str} on {self.node_id}: {reason}")
                    # Track blocked spawn via JobOrchestrationManager
                    if hasattr(self, "job_orchestration") and self.job_orchestration:
                        self.job_orchestration.record_spawn_blocked(f"safeguard:{reason}")
                    return None

                # Apply backpressure delay
                delay = _safeguards.get_delay()
                if delay > 0:
                    logger.info(f"SAFEGUARD applying {delay:.1f}s backpressure delay")
                    await asyncio.sleep(delay)

            if job_id:
                job_id = str(job_id)
                with self.jobs_lock:
                    existing = self.local_jobs.get(job_id)
                if existing and existing.status == "running":
                    return existing
            else:
                job_id = str(uuid.uuid4())[:8]

            if job_type == JobType.SELFPLAY:
                # Normalize engine_mode to what run_self_play_soak.py supports.
                # LEARNED LESSONS - Variety of AI methods for better training:
                # - nn-only: Uses NNUE/neural network evaluation
                # January 2026: Engine modes defined in scripts/p2p/job_spawner.py
                # - SELFPLAY_ENGINE_MODES: All supported modes
                # - GUMBEL_ENGINE_MODES: Gumbel MCTS aliases

                # Normalize engine mode - map aliases to what run_self_play_soak.py expects
                if engine_mode in GUMBEL_ENGINE_MODES:
                    engine_mode_norm = "gumbel-mcts-only"  # Actual mode name in run_self_play_soak.py
                elif engine_mode in SELFPLAY_ENGINE_MODES:
                    engine_mode_norm = engine_mode
                else:
                    engine_mode_norm = "nn-only"

                # Memory-safety defaults for large boards.
                num_games = 1000
                extra_args: list[str] = []
                if board_type in ("square19", "hexagonal"):
                    num_games = 200 if board_type == "square19" else 100
                    extra_args.extend(["--memory-constrained"])

                output_dir = Path(
                    self.ringrift_path,
                    "ai-service",
                    "data",
                    "selfplay",
                    "p2p",
                    f"{board_type}_{num_players}p",
                    job_id,
                )
                output_dir.mkdir(parents=True, exist_ok=True)

                # Use venv python if available, otherwise fall back to system python3
                venv_python = Path(self._get_ai_service_path(), "venv", "bin", "python")
                python_exec = str(venv_python) if venv_python.exists() else "python3"

                cmd = [
                    python_exec,
                    self._get_script_path("run_self_play_soak.py"),
                    "--num-games", str(num_games),
                    "--board-type", board_type,
                    "--num-players", str(num_players),
                    "--engine-mode", engine_mode_norm,
                    "--max-moves", "10000",  # LEARNED LESSONS - Avoid draws due to move limit
                    "--log-jsonl", str(output_dir / "games.jsonl"),
                    "--summary-json", str(output_dir / "summary.json"),
                    "--record-db", str(output_dir / "games.db"),
                    "--lean-db",
                    "--verbose", "0",
                    *extra_args,
                ]

                # January 2026: Use _spawn_and_track_job helper to reduce duplication
                result = self._spawn_and_track_job(
                    job_id=job_id,
                    job_type=job_type,
                    board_type=board_type,
                    num_players=num_players,
                    engine_mode=engine_mode_norm,
                    cmd=cmd,
                    output_dir=output_dir,
                    safeguard_reason=f"selfplay-{board_type}-{num_players}p",
                )
                if result is None:
                    return None

                job, proc = result

                # Dec 31, 2025: Add process monitoring to track completion/failure
                asyncio.create_task(self._monitor_selfplay_process(
                    job_id, proc, output_dir, board_type, num_players, "selfplay"
                ))

                return job

            elif job_type == JobType.CPU_SELFPLAY:
                # Pure CPU selfplay for high-CPU nodes with limited GPU VRAM
                # Uses CPU-efficient engine modes
                # This enables utilizing excess CPU capacity on Vast.ai hosts etc.

                # CPU-friendly engine modes - include descent, mcts, and nn-only (work on CPU)
                # run_self_play_soak.py supports all these modes
                cpu_engine_modes = {
                    "descent-only", "minimax-only", "mcts-only", "heuristic-only",
                    "random-only", "mixed", "nn-only", "best-vs-pool",
                    # Cross-AI asymmetric matches
                    "nn-vs-mcts", "nn-vs-minimax", "nn-vs-descent", "tournament-varied",
                    "heuristic-vs-nn", "heuristic-vs-mcts", "random-vs-mcts",
                }
                engine_mode_norm = engine_mode if engine_mode in cpu_engine_modes else "nn-only"

                # CPU-only jobs can handle more games per batch
                num_games = 2000
                extra_args: list[str] = []
                if board_type in ("square19", "hexagonal"):
                    num_games = 400 if board_type == "square19" else 200
                    extra_args.extend(["--memory-constrained"])

                output_dir = Path(
                    self.ringrift_path,
                    "ai-service",
                    "data",
                    "selfplay",
                    "p2p",
                    f"{board_type}_{num_players}p_cpu",  # Separate subdir for CPU-only
                    job_id,
                )
                output_dir.mkdir(parents=True, exist_ok=True)

                venv_python = Path(self._get_ai_service_path(), "venv", "bin", "python")
                python_exec = str(venv_python) if venv_python.exists() else "python3"

                cmd = [
                    python_exec,
                    self._get_script_path("run_self_play_soak.py"),
                    "--num-games", str(num_games),
                    "--board-type", board_type,
                    "--num-players", str(num_players),
                    "--engine-mode", engine_mode_norm,
                    "--max-moves", "10000",
                    "--log-jsonl", str(output_dir / "games.jsonl"),
                    "--summary-json", str(output_dir / "summary.json"),
                    "--record-db", str(output_dir / "games.db"),
                    "--lean-db",
                    "--verbose", "0",
                    *extra_args,
                ]

                # January 2026: Use _spawn_and_track_job helper to reduce duplication
                result = self._spawn_and_track_job(
                    job_id=job_id,
                    job_type=job_type,
                    board_type=board_type,
                    num_players=num_players,
                    engine_mode=engine_mode_norm,
                    cmd=cmd,
                    output_dir=output_dir,
                    cuda_visible_devices="",  # Disable GPU for CPU-only jobs
                    safeguard_reason=f"cpu-selfplay-{board_type}-{num_players}p",
                )
                if result is None:
                    return None

                job, proc = result

                # Dec 31, 2025: Add process monitoring to track completion/failure
                asyncio.create_task(self._monitor_selfplay_process(
                    job_id, proc, output_dir, board_type, num_players, "cpu_selfplay"
                ))

                return job

            elif job_type == JobType.GPU_SELFPLAY:
                # GPU selfplay using run_gpu_selfplay.py for high-quality training data
                # Uses vectorized GPU game simulation with heuristic-guided move selection

                # Normalize board type for CLI
                board_arg = {
                    "hex8": "hex8",
                    "hex": "hex8",
                    "square8": "square8",
                    "square19": "square19",
                    "hexagonal": "hexagonal",
                }.get(board_type, "square8")

                # Number of games per batch
                num_games = 100
                if board_arg == "square19":
                    num_games = 50  # Square19 games are longer
                elif board_arg == "hexagonal":
                    num_games = 100

                output_dir = Path(
                    self.ringrift_path,
                    "ai-service",
                    "data",
                    "games",
                )

                # Use venv python if available, otherwise fall back to system python3
                venv_python = Path(self._get_ai_service_path(), "venv", "bin", "python")
                python_exec = str(venv_python) if venv_python.exists() else "python3"

                cmd = [
                    python_exec,
                    self._get_script_path("run_gpu_selfplay.py"),
                    "--board", board_arg,
                    "--num-players", str(num_players),
                    "--num-games", str(num_games),
                    "--output-dir", str(output_dir),
                ]

                # GPU selection: use explicit setting, auto-select, or default to 0
                effective_cuda_devices = cuda_visible_devices
                if effective_cuda_devices is None or not str(effective_cuda_devices).strip():
                    gpu_count = 0
                    try:
                        out = subprocess.run(
                            ["nvidia-smi", "--query-gpu=name", "--format=csv,noheader"],
                            capture_output=True, text=True, timeout=5
                        )
                        if out.returncode == 0 and out.stdout.strip():
                            gpu_count = len([line for line in out.stdout.splitlines() if line.strip()])
                    except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError, KeyError, IndexError, AttributeError):
                        gpu_count = 0

                    if gpu_count > 0:
                        with self.jobs_lock:
                            running_gpu_jobs = sum(
                                1 for j in self.local_jobs.values()
                                if j.job_type == JobType.GPU_SELFPLAY and j.status == "running"
                            )
                        effective_cuda_devices = str(running_gpu_jobs % gpu_count)
                    else:
                        effective_cuda_devices = "0"

                # January 2026: Use _spawn_and_track_job helper to reduce duplication
                gpu_engine_mode = "gumbel-mcts"  # 177x speedup with GPU tree
                result = self._spawn_and_track_job(
                    job_id=job_id,
                    job_type=job_type,
                    board_type=board_type,
                    num_players=num_players,
                    engine_mode=gpu_engine_mode,
                    cmd=cmd,
                    output_dir=output_dir,
                    log_filename="gpu_run.log",
                    cuda_visible_devices=effective_cuda_devices,
                    safeguard_reason=f"gpu-selfplay-{board_type}-{num_players}p",
                )
                if result is None:
                    return None

                job, proc = result

                # Jan 7, 2026: Track GPU job count for adaptive dispatch decisions
                self._update_gpu_job_count(+1)

                # Track diversity metrics for monitoring (Phase 2B, Dec 2025)
                self.selfplay_scheduler.track_diversity({
                    "board_type": board_type,
                    "num_players": num_players,
                    "engine_mode": gpu_engine_mode,
                })

                # Monitor GPU selfplay and trigger CPU validation when complete
                asyncio.create_task(self._monitor_gpu_selfplay_and_validate(
                    job_id, proc, output_dir, board_type, num_players
                ))

                return job

            elif job_type == JobType.HYBRID_SELFPLAY:
                # Hybrid CPU/GPU selfplay using run_self_play_soak.py
                # Uses CPU for game rules (100% canonical) but GPU for heuristic evaluation
                # This is the recommended default for GPU nodes
                # NOTE: run_hybrid_selfplay.py doesn't exist, use run_self_play_soak.py instead

                # Normalize engine_mode
                # run_self_play_soak.py supports: random-only, heuristic-only, mixed, nnue-guided, mcts,
                # gumbel-mcts-only, maxn-only, brs-only, policy-only, diverse, diverse-cpu
                # Map profile engine modes to soak script equivalents
                hybrid_engine_modes = {"random-only", "heuristic-only", "mixed", "nnue-guided", "mcts",
                                       "gumbel-mcts-only", "maxn-only", "brs-only", "policy-only", "diverse"}
                nn_modes = {"nn-only", "best-vs-pool", "nn-vs-mcts", "nn-vs-minimax", "nn-vs-descent", "tournament-varied"}
                # Map DIVERSE_PROFILES engine_mode values to run_self_play_soak.py equivalents
                engine_mode_map = {
                    "gumbel-mcts": "gumbel-mcts-only",  # High-quality Gumbel MCTS
                    "maxn": "maxn-only",                # Max-N for multiplayer
                    "brs": "brs-only",                  # Best-Reply Search
                    "minimax": "minimax-only",          # Minimax search
                }
                if engine_mode in hybrid_engine_modes:
                    engine_mode_norm = engine_mode
                elif engine_mode in engine_mode_map:
                    engine_mode_norm = engine_mode_map[engine_mode]  # Map to soak script name
                elif engine_mode in nn_modes:
                    engine_mode_norm = "nnue-guided"  # Use neural network
                elif engine_mode in ("mcts-only", "descent-only"):
                    engine_mode_norm = "mcts"  # Use MCTS
                elif engine_mode == "minimax-only":
                    engine_mode_norm = "minimax-only"  # Minimax search
                else:
                    # Default to diverse mode for GPU nodes (uses all AI types)
                    engine_mode_norm = "diverse"

                # Game counts based on board type
                num_games = 1000
                if board_type == "square19":
                    num_games = 500
                elif board_type in ("hex", "hexagonal"):
                    num_games = 300

                # Jan 7, 2026: Use _get_ai_service_path() to avoid doubled ai-service/ path
                output_dir = Path(
                    self._get_ai_service_path(),
                    "data",
                    "selfplay",
                    "p2p_hybrid",
                    f"{board_type}_{num_players}p",
                    job_id,
                )
                output_dir.mkdir(parents=True, exist_ok=True)

                # Board type is passed directly - run_self_play_soak.py accepts:
                # 'hex8', 'hexagonal', 'square8', 'square19'
                board_arg = board_type

                # Use venv python if available, otherwise fall back to system python3
                venv_python = Path(self._get_ai_service_path(), "venv", "bin", "python")
                python_exec = str(venv_python) if venv_python.exists() else "python3"

                cmd = [
                    python_exec,
                    self._get_script_path("run_self_play_soak.py"),
                    "--board-type", board_arg,
                    "--num-players", str(num_players),
                    "--num-games", str(num_games),
                    "--log-jsonl", str(output_dir / "games.jsonl"),
                    "--summary-json", str(output_dir / "summary.json"),
                    "--record-db", str(output_dir / "games.db"),
                    "--lean-db",
                    "--engine-mode", engine_mode_norm,
                    "--max-moves", "10000",
                    "--verbose", "0",
                ]

                # GPU selection: use explicit setting, auto-select, or default to 0
                effective_cuda_devices = cuda_visible_devices
                if effective_cuda_devices is None or not str(effective_cuda_devices).strip():
                    gpu_count = 0
                    try:
                        out = subprocess.run(
                            ["nvidia-smi", "--query-gpu=name", "--format=csv,noheader"],
                            capture_output=True, text=True, timeout=5,
                        )
                        if out.returncode == 0 and out.stdout.strip():
                            gpu_count = len([line for line in out.stdout.splitlines() if line.strip()])
                    except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError, KeyError, IndexError, AttributeError):
                        gpu_count = 0

                    if gpu_count > 0:
                        with self.jobs_lock:
                            running_hybrid_jobs = sum(
                                1 for j in self.local_jobs.values()
                                if j.job_type == JobType.HYBRID_SELFPLAY and j.status == "running"
                            )
                        effective_cuda_devices = str(running_hybrid_jobs % gpu_count)
                    else:
                        effective_cuda_devices = "0"

                # January 2026: Use _spawn_and_track_job helper to reduce duplication
                result = self._spawn_and_track_job(
                    job_id=job_id,
                    job_type=job_type,
                    board_type=board_type,
                    num_players=num_players,
                    engine_mode=engine_mode_norm,
                    cmd=cmd,
                    output_dir=output_dir,
                    log_filename="hybrid_run.log",
                    cuda_visible_devices=effective_cuda_devices,
                    safeguard_reason=f"hybrid-selfplay-{board_type}-{num_players}p",
                )
                if result is None:
                    return None

                job, proc = result

                # Track diversity metrics for monitoring (Phase 2B, Dec 2025)
                self.selfplay_scheduler.track_diversity({
                    "board_type": board_type,
                    "num_players": num_players,
                    "engine_mode": engine_mode_norm,
                })

                # Dec 31, 2025: Add process monitoring to track completion/failure
                asyncio.create_task(self._monitor_selfplay_process(
                    job_id, proc, output_dir, board_type, num_players, "hybrid_selfplay"
                ))

                return job

            elif job_type == JobType.GUMBEL_SELFPLAY:
                # High-quality Gumbel MCTS selfplay with NN policy for self-improvement training
                # Uses generate_gumbel_selfplay.py with proper MCTS simulation budget
                # Budget tiers: THROUGHPUT(64), BOOTSTRAP(150), QUALITY(800), ULTIMATE(1600), MASTER(3200)
                #
                # Jan 2026: Use adaptive budget based on config Elo instead of hardcoded 150.
                # This ensures mature configs (Elo > 1400) use quality budgets for better training data.
                if simulation_budget is not None:
                    effective_budget = simulation_budget
                else:
                    # Look up config Elo and use adaptive budget
                    config_key = f"{board_type}_{num_players}p"
                    try:
                        config_elo = self.selfplay_scheduler.get_config_elo(config_key) if hasattr(self, 'selfplay_scheduler') else 1200.0
                        effective_budget = get_adaptive_budget_for_elo(config_elo)
                        logger.debug(f"[Gumbel] {config_key}: Elo={config_elo:.0f} -> budget={effective_budget}")
                    except Exception:
                        effective_budget = 150  # Fallback to bootstrap tier

                # Games based on board type and budget
                # Lower budget = can run more games in same time
                num_games = 50 if effective_budget >= 800 else 100  # More games for lower budget
                if board_type == "square19":
                    num_games = 10 if effective_budget >= 800 else 50  # Large board
                elif board_type in ("hex", "hexagonal", "hex8"):
                    num_games = 20 if effective_budget >= 800 else 100

                output_dir = Path(
                    self.ringrift_path,
                    "ai-service",
                    "data",
                    "selfplay",
                    "gumbel",
                    f"{board_type}_{num_players}p",
                    job_id,
                )
                output_dir.mkdir(parents=True, exist_ok=True)

                # Normalize board type for gumbel script
                board_arg = {
                    "hex": "hexagonal",
                    "hex8": "hex8",
                }.get(board_type, board_type)

                # Use venv python if available
                venv_python = Path(self._get_ai_service_path(), "venv", "bin", "python")
                python_exec = str(venv_python) if venv_python.exists() else "python3"

                cmd = [
                    python_exec,
                    self._get_script_path("generate_gumbel_selfplay.py"),
                    "--board", board_arg,
                    "--num-players", str(num_players),
                    "--num-games", str(num_games),
                    "--simulation-budget", str(effective_budget),
                    "--output-dir", str(output_dir),
                    "--db", str(output_dir / "games.db"),
                    "--seed", str(int(time.time() * 1000) % 2**31),
                    "--allow-fresh-weights",  # Allow running even without trained model
                ]

                # Dec 31, 2025: Check if GPU tree is disabled for this node (e.g., vGPU nodes)
                # vGPU instances don't properly accelerate GPU tree MCTS, causing CPU-bound slowdown
                node_config = self._load_distributed_hosts().get("hosts", {}).get(self.node_id, {})
                if not node_config.get("disable_gpu_tree", False):
                    cmd.append("--use-gpu-tree")  # 170x speedup with GPU tensor tree MCTS

                # GPU selection: use explicit setting, auto-select, or default to 0
                effective_cuda_devices = cuda_visible_devices
                if effective_cuda_devices is None or not str(effective_cuda_devices).strip():
                    gpu_count = 0
                    try:
                        out = subprocess.run(
                            ["nvidia-smi", "--query-gpu=name", "--format=csv,noheader"],
                            capture_output=True, text=True, timeout=5,
                        )
                        if out.returncode == 0 and out.stdout.strip():
                            gpu_count = len([line for line in out.stdout.splitlines() if line.strip()])
                    except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError, KeyError, IndexError, AttributeError):
                        gpu_count = 0

                    if gpu_count > 0:
                        with self.jobs_lock:
                            running_gumbel_jobs = sum(
                                1 for j in self.local_jobs.values()
                                if j.job_type == JobType.GUMBEL_SELFPLAY and j.status == "running"
                            )
                        effective_cuda_devices = str(running_gumbel_jobs % gpu_count)
                    else:
                        effective_cuda_devices = "0"

                # January 2026: Use _spawn_and_track_job helper to reduce duplication
                result = self._spawn_and_track_job(
                    job_id=job_id,
                    job_type=job_type,
                    board_type=board_type,
                    num_players=num_players,
                    engine_mode="gumbel-mcts",
                    cmd=cmd,
                    output_dir=output_dir,
                    log_filename="gumbel_run.log",
                    cuda_visible_devices=effective_cuda_devices,
                    safeguard_reason=f"gumbel-selfplay-{board_type}-{num_players}p",
                )
                if result is None:
                    return None

                job, proc = result

                # Track diversity metrics for monitoring (Phase 2B, Dec 2025)
                self.selfplay_scheduler.track_diversity({
                    "board_type": board_type,
                    "num_players": num_players,
                    "engine_mode": "gumbel-mcts",
                })

                # Dec 31, 2025: Add process monitoring to track completion/failure
                asyncio.create_task(self._monitor_selfplay_process(
                    job_id, proc, output_dir, board_type, num_players, "gumbel_selfplay"
                ))

                return job

            elif job_type == JobType.DATA_EXPORT:
                # CPU-intensive data export job (NPZ creation)
                # These jobs should be routed to high-CPU nodes (vast nodes preferred)
                if not export_params:
                    logger.info("DATA_EXPORT job requires export_params")
                    return None

                input_path = export_params.get("input_path")
                output_path = export_params.get("output_path")
                encoder_version = export_params.get("encoder_version", "v3")
                max_games = export_params.get("max_games", 5000)
                is_jsonl = export_params.get("is_jsonl", False)

                if not input_path or not output_path:
                    logger.info("DATA_EXPORT requires input_path and output_path")
                    return None

                # Ensure output directory exists
                output_dir = Path(output_path).parent
                output_dir.mkdir(parents=True, exist_ok=True)

                # Use venv python if available
                venv_python = Path(self._get_ai_service_path(), "venv", "bin", "python")
                python_exec = str(venv_python) if venv_python.exists() else "python3"

                if is_jsonl:
                    # Use jsonl_to_npz.py for JSONL input (GPU selfplay data)
                    export_script = self._get_script_path("jsonl_to_npz.py")
                    cmd = [
                        python_exec,
                        export_script,
                        "--input", str(input_path),
                        "--output", str(output_path),
                        "--board-type", board_type,
                        "--num-players", str(num_players),
                        "--gpu-selfplay",
                        "--max-games", str(max_games),
                    ]
                    if encoder_version and encoder_version != "default":
                        cmd.extend(["--encoder-version", encoder_version])
                else:
                    # Use export_replay_dataset.py for DB input
                    export_script = self._get_script_path("export_replay_dataset.py")
                    cmd = [
                        python_exec,
                        export_script,
                        "--db", str(input_path),
                        "--output", str(output_path),
                        "--board-type", board_type,
                        "--num-players", str(num_players),
                        "--max-games", str(max_games),
                        "--require-completed",
                        "--min-moves", "10",
                    ]
                    if encoder_version and encoder_version != "default":
                        cmd.extend(["--encoder-version", encoder_version])

                # Start export process
                env = os.environ.copy()
                env["PYTHONPATH"] = self._get_ai_service_path()
                env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

                log_path = output_dir / f"export_{job_id}.log"
                log_handle = open(log_path, "w")  # noqa: SIM115
                try:
                    proc = subprocess.Popen(
                        cmd,
                        stdout=log_handle,
                        stderr=subprocess.STDOUT,
                        env=env,
                        cwd=self._get_ai_service_path(),
                    )
                finally:
                    log_handle.close()

                job = ClusterJob(
                    job_id=job_id,
                    job_type=job_type,
                    node_id=self.node_id,
                    board_type=board_type,
                    num_players=num_players,
                    engine_mode="export",
                    pid=proc.pid,
                    started_at=time.time(),
                    status="running",
                )

                with self.jobs_lock:
                    self.local_jobs[job_id] = job

                logger.info(f"Started DATA_EXPORT job {job_id} (PID {proc.pid}): {input_path} -> {output_path}")
                self._save_state()

                # Track job start via JobOrchestrationManager (Jan 2026)
                if hasattr(self, "job_orchestration") and self.job_orchestration:
                    self.job_orchestration.record_job_started(job_type.value)

                return job

        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to start job: {e}")
        return None

    async def _dispatch_export_job(
        self,
        node: NodeInfo,
        input_path: str,
        output_path: str,
        board_type: str,
        num_players: int,
        encoder_version: str = "v3",
        max_games: int = 5000,
        is_jsonl: bool = False,
    ):
        """Dispatch a CPU-intensive export job to a high-CPU node.

        CPU-intensive jobs like NPZ export should run on vast nodes
        (256-512 CPUs) rather than lambda nodes (64 CPUs) to free
        GPU resources for training/selfplay.
        """
        try:
            job_id = f"export_{board_type}_{num_players}p_{int(time.time())}_{uuid.uuid4().hex[:6]}"

            payload = {
                "job_id": job_id,
                "job_type": JobType.DATA_EXPORT.value,
                "board_type": board_type,
                "num_players": num_players,
                "input_path": input_path,
                "output_path": output_path,
                "encoder_version": encoder_version,
                "max_games": max_games,
                "is_jsonl": is_jsonl,
            }

            # NAT-blocked nodes need relay command
            if getattr(node, "nat_blocked", False):
                cmd_id = await self._enqueue_relay_command_for_peer(node, "start_job", payload)
                if cmd_id:
                    logger.info(f"Enqueued relay export job for {node.node_id}: {job_id}")
                else:
                    logger.info(f"Relay queue full for {node.node_id}; export not dispatched")
                return

            timeout = ClientTimeout(total=30)
            async with get_client_session(timeout) as session:
                last_err: str | None = None
                for url in self._urls_for_peer(node, "/start_job"):
                    try:
                        async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                            if resp.status == 200:
                                result = await resp.json()
                                if result.get("success"):
                                    logger.info(f"Export job dispatched to {node.node_id}: {job_id}")
                                    return
                                last_err = result.get("error", "unknown")
                            else:
                                last_err = f"http_{resp.status}"
                    except Exception as e:  # noqa: BLE001
                        last_err = str(e)

                if last_err:
                    logger.info(f"Export job dispatch failed to {node.node_id}: {last_err}")

        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to dispatch export job to {node.node_id}: {e}")

    async def _request_remote_job(
        self,
        node: NodeInfo,
        job_type: JobType,
        board_type: str = "square8",
        num_players: int = 2,
        engine_mode: str = "hybrid",
    ):
        """Request a remote node to start a job with specific configuration.

        SAFEGUARD: Checks coordination safeguards before requesting remote spawn.
        """
        try:
            # SAFEGUARD: Check safeguards before requesting remote spawn
            if HAS_SAFEGUARDS and _safeguards:
                task_type_str = job_type.value if hasattr(job_type, 'value') else str(job_type)
                allowed, reason = check_before_spawn(task_type_str, node.node_id)
                if not allowed:
                    logger.info(f"SAFEGUARD blocked remote {task_type_str} on {node.node_id}: {reason}")
                    return

            job_id = f"{job_type.value}_{board_type}_{num_players}p_{int(time.time())}_{uuid.uuid4().hex[:6]}"

            # NAT-blocked nodes can't accept inbound /start_job; enqueue a relay command instead.
            if getattr(node, "nat_blocked", False):
                payload = {
                    "job_id": job_id,
                    "job_type": job_type.value,
                    "board_type": board_type,
                    "num_players": num_players,
                    "engine_mode": engine_mode,
                }
                cmd_id = await self._enqueue_relay_command_for_peer(node, "start_job", payload)
                if cmd_id:
                    print(
                        f"[P2P] Enqueued relay job for {node.node_id}: "
                        f"{job_type.value} {board_type} {num_players}p ({job_id})"
                    )
                else:
                    logger.info(f"Relay queue full for {node.node_id}; skipping enqueue")
                return

            timeout = ClientTimeout(total=10)
            async with get_client_session(timeout) as session:
                payload = {
                    "job_id": job_id,
                    "job_type": job_type.value,
                    "board_type": board_type,
                    "num_players": num_players,
                    "engine_mode": engine_mode,
                }
                last_err: str | None = None
                for url in self._urls_for_peer(node, "/start_job"):
                    try:
                        async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                            if resp.status != 200:
                                last_err = f"http_{resp.status}"
                                continue
                            data = await resp.json()
                            if data.get("success"):
                                logger.info(f"Started remote {board_type} {num_players}p job on {node.node_id}")
                                return
                            last_err = str(data.get("error") or "start_failed")
                    except Exception as e:  # noqa: BLE001
                        last_err = str(e)
                        continue
                if last_err:
                    logger.error(f"Failed to start remote job on {node.node_id}: {last_err}")
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to request remote job from {node.node_id}: {e}")

    def _enqueue_relay_command(self, node_id: str, cmd_type: str, payload: dict[str, Any]) -> str | None:
        """Leader-side: enqueue a command for a NAT-blocked node to pull."""
        now = time.time()
        cmd_type = str(cmd_type)
        payload = dict(payload or {})

        with self.relay_lock:
            queue = list(self.relay_command_queue.get(node_id, []))
            queue = [
                cmd for cmd in queue
                if float(cmd.get("expires_at", 0.0) or 0.0) > now
            ]

            if cmd_type == "start_job":
                pending = sum(1 for c in queue if str(c.get("type") or "") == "start_job")
                if pending >= RELAY_MAX_PENDING_START_JOBS:
                    self.relay_command_queue[node_id] = queue
                    return None

                job_id = str(payload.get("job_id") or "")
                if job_id:
                    for c in queue:
                        if str(c.get("payload", {}).get("job_id") or "") == job_id:
                            self.relay_command_queue[node_id] = queue
                            return str(c.get("id") or "")

            cmd_id = uuid.uuid4().hex
            queue.append(
                {
                    "id": cmd_id,
                    "type": cmd_type,
                    "payload": payload,
                    "created_at": now,
                    "expires_at": now + RELAY_COMMAND_TTL_SECONDS,
                }
            )
            self.relay_command_queue[node_id] = queue
            return cmd_id

    async def _enqueue_relay_command_for_peer(
        self,
        peer: NodeInfo,
        cmd_type: str,
        payload: dict[str, Any],
    ) -> str | None:
        """Enqueue a relay command for `peer`, forwarding via its relay hub when needed.

        Default behavior: NAT-blocked nodes poll the leader's `/relay/heartbeat`
        endpoint and the leader stores commands in-memory.

        Some nodes (notably certain containerized GPU providers) may be unable to
        reach the leader over the mesh network (e.g. TUN-less Tailscale) and also
        cannot accept inbound connections. Those nodes will instead send relay
        heartbeats to an internet-reachable hub (e.g. `aws-staging`). When
        `peer.relay_via` points to such a hub, the leader must enqueue the relay
        command on that hub so the node can pull and execute it.
        """
        if not peer or not getattr(peer, "node_id", ""):
            return None

        peer_id = str(getattr(peer, "node_id", "") or "").strip()
        if not peer_id:
            return None

        relay_node_id = str(getattr(peer, "relay_via", "") or "").strip()
        if relay_node_id and relay_node_id != self.node_id:
            with self.peers_lock:
                relay_peer = self.peers.get(relay_node_id)
            if relay_peer:
                timeout = ClientTimeout(total=10)
                async with get_client_session(timeout) as session:
                    last_err: str | None = None
                    for url in self._urls_for_peer(relay_peer, "/relay/enqueue"):
                        try:
                            async with session.post(
                                url,
                                json={
                                    "target_node_id": peer_id,
                                    "type": cmd_type,
                                    "payload": payload or {},
                                },
                                headers=self._auth_headers(),
                            ) as resp:
                                if resp.status != 200:
                                    last_err = f"http_{resp.status}"
                                    continue
                                data = await resp.json()
                                if data.get("success"):
                                    return str(data.get("id") or "")
                                last_err = str(data.get("error") or "enqueue_failed")
                        except Exception as e:  # noqa: BLE001
                            last_err = str(e)
                            continue
                    if last_err:
                        logger.info(f"Relay enqueue via {relay_node_id} failed for {peer_id}: {last_err}")
                        # Dec 30, 2025: Automatic relay failover
                        # If the current relay is unreachable, try to find a new one
                        # January 4, 2026: Pass peer_id for configured relay preferences
                        # Inline: was _select_best_relay()
                        new_relay = self.recovery_manager.select_best_relay(for_peer=peer_id)
                        if new_relay and new_relay != relay_node_id:
                            logger.info(
                                f"[RelayFailover] Switching {peer_id} relay: "
                                f"{relay_node_id} -> {new_relay}"
                            )
                            with self.peers_lock:
                                if peer_id in self.peers:
                                    self.peers[peer_id].relay_via = new_relay
                            # Try enqueue on new relay
                            with self.peers_lock:
                                new_relay_peer = self.peers.get(new_relay)
                            if new_relay_peer:
                                for url in self._urls_for_peer(new_relay_peer, "/relay/enqueue"):
                                    try:
                                        timeout = ClientTimeout(total=10)
                                        async with get_client_session(timeout) as session2:
                                            async with session2.post(
                                                url,
                                                json={
                                                    "target_node_id": peer_id,
                                                    "type": cmd_type,
                                                    "payload": payload or {},
                                                },
                                                headers=self._auth_headers(),
                                            ) as resp2:
                                                if resp2.status == 200:
                                                    data2 = await resp2.json()
                                                    if data2.get("success"):
                                                        return str(data2.get("id") or "")
                                    except Exception:  # noqa: BLE001
                                        continue

        # Fallback: enqueue locally (works when peer polls the leader directly).
        return self._enqueue_relay_command(peer_id, cmd_type, payload)

    async def _discovery_loop(self):
        """Broadcast UDP discovery messages to find peers on local network."""
        # Phase 3.1 Dec 29, 2025: Add max iterations to prevent infinite loop
        # Jan 13, 2026: Fix busy loop - add yield points and run socket ops in thread
        MAX_RECEIVE_ITERATIONS = 100
        YIELD_EVERY_N_PACKETS = 10  # Yield to event loop every N packets

        def _do_udp_discovery() -> list[dict]:
            """Run blocking UDP discovery in thread pool to avoid blocking event loop."""
            discovered = []
            try:
                sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
                sock.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)
                sock.settimeout(1.0)

                # Broadcast our presence
                message = json.dumps({
                    "type": "p2p_discovery",
                    "node_id": self.node_id,
                    "host": self.self_info.host,
                    "port": self.port,
                }).encode()

                with contextlib.suppress(OSError):
                    sock.sendto(message, ("<broadcast>", DISCOVERY_PORT))

                # Listen for responses with iteration limit
                receive_count = 0
                try:
                    while receive_count < MAX_RECEIVE_ITERATIONS:
                        data, _addr = sock.recvfrom(1024)
                        receive_count += 1
                        try:
                            msg = json.loads(data.decode())
                            if msg.get("type") == "p2p_discovery" and msg.get("node_id") != self.node_id:
                                discovered.append(msg)
                        except (json.JSONDecodeError, UnicodeDecodeError):
                            continue
                except TimeoutError:
                    pass

                if receive_count >= MAX_RECEIVE_ITERATIONS:
                    logger.warning(f"[UdpDiscovery] Hit max receive limit ({MAX_RECEIVE_ITERATIONS})")

                sock.close()
            except OSError as e:
                logger.debug(f"[UdpDiscovery] Socket error: {e}")
            return discovered

        while self.running:
            try:
                # Run blocking socket operations in thread pool
                discovered = await asyncio.to_thread(_do_udp_discovery)

                # Process discovered peers (yield periodically to prevent busy loop)
                for i, msg in enumerate(discovered):
                    peer_addr = f"{msg.get('host')}:{msg.get('port')}"
                    if peer_addr not in self.known_peers:
                        self.known_peers.append(peer_addr)
                        logger.info(f"Discovered peer: {msg.get('node_id')} at {peer_addr}")
                    # Yield to event loop every N packets to prevent blocking
                    if (i + 1) % YIELD_EVERY_N_PACKETS == 0:
                        await asyncio.sleep(0)

            except Exception as e:  # noqa: BLE001
                logger.debug(f"[UdpDiscovery] Error: {e}")
                # Brief sleep on error to prevent tight retry loop
                await asyncio.sleep(1.0)
                continue

            await asyncio.sleep(DISCOVERY_INTERVAL)

    def _validate_critical_subsystems(self) -> dict:
        """Validate critical subsystems at startup.

        Returns a status dict with protocol and manager availability.
        Logs clear messages about which protocols are active.

        December 2025: Added to address silent fallback behavior
        where operators couldn't tell if SWIM/Raft was running.
        """
        from app.p2p.constants import (
            SWIM_ENABLED, RAFT_ENABLED, MEMBERSHIP_MODE, CONSENSUS_MODE
        )

        status = {
            "protocols": {
                "membership_mode": MEMBERSHIP_MODE,
                "consensus_mode": CONSENSUS_MODE,
                "swim_enabled": SWIM_ENABLED,
                "raft_enabled": RAFT_ENABLED,
            },
            "managers": {},
            "warnings": [],
            "errors": [],
        }

        # Check SWIM availability
        try:
            from app.p2p.swim_adapter import SWIM_AVAILABLE
            status["protocols"]["swim_available"] = SWIM_AVAILABLE
            if SWIM_ENABLED and not SWIM_AVAILABLE:
                msg = "SWIM_ENABLED=true but swim-p2p not installed. Install: pip install swim-p2p>=1.2.0"
                status["warnings"].append(msg)
                logger.warning(f"[Startup Validation] {msg}")
            elif SWIM_AVAILABLE:
                logger.info(f"[Startup Validation] SWIM protocol available (membership_mode={MEMBERSHIP_MODE})")
        except ImportError:
            status["protocols"]["swim_available"] = False
            if SWIM_ENABLED:
                status["warnings"].append("swim_adapter import failed")

        # Check Raft availability
        try:
            from app.p2p.raft_state import PYSYNCOBJ_AVAILABLE
            status["protocols"]["raft_available"] = PYSYNCOBJ_AVAILABLE
            if RAFT_ENABLED and not PYSYNCOBJ_AVAILABLE:
                msg = "RAFT_ENABLED=true but pysyncobj not installed. Install: pip install pysyncobj>=0.3.14"
                status["warnings"].append(msg)
                logger.warning(f"[Startup Validation] {msg}")
            elif PYSYNCOBJ_AVAILABLE:
                logger.info(f"[Startup Validation] Raft protocol available (consensus_mode={CONSENSUS_MODE})")
        except ImportError:
            status["protocols"]["raft_available"] = False
            if RAFT_ENABLED:
                status["warnings"].append("raft_state import failed")

        # Log active protocol configuration
        logger.info(
            f"[Startup Validation] Protocol config: membership={MEMBERSHIP_MODE}, consensus={CONSENSUS_MODE}"
        )

        # Check critical managers (lazy load check - don't fail, just report)
        manager_checks = [
            ("work_queue", "app.coordination.work_queue", "get_work_queue"),
            ("health_manager", "app.coordination.unified_health_manager", "get_unified_health_manager"),
            ("sync_router", "app.coordination.sync_router", "get_sync_router"),
        ]

        for name, module_path, getter_name in manager_checks:
            try:
                module = importlib.import_module(module_path)
                getter = getattr(module, getter_name, None)
                status["managers"][name] = getter is not None
                if getter:
                    logger.debug(f"[Startup Validation] Manager {name} available")
            except ImportError as e:
                status["managers"][name] = False
                status["warnings"].append(f"{name} import failed: {e}")
                logger.warning(f"[Startup Validation] Manager {name} unavailable: {e}")

        # December 2025: P2P voter connectivity validation
        # Check that at least quorum voters are reachable before starting
        status["voters"] = {
            "configured": len(self.voter_node_ids),
            "quorum": self.voter_quorum_size,
            "reachable": 0,
            "unreachable": [],
        }

        if self.voter_node_ids:
            import socket
            import contextlib

            reachable_count = 0
            for voter_id in self.voter_node_ids:
                # Try to resolve voter IP from config
                try:
                    from app.config.cluster_config import get_cluster_nodes
                    nodes = get_cluster_nodes()
                    node = nodes.get(voter_id)
                    if node:
                        voter_ip = node.best_ip
                        if voter_ip:
                            # Quick TCP connect test to P2P port
                            with contextlib.suppress(Exception):
                                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                                sock.settimeout(2.0)
                                result = sock.connect_ex((voter_ip, self.port))
                                sock.close()
                                if result == 0:
                                    reachable_count += 1
                                    continue
                    status["voters"]["unreachable"].append(voter_id)
                except (socket.error, socket.timeout, OSError, TimeoutError, ConnectionRefusedError):
                    status["voters"]["unreachable"].append(voter_id)

            status["voters"]["reachable"] = reachable_count

            # Log voter connectivity status
            if reachable_count < self.voter_quorum_size:
                msg = (
                    f"Only {reachable_count}/{len(self.voter_node_ids)} voters reachable, "
                    f"need {self.voter_quorum_size} for quorum. Unreachable: {status['voters']['unreachable']}"
                )
                status["warnings"].append(msg)
                logger.warning(f"[Startup Validation] {msg}")
                # January 4, 2026: Emit QUORUM_VALIDATION_FAILED event for monitoring.
                # This enables dashboards and alert systems to track pre-startup quorum issues.
                try:
                    from app.distributed.data_events import DataEventType, get_event_bus
                    get_event_bus().emit(
                        DataEventType.QUORUM_VALIDATION_FAILED,
                        {
                            "node_id": self.node_id,
                            "reachable_voters": reachable_count,
                            "total_voters": len(self.voter_node_ids),
                            "quorum_required": self.voter_quorum_size,
                            "unreachable": status["voters"]["unreachable"],
                            "timestamp": datetime.now(timezone.utc).isoformat(),
                        },
                    )
                except (ImportError, Exception) as e:
                    logger.debug(f"[Startup Validation] Could not emit quorum validation event: {e}")
            else:
                logger.info(
                    f"[Startup Validation] Voter quorum OK: "
                    f"{reachable_count}/{len(self.voter_node_ids)} voters reachable"
                )
        else:
            logger.info("[Startup Validation] No voters configured - quorum checks disabled")

        # Jan 9, 2026: PyTorch CUDA validation - detect CPU-only PyTorch on GPU nodes
        # Root cause of lambda-gh200-10 running at 0% GPU utilization
        try:
            pytorch_status = self._resource_detector.validate_pytorch_cuda()
            status["pytorch"] = pytorch_status

            if pytorch_status.get("warning"):
                status["warnings"].append(pytorch_status["warning"])
                logger.warning(f"[Startup Validation] {pytorch_status['warning']}")

                # Emit event for monitoring and dashboards
                try:
                    from app.distributed.data_events import DataEventType, get_event_bus

                    get_event_bus().emit(
                        DataEventType.PYTORCH_CUDA_MISMATCH,
                        {
                            "node_id": self.node_id,
                            "warning": pytorch_status["warning"],
                            "gpu_detected": pytorch_status.get("gpu_detected", False),
                            "pytorch_cuda_available": pytorch_status.get("pytorch_cuda_available", False),
                            "timestamp": datetime.now(timezone.utc).isoformat(),
                        },
                    )
                except (ImportError, Exception) as e:
                    logger.debug(f"[Startup Validation] Could not emit PyTorch CUDA event: {e}")
            elif pytorch_status.get("pytorch_cuda_available"):
                logger.info(
                    f"[Startup Validation] PyTorch CUDA OK: "
                    f"version={pytorch_status.get('pytorch_cuda_version')}, "
                    f"devices={pytorch_status.get('cuda_device_count')}"
                )
            elif pytorch_status.get("error"):
                logger.debug(f"[Startup Validation] PyTorch not installed: {pytorch_status.get('error')}")
        except Exception as e:
            logger.debug(f"[Startup Validation] PyTorch validation failed: {e}")

        # Summary log
        available_count = sum(1 for v in status["managers"].values() if v)
        total_count = len(status["managers"])
        if status["warnings"]:
            logger.warning(
                f"[Startup Validation] Completed with {len(status['warnings'])} warnings. "
                f"Managers: {available_count}/{total_count} available"
            )
        else:
            logger.info(
                f"[Startup Validation] All checks passed. "
                f"Managers: {available_count}/{total_count} available"
            )

        return status

    def _start_isolated_health_server(self) -> None:
        """Start a lightweight health HTTP server in a separate thread.

        January 2026: This server runs in its own thread with its own event loop,
        guaranteeing that /health endpoints respond even when the main event loop
        is blocked by background tasks.

        The isolated server:
        - Listens on port 8771 (one above the main P2P port)
        - Only serves /health and /ready endpoints
        - Does not access any state that requires the main event loop
        - Responds within 100ms even under heavy load

        This fixes the "zombie P2P" issue where the main HTTP server stops
        responding due to event loop blocking, but the process remains alive.
        """
        orchestrator = self  # Capture self for the inner function
        # Use port + 2 for isolated health server (8772 for P2P on 8770)
        # Port + 1 (8771) is used by daemon_manager's isolated health server
        health_port = self.port + 2

        def _run_health_server_in_thread() -> None:
            """Run the health server in a separate thread with its own event loop."""
            import asyncio as thread_asyncio

            async def handle_health(request: web.Request) -> web.Response:
                """Liveness probe - returns 200 if P2P process is alive.

                This is a lightweight check that doesn't access heavy state.
                """
                uptime = time.time() - getattr(orchestrator, "start_time", time.time())
                return web.json_response({
                    "alive": True,
                    "node_id": orchestrator.node_id,
                    "role": orchestrator.role.value if hasattr(orchestrator.role, 'value') else str(orchestrator.role),
                    "uptime_seconds": uptime,
                    "main_port": orchestrator.port,
                    "isolated_health_server": True,
                    "timestamp": datetime.utcnow().isoformat(),
                })

            async def handle_ready(request: web.Request) -> web.Response:
                """Readiness probe - returns 200 if P2P has started up.

                Checks minimal state without blocking.
                """
                uptime = time.time() - getattr(orchestrator, "start_time", time.time())
                # Consider ready after 30 seconds of uptime
                is_ready = uptime >= 30.0
                return web.json_response({
                    "ready": is_ready,
                    "node_id": orchestrator.node_id,
                    "uptime_seconds": uptime,
                    "startup_complete": is_ready,
                    "timestamp": datetime.utcnow().isoformat(),
                }, status=200 if is_ready else 503)

            async def run_server() -> None:
                """Set up and run the health server."""
                app = web.Application()
                app.router.add_get('/health', handle_health)
                app.router.add_get('/ready', handle_ready)

                runner = web.AppRunner(app)
                await runner.setup()

                try:
                    site = web.TCPSite(runner, '0.0.0.0', health_port, reuse_address=True)
                    await site.start()
                    logger.info(f"Isolated health server started on 0.0.0.0:{health_port}")

                    # Keep the server running forever
                    while True:
                        await thread_asyncio.sleep(3600)
                except OSError as e:
                    if "Address already in use" in str(e):
                        logger.warning(f"Isolated health server port {health_port} in use, skipping")
                    else:
                        logger.error(f"Isolated health server failed: {e}")
                except Exception as e:
                    logger.error(f"Isolated health server error: {e}")

            # Create and run a new event loop in this thread
            loop = thread_asyncio.new_event_loop()
            thread_asyncio.set_event_loop(loop)
            try:
                loop.run_until_complete(run_server())
            except Exception as e:
                logger.error(f"Isolated health server thread failed: {e}")
            finally:
                loop.close()

        # Start the health server in a daemon thread
        health_thread = threading.Thread(
            target=_run_health_server_in_thread,
            name="isolated-health-server",
            daemon=True,
        )
        health_thread.start()
        logger.info(f"Started isolated health server thread (port {health_port})")

    async def restart_http_server(self) -> bool:
        """Restart the HTTP server gracefully without terminating the process.

        January 2026: Added to enable recovery from HTTP server failures without
        requiring full process restart. Called by HttpServerHealthLoop when the
        server becomes unresponsive.

        Returns:
            True if restart succeeded, False otherwise
        """
        async with self._http_restart_lock:
            self._http_restart_count += 1
            attempt = self._http_restart_count
            logger.warning(f"[P2P] HTTP server restart attempt {attempt}")

            try:
                # Stop existing sites
                for site in self._http_sites:
                    try:
                        await site.stop()
                    except Exception as e:
                        logger.debug(f"[P2P] Error stopping site: {e}")
                self._http_sites.clear()

                # Cleanup runner
                if self._http_runner is not None:
                    try:
                        await self._http_runner.cleanup()
                    except Exception as e:
                        logger.debug(f"[P2P] Error cleaning up runner: {e}")

                # Wait briefly for port to be released
                await asyncio.sleep(1.0)

                # Create new runner from existing app
                if self._http_app is None:
                    logger.error("[P2P] Cannot restart: HTTP app not initialized")
                    return False

                self._http_runner = web.AppRunner(self._http_app)
                await self._http_runner.setup()

                # Re-bind ports
                site_v4 = web.TCPSite(
                    self._http_runner, '0.0.0.0', self.port,
                    reuse_address=True, backlog=1024
                )
                await site_v4.start()
                self._http_sites.append(site_v4)
                logger.info(f"[P2P] HTTP server restarted on 0.0.0.0:{self.port}")

                # Try IPv6 as well
                try:
                    site_v6 = web.TCPSite(
                        self._http_runner, '::', self.port,
                        reuse_address=True, backlog=1024
                    )
                    await site_v6.start()
                    self._http_sites.append(site_v6)
                    logger.info(f"[P2P] HTTP server also listening on [::]:{self.port}")
                except OSError:
                    pass  # IPv6 optional

                logger.info(f"[P2P] HTTP server restart {attempt} successful")
                return True

            except Exception as e:
                logger.error(f"[P2P] HTTP server restart {attempt} failed: {e}")
                return False

    async def run(self):
        """Main entry point - start the orchestrator."""
        if not HAS_AIOHTTP:
            logger.error("aiohttp is required. Install with: pip install aiohttp")
            raise RuntimeError("aiohttp is required but not available - install with: pip install aiohttp")

        # Start isolated health server FIRST (January 2026)
        # This ensures /health endpoint is always responsive even if main loop blocks
        self._start_isolated_health_server()

        # Validate critical subsystems before starting (December 2025)
        self._startup_validation = self._validate_critical_subsystems()

        # Set up HTTP server
        @web.middleware
        async def auth_middleware(request: web.Request, handler):
            if self.auth_token and request.method not in ("GET", "HEAD", "OPTIONS") and not self._is_request_authorized(request):
                return web.json_response({"error": "unauthorized"}, status=401)
            return await handler(request)

        # Increase max body size for large file uploads (100MB)
        # Fixes "Request Entity Too Large" for Elo DB and other file uploads
        app = web.Application(
            middlewares=[auth_middleware],
            client_max_size=100 * 1024 * 1024,  # 100 MB
        )
        # Store app for graceful restart (Jan 2026)
        self._http_app = app

        # Register all routes from centralized route registry (December 2025)
        # Replaces 200+ individual route registrations with declarative registry
        _routes_registered = False
        try:
            from scripts.p2p.routes import register_all_routes
            route_count = register_all_routes(app, self)
            logger.info(f"Registered {route_count} HTTP routes from route registry")
            _routes_registered = True
        except ImportError as e:
            logger.warning(f"Route registry not available, using inline routes: {e}")
            _routes_registered = False

        # Register file download routes (December 2025)
        # HTTP-based file sync for nodes with unreliable SSH
        try:
            from scripts.p2p.handlers.file_download import register_file_download_routes
            file_routes = register_file_download_routes(app, self)
            logger.info(f"Registered {file_routes} file download routes for HTTP-based sync")
        except ImportError as e:
            logger.debug(f"File download handler not available: {e}")

        # Register network health routes (December 30, 2025)
        # Cross-verification between P2P mesh and Tailscale connectivity
        try:
            setup_network_health_routes(app, self)
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Network health routes not registered: {e}")

        # Register model inventory routes (January 2026)
        # Used by ClusterModelEnumerator for comprehensive model evaluation
        try:
            model_routes = setup_model_routes(app, self)
            logger.info(f"Registered {model_routes} model inventory routes")
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Model inventory routes not registered: {e}")

        # January 2026: Fallback route registrations removed.
        # Routes are now exclusively managed by scripts/p2p/routes.py.
        # If route registry fails, startup will continue with partial functionality.
        if not _routes_registered:
            logger.error(
                "Route registry failed to load - P2P will have limited functionality. "
                "Check scripts/p2p/routes.py for import errors."
            )

        runner = web.AppRunner(app)
        await runner.setup()
        # Store runner for graceful restart (Jan 2026)
        self._http_runner = runner

        # Verify NFS sync before starting (prevents import errors from stale code)
        try:
            from scripts.verify_nfs_sync import verify_before_startup
            if not verify_before_startup():
                logger.warning("NFS sync verification found mismatches - check logs for details")
        except ImportError:
            logger.debug("NFS sync verification not available")
        except Exception as e:  # noqa: BLE001
            logger.warning(f"NFS sync verification failed: {e}")

        # Wire SyncRouter to event system for real-time sync triggers (December 2025)
        self._wire_sync_router_events()

        # Wire DeadPeerCooldownManager probe function (January 2026)
        # Enables probe-based early recovery from adaptive cooldown
        self._wire_cooldown_manager_probe()

        # Wire connection pool dynamic sizing (January 2026)
        # Scales pool limits based on cluster size to prevent exhaustion
        self._wire_connection_pool_dynamic_sizing()

        # Wire LeadershipStateMachine broadcast callback (ULSM - Jan 2026)
        # This enables the state machine to broadcast step-down to peers
        self._leadership_sm._broadcast_callback = self._broadcast_leader_state_change
        logger.info("ULSM: Leadership state machine broadcast callback wired")

        # Increase backlog to handle burst of connections from many nodes
        # Default is ~128, which can overflow when many vast nodes heartbeat simultaneously
        #
        # Jan 2, 2026: Bind to BOTH IPv4 and IPv6 explicitly
        # Python's asyncio/aiohttp doesn't properly implement dual-stack sockets
        # (IPV6_V6ONLY is not automatically disabled), so we bind to both addresses.
        #
        # Jan 8, 2026: Added retry with exponential backoff for TIME_WAIT state.
        # After a crash, the port may be in TIME_WAIT for up to 60s. Retry instead of failing.

        # Port binding retry configuration (January 2026)
        PORT_BIND_MAX_RETRIES = 5
        PORT_BIND_INITIAL_DELAY = 2.0  # seconds
        PORT_BIND_MAX_DELAY = 30.0  # seconds

        async def _try_bind_port(site: web.TCPSite, host: str, port: int) -> bool:
            """Try to bind port with exponential backoff for TIME_WAIT state."""
            delay = PORT_BIND_INITIAL_DELAY
            for attempt in range(PORT_BIND_MAX_RETRIES):
                try:
                    await site.start()
                    return True
                except OSError as e:
                    errno_val = getattr(e, 'errno', 0)
                    is_addr_in_use = "Address already in use" in str(e) or errno_val == 98

                    if is_addr_in_use and attempt < PORT_BIND_MAX_RETRIES - 1:
                        # Likely TIME_WAIT state - retry with backoff
                        logger.warning(
                            f"Port {port} busy (attempt {attempt + 1}/{PORT_BIND_MAX_RETRIES}), "
                            f"retrying in {delay:.1f}s (likely TIME_WAIT state)..."
                        )
                        await asyncio.sleep(delay)
                        delay = min(delay * 2, PORT_BIND_MAX_DELAY)
                        continue
                    elif is_addr_in_use:
                        # Final attempt failed
                        logger.error(f"Port {port} still in use after {PORT_BIND_MAX_RETRIES} attempts.")
                        logger.error(f"Try: lsof -i :{port} or pkill -f p2p_orchestrator")
                        raise RuntimeError(f"Port {port} bound after retries - cannot start P2P") from e
                    elif "Invalid argument" in str(e):
                        # macOS TCP keepalive socket option issue - don't retry
                        logger.warning(f"TCP socket configuration failed on {host}:{port}: {e}")
                        logger.warning("This may be a macOS TCP keepalive compatibility issue")
                        raise
                    else:
                        # Other errors - don't retry
                        logger.error(f"Failed to bind to {host}:{port}: {e}")
                        raise
            return False  # Should not reach here

        bind_host = self.host
        if self.host == "0.0.0.0":
            # Bind to IPv4 first (always needed)
            site_v4 = web.TCPSite(runner, '0.0.0.0', self.port, reuse_address=True, backlog=1024)
            await _try_bind_port(site_v4, '0.0.0.0', self.port)
            self._http_sites.append(site_v4)  # Store for graceful restart (Jan 2026)
            logger.info(f"HTTP server started on 0.0.0.0:{self.port} (IPv4, backlog=1024)")

            # Also try to bind to IPv6 (optional, for IPv6-only clients)
            try:
                site_v6 = web.TCPSite(runner, '::', self.port, reuse_address=True, backlog=1024)
                await site_v6.start()
                self._http_sites.append(site_v6)  # Store for graceful restart (Jan 2026)
                bind_host = "0.0.0.0 + [::]"
                logger.info(f"HTTP server also listening on [::]:{self.port} (IPv6)")
                print("[DEBUG] IPv6 server started", flush=True)
            except OSError as v6_err:
                # IPv6 binding failed - that's OK, IPv4 is already working
                logger.debug(f"IPv6 binding failed (OK, IPv4 is active): {v6_err}")
                bind_host = "0.0.0.0"
        else:
            # Specific host requested - bind directly with retry
            site = web.TCPSite(runner, self.host, self.port, reuse_address=True, backlog=1024)
            await _try_bind_port(site, self.host, self.port)
            self._http_sites.append(site)  # Store for graceful restart (Jan 2026)
            logger.info(f"HTTP server started on {self.host}:{self.port} (backlog=1024)")

        # Notify systemd that we're ready to serve
        systemd_notify_ready()

        # Jan 5, 2026: Send immediate relay heartbeats for NAT-blocked nodes
        # This ensures relay nodes discover us before the regular heartbeat loop kicks in
        if self._force_relay_mode:
            await self._send_initial_relay_heartbeats()

        # Jan 7, 2026: Send immediate peer announcements for ALL nodes
        # This reduces discovery latency from 15-30s to 2-5s after startup
        print("[DEBUG] About to call _send_startup_peer_announcements()", flush=True)
        await self._send_startup_peer_announcements()
        print("[DEBUG] _send_startup_peer_announcements() returned", flush=True)

        # Jan 23, 2026: Initialize HybridCoordinator for Raft-based leader election
        # This replaces the buggy Bully algorithm when CONSENSUS_MODE=raft or hybrid.
        # The HybridCoordinator provides sub-second leader failover via PySyncObj's Raft.
        print("[DEBUG] About to call _init_hybrid_coordinator()", flush=True)
        await self._init_hybrid_coordinator()
        print("[DEBUG] _init_hybrid_coordinator() returned", flush=True)

        # Jan 9, 2026: Async fallback for game count seeding from peers
        # Cluster nodes don't have local canonical DBs, so fetch from coordinator
        # This fixes underserved config prioritization on worker nodes
        await self._async_seed_game_counts_from_peers_if_needed()

        # Start background tasks with exception isolation and restart support
        # CRITICAL FIX (Dec 2025): Each task is wrapped to prevent cascade failures.
        # Previously, a single exception in any task would crash all 18+ tasks.
        # Dec 2025 Update: Added factory functions for auto-restart on critical tasks.
        tasks = [
            # Critical heartbeat loop - auto-restart on failure
            self._create_safe_task(
                self._heartbeat_loop(), "heartbeat", factory=self._heartbeat_loop
            ),
            # Job management - auto-restart on failure
            self._create_safe_task(
                self._job_management_loop(), "job_management", factory=self._job_management_loop
            ),
            # Discovery - auto-restart on failure
            self._create_safe_task(
                self._discovery_loop(), "discovery", factory=self._discovery_loop
            ),
            # IMPROVED: Dedicated voter heartbeat loop for reliable leader election - auto-restart
            self._create_safe_task(
                self._voter_heartbeat_loop(), "voter_heartbeat", factory=self._voter_heartbeat_loop
            ),
            # Jan 11, 2026: Dead peer reconnection loop (Phase 1 P2P stability fix)
            # Actively probes dead peers with exponential backoff to prevent cluster fragmentation
            self._create_safe_task(
                self._reconnect_dead_peers_loop(), "dead_peer_reconnect", factory=self._reconnect_dead_peers_loop
            ),
            # SWIM gossip membership for leaderless failure detection (<5s vs 60s+)
            self._create_safe_task(
                self._swim_membership_loop(), "swim_membership", factory=self._swim_membership_loop
            ),
        ]

        # Add git update loop if enabled
        if AUTO_UPDATE_ENABLED:
            tasks.append(self._create_safe_task(
                self._git_update_loop(), "git_update", factory=self._git_update_loop
            ))

        # Add cloud IP refresh loops (best-effort; no-op if not configured).
        # Jan 2026: Delegated to IPDiscoveryManager for better modularity
        if HAS_DYNAMIC_REGISTRY:
            self.ip_discovery_manager.start()
            tasks.append(self._create_safe_task(self.ip_discovery_manager.vast_ip_update_loop(), "vast_ip_update"))
            tasks.append(self._create_safe_task(self.ip_discovery_manager.aws_ip_update_loop(), "aws_ip_update"))
            tasks.append(self._create_safe_task(self.ip_discovery_manager.tailscale_ip_update_loop(), "tailscale_ip_update"))

        # Phase 26: Continuous bootstrap loop - ensures isolated nodes can rejoin
        tasks.append(self._create_safe_task(self._continuous_bootstrap_loop(), "continuous_bootstrap"))

        # Dec 31, 2025: Periodic IP revalidation for late Tailscale availability
        # Fixes nodes advertising private IPs when Tailscale wasn't ready at startup
        tasks.append(self._create_safe_task(
            self._periodic_ip_validation_loop(), "ip_validation", factory=self._periodic_ip_validation_loop
        ))

        # Jan 9, 2026: Periodic game count refresh for underserved config prioritization
        # Keeps scheduler game counts up-to-date as games are generated and consolidated
        tasks.append(self._create_safe_task(
            self._game_count_refresh_loop(), "game_count_refresh", factory=self._game_count_refresh_loop
        ))

        # Jan 22, 2026: Periodic cluster health snapshots for Phase 2 P2P stability instrumentation
        # Logs detailed peer counts, voter health, and election state every 60 seconds
        tasks.append(self._create_safe_task(
            self._cluster_health_snapshot_loop(), "cluster_health_snapshot", factory=self._cluster_health_snapshot_loop
        ))

        # Jan 23, 2026: Event loop latency monitor for diagnosing HTTP unresponsiveness
        # Detects when synchronous operations block the event loop, causing health checks to fail
        tasks.append(self._create_safe_task(
            self._event_loop_latency_monitor(), "event_loop_monitor", factory=self._event_loop_latency_monitor
        ))

        # Dec 2025: 11 loops extracted to LoopManager - see scripts/p2p/loops/

        # Store tasks for shutdown handling
        self._background_tasks = tasks

        # Phase 4: Start extracted loops via LoopManager (Dec 2025)
        # These 11 loops now ONLY run via LoopManager (inline versions removed):
        # - EloSyncLoop, IdleDetectionLoop, AutoScalingLoop, JobReaperLoop, QueuePopulatorLoop
        # - WorkQueueMaintenanceLoop, NATManagementLoop, ManifestCollectionLoop, ValidationLoop
        # - DataManagementLoop, ModelSyncLoop
        job_reaper_started = False
        logger.info(f"[LoopManager] Phase 4 startup: EXTRACTED_LOOPS_ENABLED={EXTRACTED_LOOPS_ENABLED}")
        if EXTRACTED_LOOPS_ENABLED and self._register_extracted_loops():
            loop_manager = self._get_loop_manager()
            if loop_manager is not None:
                # Dec 27, 2025: start_all() now returns dict of {loop_name: started_successfully}
                # Check if job_reaper specifically started to avoid duplicate reapers
                startup_results = await loop_manager.start_all()
                job_reaper_started = startup_results.get("job_reaper", False)
                started_count = sum(1 for v in startup_results.values() if v)
                logger.info(
                    f"LoopManager: started {started_count}/{len(startup_results)} loops, "
                    f"job_reaper={'running' if job_reaper_started else 'FAILED'}"
                )

                # Jan 22, 2026: Verify StabilityController started (critical for self-healing)
                stability_started = startup_results.get("stability_controller", False)
                if not stability_started and self._stability_controller is not None:
                    logger.warning("[P2P] StabilityController failed to start via LoopManager - attempting direct start")
                    try:
                        self._stability_controller.start_background()
                        await asyncio.sleep(0.5)
                        if self._stability_controller.running:
                            logger.info("[P2P] StabilityController started via direct fallback")
                        else:
                            logger.error("[P2P] StabilityController direct start failed - self-healing disabled")
                    except Exception as e:
                        logger.error(f"[P2P] StabilityController fallback start error: {e}")

        # Phase 4.1: Inline job reaper fallback (Dec 27, 2025)
        # If JobReaperLoop specifically failed to start, run inline fallback for job cleanup
        # This ensures stuck jobs get cleaned up even if the modular loop system fails
        # Dec 27, 2025: Fixed race condition - now checks job_reaper loop status, not just
        # whether LoopManager.start_all() completed (which could mask loop startup failures)
        if JOB_REAPER_FALLBACK_ENABLED and not job_reaper_started:
            logger.info("[JobReaper] LoopManager not available, starting inline fallback")
            tasks.append(
                self._create_safe_task(
                    self._inline_job_reaper_fallback_loop(),
                    "job_reaper_fallback"
                )
            )

        # Best-effort bootstrap from seed peers before running elections. This
        # helps newly started cloud nodes quickly learn about the full cluster.
        # Jan 15, 2026 (Phase 6 P2P Resilience): Add retry logic with exponential backoff
        bootstrap_success = False
        bootstrap_attempts = 0
        max_bootstrap_attempts = 3
        bootstrap_backoff = [2, 5, 10]  # Exponential backoff in seconds

        while bootstrap_attempts < max_bootstrap_attempts and not bootstrap_success:
            try:
                bootstrap_success = await self._bootstrap_from_known_peers()
                if bootstrap_success:
                    logger.info(
                        f"[Bootstrap] Successfully bootstrapped from peers "
                        f"(attempt {bootstrap_attempts + 1}/{max_bootstrap_attempts})"
                    )
                    break
            except Exception as e:
                logger.warning(f"[Bootstrap] Attempt {bootstrap_attempts + 1} failed: {e}")

            bootstrap_attempts += 1
            if bootstrap_attempts < max_bootstrap_attempts:
                wait_time = bootstrap_backoff[bootstrap_attempts - 1]
                logger.info(f"[Bootstrap] Retrying in {wait_time}s...")
                await asyncio.sleep(wait_time)

        if not bootstrap_success:
            logger.warning(
                f"[Bootstrap] Failed to bootstrap after {max_bootstrap_attempts} attempts"
            )
            # Emit bootstrap failure event for monitoring
            self._safe_emit_event("BOOTSTRAP_FAILED", {
                "node_id": self.node_id,
                "attempts": bootstrap_attempts,
                "seed_count": len(self.known_peers or []),
                "message": "Failed to bootstrap from any seed peer",
            })

        # December 30, 2025: Immediate Tailscale discovery when no --peers provided
        # This fixes the bootstrap problem where nodes started without --peers
        # couldn't join the mesh because continuous_bootstrap_loop has a 30s delay.
        if not self.known_peers:
            logger.info("[Bootstrap] No --peers provided, running immediate Tailscale discovery...")
            with self.peers_lock:
                peers_before = len(self.peers)

            # Try direct Tailscale peer discovery first
            with contextlib.suppress(Exception):
                await self._discover_tailscale_peers()

            with self.peers_lock:
                peers_after = len(self.peers)

            if peers_after > peers_before:
                logger.info(f"[Bootstrap] Tailscale discovery found {peers_after - peers_before} new peer(s)")
                # January 2026: Force reconnect to any peers online in Tailscale but missing from P2P
                # This fixes peer discovery asymmetry where P2P shows 5-7 peers while Tailscale shows 40
                await self._reconnect_missing_tailscale_peers()
            else:
                # Tailscale discovery didn't find peers - try config-based seeds
                logger.info("[Bootstrap] Tailscale discovery found no peers, trying config-based seeds...")
                config_seeds = self._load_bootstrap_seeds_from_config()
                if config_seeds:
                    logger.info(f"[Bootstrap] Loaded {len(config_seeds)} seed(s) from config")
                    self.known_peers = config_seeds
                    with contextlib.suppress(Exception):
                        await self._bootstrap_from_known_peers()

        # December 29, 2025: Extended startup election with retry mechanism
        # If no leader known, start election after allowing time for peer discovery.
        # Previously used 5s which was too short for cluster discovery.
        await asyncio.sleep(15)  # Increased from 5s to allow peer discovery
        if not self.leader_id and not self._maybe_adopt_leader_from_peers():
            # CRITICAL: Check quorum before starting election to prevent quorum bypass
            if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                logger.warning("Skipping startup election: no voter quorum available (will retry)")
            else:
                await self._start_election()

        # December 29, 2025: Add background task to retry election if still no leader
        # This handles cases where initial election fails or quorum wasn't available
        async def _delayed_election_retry():
            """Retry election periodically if no leader after startup."""
            retry_intervals = [30, 60, 120, 300]  # Exponential backoff: 30s, 1m, 2m, 5m
            retry_count = 0

            while self.running and retry_count < len(retry_intervals):
                wait_time = retry_intervals[retry_count]
                await asyncio.sleep(wait_time)

                if not self.running:
                    break

                if self.leader_id:
                    # Leader found, no need to retry
                    logger.info(f"Leader established ({self.leader_id}), stopping election retry task")
                    break

                # Still no leader, try to adopt from peers or start election
                if self._maybe_adopt_leader_from_peers():
                    logger.info(f"Adopted leader from peers: {self.leader_id}")
                    break

                # Check quorum and start election if possible
                if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                    retry_count += 1
                    # Jan 2, 2026: Use _count_alive_voters() to check IP:port matches
                    voters_alive = self._count_alive_voters()
                    logger.warning(
                        f"No voter quorum for election retry {retry_count}/{len(retry_intervals)} "
                        f"(alive={voters_alive}, need={getattr(self, 'voter_quorum_size', 3)})"
                    )
                    continue

                if not getattr(self, "election_in_progress", False):
                    logger.info(f"No leader after {wait_time}s, triggering election retry {retry_count + 1}")
                    await self._start_election()
                    retry_count += 1
                else:
                    logger.debug("Election already in progress, skipping retry")

            if not self.leader_id and self.running:
                logger.warning("Exhausted election retries, operating in leaderless mode")

        tasks.append(
            self._create_safe_task(
                _delayed_election_retry(),
                "delayed_election_retry"
            )
        )

        # Session 17.41: Deferred game counts fetch from peers
        # If local seeding returned empty (no canonical DBs), fetch from coordinator
        async def _deferred_game_counts_fetch():
            """Fetch game counts from coordinator after peer discovery."""
            try:
                await asyncio.sleep(30)  # Wait for peer discovery to complete
                if not self.running:
                    return

                # Check if we already have game counts seeded
                if self.selfplay_scheduler and hasattr(self.selfplay_scheduler, "_p2p_game_counts"):
                    existing_counts = getattr(self.selfplay_scheduler, "_p2p_game_counts", {})
                    if existing_counts:
                        logger.debug(f"[P2P] Already have {len(existing_counts)} game counts, skipping peer fetch")
                        return

                # Fetch from coordinator/peers
                game_counts = await self._fetch_game_counts_from_peers()
                if game_counts and self.selfplay_scheduler:
                    self.selfplay_scheduler.update_p2p_game_counts(game_counts)
                    logger.info(f"[P2P] Deferred fetch: seeded SelfplayScheduler with {len(game_counts)} game counts from peers")
                    for config_key, count in sorted(game_counts.items(), key=lambda x: x[1]):
                        if count < 500:  # Log underserved configs
                            logger.info(f"[P2P] Underserved config (from peers): {config_key} = {count} games")
            except Exception as e:  # noqa: BLE001
                logger.debug(f"[P2P] Deferred game counts fetch failed: {e}")

        tasks.append(
            self._create_safe_task(
                _deferred_game_counts_fetch(),
                "deferred_game_counts_fetch"
            )
        )

        # Session 17.48: Periodic game counts refresh loop
        # The deferred fetch only runs once at startup. This loop ensures game counts
        # are kept fresh on leader nodes that don't have local canonical databases.
        # Without fresh game counts, starvation multipliers can't be applied correctly.
        async def _periodic_game_counts_refresh():
            """Periodically refresh game counts from peers (runs every 5 minutes)."""
            refresh_interval = 300  # 5 minutes
            # Wait for initial deferred fetch to complete
            await asyncio.sleep(60)

            while self.running:
                try:
                    # Only refresh if we don't have local canonical DBs
                    local_counts = await asyncio.to_thread(self._seed_selfplay_scheduler_game_counts_sync)

                    if not local_counts:
                        # No local DBs, fetch from peers
                        peer_counts = await self._fetch_game_counts_from_peers()

                        if peer_counts and self.selfplay_scheduler:
                            self.selfplay_scheduler.update_p2p_game_counts(peer_counts)
                            underserved = sum(1 for c in peer_counts.values() if c < 2000)
                            logger.info(f"[P2P] Periodic refresh: {len(peer_counts)} configs, {underserved} underserved")
                            # Log critically underserved configs
                            for config_key, count in sorted(peer_counts.items(), key=lambda x: x[1]):
                                if count < 500:
                                    logger.warning(f"[P2P] CRITICAL: {config_key} has only {count} games (ULTRA starvation)")

                except Exception as e:  # noqa: BLE001
                    logger.debug(f"[P2P] Periodic game counts refresh failed: {e}")

                await asyncio.sleep(refresh_interval)

        tasks.append(
            self._create_safe_task(
                _periodic_game_counts_refresh(),
                "periodic_game_counts_refresh"
            )
        )

        # January 14, 2026: Unified game counts refresh loop
        # This loop uses UnifiedGameAggregator to get counts from ALL sources:
        # LOCAL, CLUSTER, S3, and OWC external drive on mac-studio.
        # Runs less frequently (10 min) since it's more expensive than peer-only refresh.
        async def _unified_game_counts_refresh():
            """Refresh game counts from all sources including OWC and S3."""
            refresh_interval = 600  # 10 minutes
            # Wait for initial peer-based fetch to complete first
            await asyncio.sleep(120)

            while self.running:
                try:
                    if self.selfplay_scheduler:
                        counts = await self.selfplay_scheduler.refresh_from_unified_aggregator()
                        if counts:
                            total = sum(counts.values())
                            underserved = sum(1 for c in counts.values() if c < 5000)
                            logger.info(
                                f"[P2P] Unified refresh: {total:,} total games across all sources "
                                f"({underserved} configs underserved)"
                            )
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"[P2P] Unified game counts refresh failed: {e}")

                await asyncio.sleep(refresh_interval)

        tasks.append(
            self._create_safe_task(
                _unified_game_counts_refresh(),
                "unified_game_counts_refresh"
            )
        )

        # Run forever
        # December 2025: Added return_exceptions=True to prevent task exceptions from crashing orchestrator
        try:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            # Log any task failures
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"Background task {i} failed: {result}")
        except asyncio.CancelledError:
            pass
        finally:
            self.running = False
            # Stop extracted loops via LoopManager (Dec 2025)
            loop_manager = self._get_loop_manager()
            if loop_manager is not None and loop_manager.is_started:
                try:
                    results = await loop_manager.stop_all(timeout=15.0)
                    stopped = sum(1 for ok in results.values() if ok)
                    logger.info(f"LoopManager: stopped {stopped}/{len(results)} loops")
                except Exception as e:  # noqa: BLE001
                    logger.warning(f"LoopManager: stop failed: {e}")

            # Jan 2026: Shutdown loop executor thread pools (Phase 2)
            try:
                from scripts.p2p.loop_executors import LoopExecutors
                LoopExecutors.shutdown_all(wait=True)
            except ImportError:
                pass  # Module not available
            except Exception as e:  # noqa: BLE001
                logger.warning(f"LoopExecutors shutdown failed: {e}")

            # Jan 2026: Shutdown threaded loop runners (Phase 3)
            try:
                from scripts.p2p.threaded_loop_runner import ThreadedLoopRegistry
                results = await ThreadedLoopRegistry.stop_all(timeout=15.0)
                stopped = sum(1 for ok in results.values() if ok)
                if results:
                    logger.info(f"ThreadedLoopRegistry: stopped {stopped}/{len(results)} runners")
            except ImportError:
                pass  # Module not available
            except Exception as e:  # noqa: BLE001
                logger.warning(f"ThreadedLoopRegistry shutdown failed: {e}")

            # Jan 23, 2026: Shutdown health check executor (singleton efficiency fix)
            try:
                if hasattr(self, "_health_check_executor") and self._health_check_executor:
                    self._health_check_executor.shutdown(wait=False)
                    logger.debug("Health check executor shutdown complete")
            except Exception as e:  # noqa: BLE001
                logger.debug(f"Health check executor shutdown failed: {e}")

            try:
                await asyncio.wait_for(runner.cleanup(), timeout=30)
            except asyncio.TimeoutError:
                logger.warning("HTTP server cleanup timed out after 30s")


def _wait_for_tailscale_ip(timeout_seconds: int = 90, interval_seconds: float = 1.0) -> str:
    """Wait for Tailscale IP to become available at startup.

    Jan 12, 2026: Increased timeout from 30s to 90s after observing mac-studio
    consistently advertising local IP (10.0.0.62) instead of Tailscale IP.

    Root cause: When P2P starts before Tailscale CLI is ready, _get_tailscale_ip()
    returns empty and the code falls back to local IP (e.g., 10.0.0.62). This
    persists even after Tailscale becomes available later, causing P2P connectivity
    issues since other nodes can't reach the local IP. On mac-studio specifically,
    Tailscale can take 45-60s to initialize after boot.

    Fix: Retry Tailscale IP detection with exponential backoff for up to 90 seconds
    at startup with faster initial polling (1s intervals). This gives Tailscale
    enough time to initialize even on slow boot scenarios.

    Args:
        timeout_seconds: Maximum time to wait for Tailscale (default 90s)
        interval_seconds: Initial retry interval (doubles with each retry, max 5s)

    Returns:
        Tailscale IP if available within timeout, else empty string
    """
    from scripts.p2p.resource_detector import ResourceDetector

    detector = ResourceDetector()
    start_time = time.time()
    attempt = 0
    current_interval = interval_seconds

    while (time.time() - start_time) < timeout_seconds:
        attempt += 1
        ts_ip = detector.get_tailscale_ip()
        if ts_ip:
            if attempt > 1:
                logger.info(f"[TAILSCALE] IP acquired after {attempt} attempts: {ts_ip}")
            return ts_ip

        elapsed = time.time() - start_time
        remaining = timeout_seconds - elapsed

        if elapsed >= 5 and attempt <= 3:
            logger.warning(f"[TAILSCALE] Still waiting for IP (attempt {attempt}, {elapsed:.1f}s elapsed)")

        if remaining <= 0:
            break

        # Sleep with exponential backoff (max 5s between retries)
        sleep_time = min(current_interval, remaining, 5.0)
        time.sleep(sleep_time)
        current_interval = min(current_interval * 1.5, 5.0)

    logger.warning(f"[TAILSCALE] Timed out waiting for IP after {timeout_seconds}s ({attempt} attempts)")
    return ""


def _auto_detect_node_id() -> str | None:
    """Auto-detect node ID using unified identity resolution.

    Jan 2, 2026: Added to prevent startup failures when --node-id is forgotten.
    Jan 12, 2026: Added /etc/ringrift/node-id file support and IP normalization.
    Jan 13, 2026: Delegated to app.config.node_identity module (P2P Cluster Stability Plan).

    Detection order (from node_identity module):
    0. /etc/ringrift/node-id file (canonical source, written by deployment)
    1. RINGRIFT_NODE_ID environment variable
    2. /etc/default/ringrift-p2p file (legacy compatibility)
    3. Hostname match against distributed_hosts.yaml
    4. Tailscale IP match against distributed_hosts.yaml
    5. Fall back to get_node_id_safe() which uses hostname

    Returns:
        Detected node_id string, or None if detection failed
    """
    try:
        from app.config.node_identity import (
            get_node_identity,
            get_node_id_safe,
            NodeIdentityError,
        )

        # Try strict resolution first
        try:
            identity = get_node_identity()
            logger.info(
                f"[NODE-ID] Resolved node ID via {identity.resolution_method}: "
                f"{identity.canonical_id}"
            )
            return identity.canonical_id
        except NodeIdentityError as e:
            # Strict resolution failed, use safe fallback
            logger.warning(f"[NODE-ID] Strict resolution failed: {e}")
            node_id = get_node_id_safe()
            logger.warning(
                f"[NODE-ID] Using fallback node ID: {node_id} - "
                f"Run 'python scripts/provision_node_id.py --auto-detect' to fix"
            )
            return node_id

    except ImportError as e:
        # Module not available (running standalone or tests)
        logger.debug(f"[NODE-ID] node_identity module not available: {e}")

        # Minimal fallback: check canonical file and env var
        try:
            with open("/etc/ringrift/node-id") as f:
                node_id = f.read().strip()
                if node_id:
                    logger.info(f"[NODE-ID] Using node-id from /etc/ringrift/node-id: {node_id}")
                    return node_id
        except (FileNotFoundError, PermissionError):
            pass

        node_id = os.environ.get("RINGRIFT_NODE_ID")
        if node_id:
            return node_id

        # Fall back to hostname
        import socket
        hostname = socket.gethostname()
        if "." in hostname:
            hostname = hostname.split(".")[0]
        logger.warning(
            f"[NODE-ID] Falling back to hostname '{hostname}' - "
            f"Set RINGRIFT_NODE_ID or run provision_node_id.py"
        )
        return hostname


def _acquire_singleton_lock(
    kill_duplicates: bool = False,
    force_takeover: bool = False,
) -> bool:
    """Acquire singleton lock to prevent duplicate P2P orchestrator instances.

    Uses atomic file locking (fcntl) which is more reliable than PID file checks.
    Automatically handles stale locks from crashed processes.

    Args:
        kill_duplicates: If True, kill any duplicate P2P processes before acquiring
        force_takeover: If True, force-kill any lock holder (even if not P2P).
                        Use when lock is held by a recycled PID.

    Returns:
        True if lock acquired successfully
    """
    global _P2P_LOCK

    lock_dir = Path(__file__).parent.parent / "data" / "coordination"
    lock_dir.mkdir(parents=True, exist_ok=True)

    if kill_duplicates:
        # Find and kill any existing p2p_orchestrator processes
        pattern = r"p2p_orchestrator\.py"
        existing = find_processes_by_pattern(pattern, exclude_self=True)
        if existing:
            logger.info(f"[P2P] Found {len(existing)} duplicate processes, killing...")
            for proc in existing:
                logger.info(f"[P2P] Killing duplicate: PID {proc.pid}")
                if kill_process(proc.pid, wait=True, timeout=5.0):
                    logger.info(f"[P2P] Killed PID {proc.pid}")
                else:
                    logger.warning(f"[P2P] Failed to kill PID {proc.pid}")
            # Wait a moment for locks to release
            time.sleep(0.5)

    # Create lock with auto-cleanup of stale locks (from dead processes)
    _P2P_LOCK = SingletonLock(
        "p2p_orchestrator",
        lock_dir=lock_dir,
        auto_cleanup_stale=True,  # Automatically handle dead process locks
    )

    if not _P2P_LOCK.acquire():
        # Lock acquisition failed - provide detailed diagnostics
        status = _P2P_LOCK.get_lock_status()
        holder_pid = status.get("holder_pid")
        holder_alive = status.get("holder_alive", False)
        holder_command = status.get("holder_command", "")
        is_stale = status.get("is_stale", False)

        if is_stale:
            # This shouldn't happen with auto_cleanup_stale=True, but handle it
            logger.warning(
                f"[P2P] Stale lock detected (dead PID {holder_pid}). "
                f"Attempting force cleanup..."
            )
            if _P2P_LOCK.force_release():
                # Retry acquisition after cleanup
                if _P2P_LOCK.acquire():
                    logger.info(f"[P2P] Acquired lock after stale cleanup (PID {os.getpid()})")
                    return True
            logger.error("[P2P] Failed to clean up stale lock")
            return False

        if holder_pid and holder_alive:
            # Another live process is holding the lock
            is_p2p = _P2P_LOCK.is_holder_expected_process("p2p_orchestrator")
            if is_p2p:
                logger.error(
                    f"[P2P] Another P2P orchestrator is already running (PID {holder_pid}). "
                    f"Use --kill-duplicates to automatically terminate it."
                )
            else:
                # PID reuse - different process now holds the lock file
                # This happens when the old P2P crashed and the PID was reused
                if force_takeover:
                    logger.warning(
                        f"[P2P] Lock held by unexpected process (PID {holder_pid}: {holder_command[:80] if holder_command else 'unknown'}). "
                        f"Force takeover requested - killing holder."
                    )
                    if _P2P_LOCK.force_release(kill_holder=True):
                        if _P2P_LOCK.acquire():
                            logger.info(f"[P2P] Acquired lock after force takeover (PID {os.getpid()})")
                            return True
                    logger.error("[P2P] Force takeover failed")
                else:
                    logger.warning(
                        f"[P2P] Lock held by unexpected process (PID {holder_pid}: {holder_command[:80] if holder_command else 'unknown'}). "
                        f"This may indicate PID reuse after a crash. "
                        f"Use --force-takeover to automatically recover."
                    )
        else:
            logger.error(
                "[P2P] Failed to acquire lock (unknown reason). "
                f"Lock status: {status}"
            )
        return False

    logger.info(f"[P2P] Acquired singleton lock (PID {os.getpid()})")
    return True


def _release_singleton_lock() -> None:
    """Release the singleton lock on shutdown."""
    global _P2P_LOCK
    if _P2P_LOCK:
        _P2P_LOCK.release()
        logger.debug("[P2P] Released singleton lock")
        _P2P_LOCK = None


# =============================================================================
# PORT-FIRST CHECK (January 21, 2026 - Phase 1)
# =============================================================================
# This provides fast-fail duplicate detection BEFORE zombie detection or lock
# acquisition. If a healthy P2P is already running, exit immediately.

def _check_port_available_and_responsive(port: int = 8770, timeout: float = 3.0) -> tuple[bool, str]:
    """Check if port is available or if existing P2P is healthy.

    January 21, 2026: Added as Phase 1 of duplicate process prevention.
    This is the FIRST check at startup, before zombie detection or lock acquisition.
    Provides fast-fail when a healthy P2P is already running.

    Args:
        port: The P2P HTTP port to check (default 8770)
        timeout: HTTP health check timeout in seconds

    Returns:
        (should_continue, reason) tuple:
        - (True, "port_free") - Port is free, proceed with startup
        - (True, "port_check_failed") - Couldn't determine, proceed cautiously
        - (False, "healthy_p2p_running") - Another healthy P2P is running, exit
    """
    import socket
    import urllib.request
    import urllib.error

    # Step 1: Try to bind to port (instant availability check)
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        sock.bind(("0.0.0.0", port))
        sock.close()
        return (True, "port_free")
    except OSError:
        pass  # Port in use, check if healthy

    # Step 2: Check if existing process on port is responsive
    try:
        req = urllib.request.Request(
            f"http://127.0.0.1:{port}/health",
            headers={"User-Agent": "p2p-startup-check"},
        )
        with urllib.request.urlopen(req, timeout=timeout) as resp:
            if resp.status == 200:
                return (False, "healthy_p2p_running")
    except urllib.error.URLError as e:
        # Connection refused means port not actually listening
        if "Connection refused" in str(e):
            return (True, "port_check_failed")
        # Timeout or other error - proceed cautiously
        return (True, "port_check_failed")
    except Exception:
        # Unexpected error - proceed cautiously
        return (True, "port_check_failed")

    # Should not reach here, but proceed if we do
    return (True, "port_check_failed")


# =============================================================================
# SUPERVISOR COORDINATION (January 21, 2026 - Phase 2)
# =============================================================================
# This section prevents conflicts between manual P2P starts and master_loop
# automated recovery by creating a coordination file that tracks which
# management path is in control.

SUPERVISOR_FILE_PATH = Path(__file__).parent.parent / "data" / "coordination" / "p2p_supervisor.json"


def _read_supervisor_file() -> dict | None:
    """Read the supervisor coordination file."""
    try:
        if SUPERVISOR_FILE_PATH.exists():
            content = SUPERVISOR_FILE_PATH.read_text()
            return json.loads(content)
    except (OSError, json.JSONDecodeError) as e:
        logger.debug(f"[P2P] Could not read supervisor file: {e}")
    return None


def _write_supervisor_file(managed_by: str, pid: int, force: bool = False) -> bool:
    """Write the supervisor coordination file."""
    from datetime import datetime

    if not force:
        existing = _read_supervisor_file()
        if existing and existing.get("managed_by") not in ("none", None):
            existing_pid = existing.get("pid")
            if existing_pid and _is_process_running_check(existing_pid):
                return False

    try:
        SUPERVISOR_FILE_PATH.parent.mkdir(parents=True, exist_ok=True)
        state = {
            "managed_by": managed_by,
            "pid": pid,
            "started_at": datetime.utcnow().isoformat() + "Z",
            "last_health_check": datetime.utcnow().isoformat() + "Z",
        }
        SUPERVISOR_FILE_PATH.write_text(json.dumps(state, indent=2))
        logger.info(f"[P2P] Claimed supervisor role: {managed_by} (PID {pid})")
        return True
    except OSError as e:
        logger.warning(f"[P2P] Failed to write supervisor file: {e}")
        return False


def _is_process_running_check(pid: int) -> bool:
    """Check if a process with the given PID is running."""
    try:
        os.kill(pid, 0)
        return True
    except (ProcessLookupError, PermissionError, OSError):
        return False


def _claim_supervisor_role(mode: str = "manual", force: bool = False) -> tuple[bool, str]:
    """Claim P2P management role."""
    from datetime import datetime, timedelta

    existing = _read_supervisor_file()

    if existing is None or existing.get("managed_by") in ("none", None):
        if _write_supervisor_file(mode, os.getpid(), force=True):
            return (True, "claimed")
        return (False, "write_failed")

    existing_manager = existing.get("managed_by")
    existing_pid = existing.get("pid")

    # Check if PID is dead
    if existing_pid and not _is_process_running_check(existing_pid):
        logger.info(f"[P2P] Previous manager (PID {existing_pid}) is dead, taking over")
        if _write_supervisor_file(mode, os.getpid(), force=True):
            return (True, "claimed")
        return (False, "write_failed")

    # Jan 23, 2026: Check for stale claims based on timestamp
    # If last_health_check is older than 10 minutes, consider it stale
    last_health = existing.get("last_health_check") or existing.get("started_at")
    if last_health:
        try:
            last_health_dt = datetime.fromisoformat(last_health.replace("Z", "+00:00"))
            now = datetime.now(last_health_dt.tzinfo) if last_health_dt.tzinfo else datetime.utcnow()
            stale_threshold = timedelta(minutes=10)
            if now - last_health_dt > stale_threshold:
                logger.info(f"[P2P] Previous manager (PID {existing_pid}) has stale health check ({last_health}), taking over")
                if _write_supervisor_file(mode, os.getpid(), force=True):
                    return (True, "claimed")
                return (False, "write_failed")
        except (ValueError, TypeError) as e:
            logger.debug(f"[P2P] Could not parse health check timestamp: {e}")

    if existing_pid == os.getpid():
        return (True, "already_manager")

    if existing_manager == "master_loop" and mode == "manual":
        if not force:
            return (False, "master_loop_managing")
        logger.warning("[P2P] Forcing takeover from master_loop")

    if force:
        if _write_supervisor_file(mode, os.getpid(), force=True):
            return (True, "claimed")
        return (False, "write_failed")

    return (False, "other_manager")


def _release_supervisor_role() -> None:
    """Release P2P management role on shutdown."""
    try:
        existing = _read_supervisor_file()
        if existing and existing.get("pid") == os.getpid():
            _write_supervisor_file("none", 0, force=True)
            logger.info("[P2P] Released supervisor role")
    except Exception as e:
        logger.debug(f"[P2P] Could not release supervisor role: {e}")


def should_master_loop_manage_p2p() -> tuple[bool, str]:
    """Check if master_loop should manage P2P or defer to manual management."""
    from datetime import datetime, timedelta

    existing = _read_supervisor_file()

    if existing is None:
        return (True, "no_manager")

    managed_by = existing.get("managed_by")
    if managed_by in ("none", None):
        return (True, "no_manager")

    existing_pid = existing.get("pid")

    if existing_pid and not _is_process_running_check(existing_pid):
        return (True, "manager_dead")

    if managed_by == "manual":
        started_at_str = existing.get("started_at", "")
        try:
            started_at = datetime.fromisoformat(started_at_str.rstrip("Z"))
            age = datetime.utcnow() - started_at
            if age < timedelta(hours=1):
                return (False, "manual_manager")
            return (True, "manual_expired")
        except (ValueError, TypeError):
            return (False, "manual_manager")

    if managed_by == "master_loop":
        return (True, "master_loop_manager")

    return (False, "manager_healthy")


def _check_and_kill_zombie_p2p(port: int = 8770, timeout: float = 5.0) -> bool:
    """Check for zombie P2P process and kill it if found.

    A zombie P2P process is one that is bound to the port but not responding
    to HTTP requests. This can happen when the process is stuck in a bad state.

    Args:
        port: The P2P HTTP port to check (default 8770)
        timeout: HTTP request timeout in seconds

    Returns:
        True if a zombie was found and killed, False otherwise
    """
    import urllib.request
    import urllib.error

    # Step 1: Check if anything is listening on the port
    try:
        result = subprocess.run(
            ["lsof", "-ti", f":{port}"],
            capture_output=True,
            text=True,
            timeout=5.0,
        )
        if result.returncode != 0 or not result.stdout.strip():
            # Nothing listening on the port
            return False
        pids = [int(p) for p in result.stdout.strip().split("\n") if p.strip()]
        if not pids:
            return False
    except (subprocess.TimeoutExpired, subprocess.SubprocessError, ValueError):
        # lsof failed or timed out, assume no zombie
        return False

    # Step 2: Try to hit the /status endpoint
    try:
        req = urllib.request.Request(
            f"http://127.0.0.1:{port}/status",
            headers={"User-Agent": "zombie-detector"},
        )
        with urllib.request.urlopen(req, timeout=timeout) as resp:
            if resp.status == 200:
                # Process is responding, not a zombie
                return False
    except urllib.error.URLError as e:
        # Connection refused means nothing is really listening (lsof race)
        if "Connection refused" in str(e):
            return False
        # Other errors (timeout, etc.) mean zombie detected
        logger.warning(f"[P2P] Port {port} occupied but unresponsive: {e}")
    except Exception as e:
        # Timeout or other error - this is a zombie
        logger.warning(f"[P2P] Port {port} occupied but unresponsive: {e}")

    # Step 3: Kill the zombie process(es)
    logger.warning(f"[P2P] Detected zombie P2P process on port {port}, killing PIDs: {pids}")
    killed = False
    for pid in pids:
        # Skip ourselves
        if pid == os.getpid():
            continue
        try:
            if kill_process(pid, wait=True, timeout=5.0):
                logger.info(f"[P2P] Killed zombie process PID {pid}")
                killed = True
            else:
                logger.warning(f"[P2P] Failed to kill zombie PID {pid}")
        except Exception as e:
            logger.error(f"[P2P] Error killing zombie PID {pid}: {e}")

    if killed:
        # Give the port time to be released
        time.sleep(0.5)

    return killed


def main():
    # ==========================================================================
    # PRE-FLIGHT VALIDATION (January 2026)
    # ==========================================================================
    # Validate critical dependencies before any complex initialization.
    # This prevents cryptic runtime errors from missing packages.
    deps_ok, dep_errors = _validate_preflight_dependencies()
    if not deps_ok:
        print("[P2P] FATAL: Missing critical dependencies", file=sys.stderr)
        for err in dep_errors:
            print(f"  {err}", file=sys.stderr)
        print("\n[P2P] Fix: pip install aiohttp psutil pyyaml", file=sys.stderr)
        sys.exit(1)

    # Parse lock-related args early (before full argparse)
    import sys
    kill_duplicates = "--kill-duplicates" in sys.argv
    force_takeover = "--force-takeover" in sys.argv
    skip_zombie_check = "--no-zombie-check" in sys.argv
    skip_port_check = "--skip-port-check" in sys.argv
    ignore_supervisor = "--ignore-supervisor" in sys.argv
    force_supervisor = "--force-supervisor" in sys.argv
    is_master_loop = "--managed-by-master-loop" in sys.argv

    # ==========================================================================
    # PORT-FIRST CHECK (January 21, 2026)
    # ==========================================================================
    # Check if port is available or if a healthy P2P is already running.
    # This provides fast-fail before zombie detection or lock acquisition.
    if not skip_port_check:
        can_start, reason = _check_port_available_and_responsive(DEFAULT_PORT)
        if not can_start:
            print(f"[P2P] Exiting: {reason} - another healthy P2P is already running on port {DEFAULT_PORT}")
            sys.exit(0)
        elif reason == "port_free":
            print("[P2P] Port is free, proceeding with startup")

    # ==========================================================================
    # SUPERVISOR COORDINATION (January 21, 2026)
    # ==========================================================================
    # Check if another manager (master_loop or manual) is controlling P2P.
    if not ignore_supervisor:
        management_mode = "master_loop" if is_master_loop else "manual"
        claimed, claim_reason = _claim_supervisor_role(mode=management_mode, force=force_supervisor)
        if not claimed:
            if claim_reason == "master_loop_managing":
                print("[P2P] Exiting: master_loop.py is managing P2P. Use --force-supervisor to override.")
            else:
                print(f"[P2P] Exiting: Another manager is active ({claim_reason}). Use --force-supervisor to override.")
            sys.exit(0)
        print(f"[P2P] Claimed supervisor role: {management_mode}")

    # ==========================================================================
    # ZOMBIE DETECTION (January 2026)
    # ==========================================================================
    # Check for zombie P2P processes that are bound to the port but unresponsive.
    # This happens when the P2P process gets stuck in a bad state (e.g., 100% CPU).
    if not skip_zombie_check:
        if _check_and_kill_zombie_p2p():
            print("[P2P] Killed zombie P2P process, proceeding with startup")

    # Acquire singleton lock (December 2025: improved atomic locking with stale cleanup)
    if not _acquire_singleton_lock(
        kill_duplicates=kill_duplicates,
        force_takeover=force_takeover,
    ):
        sys.exit(1)

    parser = argparse.ArgumentParser(description="P2P Orchestrator for RingRift cluster")
    parser.add_argument("--node-id", required=False, help="Unique identifier for this node (auto-detects if not provided)")
    parser.add_argument("--host", default="0.0.0.0", help="Host to bind to")
    parser.add_argument("--port", type=int, default=DEFAULT_PORT, help="Port to listen on")
    parser.add_argument(
        "--advertise-host",
        default=None,
        help=f"Host to advertise to peers (or set {ADVERTISE_HOST_ENV})",
    )
    parser.add_argument(
        "--advertise-port",
        type=int,
        default=None,
        help=f"Port to advertise to peers (or set {ADVERTISE_PORT_ENV})",
    )
    parser.add_argument("--peers", help="Comma-separated list of known peers (host[:port] or http(s)://host[:port])")
    parser.add_argument("--relay-peers", help="Comma-separated list of peers to use relay heartbeats with (for NAT-blocked nodes)")
    parser.add_argument("--ringrift-path", help="Path to RingRift installation")
    parser.add_argument("--auth-token", help=f"Shared auth token (or set {AUTH_TOKEN_ENV})")
    parser.add_argument("--require-auth", action="store_true", help="Require auth token to be set")
    parser.add_argument("--storage-type", choices=["disk", "ramdrive", "auto"], default="auto",
                        help="Storage type: 'disk', 'ramdrive' (/dev/shm), or 'auto' (detect based on RAM/disk)")
    parser.add_argument("--sync-to-disk-interval", type=int, default=300,
                        help="When using ramdrive, sync to disk every N seconds (0 = no sync, default: 300)")
    parser.add_argument("--supervised", action="store_true",
                        help="Running under cluster_supervisor.py - disable self-restart logic")
    parser.add_argument("--kill-duplicates", action="store_true",
                        help="Kill any existing P2P orchestrator processes before starting")
    parser.add_argument("--force-takeover", action="store_true",
                        help="Force acquire lock even if held by another process (use when PID was recycled after crash)")
    parser.add_argument("--no-zombie-check", action="store_true",
                        help="Skip automatic zombie P2P detection (zombies are processes bound to port but not responding)")
    parser.add_argument("--skip-port-check", action="store_true",
                        help="Skip the port availability check at startup (Jan 21, 2026)")
    parser.add_argument("--ignore-supervisor", action="store_true",
                        help="Skip supervisor coordination file check (Jan 21, 2026)")
    parser.add_argument("--force-supervisor", action="store_true",
                        help="Force takeover of supervisor role even if another manager is active")
    parser.add_argument("--managed-by-master-loop", action="store_true",
                        help="Internal flag: indicates P2P was started by master_loop.py")
    parser.add_argument("--training-only", action="store_true",
                        help="Run as training-only node (no selfplay dispatch). Prevents OOM from training + selfplay conflicts.")

    args = parser.parse_args()

    # Jan 2026: Set training-only mode if flag is set
    if args.training_only:
        set_selfplay_disabled_override(disabled=True)
        logger.info("[P2P] Running in training-only mode - selfplay disabled")

    # Jan 2, 2026: Auto-detect node_id if not provided
    if not args.node_id:
        args.node_id = _auto_detect_node_id()
        if not args.node_id:
            logger.error("Could not auto-detect node-id. Please provide --node-id explicitly.")
            sys.exit(1)
        logger.info(f"Auto-detected node-id: {args.node_id}")

    known_peers = []
    if args.peers:
        known_peers = [p.strip() for p in args.peers.split(',')]

    relay_peers = []
    if args.relay_peers:
        relay_peers = [p.strip() for p in args.relay_peers.split(',')]

    # Wrap orchestrator creation and run in try/except to ensure crashes are logged
    orchestrator = None
    try:
        logger.info(f"Initializing P2P orchestrator: node_id={args.node_id}")
        orchestrator = P2POrchestrator(
            node_id=args.node_id,
            host=args.host,
            port=args.port,
            known_peers=known_peers,
            relay_peers=relay_peers,
            ringrift_path=args.ringrift_path,
            advertise_host=args.advertise_host,
            advertise_port=args.advertise_port,
            auth_token=args.auth_token,
            require_auth=args.require_auth,
            storage_type=args.storage_type,
            sync_to_disk_interval=args.sync_to_disk_interval,
        )
        logger.info(f"P2P orchestrator initialized successfully: {args.node_id}")

        # December 28, 2025: Validate event emitters at startup
        # This provides early warning if event system is not properly configured
        if _check_event_emitters():
            logger.info("[P2P] Event emitters available - P2P events will be published")
        else:
            logger.warning(
                "[P2P] Event emitters NOT available - P2P events will be silent. "
                "Ensure app.coordination.event_emitters is importable for full integration."
            )
    except Exception as e:  # noqa: BLE001
        logger.exception(f"Failed to initialize P2P orchestrator: {e}")
        # January 2026: Release lock on initialization failure to prevent
        # stale locks that block future startups
        _release_singleton_lock()
        sys.exit(1)

    # Handle shutdown gracefully - avoid race conditions with async tasks
    # December 2025: Fixed signal handler race condition that caused threading exceptions
    _shutdown_requested = False
    _start_time = time.time()

    def signal_handler(sig, frame):
        nonlocal _shutdown_requested
        import traceback

        uptime = time.time() - _start_time
        sig_name = signal.Signals(sig).name if hasattr(signal, 'Signals') else f"signal {sig}"

        if _shutdown_requested:
            # Force exit on second signal
            logger.warning(f"Forced shutdown (second {sig_name}) after {uptime:.1f}s uptime")
            os._exit(1)
        _shutdown_requested = True

        # Enhanced logging to identify what's sending signals
        logger.warning(f"=== SIGNAL RECEIVED: {sig_name} ===")
        logger.warning(f"PID: {os.getpid()}, Uptime: {uptime:.1f}s, Node: {args.node_id}")
        logger.warning(f"Stack trace at signal:\n{''.join(traceback.format_stack(frame))}")
        logger.info("Shutdown requested, stopping gracefully...")
        if orchestrator:
            orchestrator.running = False
            # Cancel all background tasks for graceful shutdown (Dec 2025)
            if hasattr(orchestrator, '_background_tasks'):
                for task in orchestrator._background_tasks:
                    if not task.done():
                        task.cancel()
            # Schedule ramdrive sync in a thread to avoid blocking signal handler
            # Don't call sys.exit() - let asyncio loop exit cleanly

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    # Run with exception logging
    try:
        logger.info(f"Starting P2P orchestrator main loop: {args.node_id}")
        asyncio.run(orchestrator.run())
    except Exception as e:  # noqa: BLE001
        logger.exception(f"P2P orchestrator crashed: {e}")
        sys.exit(1)
    finally:
        # Ensure ramdrive is synced on exit (moved from signal handler to avoid race)
        if orchestrator:
            try:
                orchestrator.stop_ramdrive_syncer(final_sync=True)
                logger.info("Ramdrive sync completed on shutdown")
            except Exception as e:  # noqa: BLE001
                logger.warning(f"Ramdrive sync on shutdown failed: {e}")
            # December 2025: Close webhook notifier to prevent memory leaks
            try:
                if hasattr(orchestrator, 'notifier') and orchestrator.notifier:
                    orchestrator.notifier.close_sync()
            except (RuntimeError, OSError, AttributeError) as e:
                # Dec 2025: Narrowed from bare Exception; best effort cleanup
                logger.debug(f"Notifier close failed (best effort): {e}")

            # December 2025: Close work queue to persist final stats
            try:
                from app.coordination.work_queue import reset_work_queue
                reset_work_queue()
            except (ImportError, RuntimeError, sqlite3.Error) as e:
                logger.debug(f"Work queue cleanup failed (best effort): {e}")

            # December 2025: Release singleton lock on shutdown
            _release_singleton_lock()

            # January 2026: Release supervisor role on shutdown
            _release_supervisor_role()


if __name__ == "__main__":
    main()
