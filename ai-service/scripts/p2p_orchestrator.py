#!/usr/bin/env python3
"""Distributed P2P Orchestrator - Self-healing compute cluster for RingRift AI training.

This orchestrator runs on each node in the cluster and:
1. Discovers other nodes via broadcast UDP or known peer list
2. Participates in leader election for coordination tasks
3. Monitors local resources and shares status with peers
4. Auto-starts selfplay/training jobs based on cluster needs
5. Self-heals when nodes go offline or IPs change

Architecture:
- Each node runs this script as a daemon
- Nodes communicate via HTTP REST API (port 8770)
- Leader election uses Bully algorithm (highest node_id wins)
- Heartbeats every 30 seconds detect failures
- Nodes maintain local SQLite state for crash recovery

Usage:
    # On each node:
    python scripts/p2p_orchestrator.py --node-id mac-studio
    python scripts/p2p_orchestrator.py --node-id vast-5090-quad --port 8770

    # With known peers (for cloud nodes without broadcast):
    python scripts/p2p_orchestrator.py --node-id vast-3090 --peers <peer-ip>:8770,<peer-ip>:8770
"""

from __future__ import annotations

import argparse
import asyncio
import contextlib
import gzip
import ipaddress
import json
import os
import secrets
import shutil
import signal
import socket
import sqlite3
import subprocess
import sys
import threading
import time
import uuid
from collections.abc import Generator
from dataclasses import asdict
from datetime import datetime
from pathlib import Path
from typing import TYPE_CHECKING, Any, Optional
from urllib.parse import urlparse

if TYPE_CHECKING:
    from app.coordination.queue_populator import QueuePopulator
    from app.coordination.p2p_auto_deployer import P2PAutoDeployer

# Work queue for centralized work distribution (lazy import to avoid circular deps)
_work_queue = None
def get_work_queue():
    """Get the work queue singleton (lazy load)."""
    global _work_queue
    if _work_queue is None:
        try:
            from app.coordination.work_queue import get_work_queue as _get_wq
            _work_queue = _get_wq()
        except ImportError:
            _work_queue = None
    return _work_queue

# Automation managers (lazy imports to avoid circular deps)
_auto_scaler = None
_health_manager = None  # December 2025: Consolidated from recovery_manager
_predictive_alerts = None
_tier_calibrator = None

def get_auto_scaler():
    """Get the auto-scaler singleton (lazy load)."""
    global _auto_scaler
    if _auto_scaler is None:
        try:
            from app.coordination.auto_scaler import AutoScaler
            _auto_scaler = AutoScaler()
        except ImportError:
            _auto_scaler = None
    return _auto_scaler

def get_health_manager():
    """Get the health manager singleton (lazy load).

    December 2025: Consolidated from get_recovery_manager().
    Uses UnifiedHealthManager which combines recovery + error coordination.
    """
    global _health_manager
    if _health_manager is None:
        try:
            from app.coordination.unified_health_manager import UnifiedHealthManager
            _health_manager = UnifiedHealthManager.get_instance()
        except ImportError:
            _health_manager = None
    return _health_manager


# Backward compatibility alias (deprecated)
def get_recovery_manager():
    """DEPRECATED: Use get_health_manager() instead."""
    return get_health_manager()

# Job Reaper Daemon (leader-only, kills stuck jobs and reassigns work)
_job_reaper = None
def get_job_reaper(work_queue=None, ssh_config=None):
    """Get the job reaper singleton (lazy load).

    The JobReaperDaemon enforces job timeouts by:
    1. Detecting jobs past their timeout
    2. Killing stuck processes via SSH
    3. Marking jobs as TIMEOUT
    4. Reassigning failed work to other nodes
    5. Blacklisting nodes that repeatedly fail
    """
    global _job_reaper
    if _job_reaper is None and work_queue is not None:
        try:
            from app.coordination.job_reaper import JobReaperDaemon
            _job_reaper = JobReaperDaemon(
                work_queue=work_queue,
                ssh_config=ssh_config,
            )
        except ImportError as e:
            logger.warning(f"JobReaperDaemon not available: {e}")
            _job_reaper = None
    return _job_reaper

def get_predictive_alerts():
    """Get the predictive alerts manager (lazy load)."""
    global _predictive_alerts
    if _predictive_alerts is None:
        try:
            from app.monitoring.predictive_alerts import PredictiveAlertManager
            _predictive_alerts = PredictiveAlertManager()
        except ImportError:
            _predictive_alerts = None
    return _predictive_alerts

def get_tier_calibrator():
    """Get the tier calibrator singleton (lazy load)."""
    global _tier_calibrator
    if _tier_calibrator is None:
        try:
            from app.training.tier_calibrator import TierCalibrator
            _tier_calibrator = TierCalibrator()
        except ImportError:
            _tier_calibrator = None
    return _tier_calibrator


# Board priority overrides from unified_loop.yaml
# 0=CRITICAL, 1=HIGH, 2=MEDIUM, 3=LOW (lower value = higher priority)
_board_priority_cache: dict[str, int] | None = None
_board_priority_cache_time: float = 0


def get_board_priority_overrides() -> dict[str, int]:
    """Load board priority overrides from config, cached for 60 seconds.

    Returns dict mapping config keys (e.g., 'hexagonal_2p') to priority levels.
    Priority levels: 0=CRITICAL, 1=HIGH, 2=MEDIUM, 3=LOW
    """
    global _board_priority_cache, _board_priority_cache_time
    now = time.time()

    # Return cached value if fresh (60 second TTL)
    if _board_priority_cache is not None and now - _board_priority_cache_time < 60:
        return _board_priority_cache

    try:
        import yaml
        config_path = Path(__file__).parent.parent / "config" / "unified_loop.yaml"
        if config_path.exists():
            with open(config_path) as f:
                yaml_config = yaml.safe_load(f)
            selfplay_config = yaml_config.get("selfplay", {})
            overrides = selfplay_config.get("board_priority_overrides", {})
            # Convert config keys like "hexagonal_2p" -> priority int
            _board_priority_cache = {k: int(v) for k, v in overrides.items()}
            _board_priority_cache_time = now
            return _board_priority_cache
    except Exception:
        pass

    # Default: empty (no overrides)
    return {}


# Add project root to path for scripts.lib imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scripts.lib.file_formats import open_jsonl_file
from scripts.lib.logging_config import setup_script_logging

logger = setup_script_logging("p2p_orchestrator")


@contextlib.contextmanager
def db_connection(db_path: str | Path, timeout: float = 30.0) -> Generator[sqlite3.Connection]:
    """Context manager for SQLite connections to prevent leaks.

    Usage:
        with db_connection(db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(...)
    """
    conn = None
    try:
        conn = sqlite3.connect(str(db_path), timeout=timeout)
        yield conn
    finally:
        if conn:
            with contextlib.suppress(Exception):
                conn.close()


# Centralized ramdrive utilities for auto-detection
# Shared database integrity utilities
from app.db.integrity import (
    check_and_repair_databases,
)

# Circuit breaker for fault-tolerant network operations
from app.distributed.circuit_breaker import (
    CircuitState,
    get_circuit_registry,
)
from app.utils.ramdrive import (
    RamdriveSyncer,
    get_system_resources,
    log_storage_recommendation,
    should_use_ramdrive,
)
from scripts.p2p.cluster_config import (
    get_cluster_config,
    get_webhook_urls,
)

# Import constants from the refactored module (Phase 2 refactoring - consolidated)
from scripts.p2p.constants import (
    ADVERTISE_HOST_ENV,
    ADVERTISE_PORT_ENV,
    AGENT_MODE_ENABLED,
    ARBITER_URL,
    # Auth and build info
    AUTH_TOKEN_ENV,
    AUTH_TOKEN_FILE_ENV,
    AUTO_ASSIGN_ENABLED,
    AUTO_TRAINING_THRESHOLD_MB,
    AUTO_UPDATE_ENABLED,
    AUTO_WORK_BATCH_SIZE,
    BUILD_VERSION_ENV,
    COORDINATOR_URL,
    DATA_MANAGEMENT_INTERVAL,
    DB_EXPORT_THRESHOLD_MB,
    # Network configuration
    DEFAULT_PORT,
    DISCOVERY_INTERVAL,
    DISCOVERY_PORT,
    DISK_CLEANUP_THRESHOLD,
    # Resource thresholds
    DISK_CRITICAL_THRESHOLD,
    DISK_WARNING_THRESHOLD,
    # Dynamic voter management
    DYNAMIC_VOTER_ENABLED,
    DYNAMIC_VOTER_MAX_QUORUM,
    DYNAMIC_VOTER_MIN,
    DYNAMIC_VOTER_TARGET,
    ELECTION_TIMEOUT,
    ELO_K_FACTOR,
    GH200_MAX_SELFPLAY,
    GH200_MIN_SELFPLAY,
    GIT_BRANCH_NAME,
    GIT_REMOTE_NAME,
    # Auto-update settings
    GIT_UPDATE_CHECK_INTERVAL,
    # Safeguards
    GPU_IDLE_RESTART_TIMEOUT,
    GPU_IDLE_THRESHOLD,
    GPU_POWER_RANKINGS,
    GRACEFUL_SHUTDOWN_BEFORE_UPDATE,
    HEARTBEAT_INTERVAL,
    # Connection robustness
    HTTP_CONNECT_TIMEOUT,
    HTTP_TOTAL_TIMEOUT,
    IDLE_CHECK_INTERVAL,
    IDLE_GPU_THRESHOLD,
    IDLE_GRACE_PERIOD,
    # Elo constants (from app.config.thresholds)
    BASELINE_ELO_RANDOM,  # Random AI pinned at 400 Elo
    INITIAL_ELO_RATING,
    JOB_CHECK_INTERVAL,
    LEADER_DEGRADED_STEPDOWN_DELAY,
    LEADER_HEALTH_CHECK_INTERVAL,
    LEADER_LEASE_DURATION,
    LEADER_LEASE_RENEW_INTERVAL,
    LEADER_MIN_RESPONSE_RATE,
    LEADERLESS_TRAINING_TIMEOUT,
    LOAD_AVERAGE_MAX_MULTIPLIER,
    LOAD_MAX_FOR_NEW_JOBS,
    MANIFEST_JSONL_LINECOUNT_CHUNK_BYTES,
    # Data management
    MANIFEST_JSONL_LINECOUNT_MAX_BYTES,
    MANIFEST_JSONL_SAMPLE_BYTES,
    MAX_CONCURRENT_EXPORTS,
    MAX_CONSECUTIVE_FAILURES,
    MAX_DISK_USAGE_PERCENT,
    MAX_GAUNTLET_RUNTIME,
    # Stale process cleanup
    MAX_SELFPLAY_RUNTIME,
    MAX_TOURNAMENT_RUNTIME,
    MAX_TRAINING_RUNTIME,
    MEMORY_CRITICAL_THRESHOLD,
    MEMORY_WARNING_THRESHOLD,
    MIN_GAMES_FOR_SYNC,
    MIN_MEMORY_GB_FOR_TASKS,
    MODEL_SYNC_INTERVAL,
    NAT_BLOCKED_PROBE_INTERVAL,
    NAT_BLOCKED_PROBE_TIMEOUT,
    NAT_BLOCKED_RECOVERY_TIMEOUT,
    NAT_EXTERNAL_IP_CACHE_TTL,
    NAT_HOLE_PUNCH_RETRY_COUNT,
    # NAT/Relay settings
    NAT_INBOUND_HEARTBEAT_STALE_SECONDS,
    NAT_RELAY_PREFERENCE_THRESHOLD,
    NAT_STUN_LIKE_PROBE_INTERVAL,
    NAT_SYMMETRIC_DETECTION_ENABLED,
    P2P_DATA_SYNC_BASE,
    P2P_DATA_SYNC_MAX,
    P2P_DATA_SYNC_MIN,
    P2P_MODEL_SYNC_BASE,
    P2P_MODEL_SYNC_MAX,
    P2P_MODEL_SYNC_MIN,
    P2P_SYNC_BACKOFF_FACTOR,
    P2P_SYNC_SPEEDUP_FACTOR,
    P2P_TRAINING_DB_SYNC_BASE,
    P2P_TRAINING_DB_SYNC_MAX,
    P2P_TRAINING_DB_SYNC_MIN,
    PEER_BOOTSTRAP_INTERVAL,
    PEER_BOOTSTRAP_MIN_PEERS,
    PEER_PURGE_AFTER_SECONDS,
    PEER_RETIRE_AFTER_SECONDS,
    PEER_TIMEOUT,
    RELAY_COMMAND_MAX_ATTEMPTS,
    RELAY_COMMAND_MAX_BATCH,
    RELAY_COMMAND_TTL_SECONDS,
    RELAY_HEARTBEAT_INTERVAL,
    RELAY_MAX_PENDING_START_JOBS,
    RETRY_DEAD_NODE_INTERVAL,
    RETRY_RETIRED_NODE_INTERVAL,
    RUNAWAY_SELFPLAY_PROCESS_THRESHOLD,
    SPAWN_RATE_LIMIT_PER_MINUTE,
    STALE_PROCESS_CHECK_INTERVAL,
    STALE_PROCESS_PATTERNS,
    STARTUP_JSONL_GRACE_PERIOD_SECONDS,
    # State directory
    STATE_DIR,
    TAILSCALE_CGNAT_NETWORK,
    TARGET_GPU_UTIL_MAX,
    # GPU configuration
    TARGET_GPU_UTIL_MIN,
    TRAINING_DATA_SYNC_THRESHOLD_MB,
    # Training node sync
    TRAINING_NODE_COUNT,
    TRAINING_SYNC_INTERVAL,
    # Unified inventory / Idle detection
    UNIFIED_DISCOVERY_INTERVAL,
    VOTER_DEMOTION_FAILURES,
    VOTER_HEALTH_THRESHOLD,
    VOTER_HEARTBEAT_INTERVAL,
    VOTER_HEARTBEAT_TIMEOUT,
    VOTER_MESH_REFRESH_INTERVAL,
    VOTER_MIN_QUORUM,
    VOTER_NAT_RECOVERY_AGGRESSIVE,
    VOTER_PROMOTION_UPTIME,
    # Phase 26: Multi-seed bootstrap and mesh resilience
    BOOTSTRAP_SEEDS,
    MIN_BOOTSTRAP_ATTEMPTS,
    ISOLATED_BOOTSTRAP_INTERVAL,
    MIN_CONNECTED_PEERS,
    # Phase 28: Gossip protocol
    GOSSIP_FANOUT,
    GOSSIP_INTERVAL,
    GOSSIP_MAX_PEER_ENDPOINTS,
    # Phase 27: Peer cache
    PEER_CACHE_TTL_SECONDS,
    PEER_CACHE_MAX_ENTRIES,
    PEER_REPUTATION_ALPHA,
    # Phase 29: Cluster epochs
    INITIAL_CLUSTER_EPOCH,
)
from scripts.p2p.models import (
    ClusterDataManifest,
    ClusterJob,
    ClusterSyncPlan,
    DataFileInfo,
    DataSyncJob,
    DistributedCMAESState,
    DistributedTournamentState,
    ImprovementLoopState,
    NodeDataManifest,
    NodeInfo,
    SSHTournamentRun,
    TrainingJob,
    TrainingThresholds,
)
from scripts.p2p.network import (
    AsyncLockWrapper,
    get_client_session,
)

# Import refactored utilities (Phase 2 refactoring)
from scripts.p2p.resource import (
    check_disk_has_capacity,
)

# Import refactored P2P types and models
# These were extracted from this file for modularity (Phase 1 refactoring)
from scripts.p2p.types import JobType, NodeRole
from scripts.p2p.utils import (
    systemd_notify_ready,
    systemd_notify_watchdog,
)

# Unified resource checking utilities (80% max utilization)
# Includes graceful degradation for dynamic workload management
try:
    from app.utils.resource_guard import (
        LIMITS as RESOURCE_LIMITS,
        OperationPriority,
        check_cpu as unified_check_cpu,
        check_disk_space as unified_check_disk,
        check_memory as unified_check_memory,
        get_degradation_level,
        should_proceed_with_priority,
    )
    HAS_RESOURCE_GUARD = True
except ImportError:
    HAS_RESOURCE_GUARD = False
    unified_check_disk = None
    unified_check_memory = None
    unified_check_cpu = None
    RESOURCE_LIMITS = None
    should_proceed_with_priority = None
    OperationPriority = None
    get_degradation_level = None

# ELO database sync manager for cluster-wide consistency
try:
    from app.tournament.elo_sync_manager import (
        EloSyncManager,
        ensure_elo_synced,
        get_elo_sync_manager,
        sync_elo_after_games,
    )
    HAS_ELO_SYNC = True
except ImportError:
    HAS_ELO_SYNC = False
    EloSyncManager = None
    get_elo_sync_manager = None
    sync_elo_after_games = None
    ensure_elo_synced = None

# Distributed data sync manager for model/data distribution
# Prefer new sync_coordinator, fallback to deprecated data_sync
try:
    from app.distributed.sync_coordinator import SyncCoordinator, full_cluster_sync
    HAS_SYNC_COORDINATOR = True

    def get_sync_coordinator():
        return SyncCoordinator.get_instance()
except ImportError:
    HAS_SYNC_COORDINATOR = False
    SyncCoordinator = None
    full_cluster_sync = None

# Phase 3.1: Curriculum weights integration for selfplay prioritization
try:
    from scripts.unified_loop.curriculum import load_curriculum_weights
    HAS_CURRICULUM_WEIGHTS = True
except ImportError:
    HAS_CURRICULUM_WEIGHTS = False
    load_curriculum_weights = None

# Unified node inventory for multi-CLI discovery (Vast, Tailscale, Lambda, Hetzner)
try:
    from app.coordination.unified_inventory import UnifiedInventory, get_inventory
    HAS_UNIFIED_INVENTORY = True
except ImportError:
    HAS_UNIFIED_INVENTORY = False
    UnifiedInventory = None
    get_inventory = None

# HTTP server imports
try:
    import aiohttp
    from aiohttp import ClientSession, ClientTimeout, web
    HAS_AIOHTTP = True
except ImportError:
    HAS_AIOHTTP = False
    aiohttp = None
    logger.warning("aiohttp not installed. Install with: pip install aiohttp")

# SOCKS proxy support for userspace Tailscale networking
try:
    from aiohttp_socks import ProxyConnector
    HAS_SOCKS = True
except ImportError:
    HAS_SOCKS = False
    ProxyConnector = None

# Get SOCKS proxy from environment (e.g., socks5://localhost:1055)
SOCKS_PROXY = os.environ.get("RINGRIFT_SOCKS_PROXY", "")

# Systemd watchdog support for service health monitoring
# When running under systemd with WatchdogSec set, we need to periodically
# notify systemd that the service is healthy. If we miss the deadline,
# systemd will restart the service.
try:
    import sdnotify
    SYSTEMD_NOTIFIER = sdnotify.SystemdNotifier()
    HAS_SYSTEMD = True
except ImportError:
    SYSTEMD_NOTIFIER = None
    HAS_SYSTEMD = False


# ============================================
# Utilities (Refactored - Phase 2)
# ============================================
# The following utilities have been moved to scripts/p2p/ for modularity:
# - systemd_notify_watchdog, systemd_notify_ready (scripts/p2p/utils.py)
# - AsyncLockWrapper, get_client_session (scripts/p2p/network.py)
# - check_peer_circuit, record_peer_success, record_peer_failure (scripts/p2p/network.py)
# - peer_request (scripts/p2p/network.py)
# - get_disk_usage_percent, check_disk_has_capacity, check_all_resources (scripts/p2p/resource.py)
#
# They are imported at the top of this file for backward compatibility.
# ============================================

# Dynamic host registry for IP auto-update
try:
    from app.distributed.dynamic_registry import (
        NodeState,
        get_registry,
    )
    HAS_DYNAMIC_REGISTRY = True
except ImportError:
    HAS_DYNAMIC_REGISTRY = False
    get_registry = None
    NodeState = None

# Hybrid transport layer for HTTP/SSH fallback (self-healing Vast connectivity)
try:
    from app.distributed.hybrid_transport import (
        HybridTransport,
        diagnose_node_connectivity,
        get_hybrid_transport,
    )
    from app.distributed.ssh_transport import (
        SSHTransport,
        get_ssh_transport,
        probe_vast_nodes_via_ssh,
    )
    HAS_HYBRID_TRANSPORT = True
except ImportError:
    HAS_HYBRID_TRANSPORT = False
    HybridTransport = None
    get_hybrid_transport = None
    diagnose_node_connectivity = None
    SSHTransport = None
    get_ssh_transport = None
    probe_vast_nodes_via_ssh = None

# Improvement cycle manager for automated training
# Note: ImprovementCycleManager is deprecated - unified_ai_loop.py is the new approach
# Kept for backwards compatibility with older scripts
try:
    from scripts.improvement_cycle_manager import ImprovementCycleManager
    HAS_IMPROVEMENT_MANAGER = True
except ImportError:
    # Fallback - deprecated archive location removed in 2025-12
    HAS_IMPROVEMENT_MANAGER = False
    ImprovementCycleManager = None

# Task coordination safeguards - prevents runaway spawning
try:
    from app.coordination.safeguards import Safeguards, check_before_spawn
    HAS_SAFEGUARDS = True
    _safeguards = Safeguards.get_instance()
except ImportError:
    HAS_SAFEGUARDS = False
    _safeguards = None
    def check_before_spawn(task_type, node_id):
        return True, ""

# New coordination features: OrchestratorRole, backpressure, sync_lock, bandwidth
try:
    from app.coordination import (
        NodeResources,
        # Orchestrator role management (SQLite-backed with heartbeat)
        OrchestratorRole,
        # Queue backpressure
        QueueType,
        # Resource optimizer for cluster-wide PID-controlled optimization
        ResourceOptimizer,
        TransferPriority,
        acquire_orchestrator_role,
        get_cluster_utilization,
        get_host_targets,
        get_optimal_concurrency,
        get_resource_optimizer,
        # Resource targets for unified utilization management
        get_resource_targets,
        get_target_job_count,
        get_throttle_factor,
        record_utilization,
        release_bandwidth,
        release_orchestrator_role,
        # Bandwidth management
        request_bandwidth,
        should_scale_down,
        should_scale_up,
        should_stop_production,
        should_throttle_production,
        # Sync mutex for data transfer coordination
        sync_lock,
    )

    # Import rate negotiation functions for cooperative utilization (60-80% target)
    from app.coordination.resource_optimizer import (
        apply_feedback_adjustment,
        get_config_weights,
        get_current_selfplay_rate,
        get_hybrid_selfplay_limits,
        get_max_cpu_only_selfplay,
        # Hardware-aware selfplay limits (single source of truth)
        get_max_selfplay_for_node,
        get_utilization_status,
        negotiate_selfplay_rate,
        update_config_weights,
    )
    HAS_RATE_NEGOTIATION = True
    HAS_NEW_COORDINATION = True
    HAS_HW_AWARE_LIMITS = True
    # Get targets from unified source
    _unified_targets = get_resource_targets()
except ImportError:
    HAS_NEW_COORDINATION = False
    HAS_RATE_NEGOTIATION = False
    HAS_HW_AWARE_LIMITS = False
    OrchestratorRole = None
    _unified_targets = None
    negotiate_selfplay_rate = None
    get_current_selfplay_rate = None
    apply_feedback_adjustment = None
    get_utilization_status = None
    update_config_weights = None
    get_config_weights = None
    get_max_selfplay_for_node = None
    get_hybrid_selfplay_limits = None
    get_max_cpu_only_selfplay = None

# P2P-integrated monitoring management
try:
    from app.monitoring.p2p_monitoring import MonitoringManager
    HAS_P2P_MONITORING = True
except ImportError:
    HAS_P2P_MONITORING = False
    MonitoringManager = None

# Model sync across cluster
try:
    from scripts.sync_models import (
        HOSTS_MODULE_AVAILABLE as HAS_HOSTS_FOR_SYNC,
        ClusterModelState,
        scan_cluster as scan_cluster_models,
        sync_missing_models,
    )
    # Also import load_remote_hosts for scanning
    if HAS_HOSTS_FOR_SYNC:
        from app.distributed.hosts import filter_ready_hosts, load_remote_hosts
    HAS_MODEL_SYNC = True
except ImportError:
    HAS_MODEL_SYNC = False
    scan_cluster_models = None
    sync_missing_models = None
    ClusterModelState = None
    HAS_HOSTS_FOR_SYNC = False
    load_remote_hosts = None
    filter_ready_hosts = None

# PFSP (Prioritized Fictitious Self-Play) opponent pool
try:
    from app.training.advanced_training import (
        CMAESAutoTuner,
        OpponentStats,
        PFSPOpponentPool,
        PlateauConfig,
    )
    HAS_PFSP = True
except ImportError:
    HAS_PFSP = False
    PFSPOpponentPool = None
    OpponentStats = None
    CMAESAutoTuner = None
    PlateauConfig = None

# ============================================
# Configuration
# ============================================
# NOTE: All constants have been consolidated into scripts/p2p/constants.py
# and are imported at the top of this file (Phase 2 refactoring).
# See scripts/p2p/constants.py for configuration values and documentation.
# ============================================


# ============================================
# Types and Models (Refactored)
# ============================================
# The following types have been moved to scripts/p2p/ for modularity:
# - NodeRole, JobType (scripts/p2p/types.py)
# - NodeInfo, ClusterJob, DistributedCMAESState, DistributedTournamentState,
#   SSHTournamentRun, ImprovementLoopState, TrainingJob, TrainingThresholds,
#   DataFileInfo, NodeDataManifest, ClusterDataManifest, DataSyncJob,
#   ClusterSyncPlan (scripts/p2p/models.py)
#
# They are imported at the top of this file for backward compatibility.
# ============================================


class WebhookNotifier:
    """Sends alerts to Slack/Discord webhooks for important events.

    Configure via environment variables:
    - RINGRIFT_SLACK_WEBHOOK: Slack incoming webhook URL
    - RINGRIFT_DISCORD_WEBHOOK: Discord webhook URL
    - RINGRIFT_ALERT_LEVEL: Minimum level to alert (debug/info/warning/error) default: warning
    """

    LEVELS = {"debug": 0, "info": 1, "warning": 2, "error": 3}

    def __init__(self):
        # Try environment variables first, then fall back to cluster.yaml
        self.slack_webhook = os.environ.get("RINGRIFT_SLACK_WEBHOOK", "")
        self.discord_webhook = os.environ.get("RINGRIFT_DISCORD_WEBHOOK", "")

        # Fall back to cluster.yaml config if env vars not set
        if not self.slack_webhook or not self.discord_webhook:
            try:
                yaml_webhooks = get_webhook_urls()
                if not self.slack_webhook and "slack" in yaml_webhooks:
                    self.slack_webhook = yaml_webhooks["slack"]
                if not self.discord_webhook and "discord" in yaml_webhooks:
                    self.discord_webhook = yaml_webhooks["discord"]
            except Exception:
                pass  # Ignore config loading errors

        self.min_level = self.LEVELS.get(
            os.environ.get("RINGRIFT_ALERT_LEVEL", "warning").lower(), 2
        )
        self._session: ClientSession | None = None
        self._last_alert: dict[str, float] = {}  # Throttle repeated alerts
        self._throttle_seconds = 300  # 5 minutes between duplicate alerts

    async def _get_session(self) -> ClientSession:
        if self._session is None or self._session.closed:
            self._session = ClientSession(timeout=ClientTimeout(total=10))
        return self._session

    def _should_throttle(self, alert_key: str) -> bool:
        """Check if this alert should be throttled (duplicate within window)."""
        now = time.time()
        if alert_key in self._last_alert and now - self._last_alert[alert_key] < self._throttle_seconds:
            return True
        self._last_alert[alert_key] = now
        return False

    async def send(
        self,
        title: str,
        message: str,
        level: str = "warning",
        fields: dict[str, str] | None = None,
        node_id: str = "",
    ):
        """Send an alert to configured webhooks.

        Args:
            title: Alert title/subject
            message: Alert body text
            level: debug/info/warning/error
            fields: Additional key-value pairs to include
            node_id: Node ID for deduplication
        """
        if self.LEVELS.get(level, 2) < self.min_level:
            return

        if not self.slack_webhook and not self.discord_webhook:
            return

        # Throttle duplicate alerts
        alert_key = f"{title}:{node_id}"
        if self._should_throttle(alert_key):
            return

        try:
            session = await self._get_session()

            # Color based on level
            colors = {"debug": "#808080", "info": "#36a64f", "warning": "#ff9800", "error": "#ff0000"}
            color = colors.get(level, "#808080")

            # Send to Slack
            if self.slack_webhook:
                slack_fields = []
                if fields:
                    for k, v in fields.items():
                        slack_fields.append({"title": k, "value": str(v), "short": True})

                slack_payload = {
                    "attachments": [{
                        "color": color,
                        "title": f"[{level.upper()}] {title}",
                        "text": message,
                        "fields": slack_fields,
                        "footer": f"RingRift AI | {node_id}" if node_id else "RingRift AI",
                        "ts": int(time.time()),
                    }]
                }
                try:
                    async with session.post(self.slack_webhook, json=slack_payload) as resp:
                        if resp.status != 200:
                            logger.warning(f"[Webhook] Slack alert failed: {resp.status}")
                except Exception as e:
                    logger.error(f"[Webhook] Slack error: {e}")

            # Send to Discord
            if self.discord_webhook:
                discord_fields = []
                if fields:
                    for k, v in fields.items():
                        discord_fields.append({"name": k, "value": str(v), "inline": True})

                discord_payload = {
                    "embeds": [{
                        "title": f"[{level.upper()}] {title}",
                        "description": message,
                        "color": int(color.lstrip("#"), 16),
                        "fields": discord_fields,
                        "footer": {"text": f"RingRift AI | {node_id}" if node_id else "RingRift AI"},
                        "timestamp": datetime.utcnow().isoformat(),
                    }]
                }
                try:
                    async with session.post(self.discord_webhook, json=discord_payload) as resp:
                        if resp.status not in (200, 204):
                            logger.warning(f"[Webhook] Discord alert failed: {resp.status}")
                except Exception as e:
                    logger.error(f"[Webhook] Discord error: {e}")

        except Exception as e:
            logger.error(f"[Webhook] Alert send error: {e}")

    async def close(self):
        if self._session and not self._session.closed:
            await self._session.close()


class P2POrchestrator:
    """Main P2P orchestrator class that runs on each node."""

    def __init__(
        self,
        node_id: str,
        host: str = "0.0.0.0",
        port: int = DEFAULT_PORT,
        known_peers: list[str] | None = None,
        relay_peers: list[str] | None = None,
        ringrift_path: str | None = None,
        advertise_host: str | None = None,
        advertise_port: int | None = None,
        auth_token: str | None = None,
        require_auth: bool = False,
        storage_type: str = "auto",  # "disk", "ramdrive", or "auto"
        sync_to_disk_interval: int = 300,  # Sync ramdrive to disk every N seconds
    ):
        self.node_id = node_id
        self.host = host
        self.port = port

        # Phase 26: Multi-seed bootstrap - merge CLI peers with hardcoded seeds
        # Priority: CLI peers first, then hardcoded seeds
        # Shuffle seeds to distribute load across bootstrap attempts
        import random
        cli_peers = known_peers or []
        merged_seeds = list(cli_peers)  # CLI peers have highest priority
        for seed in BOOTSTRAP_SEEDS:
            if seed not in merged_seeds:
                merged_seeds.append(seed)
        # Shuffle only the hardcoded portion to avoid overloading any single seed
        if len(merged_seeds) > len(cli_peers):
            hardcoded_portion = merged_seeds[len(cli_peers):]
            random.shuffle(hardcoded_portion)
            merged_seeds = merged_seeds[:len(cli_peers)] + hardcoded_portion
        self.known_peers = merged_seeds
        self.bootstrap_seeds = list(BOOTSTRAP_SEEDS)  # Store original for reference
        logger.info(f"Bootstrap seeds: {len(cli_peers)} CLI + {len(BOOTSTRAP_SEEDS)} hardcoded = {len(self.known_peers)} total")

        # Peers that should always receive relay heartbeats (for NAT-blocked nodes)
        self.relay_peers: set[str] = set(relay_peers or [])
        self.ringrift_path = ringrift_path or self._detect_ringrift_path()

        # Phase 29: Cluster epoch tracking for split-brain resolution
        self._cluster_epoch: int = INITIAL_CLUSTER_EPOCH
        # Gossip-learned peer endpoints (Phase 28)
        self._gossip_learned_endpoints: dict[str, dict[str, Any]] = {}

        # Storage configuration: "disk", "ramdrive", or "auto" (detected)
        self.sync_to_disk_interval = sync_to_disk_interval
        self.ramdrive_path = "/dev/shm/ringrift/data"  # Standard ramdrive location
        self.ramdrive_syncer: RamdriveSyncer | None = None

        # Resolve "auto" storage type based on system resources
        if storage_type == "auto":
            resources = get_system_resources()
            if should_use_ramdrive():
                self.storage_type = "ramdrive"
                logger.info(f"Auto-detected storage: RAMDRIVE "
                      f"(RAM: {resources.total_ram_gb:.0f}GB, "
                      f"Disk: {resources.free_disk_gb:.0f}GB free / {resources.disk_usage_percent:.0f}% used)")
            else:
                self.storage_type = "disk"
                logger.info(f"Auto-detected storage: DISK "
                      f"(RAM: {resources.total_ram_gb:.0f}GB, "
                      f"Disk: {resources.free_disk_gb:.0f}GB free / {resources.disk_usage_percent:.0f}% used)")
            log_storage_recommendation()
        else:
            self.storage_type = storage_type
        # Git 2.35+ enforces safe.directory for repos with different ownership.
        # Many nodes run the orchestrator as root against a checkout owned by
        # another user (e.g. ubuntu), so always provide a safe.directory override
        # for all git operations.
        self._git_safe_directory = os.path.abspath(self.ringrift_path)
        self.build_version = self._detect_build_version()
        self.start_time = time.time()
        self.last_peer_bootstrap = 0.0

        # Public endpoint peers should use to reach us. Peers learn our host from
        # the heartbeat socket address, but the port must be self-reported. This
        # matters for port-mapped environments like Vast.ai.
        self.advertise_host = (advertise_host or os.environ.get(ADVERTISE_HOST_ENV, "")).strip()
        if not self.advertise_host:
            # Prefer a stable mesh address (Tailscale) when available so nodes
            # behind NAT remain reachable and the cluster converges on a single
            # view of peer endpoints.
            ts_ip = self._get_tailscale_ip()
            self.advertise_host = ts_ip or self._get_local_ip()
        self.advertise_port = advertise_port if advertise_port is not None else self._infer_advertise_port()

        # Optional auth token used to protect mutating endpoints and cluster control.
        # Default is allow-all unless a token is configured.
        env_token = (os.environ.get(AUTH_TOKEN_ENV, "")).strip()
        token_from_arg = (auth_token or "").strip()
        token = token_from_arg or env_token

        if not token:
            token_file = (os.environ.get(AUTH_TOKEN_FILE_ENV, "")).strip()
            if token_file:
                try:
                    token = Path(token_file).read_text().strip()
                except Exception as e:
                    logger.info(f"Auth: failed to read {AUTH_TOKEN_FILE_ENV}={token_file}: {e}")

        self.auth_token = token.strip()
        self.require_auth = bool(require_auth)
        if self.require_auth and not self.auth_token:
            raise ValueError(
                f"--require-auth set but {AUTH_TOKEN_ENV}/{AUTH_TOKEN_FILE_ENV}/--auth-token is empty"
            )

        # Optional split-brain mitigation: require a majority of "voter" nodes
        # to be visible before assuming or renewing leadership.
        #
        # Voters can be configured via:
        # - env: RINGRIFT_P2P_VOTERS="node-a,node-b,..."
        # - ai-service/config/distributed_hosts.yaml: per-host `p2p_voter: true`
        self.voter_config_source: str = "none"  # env|config|state|learned|none
        self.voter_node_ids: list[str] = self._load_voter_node_ids()
        # SIMPLIFIED QUORUM: Fixed at 3 voters (or less if fewer voters exist)
        self.voter_quorum_size: int = min(VOTER_MIN_QUORUM, len(self.voter_node_ids)) if self.voter_node_ids else 0
        if self.voter_node_ids:
            print(
                f"[P2P] Voter quorum enabled: voters={len(self.voter_node_ids)}, "
                f"quorum={self.voter_quorum_size} ({', '.join(self.voter_node_ids)})"
            )

        # Node state
        self.role = NodeRole.FOLLOWER
        self.leader_id: str | None = None
        self.verbose = bool(os.environ.get("RINGRIFT_P2P_VERBOSE", "").strip())
        self.peers: dict[str, NodeInfo] = {}
        self.local_jobs: dict[str, ClusterJob] = {}
        self.active_jobs: dict[str, dict[str, Any]] = {}  # Track running jobs by type (selfplay, training, etc.)

        # Distributed job state tracking (leader-only)
        self.distributed_cmaes_state: dict[str, DistributedCMAESState] = {}
        self.distributed_tournament_state: dict[str, DistributedTournamentState] = {}
        self.ssh_tournament_runs: dict[str, SSHTournamentRun] = {}
        self.improvement_loop_state: dict[str, ImprovementLoopState] = {}
        # Limit CPU-heavy CMA-ES local evaluations to avoid runaway process
        # explosions that can starve the orchestrator (especially on relay hubs).
        try:
            raw = (os.environ.get("RINGRIFT_P2P_MAX_CONCURRENT_CMAES_EVALS", "") or "").strip()
            self.max_concurrent_cmaes_evals = max(1, int(raw)) if raw else 2
        except Exception:
            self.max_concurrent_cmaes_evals = 2
        self._cmaes_eval_semaphore = asyncio.Semaphore(int(self.max_concurrent_cmaes_evals))

        # Tournament match semaphore - limit concurrent Elo calibration matches to prevent OOM
        # Each match can potentially load neural networks which use significant memory
        # NOTE: Set to None here, created lazily in async context to avoid event loop issues
        self._tournament_match_semaphore: asyncio.Semaphore | None = None

        # Phase 2: Distributed data sync state
        self.local_data_manifest: NodeDataManifest | None = None
        self.cluster_data_manifest: ClusterDataManifest | None = None  # Leader-only
        self.manifest_collection_interval = 300.0  # Collect manifests every 5 minutes
        self.last_manifest_collection = 0.0

        # Dashboard/selfplay stats history (leader-only). Stored in-memory to
        # enable lightweight throughput charts without adding DB migrations.
        self.selfplay_stats_history: list[dict[str, Any]] = []
        self.selfplay_stats_history_max_samples: int = 288  # ~24h @ 5-min cadence

        # Canonical gate jobs (leader-only): dashboard-triggered runs of
        # scripts/generate_canonical_selfplay.py.
        self.canonical_gate_jobs: dict[str, dict[str, Any]] = {}
        self.canonical_gate_jobs_lock = threading.RLock()

        # Phase 2: P2P rsync coordination state
        self.active_sync_jobs: dict[str, DataSyncJob] = {}
        self.current_sync_plan: ClusterSyncPlan | None = None  # Leader-only
        self.pending_sync_requests: list[dict[str, Any]] = []  # Requests from non-leader nodes
        self.sync_in_progress = False
        self.last_sync_time = 0.0
        self.auto_sync_interval = 600.0  # Auto-sync every 10 minutes when data is missing

        # Training node priority sync state (leader-only)
        self.training_sync_interval = TRAINING_SYNC_INTERVAL
        self.last_training_sync_time = 0.0
        self.training_nodes_cache: list[str] = []  # Cached list of top GPU nodes
        self.training_nodes_cache_time = 0.0
        self.games_synced_to_training: dict[str, int] = {}  # node_id -> last synced game count

        # Circuit breaker for fault-tolerant peer communication
        self._circuit_registry = get_circuit_registry()

        # Phase 3: Training pipeline state (leader-only)
        self.training_jobs: dict[str, TrainingJob] = {}
        self.training_thresholds: TrainingThresholds = TrainingThresholds()
        self.last_training_check: float = 0.0
        self.training_check_interval: float = 300.0  # Check every 5 minutes
        self.games_at_last_nnue_train: dict[str, int] = {}  # board_type -> game_count
        self.games_at_last_cmaes_train: dict[str, int] = {}

        # Phase 5: Automated improvement cycle manager (leader-only)
        self.improvement_cycle_manager: ImprovementCycleManager | None = None
        if HAS_IMPROVEMENT_MANAGER:
            try:
                self.improvement_cycle_manager = ImprovementCycleManager(
                    db_path=STATE_DIR / f"{node_id}_improvement.db",
                    ringrift_path=self.ringrift_path,
                )
                logger.info("ImprovementCycleManager initialized")
            except Exception as e:
                logger.error(f"Failed to initialize ImprovementCycleManager: {e}")
        self.last_improvement_cycle_check: float = 0.0

        # P2P-integrated monitoring (leader starts Prometheus/Grafana)
        self.monitoring_manager: MonitoringManager | None = None
        if HAS_P2P_MONITORING:
            try:
                self.monitoring_manager = MonitoringManager(
                    node_id=node_id,
                    prometheus_port=9090,
                    grafana_port=3000,
                    config_dir=Path(self.ringrift_path) / "monitoring",
                )
                logger.info("MonitoringManager initialized")
            except Exception as e:
                logger.error(f"Failed to initialize MonitoringManager: {e}")
        self._monitoring_was_leader = False  # Track leadership changes
        self.improvement_cycle_check_interval: float = 600.0  # Check every 10 minutes

        # P2P Auto-Deployer (leader-only): ensures P2P runs on all cluster nodes
        self.p2p_auto_deployer: P2PAutoDeployer | None = None
        self._auto_deployer_task: asyncio.Task | None = None

        # Webhook notifications for alerts
        self.notifier = WebhookNotifier()

        # Diversity tracking metrics
        self.diversity_metrics = {
            "games_by_engine_mode": {},      # engine_mode -> count
            "games_by_board_config": {},     # "board_players" -> count
            "games_by_difficulty": {},       # difficulty -> count
            "asymmetric_games": 0,           # count of asymmetric games scheduled
            "symmetric_games": 0,            # count of symmetric games scheduled
            "training_triggers": 0,          # count of training triggers
            "cmaes_triggers": 0,             # count of CMA-ES triggers
            "promotions": 0,                 # count of model promotions
            "rollbacks": 0,                  # count of rollbacks
            "last_reset": time.time(),       # when metrics were last reset
        }

        # === CRITICAL SELF-IMPROVEMENT LOOP METRICS ===
        # Training progress tracking (populated by training callbacks)
        self.training_metrics: dict[str, dict[str, float]] = {}  # config -> {loss, val_loss, epoch}

        # Selfplay throughput tracking
        self.selfplay_throughput: dict[str, float] = {}  # config -> games/hour

        # Cost efficiency metrics
        self.cost_metrics: dict[str, float] = {
            "gpu_hours_total": 0.0,
            "estimated_cost_usd": 0.0,
            "elo_per_gpu_hour": 0.0,
        }

        # Promotion quality metrics
        self.promotion_metrics: dict[str, Any] = {
            "success_rate": 0.0,
            "avg_elo_gain": 0.0,
            "rejections": {},  # reason -> count
            "total_attempts": 0,
            "successful": 0,
        }

        # LEARNED LESSONS - Stuck job detection (leader-only)
        # Track when each node's GPU first went idle with running jobs
        self.gpu_idle_since: dict[str, float] = {}  # node_id -> timestamp when GPU went idle

        # A/B Testing Framework - Compare models head-to-head with statistical significance
        # Key: test_id (UUID), Value: ABTestState dict
        self.ab_tests: dict[str, dict[str, Any]] = {}
        self.ab_test_lock = threading.RLock()

        # Elo Sync Manager - Keeps unified_elo.db consistent across cluster
        self.elo_sync_manager: EloSyncManager | None = None
        if HAS_ELO_SYNC:
            try:
                db_path = Path(self.ringrift_path) / "ai-service" / "data" / "unified_elo.db"
                # Use env var for coordinator, fallback to nebius-backbone-1 (stable backbone node)
                elo_coordinator = os.environ.get("RINGRIFT_ELO_COORDINATOR", "nebius-backbone-1")
                self.elo_sync_manager = EloSyncManager(
                    db_path=db_path,
                    coordinator_host=elo_coordinator,
                    sync_interval=300,  # Sync every 5 minutes
                )
                logger.info(f"EloSyncManager initialized (db: {db_path})")
            except Exception as e:
                logger.error(f"Failed to initialize EloSyncManager: {e}")

        # Queue Populator - Maintains 50+ work items until 2000 Elo target met
        self._queue_populator: QueuePopulator | None = None

        # PFSP (Prioritized Fictitious Self-Play) opponent pool (leader-only)
        # Maintains a pool of historical models weighted by difficulty for diverse training
        self.pfsp_pools: dict[str, Any] = {}  # config_key -> PFSPOpponentPool
        if HAS_PFSP:
            try:
                for config_key in ["square8_2p", "square8_4p", "hex8_2p", "hexagonal_2p"]:
                    self.pfsp_pools[config_key] = PFSPOpponentPool(
                        max_pool_size=30,
                        hard_opponent_weight=0.6,
                        diversity_weight=0.25,
                        recency_weight=0.15,
                    )
                logger.info(f"PFSP opponent pools initialized for {len(self.pfsp_pools)} configs")
            except Exception as e:
                logger.error(f"Failed to initialize PFSP pools: {e}")

        # CMA-ES Auto-Tuner (leader-only)
        # Automatically triggers hyperparameter optimization when Elo plateaus
        self.cmaes_auto_tuners: dict[str, Any] = {}  # config_key -> CMAESAutoTuner
        self.last_cmaes_elo: dict[str, float] = {}  # config_key -> last recorded Elo
        if HAS_PFSP and CMAESAutoTuner:
            try:
                for config_key in ["square8_2p", "square8_4p", "hex8_2p", "hexagonal_2p"]:
                    parts = config_key.rsplit("_", 1)
                    board_type = parts[0]
                    num_players = int(parts[1].replace("p", ""))
                    plateau_cfg = PlateauConfig(patience=10)
                    self.cmaes_auto_tuners[config_key] = CMAESAutoTuner(
                        board_type=board_type,
                        num_players=num_players,
                        plateau_config=plateau_cfg,
                        min_epochs_between_tuning=50,
                        max_auto_tunes=3,
                    )
                logger.info(f"CMA-ES auto-tuners initialized for {len(self.cmaes_auto_tuners)} configs")
            except Exception as e:
                logger.error(f"Failed to initialize CMA-ES auto-tuners: {e}")

        # Locks for thread safety
        # Use RLock (reentrant lock) to allow nested acquisitions from same thread
        # This prevents deadlocks when helper methods like _select_best_relay are
        # called while already holding the lock
        self.peers_lock = threading.RLock()
        self.jobs_lock = threading.RLock()
        self.manifest_lock = threading.RLock()
        self.sync_lock = threading.RLock()
        self.training_lock = threading.RLock()
        self.ssh_tournament_lock = threading.RLock()
        self.relay_lock = threading.RLock()

        # State persistence
        self.db_path = STATE_DIR / f"{node_id}_state.db"
        self._init_database()
        self._load_cluster_epoch()  # Phase 29: Load persisted cluster epoch

        # Event flags
        self.running = True
        self.election_in_progress = False
        self.last_election_attempt: float = 0.0

        # LEARNED LESSONS - Lease-based leadership to prevent split-brain
        # Leader must continuously renew lease; if lease expires, leadership is void
        self.leader_lease_expires: float = 0.0  # timestamp when current leader's lease expires
        self.last_lease_renewal: float = 0.0  # when we last renewed our lease (if leader)
        self.leader_lease_id: str = ""  # unique ID for current leadership term
        # LEADERLESS FALLBACK: Track when we last had a functioning leader.
        # If leaderless for too long, nodes can trigger local training independently.
        self.last_leader_seen: float = time.time()  # When we last saw a functioning leader
        self.last_local_training_fallback: float = 0.0  # When we last triggered local training fallback

        # Voter-backed lease grants (split-brain resistance).
        #
        # When quorum gating is enabled, voters act as a lightweight consensus
        # group by granting an exclusive leader lease to a single node at a time.
        # A leader must renew its lease with a quorum of voters; otherwise it
        # steps down. This prevents split-brain even if multiple nodes think
        # they are eligible leaders.
        self.voter_grant_leader_id: str = ""
        self.voter_grant_lease_id: str = ""
        self.voter_grant_expires: float = 0.0

        # Job completion tracking for auto-restart
        self.completed_jobs: dict[str, float] = {}  # node_id -> last job completion time
        self.jobs_started_at: dict[str, dict[str, float]] = {}  # node_id -> {job_id: start_time}

        # NAT/relay support (for nodes without inbound connectivity).
        # NAT-blocked nodes poll a relay endpoint for commands; the leader enqueues
        # commands keyed by node_id.
        self.last_inbound_heartbeat: float = 0.0
        self.last_relay_heartbeat: float = 0.0
        self.relay_command_queue: dict[str, list[dict[str, Any]]] = {}
        self.pending_relay_acks: set[str] = set()
        self.pending_relay_results: list[dict[str, Any]] = []
        self.relay_command_attempts: dict[str, int] = {}

        # SAFEGUARDS - Rate limiting and coordinator integration (added 2025-12-15)
        self.spawn_timestamps: list[float] = []  # Timestamps of recent process spawns
        self.agent_mode = AGENT_MODE_ENABLED
        self.coordinator_url = COORDINATOR_URL
        self.last_coordinator_check: float = 0.0
        self.coordinator_available: bool = False
        logger.info(f"Safeguards: rate_limit={SPAWN_RATE_LIMIT_PER_MINUTE}/min, "
              f"load_max={LOAD_AVERAGE_MAX_MULTIPLIER}x, agent_mode={self.agent_mode}")

        # Load persisted state
        self._load_state()
        if self.leader_id == self.node_id:
            self.role = NodeRole.LEADER

        # Self info
        self.self_info = self._create_self_info()

        print(
            f"[P2P] Initialized node {node_id} on {host}:{port} "
            f"(advertise {self.advertise_host}:{self.advertise_port})"
        )
        logger.info(f"RingRift path: {self.ringrift_path}")
        logger.info(f"Version: {self.build_version}")
        logger.info(f"Known peers: {self.known_peers}")
        if self.relay_peers:
            logger.info(f"Relay peers (forced relay mode): {list(self.relay_peers)}")
        if self.auth_token:
            logger.info(f"Auth: enabled via {AUTH_TOKEN_ENV}")
        else:
            logger.info(f"Auth: disabled (set {AUTH_TOKEN_ENV} to enable)")

        # Hybrid transport for HTTP/SSH fallback (self-healing Vast connectivity)
        self.hybrid_transport: HybridTransport | None = None
        if HAS_HYBRID_TRANSPORT:
            try:
                self.hybrid_transport = get_hybrid_transport()
                logger.info("HybridTransport: enabled (HTTP with SSH fallback for Vast)")
            except Exception as e:
                logger.info(f"HybridTransport: failed to initialize: {e}")

    def _is_leader(self) -> bool:
        """Check if this node is the current cluster leader with valid lease."""
        if self.leader_id != self.node_id:
            # Consistency: we should never claim role=leader while leader_id points elsewhere (or is None).
            if self.role == NodeRole.LEADER:
                logger.info("Inconsistent leadership state (role=leader but leader_id!=self); stepping down")
                self.role = NodeRole.FOLLOWER
                self.last_lease_renewal = 0.0
                if not self.leader_id:
                    self.leader_lease_id = ""
                    self.leader_lease_expires = 0.0
                self._release_voter_grant_if_self()
                self._save_state()
                # Only force an election when we have no known leader; otherwise we
                # may already be following a healthy leader and shouldn't flap.
                if not self.leader_id:
                    with contextlib.suppress(RuntimeError):
                        asyncio.get_running_loop().create_task(self._start_election())
            return False
        # Consistency: we should never claim leader_id=self while being a follower/candidate.
        if self.role != NodeRole.LEADER:
            logger.info("Inconsistent leadership state (leader_id=self but role!=leader); clearing leader_id")
            self.role = NodeRole.FOLLOWER
            self.leader_id = None
            self.leader_lease_id = ""
            self.leader_lease_expires = 0.0
            self.last_lease_renewal = 0.0
            self._release_voter_grant_if_self()
            self._save_state()
            with contextlib.suppress(RuntimeError):
                asyncio.get_running_loop().create_task(self._start_election())
            return False

        # LEARNED LESSONS - Lease-based leadership prevents split-brain
        # Must have valid lease to act as leader
        if self.leader_lease_expires > 0 and time.time() >= self.leader_lease_expires:
            logger.info("Leadership lease expired, stepping down")
            self.role = NodeRole.FOLLOWER
            self.leader_id = None
            self.leader_lease_id = ""
            self.leader_lease_expires = 0.0
            self.last_lease_renewal = 0.0
            self._release_voter_grant_if_self()
            self._save_state()
            with contextlib.suppress(RuntimeError):
                asyncio.get_running_loop().create_task(self._start_election())
            return False
        if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
            logger.info("Leadership without voter quorum, stepping down")
            self.role = NodeRole.FOLLOWER
            self.leader_id = None
            self.leader_lease_id = ""
            self.leader_lease_expires = 0.0
            self.last_lease_renewal = 0.0
            self._release_voter_grant_if_self()
            self._save_state()
            with contextlib.suppress(RuntimeError):
                asyncio.get_running_loop().create_task(self._start_election())
            return False
        return True

    @property
    def is_leader(self) -> bool:
        """Property wrapper for _is_leader() method."""
        return self._is_leader()


    # =========================================================================
    # SAFEGUARDS - Load, rate limiting, and coordinator integration
    # =========================================================================

    def _check_spawn_rate_limit(self) -> tuple[bool, str]:
        """Check if we're within the spawn rate limit.

        SAFEGUARD: Prevents runaway process spawning by limiting spawns per minute.

        Returns:
            (can_spawn, reason) - True if within rate limit
        """
        now = time.time()
        # Clean old timestamps (older than 60 seconds)
        self.spawn_timestamps = [t for t in self.spawn_timestamps if now - t < 60]

        if len(self.spawn_timestamps) >= SPAWN_RATE_LIMIT_PER_MINUTE:
            return False, f"Rate limit: {len(self.spawn_timestamps)}/{SPAWN_RATE_LIMIT_PER_MINUTE} spawns in last minute"

        return True, f"Rate OK: {len(self.spawn_timestamps)}/{SPAWN_RATE_LIMIT_PER_MINUTE}"

    def _record_spawn(self) -> None:
        """Record a process spawn for rate limiting."""
        self.spawn_timestamps.append(time.time())

    async def _check_coordinator_available(self) -> bool:
        """Check if the unified coordinator is available.

        SAFEGUARD: In agent mode, defer job decisions to coordinator.

        Returns:
            True if coordinator is reachable
        """
        if not self.coordinator_url:
            return False

        # Cache check for 30 seconds
        now = time.time()
        if now - self.last_coordinator_check < 30:
            return self.coordinator_available

        self.last_coordinator_check = now

        try:
            async with get_client_session(timeout=ClientTimeout(total=5)) as session:
                async with session.get(f"{self.coordinator_url}/api/health") as resp:
                    self.coordinator_available = resp.status == 200
                    if self.coordinator_available:
                        logger.info(f"Coordinator available at {self.coordinator_url}")
                    return self.coordinator_available
        except Exception:
            self.coordinator_available = False
            return False

    def _can_spawn_process(self, reason: str = "job") -> tuple[bool, str]:
        """Combined safeguard check before spawning any process.

        SAFEGUARD: Checks load average, rate limit, and agent mode.

        Args:
            reason: Description of why we want to spawn (for logging)

        Returns:
            (can_spawn, explanation) - True if all checks pass
        """
        # Check 1: Load average
        load_ok, load_reason = self.self_info.check_load_average_safe()
        if not load_ok:
            logger.info(f"BLOCKED spawn ({reason}): {load_reason}")
            return False, load_reason

        # Check 2: Rate limit
        rate_ok, rate_reason = self._check_spawn_rate_limit()
        if not rate_ok:
            logger.info(f"BLOCKED spawn ({reason}): {rate_reason}")
            return False, rate_reason

        # Check 3: Agent mode - if coordinator is available and we're in agent mode,
        # we should not autonomously spawn jobs (let coordinator decide)
        if self.agent_mode and self.coordinator_available:
            msg = "Agent mode: deferring to coordinator"
            logger.info(f"BLOCKED spawn ({reason}): {msg}")
            return False, msg

        # Check 4: Backpressure (new coordination) - if training queue is saturated,
        # don't spawn more selfplay jobs that would produce more data
        if HAS_NEW_COORDINATION and "selfplay" in reason.lower():
            if should_stop_production(QueueType.TRAINING_DATA):
                msg = "Backpressure: training queue at STOP level"
                logger.info(f"BLOCKED spawn ({reason}): {msg}")
                return False, msg
            if should_throttle_production(QueueType.TRAINING_DATA):
                throttle = get_throttle_factor(QueueType.TRAINING_DATA)
                import random
                if random.random() > throttle:
                    msg = f"Backpressure: throttled (factor={throttle:.2f})"
                    logger.info(f"BLOCKED spawn ({reason}): {msg}")
                    return False, msg

        # Check 5: Graceful degradation - don't spawn under heavy resource pressure
        if HAS_RESOURCE_GUARD and get_degradation_level is not None:
            degradation = get_degradation_level()
            if degradation >= 4:  # CRITICAL - resources at/above limits
                msg = f"Graceful degradation: critical resource pressure (level {degradation})"
                logger.info(f"BLOCKED spawn ({reason}): {msg}")
                return False, msg
            elif degradation >= 3:  # HEAVY - only critical ops proceed
                # Selfplay is NORMAL priority, blocked under heavy pressure
                if should_proceed_with_priority is not None and not should_proceed_with_priority(OperationPriority.NORMAL):
                    msg = f"Graceful degradation: heavy resource pressure (level {degradation})"
                    logger.info(f"BLOCKED spawn ({reason}): {msg}")
                    return False, msg

        return True, "All safeguards passed"

    def _detect_build_version(self) -> str:
        env_version = (os.environ.get(BUILD_VERSION_ENV, "") or "").strip()
        if env_version:
            return env_version

        commit = ""
        branch = ""
        try:
            result = subprocess.run(
                self._git_cmd("rev-parse", "--short", "HEAD"),
                cwd=self.ringrift_path,
                capture_output=True,
                text=True,
                timeout=3,
            )
            if result.returncode == 0:
                commit = result.stdout.strip()
        except Exception:
            commit = ""

        try:
            result = subprocess.run(
                self._git_cmd("rev-parse", "--abbrev-ref", "HEAD"),
                cwd=self.ringrift_path,
                capture_output=True,
                text=True,
                timeout=3,
            )
            if result.returncode == 0:
                branch = result.stdout.strip()
        except Exception:
            branch = ""

        if commit and branch:
            return f"{branch}@{commit}"
        return commit or "unknown"

    def _git_cmd(self, *args: str) -> list[str]:
        safe_dir = getattr(self, "_git_safe_directory", "") or os.path.abspath(self.ringrift_path)
        return ["git", "-c", f"safe.directory={safe_dir}", *args]

    def _detect_ringrift_path(self) -> str:
        """Detect the RingRift installation path."""
        # Try common locations
        candidates = [
            Path.home() / "Development" / "RingRift",
            Path.home() / "ringrift",
            Path("/home/ubuntu/ringrift"),
            Path("/root/ringrift"),
        ]
        for path in candidates:
            if (path / "ai-service").exists():
                return str(path)
        return str(Path(__file__).parent.parent.parent)

    def get_data_directory(self) -> Path:
        """Get the data directory path based on storage configuration.

        Returns:
            Path to data directory:
            - ramdrive: /dev/shm/ringrift/data (for disk-constrained Vast instances)
            - disk: {ringrift_path}/ai-service/data (default)

        The ramdrive option uses tmpfs for high-speed I/O and to work around
        limited disk space on some cloud instances. Data stored in ramdrive
        is volatile and should be synced to permanent storage periodically.
        """
        if self.storage_type == "ramdrive":
            ramdrive = Path(self.ramdrive_path)
            ramdrive.mkdir(parents=True, exist_ok=True)

            # Set up automatic sync to persistent storage
            if self.ramdrive_syncer is None and self.sync_to_disk_interval > 0:
                persistent_path = Path(self.ringrift_path) / "ai-service" / "data"
                persistent_path.mkdir(parents=True, exist_ok=True)
                self.ramdrive_syncer = RamdriveSyncer(
                    source_dir=ramdrive,
                    target_dir=persistent_path,
                    interval=self.sync_to_disk_interval,
                    patterns=["*.db", "*.jsonl", "*.json", "*.npz"],
                )
                self.ramdrive_syncer.start()
                logger.info(f"Started ramdrive -> disk sync: {ramdrive} -> {persistent_path} "
                           f"every {self.sync_to_disk_interval}s")

            return ramdrive
        return Path(self.ringrift_path) / "ai-service" / "data"

    def stop_ramdrive_syncer(self, final_sync: bool = True) -> None:
        """Stop the ramdrive syncer and optionally perform final sync."""
        if self.ramdrive_syncer:
            logger.info("Stopping ramdrive syncer...")
            self.ramdrive_syncer.stop(final_sync=final_sync)
            logger.info(f"Ramdrive sync stats: {self.ramdrive_syncer.stats}")
            self.ramdrive_syncer = None

    def _infer_advertise_port(self) -> int:
        """Infer the externally reachable port for this node.

        - Explicit `RINGRIFT_ADVERTISE_PORT` always wins.
        - Vast.ai exposes container ports via `VAST_TCP_PORT_<PORT>`; when set,
          use that public port so peers can reach us.
        - Default to the listening port.
        """
        explicit = (os.environ.get(ADVERTISE_PORT_ENV, "")).strip()
        if explicit:
            try:
                return int(explicit)
            except ValueError:
                pass

        vast_key = f"VAST_TCP_PORT_{self.port}"
        mapped = (os.environ.get(vast_key, "")).strip()
        if mapped:
            try:
                return int(mapped)
            except ValueError:
                pass

        return int(self.port)

    def _load_voter_node_ids(self) -> list[str]:
        """Load the set of P2P voter node_ids (for quorum-based leadership).

        If no voters are configured, returns an empty list and quorum checks are
        disabled (backwards compatible).
        """
        env = (os.environ.get("RINGRIFT_P2P_VOTERS") or "").strip()
        if env:
            self.voter_config_source = "env"
            voters = [t.strip() for t in env.split(",") if t.strip()]
            return sorted(set(voters))

        cfg_path = Path(self.ringrift_path) / "ai-service" / "config" / "distributed_hosts.yaml"
        if not cfg_path.exists():
            self.voter_config_source = "none"
            return []

        try:
            import yaml  # type: ignore
        except Exception:
            return []

        try:
            data = yaml.safe_load(cfg_path.read_text()) or {}
        except Exception:
            return []

        hosts = data.get("hosts", {}) or {}
        voters: list[str] = []
        for node_id, cfg in hosts.items():
            if not isinstance(cfg, dict):
                continue
            raw = cfg.get("p2p_voter", False)
            if raw is True:
                voters.append(str(node_id))
                continue
            if isinstance(raw, (int, float)) and int(raw) == 1:
                voters.append(str(node_id))
                continue
            if isinstance(raw, str) and raw.strip().lower() in {"1", "true", "yes", "y"}:
                voters.append(str(node_id))
        voters = sorted(set(voters))
        self.voter_config_source = "config" if voters else "none"
        return voters

    def _maybe_adopt_voter_node_ids(self, voter_node_ids: list[str], *, source: str) -> bool:
        """Adopt/override the voter set when it's not explicitly configured via env.

        This is a convergence mechanism: some nodes may boot without local
        config (or with stale config), which would disable quorum gating and
        allow non-voter nodes to become leaders. Leaders propagate the stable
        voter set via `/coordinator` so the cluster converges.
        """
        if (os.environ.get("RINGRIFT_P2P_VOTERS") or "").strip():
            return False

        normalized = sorted({str(v).strip() for v in (voter_node_ids or []) if str(v).strip()})
        if not normalized:
            return False

        current = sorted(set(getattr(self, "voter_node_ids", []) or []))
        if current == normalized:
            return False

        self.voter_node_ids = normalized
        # SIMPLIFIED QUORUM: Fixed at 3 voters (or less if fewer voters exist)
        self.voter_quorum_size = min(VOTER_MIN_QUORUM, len(normalized))
        self.voter_config_source = source or "learned"
        print(
            f"[P2P] Updated voter set ({self.voter_config_source}): voters={len(normalized)}, "
            f"quorum={self.voter_quorum_size} ({', '.join(normalized)})"
        )
        return True

    def _has_voter_quorum(self) -> bool:
        """Return True if we currently see enough voter nodes alive.

        SIMPLIFIED QUORUM: Uses fixed minimum of 3 voters instead of majority.
        This makes leader election more resilient - as long as 3 voters agree,
        leadership can be established regardless of total voter count.
        """
        voters = list(getattr(self, "voter_node_ids", []) or [])
        if not voters:
            return True

        # Use fixed minimum quorum of 3 (or fewer if we have fewer voters)
        quorum = min(VOTER_MIN_QUORUM, len(voters))

        alive = 0
        with self.peers_lock:
            peers = dict(self.peers)
        for node_id in voters:
            if node_id == self.node_id:
                alive += 1
                continue
            peer = peers.get(node_id)
            if peer and peer.is_alive():
                alive += 1
        return alive >= quorum

    def _release_voter_grant_if_self(self) -> None:
        """Release our voter-side lease grant when stepping down.

        This shortens failover time when the leader voluntarily steps down (e.g.
        lost quorum) by not forcing other candidates to wait for the full lease
        TTL to expire.
        """
        if str(getattr(self, "voter_grant_leader_id", "") or "") != self.node_id:
            return
        self.voter_grant_leader_id = ""
        self.voter_grant_lease_id = ""
        self.voter_grant_expires = 0.0

    def _enable_partition_local_election(self) -> bool:
        """Enable local leader election for partitioned nodes.

        When a partition is detected and no voters are reachable, this method
        temporarily adds reachable nodes to the voter set so they can elect a
        local leader and continue operating autonomously.

        This is a self-healing mechanism for network splits. When connectivity
        is restored, the partition will merge back with the main cluster.

        Returns:
            True if local election was enabled
        """
        # Don't override env-configured voters
        if (os.environ.get("RINGRIFT_P2P_VOTERS") or "").strip():
            return False

        # Check if we have any voters configured
        voters = list(getattr(self, "voter_node_ids", []) or [])

        # Count how many voters are reachable
        with self.peers_lock:
            peers_by_id = dict(self.peers)
        reachable_voters = 0
        for voter_id in voters:
            if voter_id == self.node_id:
                reachable_voters += 1
                continue
            peer = peers_by_id.get(voter_id)
            if peer and peer.is_alive():
                reachable_voters += 1

        # If we have quorum (simplified: 3 voters), no need for partition election
        quorum = min(VOTER_MIN_QUORUM, len(voters)) if voters else 1
        if reachable_voters >= quorum:
            return False

        # Build local partition voter set from reachable nodes
        local_voters = [self.node_id]  # Always include self
        for node_id, peer in peers_by_id.items():
            if peer.is_alive() and node_id not in local_voters:
                local_voters.append(node_id)

        if len(local_voters) < 2:
            # Need at least 2 nodes for meaningful election
            return False

        # Store original voters for restoration
        if not hasattr(self, "_original_voters"):
            self._original_voters = voters.copy()
            self._partition_election_started = time.time()

        # Enable partition-local election
        self.voter_node_ids = sorted(local_voters)
        self.voter_quorum_size = min(VOTER_MIN_QUORUM, len(local_voters))
        self.voter_config_source = "partition-local"
        print(
            f"[P2P] PARTITION: Enabling local election with {len(local_voters)} nodes: "
            f"{', '.join(local_voters)} (quorum={self.voter_quorum_size})"
        )
        return True

    def _restore_original_voters(self) -> bool:
        """Restore original voter configuration after partition heals.

        Called when connectivity to the main cluster is restored.

        Returns:
            True if voters were restored
        """
        if not hasattr(self, "_original_voters"):
            return False

        original = getattr(self, "_original_voters", [])
        if not original:
            return False

        # Check if we can reach any original voters
        with self.peers_lock:
            peers_by_id = dict(self.peers)
        for voter_id in original:
            if voter_id == self.node_id:
                continue
            peer = peers_by_id.get(voter_id)
            if peer and peer.is_alive():
                # We can reach at least one original voter, restore config
                self.voter_node_ids = original.copy()
                # SIMPLIFIED QUORUM: Fixed at 3 voters (or less if fewer voters exist)
                self.voter_quorum_size = min(VOTER_MIN_QUORUM, len(original))
                self.voter_config_source = "restored"
                delattr(self, "_original_voters")
                if hasattr(self, "_partition_election_started"):
                    delattr(self, "_partition_election_started")
                logger.info(f"Partition healed: restored original voters {', '.join(original)}")
                return True
        return False

    def _get_eligible_voters(self) -> list[str]:
        """Get list of nodes eligible to be voters (GPU nodes with good health)."""
        with self.peers_lock:
            peers = dict(self.peers)

        eligible = []
        now = time.time()

        for node_id, peer in peers.items():
            # Skip retired or NAT-blocked without relay
            if getattr(peer, "retired", False):
                continue

            # Must be alive
            if not peer.is_alive():
                continue

            # Must have GPU (CUDA or MPS)
            has_gpu = getattr(peer, "has_gpu", False)
            gpu_name = str(getattr(peer, "gpu_name", "") or "")
            if not has_gpu and "GH200" not in gpu_name and "H100" not in gpu_name and "A10" not in gpu_name and "aws" not in node_id.lower():
                continue

            # Must have been up for minimum time
            first_seen = getattr(peer, "first_seen", 0) or peer.last_heartbeat
            if now - first_seen < VOTER_PROMOTION_UPTIME:
                continue

            # Check health score (response rate)
            failures = getattr(peer, "consecutive_failures", 0)
            if failures >= VOTER_DEMOTION_FAILURES:
                continue

            eligible.append(node_id)

        # Always include self if we have GPU
        if self.node_id not in eligible:
            self_gpu = getattr(self, "has_gpu", False)
            if self_gpu or "aws" in self.node_id.lower() or "lambda" in self.node_id.lower():
                eligible.append(self.node_id)

        return sorted(eligible)

    def _manage_dynamic_voters(self) -> bool:
        """Manage dynamic voter pool - promote/demote voters as needed.

        Returns True if voter set was changed.
        """
        if not DYNAMIC_VOTER_ENABLED:
            return False

        # Don't override env-configured voters
        if (os.environ.get("RINGRIFT_P2P_VOTERS") or "").strip():
            return False

        current_voters = list(getattr(self, "voter_node_ids", []) or [])
        eligible = self._get_eligible_voters()

        # Count how many current voters are healthy
        with self.peers_lock:
            peers = dict(self.peers)

        healthy_voters = []
        unhealthy_voters = []

        for voter_id in current_voters:
            if voter_id == self.node_id:
                healthy_voters.append(voter_id)
                continue
            peer = peers.get(voter_id)
            if peer and peer.is_alive():
                failures = getattr(peer, "consecutive_failures", 0)
                if failures < VOTER_DEMOTION_FAILURES:
                    healthy_voters.append(voter_id)
                else:
                    unhealthy_voters.append(voter_id)
            else:
                unhealthy_voters.append(voter_id)

        changed = False
        new_voters = healthy_voters.copy()

        # Demote unhealthy voters
        if unhealthy_voters:
            logger.info(f"Dynamic voters: demoting unhealthy voters: {unhealthy_voters}")
            changed = True

        # Promote new voters if below target
        if len(new_voters) < DYNAMIC_VOTER_TARGET:
            candidates = [n for n in eligible if n not in new_voters]
            # Sort by reliability (fewer failures = better)
            candidates.sort(key=lambda n: getattr(peers.get(n), "consecutive_failures", 0) if peers.get(n) else 999)

            needed = DYNAMIC_VOTER_TARGET - len(new_voters)
            for candidate in candidates[:needed]:
                new_voters.append(candidate)
                logger.info(f"Dynamic voters: promoting {candidate} to voter")
                changed = True

        if changed and new_voters:
            new_voters = sorted(set(new_voters))
            self.voter_node_ids = new_voters
            # SIMPLIFIED QUORUM: Fixed at 3 voters (or less if fewer voters exist)
            self.voter_quorum_size = min(VOTER_MIN_QUORUM, len(new_voters))
            self.voter_config_source = "dynamic"
            print(
                f"[P2P] Dynamic voter set updated: {len(new_voters)} voters, "
                f"quorum={self.voter_quorum_size} ({', '.join(new_voters)})"
            )
            return True

        return False

    def _check_leader_health(self) -> bool:
        """Check leader health based on peer response rates.

        Returns True if health is good, False if degraded.
        """
        if self.role != NodeRole.LEADER:
            return True

        with self.peers_lock:
            peers = list(self.peers.values())

        if not peers:
            return True

        # Calculate response rate (peers that responded recently)
        now = time.time()
        alive_count = sum(1 for p in peers if p.is_alive() and not getattr(p, "retired", False))
        total_count = sum(1 for p in peers if not getattr(p, "retired", False))

        if total_count == 0:
            return True

        response_rate = alive_count / total_count

        # Track degraded state
        if not hasattr(self, "_leader_degraded_since"):
            self._leader_degraded_since = 0.0

        if response_rate < LEADER_MIN_RESPONSE_RATE:
            if self._leader_degraded_since == 0.0:
                self._leader_degraded_since = now
                logger.info(f"Leader health degraded: {response_rate:.1%} response rate (min: {LEADER_MIN_RESPONSE_RATE:.0%})")
            elif now - self._leader_degraded_since > LEADER_DEGRADED_STEPDOWN_DELAY:
                logger.info(f"Leader health critically degraded for {LEADER_DEGRADED_STEPDOWN_DELAY}s, stepping down")
                self._leader_degraded_since = 0.0
                return False
        else:
            if self._leader_degraded_since > 0:
                logger.info(f"Leader health recovered: {response_rate:.1%} response rate")
            self._leader_degraded_since = 0.0

        return True

    async def _acquire_voter_lease_quorum(self, lease_id: str, duration: int) -> float | None:
        """Acquire/renew an exclusive leader lease from a quorum of voters.

        Returns the effective lease expiry timestamp if a quorum granted the
        lease; otherwise returns None.
        """
        voter_ids = list(getattr(self, "voter_node_ids", []) or [])
        if not voter_ids:
            return time.time() + float(duration)

        quorum = int(getattr(self, "voter_quorum_size", 0) or 0)
        if quorum <= 0:
            # SIMPLIFIED QUORUM: Fixed at 3 voters (or less if fewer voters exist)
            quorum = min(VOTER_MIN_QUORUM, len(voter_ids))

        now = time.time()
        duration = max(10, min(int(duration), int(LEADER_LEASE_DURATION * 2)))

        acks = 0
        lease_ttls: list[float] = []

        # Self-grant (as a voter).
        if self.node_id in voter_ids:
            self.voter_grant_leader_id = self.node_id
            self.voter_grant_lease_id = lease_id
            self.voter_grant_expires = now + float(duration)
            lease_ttls.append(float(duration))
            acks += 1

        with self.peers_lock:
            peers_by_id = dict(self.peers)

        timeout = ClientTimeout(total=5)
        async with get_client_session(timeout) as session:
            for voter_id in voter_ids:
                if acks >= quorum:
                    break
                if voter_id == self.node_id:
                    continue
                voter = peers_by_id.get(voter_id)
                if not voter or not voter.is_alive():
                    continue

                payload = {
                    "leader_id": self.node_id,
                    "lease_id": lease_id,
                    "lease_duration": duration,
                }

                # Use Tailscale-exclusive URLs for voter communication to avoid NAT issues
                for url in self._tailscale_urls_for_voter(voter, "/election/lease"):
                    try:
                        async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                            if resp.status != 200:
                                continue
                            data = await resp.json()
                            if not data.get("granted"):
                                break
                            ttl_raw = data.get("lease_ttl_seconds")
                            if ttl_raw is None:
                                ttl_raw = data.get("ttl_seconds")
                            ttl_val: float | None = None
                            if ttl_raw is not None:
                                try:
                                    ttl_val = float(ttl_raw)
                                except Exception:
                                    ttl_val = None
                            if ttl_val is not None and ttl_val > 0:
                                lease_ttls.append(ttl_val)
                            else:
                                lease_ttls.append(float(duration))
                            acks += 1
                            break
                    except Exception:
                        continue

        if acks < quorum:
            return None
        # Use a relative TTL (computed by each voter on its own clock) to avoid
        # leader lease flapping under clock skew. Convert back to a local expiry.
        effective_ttl = min(lease_ttls) if lease_ttls else float(duration)
        effective_ttl = max(10.0, min(float(duration), float(effective_ttl)))
        return now + float(effective_ttl)

    async def _determine_leased_leader_from_voters(self) -> str | None:
        """Return the current lease-holder as reported by a quorum of voters.

        This is a read-only reconciliation step used to resolve split-brain once
        partitions heal. It queries the current voter grant state via
        `/election/grant` and selects the leader_id that has >= quorum votes with
        non-expired grants.
        """
        voter_ids = list(getattr(self, "voter_node_ids", []) or [])
        if not voter_ids:
            return None

        quorum = int(getattr(self, "voter_quorum_size", 0) or 0)
        if quorum <= 0:
            # SIMPLIFIED QUORUM: Fixed at 3 voters (or less if fewer voters exist)
            quorum = min(VOTER_MIN_QUORUM, len(voter_ids))

        now = time.time()
        counts: dict[str, int] = {}

        # Include local voter state.
        if self.node_id in voter_ids:
            leader_id = str(getattr(self, "voter_grant_leader_id", "") or "")
            expires = float(getattr(self, "voter_grant_expires", 0.0) or 0.0)
            if leader_id and expires > now:
                counts[leader_id] = counts.get(leader_id, 0) + 1

        with self.peers_lock:
            peers_by_id = dict(self.peers)

        timeout = ClientTimeout(total=5)
        async with get_client_session(timeout) as session:
            for voter_id in voter_ids:
                if voter_id == self.node_id:
                    continue
                voter = peers_by_id.get(voter_id)
                if not voter or not voter.is_alive():
                    continue

                # Use Tailscale-exclusive URLs for voter communication to avoid NAT issues
                for url in self._tailscale_urls_for_voter(voter, "/election/grant"):
                    try:
                        async with session.get(url, headers=self._auth_headers()) as resp:
                            if resp.status != 200:
                                continue
                            data = await resp.json()
                        leader_id = str((data or {}).get("leader_id") or "")
                        if not leader_id:
                            break
                        ttl_raw = (data or {}).get("lease_ttl_seconds")
                        if ttl_raw is None:
                            ttl_raw = (data or {}).get("ttl_seconds")
                        ttl_val: float | None = None
                        if ttl_raw is not None:
                            try:
                                ttl_val = float(ttl_raw)
                            except Exception:
                                ttl_val = None

                        if ttl_val is not None:
                            if ttl_val <= 0:
                                break
                        else:
                            # Back-compat: use absolute expiry as best-effort, with
                            # a generous skew tolerance (1 lease duration).
                            expires = float((data or {}).get("lease_expires") or 0.0)
                            if expires <= 0:
                                break
                            if expires + float(LEADER_LEASE_DURATION) < now:
                                break
                        counts[leader_id] = counts.get(leader_id, 0) + 1
                        break
                    except Exception:
                        continue

        winners = [leader_id for leader_id, count in counts.items() if count >= quorum]
        if not winners:
            return None
        # Deterministic: if multiple satisfy quorum (shouldn't), pick highest node_id.
        return sorted(winners)[-1]

    async def _query_arbiter_for_leader(self) -> str | None:
        """Query the arbiter for the authoritative leader when voter quorum fails.

        The arbiter is a reliably-reachable node that maintains its view of
        who the leader should be. Used as a fallback when split-brain causes
        voter quorum to be unreachable.

        Returns:
            The leader_id from the arbiter, or None if arbiter is unreachable
        """
        arbiter_url = ARBITER_URL
        if not arbiter_url:
            return None

        # Try the configured arbiter URL
        urls_to_try = [arbiter_url]

        # Also try known peers as arbiters if main arbiter fails
        for peer_addr in (self.known_peers or []):
            if peer_addr not in urls_to_try:
                urls_to_try.append(peer_addr)

        timeout = ClientTimeout(total=5)
        try:
            async with get_client_session(timeout) as session:
                for url in urls_to_try:
                    try:
                        base_url = url.rstrip("/")
                        # Query the arbiter's election/grant endpoint to see who they think is leader
                        async with session.get(
                            f"{base_url}/election/grant",
                            headers=self._auth_headers()
                        ) as resp:
                            if resp.status == 200:
                                data = await resp.json()
                                leader_id = str((data or {}).get("leader_id") or "")
                                if leader_id:
                                    logger.info(f"Arbiter {base_url} reports leader: {leader_id}")
                                    return leader_id
                    except Exception:
                        # Try next arbiter
                        continue
        except Exception:
            pass

        return None

    def _parse_peer_address(self, peer_addr: str) -> tuple[str, str, int]:
        """Parse `--peers` entries.

        Supports:
        - `host`
        - `host:port`
        - `http://host[:port]`
        - `https://host[:port]`
        """
        peer_addr = (peer_addr or "").strip()
        if not peer_addr:
            raise ValueError("Empty peer address")

        if "://" in peer_addr:
            parsed = urlparse(peer_addr)
            scheme = (parsed.scheme or "http").lower()
            host = parsed.hostname or ""
            if not host:
                raise ValueError(f"Invalid peer URL: {peer_addr}")
            if parsed.port is not None:
                port = int(parsed.port)
            else:
                port = 443 if scheme == "https" else DEFAULT_PORT
            return scheme, host, port

        # Back-compat: host[:port]
        parts = peer_addr.split(":", 1)
        host = parts[0]
        port = int(parts[1]) if len(parts) > 1 and parts[1] else DEFAULT_PORT
        return "http", host, port

    def _url_for_peer(self, peer: NodeInfo, path: str) -> str:
        scheme = (getattr(peer, "scheme", None) or "http").lower()
        host = str(getattr(peer, "host", "") or "").strip()
        try:
            port = int(getattr(peer, "port", DEFAULT_PORT) or DEFAULT_PORT)
        except Exception:
            port = DEFAULT_PORT

        rh = (getattr(peer, "reported_host", "") or "").strip()
        try:
            rp = int(getattr(peer, "reported_port", 0) or 0)
        except Exception:
            rp = 0

        if rh and rp:
            # Prefer reported endpoints when the observed endpoint is loopback
            # (proxy/relay artifacts).
            if host in {"127.0.0.1", "localhost", "0.0.0.0", "::1"} or (self._local_has_tailscale() and self._is_tailscale_host(rh)):
                host, port = rh, rp

        return f"{scheme}://{host}:{port}{path}"

    def _urls_for_peer(self, peer: NodeInfo, path: str) -> list[str]:
        """Return candidate URLs for reaching a peer.

        Includes both the observed reachable endpoint (`host`/`port`) and the
        peer's self-reported endpoint (`reported_host`/`reported_port`) when
        available. This improves resilience in mixed network environments
        (public IP vs overlay networks like Tailscale, port-mapped listeners).
        """
        scheme = (getattr(peer, "scheme", None) or "http").lower()
        urls: list[str] = []

        def _add(host: Any, port: Any) -> None:
            try:
                h = str(host or "").strip()
                p = int(port)
            except Exception:
                return
            if not h or p <= 0:
                return
            url = f"{scheme}://{h}:{p}{path}"
            if url not in urls:
                urls.append(url)

        rh = (getattr(peer, "reported_host", "") or "").strip()
        try:
            rp = int(getattr(peer, "reported_port", 0) or 0)
        except Exception:
            rp = 0

        host = str(getattr(peer, "host", "") or "").strip()
        try:
            port = int(getattr(peer, "port", 0) or 0)
        except Exception:
            port = 0

        # Prefer Tailscale endpoints first when available locally; otherwise try
        # the observed endpoint first.
        reported_preferred = False
        if rh and rp and self._local_has_tailscale() and self._is_tailscale_host(rh):
            _add(rh, rp)
            reported_preferred = True

        _add(host, port)

        if rh and rp and (not reported_preferred) and (rh != host or rp != port):
            _add(rh, rp)

        return urls

    def _auth_headers(self) -> dict[str, str]:
        if not self.auth_token:
            return {}
        return {"Authorization": f"Bearer {self.auth_token}"}

    def _get_leader_peer(self) -> NodeInfo | None:
        if self._is_leader():
            return self.self_info

        with self.peers_lock:
            peers_snapshot = list(self.peers.values())

        conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])

        leader_id = self.leader_id
        if leader_id and self._is_leader_lease_valid():
            for peer in peers_snapshot:
                if (
                    peer.node_id == leader_id
                    and peer.role == NodeRole.LEADER
                    and peer.is_alive()
                    and self._is_leader_eligible(peer, conflict_keys)
                ):
                    return peer

        eligible_leaders = [
            peer for peer in peers_snapshot
            if peer.role == NodeRole.LEADER and self._is_leader_eligible(peer, conflict_keys)
        ]
        if eligible_leaders:
            return sorted(eligible_leaders, key=lambda p: p.node_id)[-1]

        return None

    async def _proxy_to_leader(self, request: web.Request) -> web.StreamResponse:
        """Best-effort proxy for leader-only APIs when the dashboard hits a follower."""
        leader = self._get_leader_peer()
        if not leader:
            return web.json_response(
                {"success": False, "error": "leader_unknown", "leader_id": self.leader_id},
                status=503,
            )

        candidate_urls = self._urls_for_peer(leader, request.raw_path)
        if not candidate_urls:
            candidate_urls = [self._url_for_peer(leader, request.raw_path)]
        forward_headers: dict[str, str] = {}
        for h in ("Authorization", "X-RingRift-Auth", "Content-Type"):
            if h in request.headers:
                forward_headers[h] = request.headers[h]

        body: bytes | None = None
        if request.method not in ("GET", "HEAD", "OPTIONS"):
            body = await request.read()

        # Keep leader-proxy responsive: unreachable "leaders" (often NAT/firewall)
        # should fail fast so the dashboard doesn't hang for a full minute.
        timeout = ClientTimeout(total=10)
        last_exc: Exception | None = None
        async with get_client_session(timeout) as session:
            for target_url in candidate_urls:
                try:
                    async with session.request(
                        request.method,
                        target_url,
                        data=body,
                        headers=forward_headers,
                    ) as resp:
                        payload = await resp.read()
                        content_type = resp.headers.get("Content-Type")
                        headers: dict[str, str] = {}
                        if content_type:
                            headers["Content-Type"] = content_type
                        headers["X-RingRift-Proxied-By"] = self.node_id
                        headers["X-RingRift-Proxied-To"] = target_url
                        return web.Response(body=payload, status=resp.status, headers=headers)
                except Exception as exc:
                    last_exc = exc
                    continue

        return web.json_response(
            {
                "success": False,
                "error": "leader_proxy_failed",
                "message": str(last_exc) if last_exc else "unknown_error",
                "leader_id": self.leader_id,
                "attempted_urls": candidate_urls,
            },
            status=502,
        )

    def _is_request_authorized(self, request: web.Request) -> bool:
        if not self.auth_token:
            return True

        auth_header = request.headers.get("Authorization", "")
        token = ""
        if auth_header.lower().startswith("bearer "):
            token = auth_header[7:].strip()
        if not token:
            token = request.headers.get("X-RingRift-Auth", "").strip()
        if not token:
            return False

        return secrets.compare_digest(token, self.auth_token)

    def _init_database(self):
        """Initialize SQLite database for state persistence."""
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()

        # Peers table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS peers (
                node_id TEXT PRIMARY KEY,
                host TEXT,
                port INTEGER,
                last_heartbeat REAL,
                info_json TEXT
            )
        """)

        # Jobs table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS jobs (
                job_id TEXT PRIMARY KEY,
                job_type TEXT,
                node_id TEXT,
                board_type TEXT,
                num_players INTEGER,
                engine_mode TEXT,
                pid INTEGER,
                started_at REAL,
                status TEXT
            )
        """)

        # State table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS state (
                key TEXT PRIMARY KEY,
                value TEXT
            )
        """)

        # Metrics history table for observability
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS metrics_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp REAL NOT NULL,
                metric_type TEXT NOT NULL,
                board_type TEXT,
                num_players INTEGER,
                value REAL NOT NULL,
                metadata TEXT
            )
        """)
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_metrics_type_time
            ON metrics_history(metric_type, timestamp)
        """)
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_metrics_config
            ON metrics_history(board_type, num_players, timestamp)
        """)

        # A/B Testing tables
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS ab_tests (
                test_id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                description TEXT,
                board_type TEXT NOT NULL,
                num_players INTEGER NOT NULL,
                model_a TEXT NOT NULL,
                model_b TEXT NOT NULL,
                target_games INTEGER DEFAULT 100,
                confidence_threshold REAL DEFAULT 0.95,
                status TEXT DEFAULT 'running',
                winner TEXT,
                created_at REAL NOT NULL,
                completed_at REAL,
                metadata TEXT
            )
        """)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS ab_test_games (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                test_id TEXT NOT NULL,
                game_id TEXT NOT NULL,
                model_a_result TEXT NOT NULL,
                model_a_score REAL NOT NULL,
                model_b_score REAL NOT NULL,
                game_length INTEGER,
                played_at REAL NOT NULL,
                metadata TEXT,
                FOREIGN KEY (test_id) REFERENCES ab_tests(test_id)
            )
        """)
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_ab_games_test
            ON ab_test_games(test_id, played_at)
        """)

        # Phase 27: Peer cache table for persistent peer storage
        # Peers are cached with reputation scores for reliable reconnection
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS peer_cache (
                node_id TEXT PRIMARY KEY,
                host TEXT NOT NULL,
                port INTEGER NOT NULL,
                tailscale_ip TEXT,
                last_seen REAL,
                success_count INTEGER DEFAULT 0,
                failure_count INTEGER DEFAULT 0,
                reputation_score REAL DEFAULT 0.5,
                is_bootstrap_seed BOOLEAN DEFAULT FALSE
            )
        """)
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_peer_cache_reputation
            ON peer_cache(reputation_score DESC, last_seen DESC)
        """)

        # Phase 29: Config table for cluster epoch and other persistent settings
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS config (
                key TEXT PRIMARY KEY,
                value TEXT NOT NULL
            )
        """)

        conn.commit()
        conn.close()

    def _load_state(self):
        """Load persisted state from database."""
        conn = None
        try:
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()

            # Load peers
            cursor.execute("SELECT node_id, info_json FROM peers")
            for row in cursor.fetchall():
                try:
                    if row[0] == self.node_id:
                        continue
                    info = NodeInfo.from_dict(json.loads(row[1]))
                    self.peers[row[0]] = info
                except Exception as e:
                    logger.error(f"Failed to load peer {row[0]}: {e}")

            # Load jobs
            cursor.execute("SELECT * FROM jobs WHERE status = 'running'")
            for row in cursor.fetchall():
                job = ClusterJob(
                    job_id=row[0],
                    job_type=JobType(row[1]),
                    node_id=row[2],
                    board_type=row[3],
                    num_players=row[4],
                    engine_mode=row[5],
                    pid=row[6],
                    started_at=row[7],
                    status=row[8],
                )
                self.local_jobs[job.job_id] = job

            # Load leader
            cursor.execute("SELECT key, value FROM state")
            state_rows = {row[0]: row[1] for row in cursor.fetchall() if row and row[0]}
            raw_leader_id = state_rows.get("leader_id")
            if raw_leader_id:
                self.leader_id = raw_leader_id

            raw_lease_id = state_rows.get("leader_lease_id")
            if raw_lease_id:
                self.leader_lease_id = raw_lease_id

            raw_lease_expires = state_rows.get("leader_lease_expires")
            if raw_lease_expires:
                with contextlib.suppress(Exception):
                    self.leader_lease_expires = float(raw_lease_expires)

            raw_last_renewal = state_rows.get("last_lease_renewal")
            if raw_last_renewal:
                with contextlib.suppress(Exception):
                    self.last_lease_renewal = float(raw_last_renewal)

            raw_role = state_rows.get("role")
            if raw_role:
                with contextlib.suppress(Exception):
                    self.role = NodeRole(str(raw_role))

            raw_grant_leader = state_rows.get("voter_grant_leader_id")
            if raw_grant_leader:
                self.voter_grant_leader_id = str(raw_grant_leader)
            raw_grant_lease = state_rows.get("voter_grant_lease_id")
            if raw_grant_lease:
                self.voter_grant_lease_id = str(raw_grant_lease)
            raw_grant_expires = state_rows.get("voter_grant_expires")
            if raw_grant_expires:
                with contextlib.suppress(Exception):
                    self.voter_grant_expires = float(raw_grant_expires)

            # Optional persisted voter configuration (convergence helper). Only
            # apply when voters are not explicitly configured via env/config.
            raw_voters = state_rows.get("voter_node_ids")
            if raw_voters and not (getattr(self, "voter_node_ids", []) or []) and str(getattr(self, "voter_config_source", "none") or "none") == "none":
                voters: list[str] = []
                try:
                    parsed = json.loads(raw_voters)
                    if isinstance(parsed, list):
                        voters = [str(v).strip() for v in parsed if str(v).strip()]
                except Exception:
                    voters = [t.strip() for t in str(raw_voters).split(",") if t.strip()]
                if voters:
                    self._maybe_adopt_voter_node_ids(voters, source="state")

            # Self-heal inconsistent persisted leader state (can happen after
            # abrupt shutdowns or partial writes): never keep role=leader without
            # a matching leader_id.
            if self.role == NodeRole.LEADER and not self.leader_id:
                logger.info("Loaded role=leader but leader_id is empty; stepping down to follower")
                self.role = NodeRole.FOLLOWER
                self.leader_lease_id = ""
                self.leader_lease_expires = 0.0
                self.last_lease_renewal = 0.0

            logger.info(f"Loaded state: {len(self.peers)} peers, {len(self.local_jobs)} jobs")
        except Exception as e:
            logger.error(f"Failed to load state: {e}")
        finally:
            if conn:
                conn.close()

    def _save_state(self):
        """Save current state to database."""
        conn = None
        try:
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()

            # Save peers
            cursor.execute("DELETE FROM peers WHERE node_id = ?", (self.node_id,))
            with self.peers_lock:
                for node_id, info in self.peers.items():
                    if node_id == self.node_id:
                        continue
                    cursor.execute("""
                        INSERT OR REPLACE INTO peers (node_id, host, port, last_heartbeat, info_json)
                        VALUES (?, ?, ?, ?, ?)
                    """, (node_id, info.host, info.port, info.last_heartbeat, json.dumps(info.to_dict())))

            # Save jobs
            with self.jobs_lock:
                for _job_id, job in self.local_jobs.items():
                    cursor.execute("""
                        INSERT OR REPLACE INTO jobs
                        (job_id, job_type, node_id, board_type, num_players, engine_mode, pid, started_at, status)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (job.job_id, job.job_type.value, job.node_id, job.board_type,
                          job.num_players, job.engine_mode, job.pid, job.started_at, job.status))

            # Save leader
            role_value = self.role.value if hasattr(self.role, "value") else str(self.role)
            voter_node_ids_json = json.dumps(sorted(set(getattr(self, "voter_node_ids", []) or [])))
            voter_config_source = str(getattr(self, "voter_config_source", "") or "")
            state_payload = [
                ("leader_id", self.leader_id),
                ("leader_lease_id", self.leader_lease_id or ""),
                ("leader_lease_expires", str(float(self.leader_lease_expires or 0.0))),
                ("last_lease_renewal", str(float(self.last_lease_renewal or 0.0))),
                ("role", role_value),
                ("voter_node_ids", voter_node_ids_json),
                ("voter_config_source", voter_config_source),
                ("voter_grant_leader_id", str(getattr(self, "voter_grant_leader_id", "") or "")),
                ("voter_grant_lease_id", str(getattr(self, "voter_grant_lease_id", "") or "")),
                ("voter_grant_expires", str(float(getattr(self, "voter_grant_expires", 0.0) or 0.0))),
            ]
            cursor.executemany(
                "INSERT OR REPLACE INTO state (key, value) VALUES (?, ?)",
                state_payload,
                )

            conn.commit()
        except Exception as e:
            logger.error(f"Failed to save state: {e}")
        finally:
            if conn:
                conn.close()

    # =========================================================================
    # Phase 27: Peer Cache and Reputation Tracking
    # =========================================================================

    def _update_peer_reputation(self, peer_addr_or_node_id: str, success: bool) -> None:
        """Update peer reputation based on interaction success.

        Uses exponential moving average (EMA) for smooth reputation updates.
        Higher reputation = more reliable peer for bootstrap.
        """
        conn = None
        try:
            conn = sqlite3.connect(str(self.db_path), timeout=5.0)
            cursor = conn.cursor()

            # Get current reputation
            cursor.execute(
                "SELECT reputation_score, success_count, failure_count FROM peer_cache WHERE node_id = ?",
                (peer_addr_or_node_id,)
            )
            row = cursor.fetchone()

            if row:
                current_score = float(row[0] or 0.5)
                success_count = int(row[1] or 0)
                failure_count = int(row[2] or 0)
            else:
                current_score = 0.5
                success_count = 0
                failure_count = 0

            # EMA update: new_score = alpha * outcome + (1-alpha) * current
            outcome = 1.0 if success else 0.0
            new_score = PEER_REPUTATION_ALPHA * outcome + (1 - PEER_REPUTATION_ALPHA) * current_score

            # Update counts
            if success:
                success_count += 1
            else:
                failure_count += 1

            # Upsert
            cursor.execute("""
                INSERT INTO peer_cache (node_id, host, port, reputation_score, success_count, failure_count, last_seen)
                VALUES (?, '', 0, ?, ?, ?, ?)
                ON CONFLICT(node_id) DO UPDATE SET
                    reputation_score = ?,
                    success_count = ?,
                    failure_count = ?,
                    last_seen = ?
            """, (
                peer_addr_or_node_id,
                new_score, success_count, failure_count, time.time(),
                new_score, success_count, failure_count, time.time()
            ))
            conn.commit()
        except Exception as e:
            if self.verbose:
                logger.debug(f"Error updating peer reputation: {e}")
        finally:
            if conn:
                conn.close()

    def _save_peer_to_cache(
        self,
        node_id: str,
        host: str,
        port: int,
        tailscale_ip: str | None = None
    ) -> None:
        """Save a peer to the cache for persistence across restarts."""
        if not node_id or node_id == self.node_id:
            return

        conn = None
        try:
            conn = sqlite3.connect(str(self.db_path), timeout=5.0)
            cursor = conn.cursor()

            # Check if this is a known bootstrap seed
            is_seed = f"{host}:{port}" in self.bootstrap_seeds

            cursor.execute("""
                INSERT INTO peer_cache (node_id, host, port, tailscale_ip, last_seen, is_bootstrap_seed)
                VALUES (?, ?, ?, ?, ?, ?)
                ON CONFLICT(node_id) DO UPDATE SET
                    host = COALESCE(NULLIF(?, ''), host),
                    port = CASE WHEN ? > 0 THEN ? ELSE port END,
                    tailscale_ip = COALESCE(NULLIF(?, ''), tailscale_ip),
                    last_seen = ?,
                    is_bootstrap_seed = ?
            """, (
                node_id, host, port, tailscale_ip or "", time.time(), is_seed,
                host, port, port, tailscale_ip or "", time.time(), is_seed
            ))
            conn.commit()

            # Prune old entries if needed
            cursor.execute("SELECT COUNT(*) FROM peer_cache")
            count = cursor.fetchone()[0]
            if count > PEER_CACHE_MAX_ENTRIES:
                # Delete oldest entries with lowest reputation
                cursor.execute("""
                    DELETE FROM peer_cache
                    WHERE node_id IN (
                        SELECT node_id FROM peer_cache
                        WHERE is_bootstrap_seed = 0
                        ORDER BY reputation_score ASC, last_seen ASC
                        LIMIT ?
                    )
                """, (count - PEER_CACHE_MAX_ENTRIES,))
                conn.commit()

        except Exception as e:
            if self.verbose:
                logger.debug(f"Error saving peer to cache: {e}")
        finally:
            if conn:
                conn.close()

    def _get_bootstrap_peers_by_reputation(self, limit: int = 5) -> list[str]:
        """Get most reliable cached peers for bootstrap.

        Returns list of "host:port" strings ordered by reputation.
        Filters out peers not seen in the last 7 days.
        """
        conn = None
        try:
            conn = sqlite3.connect(str(self.db_path), timeout=5.0)
            cursor = conn.cursor()

            # Get peers seen recently, ordered by seed status then reputation
            cutoff = time.time() - PEER_CACHE_TTL_SECONDS
            cursor.execute("""
                SELECT host, port, tailscale_ip FROM peer_cache
                WHERE last_seen > ? AND host != '' AND port > 0
                ORDER BY is_bootstrap_seed DESC, reputation_score DESC
                LIMIT ?
            """, (cutoff, limit))

            result = []
            for row in cursor.fetchall():
                host = row[0]
                port = row[1]
                ts_ip = row[2]
                # Prefer Tailscale IP if available
                if ts_ip:
                    result.append(f"{ts_ip}:{port}")
                elif host:
                    result.append(f"{host}:{port}")
            return result

        except Exception as e:
            if self.verbose:
                logger.debug(f"Error getting cached peers: {e}")
            return []
        finally:
            if conn:
                conn.close()

    # =========================================================================
    # Phase 29: Cluster Epoch Persistence
    # =========================================================================

    def _load_cluster_epoch(self) -> None:
        """Load cluster epoch from database."""
        conn = None
        try:
            conn = sqlite3.connect(str(self.db_path), timeout=5.0)
            cursor = conn.cursor()
            cursor.execute("SELECT value FROM config WHERE key = 'cluster_epoch'")
            row = cursor.fetchone()
            if row:
                self._cluster_epoch = int(row[0])
                logger.info(f"Loaded cluster epoch: {self._cluster_epoch}")
        except Exception as e:
            if self.verbose:
                logger.debug(f"Error loading cluster epoch: {e}")
        finally:
            if conn:
                conn.close()

    def _save_cluster_epoch(self) -> None:
        """Save cluster epoch to database."""
        conn = None
        try:
            conn = sqlite3.connect(str(self.db_path), timeout=5.0)
            cursor = conn.cursor()
            cursor.execute(
                "INSERT OR REPLACE INTO config (key, value) VALUES ('cluster_epoch', ?)",
                (str(self._cluster_epoch),)
            )
            conn.commit()
        except Exception as e:
            if self.verbose:
                logger.debug(f"Error saving cluster epoch: {e}")
        finally:
            if conn:
                conn.close()

    def _increment_cluster_epoch(self) -> None:
        """Increment cluster epoch (called on leader change)."""
        self._cluster_epoch += 1
        self._save_cluster_epoch()
        logger.info(f"Incremented cluster epoch to {self._cluster_epoch}")

    # Class-level metrics buffer for batched writes (5% speedup)
    _metrics_buffer: list[tuple] = []
    _metrics_buffer_lock = threading.Lock()
    _metrics_last_flush: float = 0.0
    _metrics_flush_interval: float = 30.0  # Flush every 30 seconds
    _metrics_max_buffer: int = 100  # Or when buffer reaches 100 entries

    def record_metric(
        self,
        metric_type: str,
        value: float,
        board_type: str | None = None,
        num_players: int | None = None,
        metadata: dict[str, Any] | None = None,
    ):
        """Record a metric to the history table for observability.

        Metric types:
        - training_loss: NNUE training loss
        - elo_rating: Model Elo rating
        - gpu_utilization: GPU utilization percentage
        - selfplay_games_per_hour: Game generation rate
        - validation_rate: GPU selfplay validation rate
        - tournament_win_rate: Tournament win rate for new model

        Uses buffered writes for better performance (batches every 30s or 100 entries).
        """
        entry = (
            time.time(),
            metric_type,
            board_type,
            num_players,
            value,
            json.dumps(metadata) if metadata else None,
        )

        with self._metrics_buffer_lock:
            self._metrics_buffer.append(entry)
            should_flush = (
                len(self._metrics_buffer) >= self._metrics_max_buffer or
                time.time() - self._metrics_last_flush > self._metrics_flush_interval
            )

        if should_flush:
            self._flush_metrics_buffer()

    def _flush_metrics_buffer(self):
        """Flush buffered metrics to database using batch insert."""
        with self._metrics_buffer_lock:
            if not self._metrics_buffer:
                return
            entries = self._metrics_buffer.copy()
            self._metrics_buffer.clear()
            self._metrics_last_flush = time.time()

        conn = None
        try:
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()
            cursor.executemany("""
                INSERT INTO metrics_history
                (timestamp, metric_type, board_type, num_players, value, metadata)
                VALUES (?, ?, ?, ?, ?, ?)
            """, entries)
            conn.commit()
        except Exception as e:
            logger.error(f"Failed to flush metrics buffer ({len(entries)} entries): {e}")
        finally:
            if conn:
                conn.close()

    def get_metrics_history(
        self,
        metric_type: str,
        board_type: str | None = None,
        num_players: int | None = None,
        hours: float = 24,
        limit: int = 1000,
    ) -> list[dict[str, Any]]:
        """Get metrics history for a specific metric type."""
        conn = None
        try:
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()

            since = time.time() - (hours * 3600)
            query = """
                SELECT timestamp, value, board_type, num_players, metadata
                FROM metrics_history
                WHERE metric_type = ? AND timestamp > ?
            """
            params: list[Any] = [metric_type, since]

            if board_type:
                query += " AND board_type = ?"
                params.append(board_type)
            if num_players:
                query += " AND num_players = ?"
                params.append(num_players)

            query += " ORDER BY timestamp DESC LIMIT ?"
            params.append(limit)

            cursor.execute(query, params)
            results = []
            for row in cursor.fetchall():
                results.append({
                    "timestamp": row[0],
                    "value": row[1],
                    "board_type": row[2],
                    "num_players": row[3],
                    "metadata": json.loads(row[4]) if row[4] else None,
                })
            return results
        except Exception as e:
            logger.error(f"Failed to get metrics history: {e}")
            return []
        finally:
            if conn:
                conn.close()

    def get_metrics_summary(self, hours: float = 24) -> dict[str, Any]:
        """Get summary of all metrics over the specified time period."""
        conn = None
        try:
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()

            since = time.time() - (hours * 3600)

            cursor.execute("""
                SELECT metric_type, COUNT(*), AVG(value), MIN(value), MAX(value)
                FROM metrics_history
                WHERE timestamp > ?
                GROUP BY metric_type
            """, (since,))

            summary: dict[str, Any] = {}
            for row in cursor.fetchall():
                summary[row[0]] = {
                    "count": row[1],
                    "avg": row[2],
                    "min": row[3],
                    "max": row[4],
                }

            cursor.execute("""
                SELECT metric_type, value, timestamp
                FROM metrics_history m1
                WHERE timestamp = (
                    SELECT MAX(timestamp) FROM metrics_history m2
                    WHERE m2.metric_type = m1.metric_type
                )
            """)
            for row in cursor.fetchall():
                if row[0] in summary:
                    summary[row[0]]["latest"] = row[1]
                    summary[row[0]]["latest_time"] = row[2]

            return {"period_hours": hours, "since": since, "metrics": summary}
        except Exception as e:
            logger.error(f"Failed to get metrics summary: {e}")
            return {}
        finally:
            if conn:
                conn.close()

    def _create_self_info(self) -> NodeInfo:
        """Create NodeInfo for this node."""
        # Detect GPU
        has_gpu, gpu_name = self._detect_gpu()

        cpu_count = int(os.cpu_count() or 0)

        # Detect memory
        memory_gb = self._detect_memory()

        # Detect capabilities based on hardware
        capabilities = ["selfplay"]
        if has_gpu:
            capabilities.extend(["training", "cmaes"])
        if memory_gb >= 64:
            capabilities.append("large_boards")

        info = NodeInfo(
            node_id=self.node_id,
            host=self.advertise_host,
            port=self.advertise_port,
            role=self.role,
            last_heartbeat=time.time(),
            cpu_count=cpu_count,
            has_gpu=has_gpu,
            gpu_name=gpu_name,
            memory_gb=memory_gb,
            capabilities=capabilities,
            version=self.build_version,
        )
        # Advertise an alternate mesh endpoint (Tailscale) for NAT traversal and
        # multi-path retries. Peers persist the observed reachable endpoint in
        # `host`/`port` but keep our `reported_host`/`reported_port` as an
        # additional candidate (see `_heartbeat_loop` multi-path retry).
        ts_ip = self._get_tailscale_ip()
        if ts_ip and ts_ip != info.host:
            info.reported_host = ts_ip
            # Use the actual listening port for mesh endpoints (port-mapped
            # advertise ports may not be reachable inside overlays).
            info.reported_port = int(self.port)
        return info

    def _detect_gpu(self) -> tuple[bool, str]:
        """Detect if GPU is available and its name."""
        try:
            # Try nvidia-smi
            result = subprocess.run(
                ["nvidia-smi", "--query-gpu=name", "--format=csv,noheader"],
                capture_output=True, text=True, timeout=5
            )
            if result.returncode == 0 and result.stdout.strip():
                return True, result.stdout.strip().split('\n')[0]
        except Exception:
            pass

        try:
            # Try MPS (Apple Silicon)
            result = subprocess.run(
                ["python3", "-c", "import torch; print(torch.backends.mps.is_available())"],
                capture_output=True, text=True, timeout=10
            )
            if "True" in result.stdout:
                return True, "Apple MPS"
        except Exception:
            pass

        return False, ""

    def _detect_memory(self) -> int:
        """Detect total system memory in GB."""
        try:
            if sys.platform == "darwin":
                result = subprocess.run(
                    ["sysctl", "-n", "hw.memsize"],
                    capture_output=True, text=True, timeout=5
                )
                return int(result.stdout.strip()) // (1024**3)
            else:
                with open("/proc/meminfo") as f:
                    for line in f:
                        if line.startswith("MemTotal:"):
                            return int(line.split()[1]) // (1024**2)
        except (OSError, ValueError):
            pass
        return 16  # Default assumption

    def _get_local_ip(self) -> str:
        """Get local IP address."""
        try:
            # Connect to external address to determine local IP
            s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            s.connect(("8.8.8.8", 80))
            ip = s.getsockname()[0]
            s.close()
            return ip
        except OSError:
            return "127.0.0.1"

    def _get_tailscale_ip(self) -> str:
        """Return this node's Tailscale IPv4 (100.x) when available."""
        try:
            result = subprocess.run(
                ["tailscale", "ip", "-4"],
                capture_output=True,
                text=True,
                timeout=5,
            )
            if result.returncode != 0:
                return ""
            ip = (result.stdout or "").strip().splitlines()[0].strip()
            return ip
        except FileNotFoundError:
            return ""
        except Exception:
            return ""

    def _is_tailscale_host(self, host: str) -> bool:
        """Return True when `host` looks like a Tailscale mesh endpoint."""
        h = (host or "").strip()
        if not h:
            return False
        if h.endswith(".ts.net"):
            return True
        try:
            ip = ipaddress.ip_address(h)
        except ValueError:
            return False
        if not isinstance(ip, ipaddress.IPv4Address):
            return False
        return ip in TAILSCALE_CGNAT_NETWORK

    def _local_has_tailscale(self) -> bool:
        """Best-effort: True when this node appears to have a Tailscale address."""
        try:
            info = getattr(self, "self_info", None)
            if not info:
                return False
            host = str(getattr(info, "host", "") or "").strip()
            reported_host = str(getattr(info, "reported_host", "") or "").strip()
            return self._is_tailscale_host(host) or self._is_tailscale_host(reported_host)
        except Exception:
            return False

    def _get_tailscale_ip_for_peer(self, node_id: str) -> str:
        """Look up a peer's Tailscale IP from the dynamic registry or cluster.yaml.

        This enables automatic fallback to Tailscale mesh when public IPs fail.
        Falls back to static config in cluster.yaml if dynamic registry unavailable.

        Args:
            node_id: The peer's node identifier

        Returns:
            Tailscale IP (100.x.x.x) if available, else empty string
        """
        # Try dynamic registry first
        if HAS_DYNAMIC_REGISTRY and get_registry is not None:
            try:
                registry = get_registry()
                with registry._lock:
                    if node_id in registry._nodes:
                        ts_ip = registry._nodes[node_id].tailscale_ip or ""
                        if ts_ip:
                            return ts_ip
            except Exception:
                pass

        # Fall back to static cluster.yaml config
        try:
            cluster_config = get_cluster_config()
            ts_ip = cluster_config.get_tailscale_ip(node_id)
            if ts_ip:
                return ts_ip
        except Exception:
            pass

        return ""

    def _detect_network_partition(self) -> bool:
        """Detect if we're in a network partition (>50% peers unreachable via primary IP).

        Used to trigger Tailscale-first connectivity mode when the public network
        is fragmented but mesh connectivity remains intact.

        Returns:
            True if partition detected (majority of peers unreachable)
        """
        with self.peers_lock:
            peers_snapshot = [p for p in self.peers.values() if p.node_id != self.node_id]

        if len(peers_snapshot) < 2:
            return False

        # Count peers with recent heartbeat failures
        now = time.time()
        unreachable = 0
        for peer in peers_snapshot:
            if peer.consecutive_failures >= 3 or (now - peer.last_heartbeat > PEER_TIMEOUT):
                unreachable += 1

        partition_ratio = unreachable / len(peers_snapshot)
        if partition_ratio > 0.5:
            logger.info(f"Network partition detected: {unreachable}/{len(peers_snapshot)} peers unreachable ({partition_ratio:.0%})")
            return True
        return False

    def _get_tailscale_priority_mode(self) -> bool:
        """Check if Tailscale-first mode is enabled (partition recovery)."""
        return getattr(self, "_tailscale_priority", False)

    def _enable_tailscale_priority(self) -> None:
        """Enable Tailscale-first mode for heartbeats during partition recovery."""
        if not getattr(self, "_tailscale_priority", False):
            logger.info("Enabling Tailscale-priority mode for partition recovery")
            self._tailscale_priority = True
            self._tailscale_priority_until = time.time() + 300  # 5 minutes

    def _disable_tailscale_priority(self) -> None:
        """Disable Tailscale-first mode when connectivity recovers."""
        if getattr(self, "_tailscale_priority", False):
            logger.info("Disabling Tailscale-priority mode (connectivity recovered)")
            self._tailscale_priority = False

    def _tailscale_urls_for_voter(self, voter: NodeInfo, path: str) -> list[str]:
        """Return Tailscale-exclusive URLs for voter communication.

        For election/lease operations between voter nodes, NAT-blocked public IPs
        cause split-brain issues. This method ensures voter communication uses only
        Tailscale mesh IPs (100.x.x.x) which bypass NAT.

        Falls back to regular `_urls_for_peer()` if no Tailscale IP is available.
        """
        scheme = (getattr(voter, "scheme", None) or "http").lower()
        urls: list[str] = []

        voter_id = str(getattr(voter, "node_id", "") or "").strip()
        port = 0
        with contextlib.suppress(Exception):
            port = int(getattr(voter, "port", 0) or 0)
        if port <= 0:
            try:
                port = int(getattr(voter, "reported_port", DEFAULT_PORT) or DEFAULT_PORT)
            except Exception:
                port = DEFAULT_PORT

        # Priority 1: Dynamic registry Tailscale IP lookup
        ts_ip = self._get_tailscale_ip_for_peer(voter_id)
        if ts_ip:
            urls.append(f"{scheme}://{ts_ip}:{port}{path}")

        # Priority 2: Check if reported_host is a Tailscale IP
        rh = str(getattr(voter, "reported_host", "") or "").strip()
        if rh and self._is_tailscale_host(rh):
            try:
                rp = int(getattr(voter, "reported_port", 0) or 0)
            except Exception:
                rp = 0
            if rp > 0:
                url = f"{scheme}://{rh}:{rp}{path}"
                if url not in urls:
                    urls.append(url)

        # Priority 3: Check if host is a Tailscale IP
        host = str(getattr(voter, "host", "") or "").strip()
        if host and self._is_tailscale_host(host):
            url = f"{scheme}://{host}:{port}{path}"
            if url not in urls:
                urls.append(url)

        # If no Tailscale URLs found, fall back to regular method
        # (allows graceful degradation if Tailscale not available)
        if not urls:
            return self._urls_for_peer(voter, path)

        return urls

    def _is_in_startup_grace_period(self) -> bool:
        """Check if we're still in the startup grace period.

        During this period, skip heavy I/O operations like JSONL scanning
        to ensure HTTP server remains responsive.
        """
        return (time.time() - self.start_time) < STARTUP_JSONL_GRACE_PERIOD_SECONDS

    def _get_resource_usage(self) -> dict[str, float]:
        """Get current resource usage."""
        result = {
            "cpu_percent": 0.0,
            "memory_percent": 0.0,
            "disk_percent": 0.0,
            "gpu_percent": 0.0,
            "gpu_memory_percent": 0.0,
        }

        try:
            # CPU
            if sys.platform == "darwin":
                out = subprocess.run(
                    ["ps", "-A", "-o", "%cpu"],
                    capture_output=True, text=True, timeout=5
                )
                cpus = [float(x) for x in out.stdout.strip().split('\n')[1:] if x.strip()]
                result["cpu_percent"] = min(100.0, sum(cpus) / os.cpu_count())
            else:
                with open("/proc/loadavg") as f:
                    load = float(f.read().split()[0])
                    result["cpu_percent"] = min(100.0, load * 100 / os.cpu_count())

            # Memory
            if sys.platform == "darwin":
                out = subprocess.run(
                    ["vm_stat"],
                    capture_output=True, text=True, timeout=5
                )
                # Parse vm_stat output
                lines = out.stdout.strip().split('\n')
                stats = {}
                for line in lines[1:]:
                    if ':' in line:
                        key, val = line.split(':')
                        stats[key.strip()] = int(val.strip().rstrip('.'))
                page_size = 16384  # Usually 16KB on M1
                free = stats.get('Pages free', 0) * page_size
                total = self._detect_memory() * (1024**3)
                result["memory_percent"] = 100.0 * (1 - free / total) if total > 0 else 0.0
            else:
                with open("/proc/meminfo") as f:
                    mem = {}
                    for line in f:
                        parts = line.split()
                        if len(parts) >= 2:
                            mem[parts[0].rstrip(':')] = int(parts[1])
                    total = mem.get('MemTotal', 1)
                    avail = mem.get('MemAvailable', mem.get('MemFree', 0))
                    result["memory_percent"] = 100.0 * (1 - avail / total)

            # Disk
            import shutil
            usage = shutil.disk_usage(self.ringrift_path)
            result["disk_percent"] = 100.0 * usage.used / usage.total
            result["disk_free_gb"] = usage.free / (1024**3)

            # GPU (NVIDIA) - handle multi-GPU by averaging
            try:
                out = subprocess.run(
                    ["nvidia-smi", "--query-gpu=utilization.gpu,memory.used,memory.total",
                     "--format=csv,noheader,nounits"],
                    capture_output=True, text=True, timeout=5
                )
                if out.returncode == 0:
                    lines = out.stdout.strip().split('\n')
                    gpu_utils = []
                    mem_percents = []
                    for line in lines:
                        parts = line.strip().split(',')
                        if len(parts) >= 3:
                            try:
                                gpu_utils.append(float(parts[0].strip()))
                                mem_used = float(parts[1].strip())
                                mem_total = float(parts[2].strip())
                                if mem_total > 0:
                                    mem_percents.append(100.0 * mem_used / mem_total)
                            except (ValueError, IndexError):
                                continue
                    if gpu_utils:
                        # Use max utilization across GPUs (more representative for parallel workloads)
                        result["gpu_percent"] = max(gpu_utils)
                    if mem_percents:
                        result["gpu_memory_percent"] = max(mem_percents)
            except Exception:
                # Silently ignore nvidia-smi errors (not all nodes have GPUs)
                pass

        except Exception as e:
            logger.info(f"Resource check error: {e}")

        return result

    def _check_nfs_accessible(self) -> bool:
        """Check if NFS mount is accessible.

        Tests common NFS mount points for accessibility.
        Returns True if NFS is accessible, False otherwise.
        """
        nfs_paths = [
            Path("/mnt/nfs/ringrift"),
            Path("/home/shared/ringrift"),
            Path(os.environ.get("RINGRIFT_NFS_PATH", "/mnt/nfs/ringrift")),
        ]

        for nfs_path in nfs_paths:
            try:
                if nfs_path.exists() and nfs_path.is_dir():
                    # Try to list directory (actual access check)
                    list(nfs_path.iterdir())[:1]
                    return True
            except Exception:
                continue

        # NFS not found or not accessible
        return False

    def _detect_local_external_work(self) -> dict[str, bool]:
        """Detect external work running on this node (not tracked by P2P orchestrator).

        This detects:
        - CMA-ES optimization (HeuristicAI weight tuning)
        - Gauntlet runs (baseline or two-stage)
        - ELO tournaments
        - Data merge/aggregation jobs

        Returns dict with boolean flags for each work type.
        """
        result = {
            'cmaes_running': False,
            'gauntlet_running': False,
            'tournament_running': False,
            'data_merge_running': False,
        }

        try:
            # Use pgrep to check for running processes (efficient)
            checks = [
                ('cmaes_running', 'HeuristicAI.*json|cmaes_distributed|run_cpu_cmaes'),
                ('gauntlet_running', 'baseline_gauntlet|two_stage_gauntlet'),
                ('tournament_running', 'run_model_elo_tournament'),
                ('data_merge_running', 'merge_game_dbs|aggregate_jsonl|export_training'),
            ]

            for key, pattern in checks:
                try:
                    proc = subprocess.run(
                        ["pgrep", "-f", pattern],
                        capture_output=True, text=True, timeout=2
                    )
                    # pgrep returns 0 if matches found
                    result[key] = proc.returncode == 0 and proc.stdout.strip() != ""
                except Exception:
                    pass

        except Exception as e:
            logger.debug(f"External work detection error: {e}")

        return result

    def _get_diversity_metrics(self) -> dict[str, Any]:
        """Get diversity tracking metrics for monitoring."""
        metrics = dict(self.diversity_metrics)
        metrics["uptime_seconds"] = time.time() - metrics.get("last_reset", time.time())

        # Calculate diversity ratios
        total_games = metrics.get("asymmetric_games", 0) + metrics.get("symmetric_games", 0)
        if total_games > 0:
            metrics["asymmetric_ratio"] = metrics["asymmetric_games"] / total_games
        else:
            metrics["asymmetric_ratio"] = 0.0

        # Engine mode distribution
        engine_total = sum(metrics.get("games_by_engine_mode", {}).values())
        if engine_total > 0:
            metrics["engine_mode_distribution"] = {
                k: v / engine_total
                for k, v in metrics.get("games_by_engine_mode", {}).items()
            }
        else:
            metrics["engine_mode_distribution"] = {}

        return metrics

    def _track_selfplay_diversity(self, config: dict[str, Any]):
        """Track diversity metrics for a scheduled selfplay game."""
        # Track engine mode
        engine_mode = config.get("engine_mode", "unknown")
        if engine_mode not in self.diversity_metrics["games_by_engine_mode"]:
            self.diversity_metrics["games_by_engine_mode"][engine_mode] = 0
        self.diversity_metrics["games_by_engine_mode"][engine_mode] += 1

        # Track board config
        board_key = f"{config.get('board_type', 'unknown')}_{config.get('num_players', 0)}p"
        if board_key not in self.diversity_metrics["games_by_board_config"]:
            self.diversity_metrics["games_by_board_config"][board_key] = 0
        self.diversity_metrics["games_by_board_config"][board_key] += 1

        # Track asymmetric vs symmetric
        if config.get("asymmetric"):
            self.diversity_metrics["asymmetric_games"] += 1
            strong = config.get("strong_config", {})
            weak = config.get("weak_config", {})
            logger.info(f"DIVERSE: Asymmetric game scheduled - "
                  f"Strong({strong.get('engine_mode')}@D{strong.get('difficulty')}) vs "
                  f"Weak({weak.get('engine_mode')}@D{weak.get('difficulty')}) "
                  f"on {board_key}")
        else:
            self.diversity_metrics["symmetric_games"] += 1

        # Track difficulty if available
        difficulty = config.get("difficulty", config.get("difficulty_band"))
        if difficulty:
            diff_key = str(difficulty)
            if diff_key not in self.diversity_metrics["games_by_difficulty"]:
                self.diversity_metrics["games_by_difficulty"][diff_key] = 0
            self.diversity_metrics["games_by_difficulty"][diff_key] += 1

    def _count_local_jobs(self) -> tuple[int, int]:
        """Count running selfplay and training jobs on this node."""
        def _pid_alive(pid: int) -> bool:
            try:
                os.kill(pid, 0)
                return True
            except ProcessLookupError:
                return False
            except PermissionError:
                return True
            except Exception:
                return False

        # Primary source of truth: jobs we started and are tracking.
        selfplay_pids: set[str] = set()
        training_pids: set[str] = set()

        stale_job_ids: list[str] = []
        try:
            with self.jobs_lock:
                jobs_snapshot = list(self.local_jobs.items())
            for job_id, job in jobs_snapshot:
                if job.status != "running":
                    continue
                pid = int(job.pid or 0)
                if pid <= 0:
                    continue
                if not _pid_alive(pid):
                    stale_job_ids.append(job_id)
                    continue
                if job.job_type in (JobType.SELFPLAY, JobType.GPU_SELFPLAY, JobType.HYBRID_SELFPLAY, JobType.CPU_SELFPLAY, JobType.GUMBEL_SELFPLAY):
                    selfplay_pids.add(str(pid))
                elif job.job_type == JobType.TRAINING:
                    training_pids.add(str(pid))

            if stale_job_ids:
                with self.jobs_lock:
                    for job_id in stale_job_ids:
                        self.local_jobs.pop(job_id, None)
        except Exception:
            pass

        # Secondary check: best-effort process scan for untracked jobs (e.g. manual runs).
        # IMPORTANT: never return (0,0) just because `pgrep` is missing or fails;
        # that can cause the leader to spawn runaway selfplay processes until disk fills.
        try:
            import shutil

            if shutil.which("pgrep"):
                for pattern in ("run_self_play_soak.py", "run_gpu_selfplay.py", "run_hybrid_selfplay.py"):
                    out = subprocess.run(
                        ["pgrep", "-f", pattern],
                        capture_output=True,
                        text=True,
                        timeout=5,
                    )
                    if out.returncode == 0 and out.stdout.strip():
                        selfplay_pids.update([p for p in out.stdout.strip().split() if p])

                for pattern in ("train_", "train.py"):
                    out = subprocess.run(
                        ["pgrep", "-f", pattern],
                        capture_output=True,
                        text=True,
                        timeout=5,
                    )
                    if out.returncode == 0 and out.stdout.strip():
                        training_pids.update([p for p in out.stdout.strip().split() if p])
        except Exception:
            pass

        return len(selfplay_pids), len(training_pids)

    def _cleanup_stale_processes(self) -> int:
        """Kill processes that have been running too long.

        Scans for known process patterns (tournaments, gauntlets, selfplay)
        and kills any that exceed their maximum runtime threshold.

        Returns:
            Number of processes killed.
        """
        import shutil

        if not shutil.which("pgrep") or not shutil.which("ps"):
            return 0

        killed_count = 0
        time.time()

        # Map patterns to their max runtimes
        pattern_max_runtime = {
            "run_model_elo_tournament.py": MAX_TOURNAMENT_RUNTIME,
            "run_gauntlet.py": MAX_GAUNTLET_RUNTIME,
            "run_self_play_soak.py": MAX_SELFPLAY_RUNTIME,
            "run_gpu_selfplay.py": MAX_SELFPLAY_RUNTIME,
            "run_hybrid_selfplay.py": MAX_SELFPLAY_RUNTIME,
            "run_diverse_selfplay.py": MAX_SELFPLAY_RUNTIME,
            "train_nnue.py": MAX_TRAINING_RUNTIME,
            "train.py": MAX_TRAINING_RUNTIME,
        }

        for pattern, max_runtime in pattern_max_runtime.items():
            try:
                # Get PIDs matching the pattern
                pgrep_result = subprocess.run(
                    ["pgrep", "-f", pattern],
                    capture_output=True,
                    text=True,
                    timeout=5,
                )
                if pgrep_result.returncode != 0 or not pgrep_result.stdout.strip():
                    continue

                pids = [p.strip() for p in pgrep_result.stdout.strip().split() if p.strip()]

                for pid in pids:
                    try:
                        # Get process start time using ps
                        ps_result = subprocess.run(
                            ["ps", "-o", "etimes=", "-p", pid],
                            capture_output=True,
                            text=True,
                            timeout=5,
                        )
                        if ps_result.returncode != 0:
                            continue

                        elapsed_seconds = int(ps_result.stdout.strip())

                        if elapsed_seconds > max_runtime:
                            # Process has exceeded max runtime - kill it
                            logger.warning(
                                f"Killing stale process {pid} ({pattern}): "
                                f"running for {elapsed_seconds/3600:.1f}h, max={max_runtime/3600:.1f}h"
                            )
                            subprocess.run(
                                ["kill", "-9", pid],
                                capture_output=True,
                                timeout=5,
                            )
                            killed_count += 1

                            # Send alert
                            if hasattr(self, 'notifier') and self.notifier:
                                asyncio.create_task(
                                    self.notifier.send(
                                        title="Stale Process Killed",
                                        message=f"Killed {pattern} (PID {pid}) after {elapsed_seconds/3600:.1f} hours",
                                        level="warning",
                                        node_id=self.node_id,
                                    )
                                )

                    except (ValueError, subprocess.TimeoutExpired):
                        continue

            except Exception as e:
                logger.debug(f"Error checking pattern {pattern}: {e}")
                continue

        if killed_count > 0:
            logger.info(f"Stale process cleanup: killed {killed_count} processes")

        return killed_count

    # ============================================
    # Phase 2: Distributed Data Sync Methods
    # ============================================

    def _collect_local_data_manifest(self) -> NodeDataManifest:
        """Collect manifest of all data files on this node.

        Scans the data directory for:
        - selfplay/ - Game replay files (.jsonl, .db)
        - models/ - Trained model files (.pt, .onnx)
        - training/ - Training data files (.npz)
        - games/ - Synced game databases (.db)

        Uses get_data_directory() to support both disk and ramdrive storage.
        """
        data_dir = self.get_data_directory()
        manifest = NodeDataManifest(
            node_id=self.node_id,
            collected_at=time.time(),
        )

        if not data_dir.exists():
            logger.info(f"Data directory not found: {data_dir}")
            return manifest

        files: list[DataFileInfo] = []

        def _count_jsonl_games(file_path: Path, file_size_bytes: int) -> int:
            """Count or estimate lines in a JSONL file.

            For small files (<=50MB), count lines directly.
            For large files, sample first 256KB to estimate average bytes/line,
            then extrapolate to estimate total lines.
            """
            if file_size_bytes == 0:
                return 0

            try:
                # For large files, estimate from sample to avoid blocking
                if file_size_bytes > MANIFEST_JSONL_LINECOUNT_MAX_BYTES:
                    with open(file_path, "rb") as f:
                        sample = f.read(MANIFEST_JSONL_SAMPLE_BYTES)
                        if not sample:
                            return 0
                        sample_lines = sample.count(b"\n")
                        if sample_lines == 0:
                            return 0
                        # Estimate total lines based on sample
                        avg_bytes_per_line = len(sample) / sample_lines
                        estimated_lines = int(file_size_bytes / avg_bytes_per_line)
                        return estimated_lines

                # For small files, count directly
                with open(file_path, "rb") as f:
                    line_count = 0
                    last_byte = b""
                    while True:
                        chunk = f.read(MANIFEST_JSONL_LINECOUNT_CHUNK_BYTES)
                        if not chunk:
                            break
                        line_count += chunk.count(b"\n")
                        last_byte = chunk[-1:]

                if file_size_bytes > 0 and last_byte != b"\n":
                    line_count += 1

                return int(line_count)
            except Exception:
                return 0

        # Scan for data files
        patterns = {
            "selfplay": ["selfplay/**/*.jsonl", "selfplay/**/*.db"],
            "model": ["models/**/*.pt", "models/**/*.onnx", "models/**/*.bin"],
            "training": ["training/**/*.npz"],
            "games": ["games/**/*.db"],
        }

        for file_type, globs in patterns.items():
            for pattern in globs:
                for file_path in data_dir.glob(pattern):
                    if not file_path.is_file():
                        continue

                    try:
                        stat = file_path.stat()
                        rel_path = str(file_path.relative_to(data_dir))

                        # Parse board_type and num_players from filename/path
                        board_type = ""
                        num_players = 0
                        path_lower = rel_path.lower()

                        if "sq8" in path_lower or "square8" in path_lower:
                            board_type = "square8"
                        elif "sq19" in path_lower or "square19" in path_lower:
                            board_type = "square19"
                        elif "hex" in path_lower:
                            board_type = "hexagonal"

                        if "_2p" in path_lower or "2p_" in path_lower:
                            num_players = 2
                        elif "_3p" in path_lower or "3p_" in path_lower:
                            num_players = 3
                        elif "_4p" in path_lower or "4p_" in path_lower:
                            num_players = 4

                        file_info = DataFileInfo(
                            path=rel_path,
                            size_bytes=stat.st_size,
                            modified_time=stat.st_mtime,
                            file_type=file_type,
                            board_type=board_type,
                            num_players=num_players,
                        )
                        files.append(file_info)

                        # Update summary stats
                        manifest.total_files += 1
                        manifest.total_size_bytes += stat.st_size

                        if file_type == "selfplay":
                            # Count games in JSONL files
                            if rel_path.endswith(".jsonl"):
                                try:
                                    line_count = _count_jsonl_games(file_path, stat.st_size)
                                    file_info.game_count = line_count
                                    manifest.selfplay_games += line_count
                                except Exception:
                                    pass
                            # Count games in SQLite databases
                            elif rel_path.endswith(".db"):
                                db_conn = None
                                try:
                                    db_conn = sqlite3.connect(str(file_path), timeout=5)
                                    cursor = db_conn.execute("SELECT COUNT(*) FROM games")
                                    game_count = cursor.fetchone()[0]
                                    file_info.game_count = game_count
                                    manifest.selfplay_games += game_count
                                except Exception:
                                    pass
                                finally:
                                    if db_conn:
                                        db_conn.close()
                        elif file_type == "model":
                            manifest.model_count += 1
                        elif file_type == "training":
                            manifest.training_data_size += stat.st_size

                    except Exception as e:
                        logger.error(f"scanning file {file_path}: {e}")

        manifest.files = files

        logger.info(f"Collected manifest: {manifest.total_files} files, "
              f"{manifest.total_size_bytes / (1024*1024):.1f}MB, "
              f"{manifest.selfplay_games} games")

        return manifest

    def _compute_file_hash(self, file_path: Path, chunk_size: int = 8192) -> str:
        """Compute MD5 hash of a file for verification."""
        import hashlib
        md5 = hashlib.md5()
        try:
            with open(file_path, 'rb') as f:
                while chunk := f.read(chunk_size):
                    md5.update(chunk)
            return md5.hexdigest()
        except Exception as e:
            logger.error(f"hashing file {file_path}: {e}")
            return ""

    async def _request_peer_manifest(self, peer_info: NodeInfo) -> NodeDataManifest | None:
        """Request data manifest from a peer node."""
        try:
            # Keep manifest requests snappy: these are advisory and should not
            # stall leader loops or external callers (e.g. the improvement
            # daemon). Prefer faster failure and rely on periodic retries.
            timeout = ClientTimeout(total=10, sock_connect=3, sock_read=7)
            async with get_client_session(timeout) as session:
                for url in self._urls_for_peer(peer_info, "/data_manifest"):
                    try:
                        async with session.get(url, headers=self._auth_headers()) as resp:
                            if resp.status != 200:
                                continue
                            data = await resp.json()
                        return NodeDataManifest.from_dict((data or {}).get("manifest", {}))
                    except Exception:
                        continue
        except Exception as e:
            logger.error(f"requesting manifest from {peer_info.node_id}: {e}")
        return None

    def _get_manifest_cache_path(self) -> Path:
        """Get path for persistent manifest cache."""
        data_dir = self.get_data_directory()
        return data_dir / ".manifest_cache.json"

    def _save_manifest_to_cache(self, manifest: NodeDataManifest) -> bool:
        """DECENTRALIZED: Save manifest to disk for faster startup.

        Persists the current manifest state so nodes can resume quickly after
        restart without needing to rescan all data files.
        """
        try:
            cache_path = self._get_manifest_cache_path()
            cache_data = {
                "version": 1,
                "saved_at": time.time(),
                "manifest": manifest.to_dict() if hasattr(manifest, "to_dict") else {},
            }
            import json
            with open(cache_path, "w") as f:
                json.dump(cache_data, f)
            return True
        except Exception as e:
            logger.error(f"Failed to save manifest cache: {e}")
            return False

    def _load_manifest_from_cache(self, max_age_seconds: int = 300) -> NodeDataManifest | None:
        """DECENTRALIZED: Load manifest from disk cache if fresh enough.

        Returns cached manifest if it exists and is not too old, otherwise None.
        This speeds up startup by avoiding full data directory scans.

        Args:
            max_age_seconds: Maximum age of cache to consider valid (default 5 minutes)
        """
        try:
            cache_path = self._get_manifest_cache_path()
            if not cache_path.exists():
                return None

            import json
            with open(cache_path) as f:
                cache_data = json.load(f)

            # Check version
            if cache_data.get("version") != 1:
                return None

            # Check age
            saved_at = cache_data.get("saved_at", 0)
            if time.time() - saved_at > max_age_seconds:
                return None

            # Parse manifest
            manifest_dict = cache_data.get("manifest", {})
            if not manifest_dict:
                return None

            manifest = NodeDataManifest.from_dict(manifest_dict)
            logger.info(f"Loaded manifest from cache (age: {int(time.time() - saved_at)}s)")
            return manifest
        except Exception as e:
            logger.error(f"Failed to load manifest cache: {e}")
            return None

    def _collect_local_data_manifest_cached(self, max_cache_age: int = 300) -> NodeDataManifest:
        """Collect manifest with caching support.

        First tries to load from cache, then falls back to full scan.
        Saves result to cache after collection.
        """
        # Try cache first
        cached = self._load_manifest_from_cache(max_age_seconds=max_cache_age)
        if cached:
            return cached

        # Full scan
        manifest = self._collect_local_data_manifest()

        # Save to cache
        self._save_manifest_to_cache(manifest)

        return manifest

    async def _collect_cluster_manifest(self) -> ClusterDataManifest:
        """Leader-only: Collect manifests from all peers and build cluster view."""
        cluster_manifest = ClusterDataManifest(
            collected_at=time.time(),
        )

        # Collect from self
        local_manifest = await asyncio.to_thread(self._collect_local_data_manifest)
        with self.manifest_lock:
            self.local_data_manifest = local_manifest
        cluster_manifest.node_manifests[self.node_id] = local_manifest

        # Collect from peers in parallel.
        #
        # Only probe peers that are currently alive and not retired; terminated
        # or long-dead nodes should not stall manifest collection. NAT-blocked
        # peers can't accept inbound /data_manifest, so they are excluded too.
        with self.peers_lock:
            peers = [
                p
                for p in self.peers.values()
                if p.is_alive()
                and not bool(getattr(p, "retired", False))
                and not bool(getattr(p, "nat_blocked", False))
            ]

        tasks = [self._request_peer_manifest(peer) for peer in peers]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for peer, result in zip(peers, results, strict=False):
            if isinstance(result, NodeDataManifest):
                cluster_manifest.node_manifests[peer.node_id] = result

        # Compute cluster-wide statistics
        cluster_manifest.total_nodes = len(cluster_manifest.node_manifests)

        all_files: set[str] = set()
        for node_id, node_manifest in cluster_manifest.node_manifests.items():
            cluster_manifest.total_files += node_manifest.total_files
            cluster_manifest.total_size_bytes += node_manifest.total_size_bytes
            cluster_manifest.total_selfplay_games += node_manifest.selfplay_games
            cluster_manifest.files_by_node[node_id] = node_manifest.total_files

            for file_info in node_manifest.files:
                all_files.add(file_info.path)

        cluster_manifest.unique_files = all_files

        # Find files missing from nodes (for sync planning)
        for file_path in all_files:
            nodes_with_file = []
            nodes_without_file = []
            for node_id, node_manifest in cluster_manifest.node_manifests.items():
                file_paths = {f.path for f in node_manifest.files}
                if file_path in file_paths:
                    nodes_with_file.append(node_id)
                else:
                    nodes_without_file.append(node_id)

            if nodes_without_file:
                cluster_manifest.missing_from_nodes[file_path] = nodes_without_file

        logger.info(f"Cluster manifest: {cluster_manifest.total_nodes} nodes, "
              f"{len(cluster_manifest.unique_files)} unique files, "
              f"{cluster_manifest.total_selfplay_games} total games")

        return cluster_manifest

    # ============================================
    # Phase 2: P2P Rsync Coordination Methods
    # ============================================

    def _generate_sync_plan(self) -> ClusterSyncPlan | None:
        """
        Leader generates a sync plan from the cluster manifest.
        Identifies which files are missing from which nodes and creates sync jobs.
        """
        if not self.cluster_data_manifest:
            logger.info("No cluster manifest available, cannot generate sync plan")
            return None

        if not self.cluster_data_manifest.missing_from_nodes:
            logger.info("All nodes have all files, no sync needed")
            return None

        plan = ClusterSyncPlan(
            plan_id=str(uuid.uuid4()),
            created_at=time.time(),
        )

        # For each missing file, find a source node and create a sync job
        for file_path, missing_nodes in self.cluster_data_manifest.missing_from_nodes.items():
            # Find a node that has this file (any node not in missing_nodes)
            source_node = None
            for node_id in self.cluster_data_manifest.manifests_by_node:
                if node_id not in missing_nodes:
                    node_manifest = self.cluster_data_manifest.manifests_by_node[node_id]
                    if file_path in node_manifest.files_by_path:
                        source_node = node_id
                        break

            if not source_node:
                continue  # No node has this file (shouldn't happen)

            # Create sync jobs for each target node
            for target_node in missing_nodes:
                job = DataSyncJob(
                    job_id=str(uuid.uuid4()),
                    source_node=source_node,
                    target_node=target_node,
                    files=[file_path],
                    status="pending",
                )

                # Get file size for tracking
                node_manifest = self.cluster_data_manifest.manifests_by_node[source_node]
                if file_path in node_manifest.files_by_path:
                    file_info = node_manifest.files_by_path[file_path]
                    plan.total_bytes_to_sync += file_info.size_bytes

                plan.sync_jobs.append(job)
                plan.total_files_to_sync += 1

        logger.info(f"Generated sync plan: {len(plan.sync_jobs)} jobs, "
              f"{plan.total_files_to_sync} files, "
              f"{plan.total_bytes_to_sync / (1024*1024):.1f} MB total")

        return plan

    async def _execute_sync_plan(self) -> None:
        """Leader executes the sync plan by dispatching jobs to nodes."""
        if not self.current_sync_plan:
            return

        # Check disk capacity before syncing
        has_capacity, disk_percent = check_disk_has_capacity()
        if not has_capacity:
            logger.info(f"SKIPPING SYNC - Disk usage {disk_percent:.1f}% exceeds limit {MAX_DISK_USAGE_PERCENT}%")
            return

        with self.sync_lock:
            if self.sync_in_progress:
                logger.info("Sync already in progress, skipping")
                return
            self.sync_in_progress = True
            self.current_sync_plan.status = "running"

        try:
            # Group jobs by target node for efficiency
            jobs_by_target: dict[str, list[DataSyncJob]] = {}
            for job in self.current_sync_plan.sync_jobs:
                if job.target_node not in jobs_by_target:
                    jobs_by_target[job.target_node] = []
                jobs_by_target[job.target_node].append(job)

            # Execute jobs for each target node
            for target_node, jobs in jobs_by_target.items():
                peer = self.peers.get(target_node)
                if target_node != self.node_id and (not peer or not peer.is_alive()):
                    logger.info(f"Target node {target_node} not available, skipping sync")
                    for job in jobs:
                        job.status = "failed"
                        job.error_message = "Target node not available"
                        self.current_sync_plan.jobs_failed += 1
                    continue

                # Send sync request to target node
                for job in jobs:
                    await self._request_node_sync(job)

            # Update plan status
            with self.sync_lock:
                if self.current_sync_plan.jobs_failed == len(self.current_sync_plan.sync_jobs):
                    self.current_sync_plan.status = "failed"
                elif self.current_sync_plan.jobs_completed == len(self.current_sync_plan.sync_jobs):
                    self.current_sync_plan.status = "completed"
                else:
                    self.current_sync_plan.status = "partial"

        finally:
            with self.sync_lock:
                self.sync_in_progress = False
                self.last_sync_time = time.time()

    async def _request_node_sync(self, job: DataSyncJob) -> bool:
        """Request a target node to pull files from a source node."""
        target_peer = self.peers.get(job.target_node)
        if job.target_node == self.node_id:
            target_peer = self.self_info

        source_peer = self.peers.get(job.source_node)
        if job.source_node == self.node_id:
            source_peer = self.self_info

        if not target_peer or not source_peer:
            job.status = "failed"
            job.error_message = "Source or target peer not found"
            return False

        job.status = "running"
        job.started_at = time.time()

        try:
            # Local target: execute the pull directly (no HTTP round-trip).
            if job.target_node == self.node_id:
                result = await self._handle_sync_pull_request(
                    source_host=source_peer.host,
                    source_port=source_peer.port,
                    source_reported_host=(getattr(source_peer, "reported_host", "") or None),
                    source_reported_port=(getattr(source_peer, "reported_port", 0) or None),
                    source_node_id=job.source_node,
                    files=job.files,
                )
            else:
                payload = {
                    "job_id": job.job_id,
                    # Back-compat: target will prefer source_node_id lookup.
                    "source_host": source_peer.host,
                    "source_port": source_peer.port,
                    "source_node_id": job.source_node,
                    "files": job.files,
                }
                rh = (getattr(source_peer, "reported_host", "") or "").strip()
                rp = int(getattr(source_peer, "reported_port", 0) or 0)
                if rh and rp and (rh != source_peer.host or rp != source_peer.port):
                    payload["source_reported_host"] = rh
                    payload["source_reported_port"] = rp

                timeout = ClientTimeout(total=600)
                async with get_client_session(timeout) as session:
                    result = None
                    last_err: str | None = None
                    for url in self._urls_for_peer(target_peer, "/sync/pull"):
                        try:
                            async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                                if resp.status != 200:
                                    last_err = f"http_{resp.status}"
                                    continue
                                result = await resp.json()
                                break
                        except Exception as e:
                            last_err = str(e)
                            continue
                    if result is None:
                        job.status = "failed"
                        job.error_message = last_err or "sync_pull_failed"
                        if self.current_sync_plan:
                            self.current_sync_plan.jobs_failed += 1
                        return False

            ok = bool(result.get("success"))
            job.status = "completed" if ok else "failed"
            job.completed_at = time.time()
            job.bytes_transferred = int(result.get("bytes_transferred", 0) or 0)
            job.files_completed = int(result.get("files_completed", 0) or 0)
            if not ok:
                job.error_message = str(result.get("error") or "Unknown error")

            if self.current_sync_plan:
                if ok:
                    self.current_sync_plan.jobs_completed += 1
                else:
                    self.current_sync_plan.jobs_failed += 1

            if ok:
                logger.info(f"Sync job {job.job_id[:8]} completed: {job.source_node} -> {job.target_node}")
            else:
                logger.info(f"Sync job {job.job_id[:8]} failed: {job.error_message}")

            return ok

        except Exception as e:
            job.status = "failed"
            job.error_message = str(e)
            job.completed_at = time.time()
            if self.current_sync_plan:
                self.current_sync_plan.jobs_failed += 1
            logger.info(f"Sync job {job.job_id[:8]} failed: {e}")
            return False

    async def _handle_sync_pull_request(
        self,
        source_host: str,
        source_port: int,
        source_node_id: str,
        files: list[str],
        source_reported_host: str | None = None,
        source_reported_port: int | None = None,
    ) -> dict[str, Any]:
        """
        Handle incoming request to pull files from a source node.
        Pulls files over the P2P HTTP channel to avoid SSH/rsync dependencies.
        Uses get_data_directory() to support both disk and ramdrive storage.
        """
        # Check disk capacity before pulling files
        has_capacity, disk_percent = check_disk_has_capacity()
        if not has_capacity:
            return {
                "success": False,
                "error": f"Disk full ({disk_percent:.1f}% >= {MAX_DISK_USAGE_PERCENT}%)",
                "disk_percent": disk_percent,
                "bytes_transferred": 0,
                "files_completed": 0,
            }

        data_dir = self.get_data_directory()
        data_dir.mkdir(parents=True, exist_ok=True)

        bytes_transferred = 0
        files_completed = 0
        errors: list[str] = []

        # Multi-path sources: prefer observed endpoint but allow a self-reported
        # endpoint (e.g. Tailscale) when the public route fails.
        candidate_sources: list[tuple[str, int]] = []
        seen_sources: set[tuple[str, int]] = set()

        def _add_source(host: str | None, port: int | None) -> None:
            if not host:
                return
            h = str(host).strip()
            if not h:
                return
            try:
                p = int(port or 0)
            except Exception:
                return
            if p <= 0:
                return
            key = (h, p)
            if key in seen_sources:
                return
            seen_sources.add(key)
            candidate_sources.append(key)

        _add_source(source_host, source_port)
        _add_source(source_reported_host, source_reported_port)

        timeout = ClientTimeout(total=None, sock_connect=HTTP_CONNECT_TIMEOUT, sock_read=600)

        async with get_client_session(timeout) as session:
            for rel_path in files:
                rel_path = (rel_path or "").lstrip("/")
                if not rel_path:
                    errors.append("empty_path")
                    continue

                # Security: keep all writes within ai-service/data.
                dest_path = (data_dir / rel_path)
                try:
                    data_root = data_dir.resolve()
                    dest_resolved = dest_path.resolve()
                    dest_resolved.relative_to(data_root)
                except Exception:
                    errors.append(f"invalid_path:{rel_path}")
                    continue

                dest_path.parent.mkdir(parents=True, exist_ok=True)
                tmp_path = dest_path.with_name(dest_path.name + ".partial")

                last_err: str | None = None
                success = False

                for host, base_port in candidate_sources:
                    # Back-compat: if caller passed an SSH-like port (22), try DEFAULT_PORT too.
                    ports_to_try: list[int] = []
                    try:
                        ports_to_try.append(int(base_port))
                    except Exception:
                        ports_to_try.append(DEFAULT_PORT)
                    if DEFAULT_PORT not in ports_to_try:
                        ports_to_try.append(DEFAULT_PORT)

                    for port in ports_to_try:
                        url = f"http://{host}:{port}/sync/file"
                        try:
                            async with session.get(
                                url,
                                params={"path": rel_path},
                                headers=self._auth_headers(),
                            ) as resp:
                                if resp.status != 200:
                                    text = ""
                                    try:
                                        text = (await resp.text())[:200]
                                    except Exception:
                                        text = ""
                                    last_err = f"{resp.status} {text}".strip()
                                    continue

                                with open(tmp_path, "wb") as out_f:
                                    async for chunk in resp.content.iter_chunked(1024 * 1024):
                                        out_f.write(chunk)
                                        bytes_transferred += len(chunk)

                                tmp_path.replace(dest_path)
                                files_completed += 1
                                success = True
                                break

                        except Exception as e:
                            last_err = str(e)
                            continue
                    if success:
                        break

                if not success:
                    errors.append(f"{rel_path}: {last_err or 'download_failed'}")
                    try:
                        if tmp_path.exists():
                            tmp_path.unlink()
                    except Exception:
                        pass

        if errors:
            return {
                "success": False,
                "files_completed": files_completed,
                "bytes_transferred": bytes_transferred,
                "error": "; ".join(errors[:5]),
            }

        return {
            "success": True,
            "files_completed": files_completed,
            "bytes_transferred": bytes_transferred,
        }

    async def start_cluster_sync(self) -> dict[str, Any]:
        """
        Leader initiates a full cluster data sync.
        Returns status of the sync operation.
        """
        if not self._is_leader():
            return {"success": False, "error": "Not the leader"}

        # First, collect fresh manifests
        logger.info("Collecting cluster manifest for sync...")
        self.cluster_data_manifest = await self._collect_cluster_manifest()

        # Generate sync plan
        self.current_sync_plan = self._generate_sync_plan()
        if not self.current_sync_plan:
            return {"success": True, "message": "No sync needed, all nodes in sync"}

        # Execute the plan
        await self._execute_sync_plan()

        return {
            "success": True,
            "plan_id": self.current_sync_plan.plan_id,
            "total_jobs": len(self.current_sync_plan.sync_jobs),
            "jobs_completed": self.current_sync_plan.jobs_completed,
            "jobs_failed": self.current_sync_plan.jobs_failed,
            "status": self.current_sync_plan.status,
        }

    # ============================================
    # Training Node Priority Sync
    # ============================================

    def _get_training_primary_nodes(self, count: int = TRAINING_NODE_COUNT) -> list[NodeInfo]:
        """Get the top N nodes by GPU power for training priority.

        Returns nodes sorted by GPU processing power (highest first).
        These nodes should receive selfplay data first for training.
        """
        with self.peers_lock:
            # Include self if we have a GPU
            all_nodes = list(self.peers.values())
            if self.self_info.has_gpu:
                all_nodes.append(self.self_info)

        # Filter to only GPU nodes that are alive and healthy
        gpu_nodes = [
            node for node in all_nodes
            if node.has_gpu and node.is_alive() and node.gpu_power_score() > 0
        ]

        # Sort by GPU power score (descending)
        gpu_nodes.sort(key=lambda n: n.gpu_power_score(), reverse=True)

        # Return top N
        return gpu_nodes[:count]

    def _get_training_nodes_ranked(self) -> list[dict[str, Any]]:
        """Get all GPU nodes with their power rankings for dashboard display."""
        with self.peers_lock:
            all_nodes = list(self.peers.values())
            if self.self_info.has_gpu:
                all_nodes.append(self.self_info)

        result = []
        for node in all_nodes:
            if node.has_gpu:
                result.append({
                    "node_id": node.node_id,
                    "gpu_name": node.gpu_name,
                    "gpu_power_score": node.gpu_power_score(),
                    "memory_gb": node.memory_gb,
                    "is_alive": node.is_alive(),
                    "is_healthy": node.is_healthy(),
                    "gpu_percent": node.gpu_percent,
                })

        # Sort by power score
        result.sort(key=lambda x: x["gpu_power_score"], reverse=True)
        return result

    # ============================================
    # CPU Node Priority for Data Processing
    # ============================================

    def _get_cpu_primary_nodes(self, count: int = 3) -> list[NodeInfo]:
        """Get the top N nodes by CPU power for CPU-intensive tasks.

        Returns nodes sorted by CPU processing power (highest first).
        These nodes should receive CPU-intensive work like NPZ export,
        data aggregation, etc. Vast nodes are strongly preferred.
        """
        with self.peers_lock:
            all_nodes = list(self.peers.values())
            all_nodes.append(self.self_info)

        # Filter to only alive and healthy nodes with CPU info
        cpu_nodes = [
            node for node in all_nodes
            if node.is_alive() and node.is_healthy() and node.cpu_power_score() > 0
        ]

        # Sort by CPU power score (descending) - vast nodes will rank highest
        cpu_nodes.sort(key=lambda n: (-n.cpu_power_score(), n.get_load_score()))

        # Return top N
        return cpu_nodes[:count]

    def _get_cpu_nodes_ranked(self) -> list[dict[str, Any]]:
        """Get all nodes with their CPU power rankings for dashboard display."""
        with self.peers_lock:
            all_nodes = list(self.peers.values())
            all_nodes.append(self.self_info)

        result = []
        for node in all_nodes:
            if node.cpu_count and node.cpu_count > 0:
                result.append({
                    "node_id": node.node_id,
                    "cpu_count": node.cpu_count,
                    "cpu_power_score": node.cpu_power_score(),
                    "cpu_percent": node.cpu_percent,
                    "memory_gb": node.memory_gb,
                    "is_alive": node.is_alive(),
                    "is_healthy": node.is_healthy(),
                    "has_gpu": node.has_gpu,
                })

        # Sort by CPU power score (descending)
        result.sort(key=lambda x: x["cpu_power_score"], reverse=True)
        return result

    # ============================================
    # Task-Specific Node Selection
    # ============================================

    def _get_best_gpu_node_for_training(self) -> NodeInfo | None:
        """Get the best GPU node for neural network training.

        Prioritizes:
        1. GPU power score (H100 > GH200 > A10 > consumer GPUs)
        2. Low current load
        3. Not already running training
        """
        with self.peers_lock:
            all_nodes = list(self.peers.values())
            all_nodes.append(self.self_info)

        # Filter to GPU nodes that are healthy and not retired
        gpu_nodes = [
            n for n in all_nodes
            if n.has_gpu
            and n.is_alive()
            and n.is_healthy()
            and not getattr(n, "retired", False)
            and n.gpu_power_score() > 0
        ]

        if not gpu_nodes:
            return None

        # Prefer nodes not already running training
        nodes_with_training = {
            j.worker_node for j in self.training_jobs.values()
            if j.status in ("running", "queued")
        }
        available = [n for n in gpu_nodes if n.node_id not in nodes_with_training]
        candidates = available if available else gpu_nodes

        # Sort by GPU power (descending), then load (ascending)
        candidates.sort(key=lambda n: (-n.gpu_power_score(), n.get_load_score()))
        return candidates[0] if candidates else None

    def _get_best_cpu_node_for_gauntlet(self) -> NodeInfo | None:
        """Get the best CPU node for gauntlet/tournament work.

        Prioritizes Vast instances with high CPU count (200+ vCPUs).
        Gauntlets are CPU-bound and benefit from massive parallelism.
        """
        with self.peers_lock:
            all_nodes = list(self.peers.values())
            all_nodes.append(self.self_info)

        # Filter to healthy nodes with high CPU count
        cpu_nodes = [
            n for n in all_nodes
            if n.is_alive()
            and n.is_healthy()
            and not getattr(n, "retired", False)
            and n.cpu_power_score() > 0
        ]

        if not cpu_nodes:
            return None

        # Strongly prefer Vast nodes (identified by "vast" in node_id or high CPU count)
        vast_nodes = [
            n for n in cpu_nodes
            if "vast" in n.node_id.lower() or n.cpu_count >= 64
        ]

        # Use vast nodes if available, otherwise fall back to any CPU node
        candidates = vast_nodes if vast_nodes else cpu_nodes

        # Sort by CPU power (descending), then load (ascending)
        candidates.sort(key=lambda n: (-n.cpu_power_score(), n.get_load_score()))
        return candidates[0] if candidates else None

    async def _dispatch_gauntlet_to_cpu_node(
        self,
        config_key: str,
        model_id: str,
        baseline_id: str,
        games_per_side: int = 4
    ) -> dict[str, Any] | None:
        """Dispatch gauntlet evaluation to a CPU-rich node.

        This ensures gauntlets run on Vast instances with high CPU count
        rather than blocking GPU-rich nodes like GH200/H100.
        """
        cpu_node = self._get_best_cpu_node_for_gauntlet()
        if not cpu_node:
            logger.info("No CPU node available for gauntlet, running locally")
            return None

        # If we're already the best CPU node, return None to run locally
        if cpu_node.node_id == self.node_id:
            return None

        try:
            # Dispatch to the CPU node's gauntlet endpoint
            payload = {
                "config_key": config_key,
                "model_id": model_id,
                "baseline_id": baseline_id,
                "games_per_side": games_per_side,
            }

            timeout = ClientTimeout(total=120)
            async with get_client_session(timeout) as session:
                for url in self._urls_for_peer(cpu_node, "/gauntlet/quick-eval"):
                    try:
                        async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                            if resp.status == 200:
                                result = await resp.json()
                                logger.info(f"Gauntlet dispatched to {cpu_node.node_id} "
                                      f"(cpu_power={cpu_node.cpu_power_score()})")
                                return result
                    except Exception:
                        continue

            logger.error(f"Failed to dispatch gauntlet to {cpu_node.node_id}")
            return None
        except Exception as e:
            logger.error(f"dispatching gauntlet: {e}")
            return None

    def _should_sync_to_node(self, node: NodeInfo) -> bool:
        """Check if we should sync data TO this node based on disk space."""
        # Don't sync to nodes with critical disk usage
        if node.disk_percent >= DISK_CRITICAL_THRESHOLD:
            logger.info(f"Skipping sync to {node.node_id}: disk critical ({node.disk_percent:.1f}%)")
            return False
        # Warn but allow sync to nodes with warning-level disk
        if node.disk_percent >= DISK_WARNING_THRESHOLD:
            logger.warning(f"{node.node_id} disk at {node.disk_percent:.1f}%")
        return True

    def _should_cleanup_source(self, node: NodeInfo) -> bool:
        """Check if source node needs disk cleanup after sync."""
        return node.disk_percent >= DISK_CLEANUP_THRESHOLD

    async def _cleanup_synced_files(self, node_id: str, files: list[str]) -> bool:
        """Delete synced files from source node to free disk space.

        Only called after successful sync to training nodes.
        """
        with self.peers_lock:
            node = self.peers.get(node_id)
        if not node or not node.is_alive():
            return False

        try:
            if getattr(node, "nat_blocked", False):
                cmd_id = await self._enqueue_relay_command_for_peer(
                    node,
                    "cleanup_files",
                    {"files": list(files or []), "reason": "post_sync_cleanup"},
                )
                if cmd_id:
                    logger.info(f"Enqueued relay cleanup_files for {node_id} ({len(files)} files)")
                    return True
                logger.info(f"Relay queue full for {node_id}; skipping cleanup_files enqueue")
                return False

            timeout = ClientTimeout(total=60)
            async with get_client_session(timeout) as session:
                last_err: str | None = None
                for url in self._urls_for_peer(node, "/cleanup/files"):
                    try:
                        async with session.post(
                            url,
                            json={"files": files, "reason": "post_sync_cleanup"},
                            headers=self._auth_headers(),
                        ) as resp:
                            if resp.status != 200:
                                last_err = f"http_{resp.status}"
                                continue
                            result = await resp.json()
                            freed_bytes = result.get("freed_bytes", 0)
                            logger.info(f"Cleanup on {node_id}: freed {freed_bytes / 1e6:.1f}MB")
                            return True
                    except Exception as e:
                        last_err = str(e)
                        continue
                if last_err:
                    logger.info(f"Cleanup files request failed on {node_id}: {last_err}")
        except Exception as e:
            logger.error(f"Failed to cleanup files on {node_id}: {e}")
        return False

    async def _sync_selfplay_to_training_nodes(self) -> dict[str, Any]:
        """Sync selfplay data to training primary nodes.

        This is called periodically by the leader to ensure training nodes
        have the latest selfplay data for model training.

        Features:
        - Prioritizes nodes by GPU power (H100 > 5090 > 4090 > etc.)
        - Skips syncing TO nodes with critical disk usage
        - Cleans up source nodes with high disk usage after successful sync
        """
        if not self._is_leader():
            return {"success": False, "error": "Not the leader"}

        # Get training primary nodes
        training_nodes = self._get_training_primary_nodes()
        if not training_nodes:
            return {"success": False, "error": "No training nodes available"}

        # Filter out nodes with critical disk space
        eligible_training_nodes = [n for n in training_nodes if self._should_sync_to_node(n)]
        if not eligible_training_nodes:
            return {"success": False, "error": "All training nodes have critical disk usage"}

        logger.info(f"Training sync: {len(eligible_training_nodes)} eligible training nodes")
        for node in eligible_training_nodes:
            logger.info(f"  - {node.node_id}: {node.gpu_name} (power={node.gpu_power_score()}, disk={node.disk_percent:.1f}%)")

        # Collect current cluster manifest if stale
        if (time.time() - self.last_manifest_collection > self.manifest_collection_interval
                or not self.cluster_data_manifest):
            logger.info("Collecting fresh cluster manifest for training sync...")
            self.cluster_data_manifest = await self._collect_cluster_manifest()
            self.last_manifest_collection = time.time()

        if not self.cluster_data_manifest:
            return {"success": False, "error": "Failed to collect cluster manifest"}

        # Track source nodes that need cleanup after sync
        sources_to_cleanup: dict[str, list[str]] = {}  # node_id -> list of synced files

        # Find selfplay files that training nodes don't have
        sync_jobs_created = 0
        for target_node in eligible_training_nodes:
            target_manifest = self.cluster_data_manifest.node_manifests.get(target_node.node_id)
            target_files = set()
            if target_manifest:
                target_files = set(target_manifest.files_by_path.keys())

            # Find source nodes with selfplay data this target doesn't have
            for source_id, source_manifest in self.cluster_data_manifest.node_manifests.items():
                if source_id == target_node.node_id:
                    continue

                # Check if source node needs disk cleanup
                source_node = self.peers.get(source_id)
                needs_cleanup = source_node and self._should_cleanup_source(source_node)

                # Find selfplay files to sync (with mtime comparison for efficiency)
                files_to_sync = []
                for file_info in source_manifest.files:
                    if file_info.file_type != "selfplay":
                        continue

                    # Check if target needs this file
                    target_file_info = target_manifest.files_by_path.get(file_info.path) if target_manifest else None

                    should_sync = False
                    if file_info.path not in target_files:
                        # Target doesn't have file at all
                        should_sync = True
                    elif target_file_info and file_info.modified_time > target_file_info.modified_time + 60:
                        # Source is newer (with 60s tolerance to avoid clock skew issues)
                        should_sync = True

                    if should_sync:
                        files_to_sync.append(file_info.path)

                if files_to_sync:
                    # Create sync job
                    job_id = f"training_sync_{source_id}_to_{target_node.node_id}_{int(time.time())}"
                    job = DataSyncJob(
                        job_id=job_id,
                        source_node=source_id,
                        target_node=target_node.node_id,
                        files=files_to_sync[:50],  # Limit files per job
                        status="pending",
                    )
                    self.active_sync_jobs[job_id] = job
                    sync_jobs_created += 1
                    logger.info(f"Created training sync job: {len(files_to_sync)} files from {source_id} to {target_node.node_id}")

                    # Track files for cleanup if source has high disk usage
                    if needs_cleanup:
                        if source_id not in sources_to_cleanup:
                            sources_to_cleanup[source_id] = []
                        sources_to_cleanup[source_id].extend(files_to_sync[:50])

        # Execute sync jobs
        successful_syncs = 0
        if sync_jobs_created > 0:
            await self._execute_pending_sync_jobs()
            # Count successful syncs
            successful_syncs = sum(
                1 for job in self.active_sync_jobs.values()
                if job.status == "completed"
            )

        # Cleanup source nodes with high disk usage after successful syncs
        cleanup_results = {}
        if successful_syncs > 0 and sources_to_cleanup:
            logger.info(f"Running post-sync cleanup on {len(sources_to_cleanup)} source nodes...")
            for source_id, files in sources_to_cleanup.items():
                success = await self._cleanup_synced_files(source_id, files)
                cleanup_results[source_id] = success

        self.last_training_sync_time = time.time()

        return {
            "success": True,
            "training_nodes": [n.node_id for n in eligible_training_nodes],
            "sync_jobs_created": sync_jobs_created,
            "successful_syncs": successful_syncs,
            "sources_cleaned": sum(cleanup_results.values()),
        }

    async def _execute_pending_sync_jobs(self):
        """Execute all pending sync jobs."""
        with self.sync_lock:
            pending_jobs = [
                job for job in self.active_sync_jobs.values()
                if job.status == "pending"
            ]

        for job in pending_jobs:
            try:
                success = await self._request_node_sync(job)
                if success:
                    job.status = "completed"
                    job.completed_at = time.time()
                else:
                    job.status = "failed"
            except Exception as e:
                logger.info(f"Sync job {job.job_id} failed: {e}")
                job.status = "failed"
                job.error_message = str(e)

    async def _training_sync_loop(self):
        """Background loop to periodically sync data to training nodes.

        Leader-only: Runs every TRAINING_SYNC_INTERVAL seconds to ensure
        training nodes have the latest selfplay data.
        """
        logger.info(f"Training sync loop started (interval: {self.training_sync_interval}s)")

        while self.running:
            try:
                await asyncio.sleep(self.training_sync_interval)

                if not self._is_leader():
                    continue

                # Check if enough time has passed since last sync
                if time.time() - self.last_training_sync_time < self.training_sync_interval:
                    continue

                # Check disk capacity before syncing
                has_capacity, disk_percent = check_disk_has_capacity()
                if not has_capacity:
                    logger.info(f"SKIPPING training sync - Disk {disk_percent:.1f}% >= {MAX_DISK_USAGE_PERCENT}%")
                    continue

                logger.info("Running periodic training node sync...")
                result = await self._sync_selfplay_to_training_nodes()
                if result.get("success"):
                    logger.info(f"Training sync completed: {result.get('sync_jobs_created', 0)} jobs created")
                else:
                    logger.info(f"Training sync failed: {result.get('error', 'Unknown error')}")

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.info(f"Training sync loop error: {e}")
                await asyncio.sleep(60)  # Wait before retrying

    async def _force_ip_refresh_all_sources(self) -> int:
        """Force immediate refresh of IPs from all CLI sources (Tailscale, Vast, AWS).

        Called when network partition is detected to aggressively discover
        alternative paths to reach peers.

        Returns:
            Total number of IPs updated across all sources
        """
        if not HAS_DYNAMIC_REGISTRY or get_registry is None:
            return 0

        registry = get_registry()
        total_updated = 0

        logger.info("Force-refreshing all IP sources for partition recovery...")

        # Refresh Tailscale first (most likely to help in partition)
        try:
            # Reset rate limit to force immediate check
            registry._last_tailscale_check = 0
            updated = await registry.update_tailscale_ips()
            if updated > 0:
                logger.info(f"Tailscale refresh: {updated} IPs updated")
                total_updated += updated
        except Exception as e:
            logger.info(f"Tailscale refresh error: {e}")

        # Refresh Vast IPs
        try:
            registry._last_vast_check = 0
            updated = await registry.update_vast_ips()
            if updated > 0:
                logger.info(f"Vast refresh: {updated} IPs updated")
                total_updated += updated
        except Exception as e:
            logger.info(f"Vast refresh error: {e}")

        # Refresh AWS IPs
        try:
            registry._last_aws_check = 0
            updated = await registry.update_aws_ips()
            if updated > 0:
                logger.info(f"AWS refresh: {updated} IPs updated")
                total_updated += updated
        except Exception as e:
            logger.info(f"AWS refresh error: {e}")

        if total_updated > 0:
            logger.info(f"Force refresh complete: {total_updated} total IPs updated")
        return total_updated

    async def _vast_ip_update_loop(self):
        """Background loop to periodically refresh Vast instance connection info.

        Uses VAST_API_KEY when available, otherwise falls back to the `vastai`
        CLI if installed (see DynamicHostRegistry.update_vast_ips).
        """
        if not HAS_DYNAMIC_REGISTRY:
            return

        logger.info("Vast IP update loop started")
        registry = get_registry()

        while self.running:
            try:
                await asyncio.sleep(300)  # Check every 5 minutes

                updated = await registry.update_vast_ips()
                if updated > 0:
                    logger.info(f"Updated {updated} Vast instance IPs from API")

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.info(f"Vast IP update loop error: {e}")
                await asyncio.sleep(60)

    async def _aws_ip_update_loop(self):
        """Background loop to periodically refresh AWS instance connection info.

        Uses the `aws` CLI (see DynamicHostRegistry.update_aws_ips). No-op when
        no AWS instances are configured in distributed_hosts.yaml properties.
        """
        if not HAS_DYNAMIC_REGISTRY:
            return

        logger.info("AWS IP update loop started")
        registry = get_registry()

        while self.running:
            try:
                await asyncio.sleep(300)  # Check every 5 minutes

                updated = await registry.update_aws_ips()
                if updated > 0:
                    logger.info(f"Updated {updated} AWS instance IPs via CLI")

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.info(f"AWS IP update loop error: {e}")
                await asyncio.sleep(60)

    async def _tailscale_ip_update_loop(self):
        """Background loop to discover and update Tailscale IPs for cluster nodes.

        Uses `tailscale status --json` to discover mesh network peers.
        Tailscale provides reliable connectivity even when public IPs change.
        """
        if not HAS_DYNAMIC_REGISTRY:
            return

        logger.info("Tailscale IP update loop started")
        registry = get_registry()

        while self.running:
            try:
                await asyncio.sleep(120)  # Check every 2 minutes

                updated = await registry.update_tailscale_ips()
                if updated > 0:
                    logger.info(f"Updated {updated} node Tailscale IPs")

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.info(f"Tailscale IP update loop error: {e}")
                await asyncio.sleep(60)

    async def _tailscale_peer_recovery_loop(self):
        """Proactively discover and connect to all Tailscale nodes.

        LEARNED LESSONS: Cross-cloud nodes (Lambda, Vast, Hetzner) can lose
        connectivity intermittently. This loop ensures all nodes stay in the
        P2P network by:
        1. Running `tailscale status --json` to find all online nodes
        2. Attempting to connect to nodes not in the peer list
        3. Retrying dead/stale nodes more aggressively
        """
        import json
        import subprocess

        logger.info("Tailscale peer recovery loop started (interval=120s)")

        # Patterns for compute nodes we want in the P2P network
        COMPUTE_PATTERNS = [
            "lambda-", "vast-", "gh200", "h100", "a100", "a10",
            "192-222-", "aws-",  # Lambda public IPs and AWS nodes
        ]

        while self.running:
            try:
                await asyncio.sleep(120)  # Every 2 minutes

                # Phase 30: All nodes participate in discovery (not just leader)
                # This ensures isolated nodes can rejoin the cluster
                # Rate limit non-leaders to every 5 minutes (3 loops)
                is_leader = self.role == NodeRole.LEADER
                if not is_leader:
                    # Non-leaders do discovery less frequently
                    loop_count = getattr(self, "_ts_recovery_loop_count", 0) + 1
                    self._ts_recovery_loop_count = loop_count
                    if loop_count % 3 != 0:  # Every 3rd iteration = 6 minutes
                        # Unless we're isolated (few peers)
                        with self.peers_lock:
                            alive_count = sum(1 for p in self.peers.values() if p.is_alive())
                        if alive_count >= MIN_CONNECTED_PEERS:
                            continue  # Skip if we have enough peers

                # Get current peer node_ids
                current_peers = set()
                with self.peers_lock:
                    current_peers = {p.node_id for p in self.peers.values()}

                # Get Tailscale peers
                try:
                    result = subprocess.run(
                        ["tailscale", "status", "--json"],
                        capture_output=True, text=True, timeout=10
                    )
                    if result.returncode != 0:
                        continue
                    ts_data = json.loads(result.stdout)
                except Exception as e:
                    logger.debug(f"Tailscale status failed: {e}")
                    continue

                # Find compute nodes not in P2P network
                missing_nodes = []
                for _peer_id, peer_info in ts_data.get("Peer", {}).items():
                    hostname = peer_info.get("HostName", "").lower()
                    online = peer_info.get("Online", False)
                    ts_ips = peer_info.get("TailscaleIPs", [])

                    if not online or not ts_ips:
                        continue

                    # Check if this is a compute node
                    is_compute = any(pat in hostname for pat in COMPUTE_PATTERNS)
                    if not is_compute:
                        continue

                    # Check if already in P2P network
                    if hostname in current_peers or hostname.replace("-", "_") in current_peers:
                        continue

                    # Also check by IP prefix
                    ip = ts_ips[0] if ts_ips else ""
                    ip_based_id = ip.replace(".", "-")
                    if ip_based_id in current_peers:
                        continue

                    missing_nodes.append((hostname, ip))

                if missing_nodes:
                    logger.info(f"Tailscale peer recovery: {len(missing_nodes)} compute nodes not in P2P network")
                    for hostname, ip in missing_nodes[:10]:  # Limit to 10 per cycle
                        logger.info(f"  Attempting to connect: {hostname} ({ip})")
                        try:
                            # Try to establish connection via heartbeat
                            url = f"http://{ip}:{DEFAULT_PORT}/health"
                            timeout = ClientTimeout(total=10)
                            async with get_client_session(timeout) as session:
                                async with session.get(url) as resp:
                                    if resp.status == 200:
                                        data = await resp.json()
                                        node_id = data.get("node_id", hostname)
                                        logger.info(f"  Connected to {node_id}, sending join request")
                                        # Send heartbeat to register
                                        await self._send_heartbeat_to_peer(ip, DEFAULT_PORT)
                        except Exception as e:
                            logger.debug(f"  Failed to connect to {hostname}: {e}")

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.warning(f"Tailscale peer recovery loop error: {e}")
                await asyncio.sleep(60)

    async def _discover_tailscale_peers(self):
        """One-shot Tailscale peer discovery for bootstrap fallback.

        Phase 30: Called when bootstrap from seeds fails. Discovers peers
        via `tailscale status --json` and attempts to connect.
        """
        import json
        import subprocess

        logger.info("Running one-shot Tailscale peer discovery...")

        try:
            result = subprocess.run(
                ["tailscale", "status", "--json"],
                capture_output=True, text=True, timeout=10
            )
            if result.returncode != 0:
                logger.warning(f"Tailscale status failed: {result.stderr}")
                return

            ts_data = json.loads(result.stdout)
            peers = ts_data.get("Peer", {})

            # Get current peer node_ids
            current_peers = set()
            with self.peers_lock:
                current_peers = {p.node_id for p in self.peers.values()}

            # Patterns for compute nodes
            COMPUTE_PATTERNS = [
                "lambda-", "vast-", "gh200", "h100", "a100", "a10",
                "nebius-", "runpod-", "vultr-", "hetzner-",
            ]

            discovered = 0
            for peer_info in peers.values():
                hostname = peer_info.get("HostName", "").lower()
                is_compute = any(pat in hostname for pat in COMPUTE_PATTERNS)
                if not is_compute:
                    continue

                # Get IP from TailscaleIPs (prefer IPv4)
                ts_ips = peer_info.get("TailscaleIPs", [])
                ipv4s = [ip for ip in ts_ips if "." in ip]
                if not ipv4s:
                    continue
                ip = ipv4s[0]

                # Skip if we already know this IP
                known = False
                with self.peers_lock:
                    for p in self.peers.values():
                        if getattr(p, "tailscale_ip", None) == ip or p.host == ip:
                            known = True
                            break
                if known:
                    continue

                # Try to connect
                try:
                    async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=5)) as session:
                        async with session.get(f"http://{ip}:{DEFAULT_PORT}/status") as resp:
                            if resp.status == 200:
                                data = await resp.json()
                                node_id = data.get("node_id", hostname)
                                if node_id not in current_peers:
                                    logger.info(f"Discovered peer {node_id} via Tailscale at {ip}")
                                    await self._send_heartbeat_to_peer(ip, DEFAULT_PORT)
                                    discovered += 1
                except Exception:
                    pass  # Silently skip unreachable nodes

            if discovered > 0:
                logger.info(f"Tailscale discovery: connected to {discovered} new peer(s)")
            else:
                logger.info("Tailscale discovery: no new peers found")

        except FileNotFoundError:
            logger.debug("Tailscale not installed on this node")
        except Exception as e:
            logger.warning(f"Tailscale discovery error: {e}")

    async def _convert_jsonl_to_db(self, data_dir: Path, games_dir: Path) -> int:
        """Convert JSONL selfplay files to SQLite DB format for training.

        This enables the training pipeline to access games stored in JSONL format.
        Converted files are tracked to avoid re-processing.

        Features:
        - Chunked reading to handle large files without memory issues
        - Limited files per cycle to avoid blocking the event loop
        - Spawns external converter for large backlogs

        Returns:
            Number of games converted.
        """
        import json
        import sqlite3

        # Configuration
        MAX_FILES_PER_CYCLE = 50  # Process max 50 files per cycle to avoid blocking
        CHUNK_SIZE = 500  # Lines to read at a time
        LARGE_BACKLOG_THRESHOLD = 200  # Spawn external converter if more files

        # Track converted files to avoid re-processing
        converted_marker_file = data_dir / ".jsonl_converted"
        converted_files: set = set()
        if converted_marker_file.exists():
            with contextlib.suppress(Exception):
                converted_files = set(converted_marker_file.read_text().strip().split("\n"))

        total_converted = 0
        newly_converted = []
        selfplay_dir = data_dir / "selfplay"

        if not selfplay_dir.exists():
            return 0

        # Find all JSONL files in selfplay subdirectories
        jsonl_files = list(selfplay_dir.rglob("*.jsonl"))
        if not jsonl_files:
            return 0

        # Filter to unconverted files and sort by size (smallest first for quick wins)
        unconverted_files = []
        for jsonl_file in jsonl_files:
            try:
                file_size = jsonl_file.stat().st_size
                if file_size < 100:
                    continue
                file_key = str(jsonl_file.relative_to(data_dir))
                if file_key not in converted_files:
                    unconverted_files.append((jsonl_file, file_size, file_key))
            except Exception:
                continue

        if not unconverted_files:
            return 0

        # Sort by size (smallest first) and limit per cycle
        unconverted_files.sort(key=lambda x: x[1])
        files_this_cycle = unconverted_files[:MAX_FILES_PER_CYCLE]

        # If large backlog, spawn external converter in background
        if len(unconverted_files) > LARGE_BACKLOG_THRESHOLD:
            logger.info(f"Large JSONL backlog ({len(unconverted_files)} files), spawning background converter")
            converter_script = self.ringrift_path / "ai-service" / "scripts" / "chunked_jsonl_converter.py"
            if converter_script.exists():
                try:
                    import subprocess
                    subprocess.Popen(
                        ["python3", str(converter_script),
                         "--input-dir", str(selfplay_dir),
                         "--output-dir", str(games_dir),
                         "--workers", "2",
                         "--chunk-size", "500"],
                        stdout=open("/tmp/chunked_converter.log", "a"),
                        stderr=subprocess.STDOUT,
                        cwd=str(self.ringrift_path / "ai-service"),
                    )
                except Exception as e:
                    logger.error(f"Failed to spawn background converter: {e}")

        # Group files by board type
        board_type_files: dict[str, list[tuple[Path, str]]] = {}
        for jsonl_file, _file_size, file_key in files_this_cycle:
            path_str = str(jsonl_file).lower()
            if "hex" in path_str:
                if "4p" in path_str:
                    board_key = "hexagonal_4p"
                elif "3p" in path_str:
                    board_key = "hexagonal_3p"
                else:
                    board_key = "hexagonal_2p"
            elif "square19" in path_str or "sq19" in path_str:
                if "4p" in path_str:
                    board_key = "square19_4p"
                elif "3p" in path_str:
                    board_key = "square19_3p"
                else:
                    board_key = "square19_2p"
            else:
                if "4p" in path_str:
                    board_key = "square8_4p"
                elif "3p" in path_str:
                    board_key = "square8_3p"
                else:
                    board_key = "square8_2p"

            if board_key not in board_type_files:
                board_type_files[board_key] = []
            board_type_files[board_key].append((jsonl_file, file_key))

        # Convert each board type to a consolidated DB (chunked reading)
        for board_key, files in board_type_files.items():
            if not files:
                continue

            db_path = games_dir / f"jsonl_converted_{board_key}.db"
            conn = None
            try:
                conn = sqlite3.connect(str(db_path), timeout=30.0)
                conn.execute("PRAGMA journal_mode=WAL")
                conn.execute("PRAGMA synchronous=NORMAL")
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS games (
                        game_id TEXT PRIMARY KEY,
                        board_type TEXT,
                        num_players INTEGER,
                        winner INTEGER,
                        move_count INTEGER,
                        game_status TEXT,
                        victory_type TEXT,
                        created_at TEXT,
                        source TEXT,
                        metadata_json TEXT
                    )
                """)
                conn.commit()

                games_added = 0
                for jsonl_file, file_key in files:
                    try:
                        # Read file in chunks to avoid memory issues
                        chunk_buffer = []
                        with open_jsonl_file(jsonl_file) as f:
                            for line_num, line in enumerate(f, 1):
                                line = line.strip()
                                if not line:
                                    continue
                                try:
                                    record = json.loads(line)
                                    game_id = f"{jsonl_file.stem}_{record.get('game_id', line_num)}"
                                    chunk_buffer.append((
                                        game_id,
                                        record.get("board_type", board_key.split("_")[0]),
                                        record.get("num_players", int(board_key[-2]) if board_key[-2].isdigit() else 2),
                                        record.get("winner", 0),
                                        record.get("move_count", 0),
                                        record.get("status", "completed"),
                                        record.get("victory_type", "unknown"),
                                        record.get("timestamp", ""),
                                        f"jsonl:{jsonl_file.name}",
                                        json.dumps(record),
                                    ))

                                    # Flush chunk when buffer is full
                                    if len(chunk_buffer) >= CHUNK_SIZE:
                                        conn.executemany("""
                                            INSERT OR IGNORE INTO games
                                            (game_id, board_type, num_players, winner, move_count,
                                             game_status, victory_type, created_at, source, metadata_json)
                                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                                        """, chunk_buffer)
                                        games_added += len(chunk_buffer)
                                        chunk_buffer = []

                                except (json.JSONDecodeError, Exception):
                                    continue

                        # Flush remaining records
                        if chunk_buffer:
                            conn.executemany("""
                                INSERT OR IGNORE INTO games
                                (game_id, board_type, num_players, winner, move_count,
                                 game_status, victory_type, created_at, source, metadata_json)
                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                            """, chunk_buffer)
                            games_added += len(chunk_buffer)

                        newly_converted.append(file_key)
                    except Exception as e:
                        logger.error(f"converting {jsonl_file.name}: {e}")
                        continue

                conn.commit()
                total_converted += games_added

                if games_added > 0:
                    logger.info(f"Converted {games_added} games to {db_path.name}")

            except Exception as e:
                logger.error(f"creating DB for {board_key}: {e}")
            finally:
                if conn:
                    conn.close()

        # Update converted files marker
        if newly_converted:
            try:
                all_converted = converted_files | set(newly_converted)
                converted_marker_file.write_text("\n".join(sorted(all_converted)))
            except Exception:
                pass

        if total_converted > 0:
            logger.info(f"JSONL conversion complete: {total_converted} total games converted")

        return total_converted

    async def _convert_jsonl_to_npz_for_training(self, data_dir: Path, training_dir: Path) -> int:
        """Convert JSONL selfplay files directly to NPZ for training.

        This bypasses the DB intermediate step and creates training-ready NPZ files
        directly from JSONL. Called periodically when JSONL backlog exists.

        Returns:
            Number of NPZ files created.
        """
        import json as json_module
        import subprocess

        # Configuration
        JSONL_THRESHOLD_GAMES = 50  # Only convert when > 50 games accumulated
        MAX_CONVERSIONS_PER_CYCLE = 3  # Limit conversions per cycle

        selfplay_dir = data_dir / "selfplay"
        canonical_dir = selfplay_dir / "canonical"

        # Track converted files
        npz_marker_file = data_dir / ".jsonl_to_npz_converted"
        converted_files: set = set()
        if npz_marker_file.exists():
            with contextlib.suppress(Exception):
                converted_files = set(npz_marker_file.read_text().strip().split("\n"))

        conversions_done = 0
        newly_converted = []

        # Board configs to check
        board_configs = [
            ("square8", 2), ("square8", 3), ("square8", 4),
            ("square19", 2), ("square19", 3), ("square19", 4),
            ("hex8", 2), ("hex8", 3), ("hex8", 4),
            ("hexagonal", 2), ("hexagonal", 3), ("hexagonal", 4),
        ]

        for board_type, num_players in board_configs:
            if conversions_done >= MAX_CONVERSIONS_PER_CYCLE:
                break

            config_key = f"{board_type}_{num_players}p"

            # Skip if already converted recently
            if config_key in converted_files:
                continue

            # Find JSONL files for this config
            jsonl_files = []
            search_dirs = [canonical_dir, selfplay_dir, data_dir / "games"]

            for search_dir in search_dirs:
                if not search_dir.exists():
                    continue
                for jsonl_file in search_dir.rglob("*.jsonl"):
                    if jsonl_file.stat().st_size < 100:
                        continue
                    jsonl_files.append(jsonl_file)

            if not jsonl_files:
                continue

            # Count games matching this config (quick check - just count lines with board type)
            game_count = 0
            valid_files = []
            for jsonl_file in jsonl_files:
                try:
                    with open_jsonl_file(jsonl_file) as f:
                        for line in f:
                            if not line.strip():
                                continue
                            try:
                                game = json_module.loads(line)
                                game_board = game.get("board_type", "")
                                game_players = game.get("num_players", 0)
                                has_moves = "moves" in game and len(game.get("moves", [])) > 0

                                # Check if matches config
                                board_match = board_type in game_board.lower() or game_board.lower() in board_type
                                if board_type == "hexagonal":
                                    board_match = "hex" in game_board.lower()

                                if board_match and game_players == num_players and has_moves:
                                    game_count += 1
                                    if jsonl_file not in valid_files:
                                        valid_files.append(jsonl_file)
                            except json_module.JSONDecodeError:
                                continue
                except Exception:
                    continue

            if game_count < JSONL_THRESHOLD_GAMES:
                continue

            if not valid_files:
                continue

            # Convert to NPZ using jsonl_to_npz.py
            output_npz = training_dir / f"jsonl_auto_{config_key}_{int(time.time())}.npz"
            converter_script = self.ringrift_path / "ai-service" / "scripts" / "jsonl_to_npz.py"

            if not converter_script.exists():
                logger.info(f"JSONLNPZ converter not found: {converter_script}")
                continue

            cmd = [
                sys.executable, str(converter_script),
                "--output", str(output_npz),
                "--board-type", board_type,
                "--num-players", str(num_players),
                "--sample-every", "5",
                "--max-games", "100",
            ]
            for vf in valid_files[:10]:  # Limit input files
                cmd.extend(["--input", str(vf)])

            env = os.environ.copy()
            env["PYTHONPATH"] = str(self.ringrift_path / "ai-service")

            try:
                logger.info(f"Converting {game_count} {config_key} JSONL games to NPZ...")
                result = subprocess.run(
                    cmd, capture_output=True, text=True, timeout=600, env=env,
                    cwd=str(self.ringrift_path / "ai-service")
                )
                if result.returncode == 0 and output_npz.exists():
                    logger.info(f"Created {output_npz.name} from JSONL")
                    conversions_done += 1
                    newly_converted.append(config_key)
                else:
                    logger.info(f"JSONLNPZ conversion failed for {config_key}: {result.stderr[:200] if result.stderr else 'no error'}")
            except subprocess.TimeoutExpired:
                logger.info(f"JSONLNPZ conversion timeout for {config_key}")
            except Exception as e:
                logger.info(f"JSONLNPZ conversion error for {config_key}: {e}")

        # Update marker file
        if newly_converted:
            try:
                all_converted = converted_files | set(newly_converted)
                npz_marker_file.write_text("\n".join(sorted(all_converted)))
            except Exception:
                pass

        return conversions_done

    async def _data_management_loop(self):
        """Background loop for automatic data management.

        LEARNED LESSONS - Automated data pipeline:
        - Triggers exports when databases exceed threshold
        - Syncs training data to GPU nodes
        - Auto-triggers training when enough data available
        """
        logger.info(f"Data management loop started (interval: {DATA_MANAGEMENT_INTERVAL}s)")

        # Track active export jobs
        active_exports: dict[str, float] = {}  # path -> start_time

        while self.running:
            try:
                await asyncio.sleep(DATA_MANAGEMENT_INTERVAL)

                # ==== LOCAL NODE OPERATIONS (run on ALL nodes) ====
                # Check disk usage and trigger cleanup if needed
                has_capacity, disk_pct = check_disk_has_capacity(DISK_WARNING_THRESHOLD)
                if not has_capacity:
                    logger.info(f"Disk at {disk_pct:.1f}% (warning threshold {DISK_WARNING_THRESHOLD}%), triggering cleanup...")
                    await self._cleanup_local_disk()
                    has_capacity, disk_pct = check_disk_has_capacity(DISK_CRITICAL_THRESHOLD)

                # Convert JSONL selfplay files to DB format (runs on ALL nodes)
                # This enables training to access local selfplay data
                data_dir = self.get_data_directory()
                games_dir = data_dir / "games"
                training_dir = data_dir / "training"
                games_dir.mkdir(parents=True, exist_ok=True)
                training_dir.mkdir(parents=True, exist_ok=True)

                try:
                    converted = await self._convert_jsonl_to_db(data_dir, games_dir)
                    if converted > 0:
                        logger.info(f"Local JSONLDB conversion: {converted} games converted")
                except Exception as conv_err:
                    logger.info(f"JSONLDB conversion error: {conv_err}")

                # Also convert JSONL directly to NPZ for training (preferred path)
                try:
                    npz_created = await self._convert_jsonl_to_npz_for_training(data_dir, training_dir)
                    if npz_created > 0:
                        logger.info(f"Local JSONLNPZ conversion: {npz_created} training files created")
                except Exception as npz_err:
                    logger.info(f"JSONLNPZ conversion error: {npz_err}")

                # ==== LEADER-ONLY OPERATIONS ====
                if not self._is_leader():
                    continue

                logger.info("Running data management check (leader)...")

                # Re-check disk after conversion
                if not has_capacity:
                    logger.info(f"Disk at {disk_pct:.1f}% after cleanup, skipping leader data operations")
                    continue

                # Check database integrity (every 6th cycle = ~30 min)
                if not hasattr(self, "_db_integrity_counter"):
                    self._db_integrity_counter = 0
                self._db_integrity_counter += 1
                if self._db_integrity_counter % 6 == 0:
                    try:
                        data_dir = Path(self.ringrift_path) / "ai-service" / "data" / "games"
                        db_results = check_and_repair_databases(
                            data_dir=data_dir,
                            auto_repair=False,  # Just move corrupted, don't attempt recovery
                            log_prefix="[P2P]"
                        )
                        if db_results["corrupted"] > 0:
                            moved = db_results.get("failed", 0)  # "failed" = moved without recovery
                            logger.info(f"DB integrity: {db_results['checked']} checked, "
                                  f"{db_results['corrupted']} corrupted, {moved} moved")
                    except Exception as db_err:
                        logger.info(f"DB integrity check error: {db_err}")

                # 1. Check local database sizes and trigger exports
                # Count current exports
                current_exports = len([p for p, t in active_exports.items()
                                       if time.time() - t < 3600])  # 1 hour timeout

                if games_dir.exists():
                    for db_file in games_dir.glob("*.db"):
                        db_size_mb = db_file.stat().st_size / (1024 * 1024)

                        if db_size_mb >= DB_EXPORT_THRESHOLD_MB:
                            # Check if already exporting
                            export_key = str(db_file)
                            if export_key in active_exports:
                                continue

                            # Check concurrent export limit
                            if current_exports >= MAX_CONCURRENT_EXPORTS:
                                logger.info(f"Skipping export for {db_file.name} (max concurrent reached)")
                                continue

                            # Determine board type from filename
                            board_type = "square8"  # default
                            if "hex" in db_file.name.lower():
                                board_type = "hexagonal"
                            elif "square19" in db_file.name.lower() or "sq19" in db_file.name.lower():
                                board_type = "square19"

                            # Start export job
                            logger.info(f"Auto-triggering export for {db_file.name} ({db_size_mb:.0f}MB)")
                            export_output = training_dir / f"auto_{db_file.stem}_{int(time.time())}.npz"

                            try:
                                cmd = [
                                    sys.executable,  # Use venv Python, not system python3
                                    f"{self.ringrift_path}/ai-service/scripts/export_replay_dataset.py",
                                    "--db", str(db_file),
                                    "--board-type", board_type,
                                    "--num-players", "2",
                                    "--board-aware-encoding",
                                    "--require-completed",
                                    "--min-moves", "10",
                                    "--output", str(export_output),
                                ]

                                env = os.environ.copy()
                                env["PYTHONPATH"] = f"{self.ringrift_path}/ai-service"

                                subprocess.Popen(
                                    cmd,
                                    stdout=open(f"/tmp/auto_export_{db_file.stem}.log", "w"),
                                    stderr=subprocess.STDOUT,
                                    env=env,
                                    cwd=f"{self.ringrift_path}/ai-service",
                                )
                                active_exports[export_key] = time.time()
                                current_exports += 1
                                logger.info(f"Started export job for {db_file.name}")

                            except Exception as e:
                                logger.error(f"Failed to start export for {db_file.name}: {e}")

                # 2. Calculate total training data size
                total_training_mb = 0.0
                if training_dir.exists():
                    for npz_file in training_dir.glob("*.npz"):
                        total_training_mb += npz_file.stat().st_size / (1024 * 1024)

                logger.info(f"Training data available: {total_training_mb:.1f}MB")

                # 3. Auto-trigger training if threshold exceeded and GPU available
                if total_training_mb >= AUTO_TRAINING_THRESHOLD_MB and self.self_info.is_gpu_node() and self.self_info.training_jobs == 0:
                    logger.info(f"Auto-triggering training ({total_training_mb:.1f}MB data available)")
                    # Find largest training file
                    largest_npz = max(
                        training_dir.glob("*.npz"),
                        key=lambda f: f.stat().st_size,
                        default=None
                    )
                    if largest_npz:
                        await self._start_auto_training(str(largest_npz))

                # 4. Request data sync from peers with large databases
                await self._request_data_from_peers()

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.info(f"Data management loop error: {e}")
                import traceback
                traceback.print_exc()
                await asyncio.sleep(60)

    async def _model_sync_loop(self):
        """Background loop for syncing NN/NNUE models across the cluster.

        LEARNED LESSONS - Model distribution:
        - Training nodes produce new models that need distribution
        - All GPU nodes should have access to latest models for selfplay
        - Use sync_models.py infrastructure for hash-based deduplication
        - Only leader runs this to avoid conflicts
        """
        logger.info(f"Model sync loop started (interval: {MODEL_SYNC_INTERVAL}s)")

        while self.running:
            try:
                await asyncio.sleep(MODEL_SYNC_INTERVAL)

                if not self._is_leader():
                    continue

                if not HAS_MODEL_SYNC or not HAS_HOSTS_FOR_SYNC:
                    if self.verbose:
                        logger.info("Model sync skipped: sync_models module not available")
                    continue

                # Check disk capacity before downloading models (enforces 70% limit)
                has_capacity, disk_pct = check_disk_has_capacity(DISK_CRITICAL_THRESHOLD)
                if not has_capacity:
                    logger.info(f"Model sync skipped: disk at {disk_pct:.1f}% (limit {DISK_CRITICAL_THRESHOLD}%)")
                    continue

                logger.info("Running model sync check...")

                # Run sync in a thread pool to avoid blocking the event loop
                loop = asyncio.get_event_loop()

                def do_model_sync():
                    try:
                        # Load remote hosts
                        hosts = load_remote_hosts()
                        if not hosts:
                            return 0, 0, ["No remote hosts configured"]

                        # Scan cluster for model inventory
                        state = scan_cluster_models(hosts, max_workers=5)

                        # Calculate current total
                        total_models = len(state.all_nn_models) + len(state.all_nnue_models)

                        # Sync missing models with deduplication
                        collected, distributed, errors = sync_missing_models(
                            state, hosts, dry_run=False, collect_first=True
                        )

                        # Save state for future reference
                        state.save()

                        return collected, distributed, errors, total_models, len(hosts)

                    except Exception as e:
                        return 0, 0, [str(e)], 0, 0

                result = await loop.run_in_executor(None, do_model_sync)

                if len(result) == 5:
                    collected, distributed, errors, total_models, num_hosts = result

                    if collected > 0 or distributed > 0:
                        logger.info(f"Model sync: collected {collected}, distributed {distributed} "
                              f"({total_models} total models across {num_hosts} hosts)")

                    if errors:
                        for err in errors[:3]:
                            logger.info(f"Model sync error: {err}")

                    # Use SyncCoordinator for additional transport methods (aria2, ssh, p2p)
                    if HAS_SYNC_COORDINATOR and errors:
                        try:
                            coordinator = get_sync_coordinator()
                            # Sync models using alternative transports
                            model_stats = await coordinator.sync_models()
                            if model_stats.files_synced > 0:
                                logger.info(f"SyncCoordinator fallback: {model_stats.files_synced} models via {model_stats.transport_used}")
                        except Exception as sync_err:
                            if self.verbose:
                                logger.info(f"SyncCoordinator fallback error: {sync_err}")
                else:
                    collected, distributed, errors = result[:3]
                    if errors:
                        logger.info(f"Model sync failed: {errors[0]}")

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.info(f"Model sync loop error: {e}")
                import traceback
                traceback.print_exc()
                await asyncio.sleep(60)

    async def _consolidate_selfplay_data(self):
        """Consolidate siloed job databases AND JSONL files into training DB.

        LEARNED LESSONS: Selfplay jobs write to job-specific databases for isolation.
        GPU selfplay jobs write JSONL files for efficiency.
        These need to be periodically merged into the training DB for:
        1. Training triggers to see accurate game counts
        2. Cross-node data sync to work correctly
        3. Training scripts to find all available data

        Runs every ~5 minutes (every 5th job check cycle) to avoid overhead.
        """
        # Only run every 5th cycle (~5 minutes with JOB_CHECK_INTERVAL=60)
        cycle_counter = getattr(self, "_consolidation_cycle", 0)
        self._consolidation_cycle = cycle_counter + 1
        if cycle_counter % 5 != 0:
            return

        try:
            import sqlite3
            import subprocess
            from pathlib import Path

            data_dir = Path(self.ringrift_path) / "ai-service" / "data"
            selfplay_dir = data_dir / "selfplay"
            games_dir = data_dir / "games"
            main_db_path = games_dir / "selfplay.db"
            jsonl_db_path = games_dir / "jsonl_aggregated.db"

            if not selfplay_dir.exists():
                return

            env = os.environ.copy()
            env["PYTHONPATH"] = str(Path(self.ringrift_path) / "ai-service")

            # --- PART 1: Aggregate JSONL files (GPU selfplay output) ---
            jsonl_files = list(selfplay_dir.glob("**/games.jsonl"))
            # Filter to files > 1KB and modified in last 24 hours
            recent_jsonl = []
            for jf in jsonl_files:
                try:
                    if jf.stat().st_size > 1024:
                        recent_jsonl.append(jf)
                except Exception:
                    pass

            if recent_jsonl:
                total_lines = 0
                for jf in recent_jsonl[:20]:  # Sample first 20
                    try:
                        with open(jf) as f:
                            total_lines += sum(1 for _ in f)
                    except Exception:
                        pass

                if total_lines > 100:  # Only run if there's meaningful data
                    aggregate_script = Path(self.ringrift_path) / "ai-service" / "scripts" / "aggregate_jsonl_to_db.py"
                    if aggregate_script.exists() and not getattr(self, "_jsonl_aggregation_running", False):
                        self._jsonl_aggregation_running = True
                        logger.info(f"JSONL aggregation: ~{total_lines * len(recent_jsonl) // 20} games in {len(recent_jsonl)} files")
                        cmd = [
                            sys.executable, str(aggregate_script),
                            "--input-dir", str(selfplay_dir),
                            "--output-db", str(jsonl_db_path),
                        ]
                        proc = subprocess.Popen(
                            cmd,
                            env=env,
                            stdout=open("/tmp/jsonl_aggregate.log", "w"),
                            stderr=subprocess.STDOUT,
                            cwd=str(Path(self.ringrift_path) / "ai-service"),
                        )
                        logger.info(f"Started JSONL aggregation (PID: {proc.pid})")
                        # Reset flag after ~10 minutes
                        asyncio.get_event_loop().call_later(
                            600, lambda: setattr(self, "_jsonl_aggregation_running", False)
                        )

            # --- PART 1b: Export NPZ from aggregated DB for training ---
            # Only run if we have a decent sized aggregated DB and not already exporting
            # LEARNED LESSONS: NPZ export is CPU-intensive. Route to high-CPU nodes
            # (vast nodes have 256-512 CPUs vs lambda's 64) to free GPU nodes for training.
            if jsonl_db_path.exists() and not getattr(self, "_npz_export_running", False):
                try:
                    conn = sqlite3.connect(str(jsonl_db_path), timeout=5)
                    game_count = conn.execute("SELECT COUNT(*) FROM games").fetchone()[0]
                    conn.close()

                    # Only export if we have enough games and it's been a while
                    training_dir = data_dir / "training"
                    training_dir.mkdir(exist_ok=True)
                    npz_output = training_dir / "auto_training_sq8_2p.npz"

                    # Check if existing NPZ is stale (older than 1 hour) or small
                    should_export = False
                    if not npz_output.exists():
                        should_export = game_count > 500
                    else:
                        npz_age_hours = (time.time() - npz_output.stat().st_mtime) / 3600
                        npz_size_mb = npz_output.stat().st_size / (1024 * 1024)
                        should_export = game_count > 1000 and (npz_age_hours > 1 or npz_size_mb < 1)

                    if should_export:
                        self._npz_export_running = True

                        # Find best CPU node for export (prefer vast nodes)
                        cpu_nodes = self._get_cpu_primary_nodes(count=3)
                        export_node = None
                        for node in cpu_nodes:
                            # Skip nodes that are already very loaded
                            if node.get_load_score() < 80 and node.cpu_percent < 90:
                                export_node = node
                                break

                        if export_node and export_node.node_id != self.node_id:
                            # Dispatch to high-CPU node
                            logger.info(f"Dispatching NPZ export ({game_count} games) to {export_node.node_id} "
                                  f"(cpu_power={export_node.cpu_power_score()}, cpus={export_node.cpu_count})")
                            asyncio.create_task(self._dispatch_export_job(
                                node=export_node,
                                input_path=str(jsonl_db_path),
                                output_path=str(npz_output),
                                board_type="square8",
                                num_players=2,
                                encoder_version="v3",
                                max_games=5000,
                                is_jsonl=False,
                            ))
                        else:
                            # Fall back to local export if no suitable CPU node
                            export_script = Path(self.ringrift_path) / "ai-service" / "scripts" / "export_replay_dataset.py"
                            if export_script.exists():
                                logger.info(f"Starting local NPZ export ({game_count} games) -> {npz_output}")
                                cmd = [
                                    sys.executable, str(export_script),
                                    "--db", str(jsonl_db_path),
                                    "--board-type", "square8",
                                    "--num-players", "2",
                                    "--output", str(npz_output),
                                    "--encoder-version", "v3",
                                    "--max-games", "5000",
                                ]
                                subprocess.Popen(
                                    cmd,
                                    env=env,
                                    stdout=open("/tmp/npz_export.log", "w"),
                                    stderr=subprocess.STDOUT,
                                    cwd=str(Path(self.ringrift_path) / "ai-service"),
                                )

                        # Reset flag after 30 minutes (export is slow)
                        asyncio.get_event_loop().call_later(
                            1800, lambda: setattr(self, "_npz_export_running", False)
                        )
                except Exception as e:
                    logger.info(f"NPZ export check error: {e}")

            # --- PART 2: Merge job DBs (CPU selfplay output) ---
            dbs_to_merge = []
            for db_path in selfplay_dir.glob("**/games.db"):
                # Skip if it's in a .tmp directory or is the main DB
                if ".tmp" in str(db_path) or db_path == main_db_path:
                    continue
                try:
                    conn = sqlite3.connect(str(db_path), timeout=5)
                    count = conn.execute("SELECT COUNT(*) FROM games").fetchone()[0]
                    conn.close()
                    if count > 0:
                        dbs_to_merge.append((db_path, count))
                except Exception:
                    pass

            if dbs_to_merge:
                total_games = sum(c for _, c in dbs_to_merge)
                logger.info(f"DB consolidation: {len(dbs_to_merge)} DBs with {total_games} games to merge")

                # Use merge script if available
                merge_script = Path(self.ringrift_path) / "ai-service" / "scripts" / "merge_game_dbs.py"
                if merge_script.exists():
                    # Build command with all DBs
                    cmd = [
                        sys.executable, str(merge_script),  # Use venv Python
                        "--output", str(main_db_path),
                        "--dedupe-by-game-id",
                    ]
                    for db_path, _ in dbs_to_merge[:50]:  # Limit to 50 DBs at a time
                        cmd.extend(["--db", str(db_path)])

                    # Run merge in background
                    proc = subprocess.Popen(
                        cmd,
                        env=env,
                        stdout=subprocess.DEVNULL,
                        stderr=subprocess.DEVNULL,
                        cwd=str(Path(self.ringrift_path) / "ai-service"),
                    )
                    logger.info(f"Started DB merge (PID: {proc.pid})")

        except Exception as e:
            logger.info(f"Data consolidation error: {e}")

    async def _start_auto_training(self, data_path: str):
        """Start automatic training job on local node."""
        try:
            run_dir = f"{self.ringrift_path}/ai-service/models/auto_train_{int(time.time())}"
            Path(run_dir).mkdir(parents=True, exist_ok=True)

            cmd = [
                sys.executable,  # Use venv Python
                f"{self.ringrift_path}/ai-service/scripts/run_nn_training_baseline.py",
                "--board", "square8",
                "--num-players", "2",
                "--run-dir", run_dir,
                "--data-path", data_path,
                "--epochs", "50",
                "--model-version", "v3",
            ]

            env = os.environ.copy()
            env["PYTHONPATH"] = f"{self.ringrift_path}/ai-service"

            subprocess.Popen(
                cmd,
                stdout=open(f"{run_dir}/training.log", "w"),
                stderr=subprocess.STDOUT,
                env=env,
                cwd=f"{self.ringrift_path}/ai-service",
            )
            logger.info(f"Started auto-training job in {run_dir}")
            self.self_info.training_jobs += 1

        except Exception as e:
            logger.error(f"Failed to start auto-training: {e}")

    async def _request_data_from_peers(self):
        """Sync training data (NPZ files) from peers with large datasets.

        This ensures training data is distributed across the cluster so any
        GPU node can run training jobs.
        """
        try:
            # Check disk capacity before requesting data
            has_capacity, disk_pct = check_disk_has_capacity(DISK_CRITICAL_THRESHOLD)
            if not has_capacity:
                if self.verbose:
                    logger.info(f"Skipping data sync request: disk at {disk_pct:.1f}% (limit {DISK_CRITICAL_THRESHOLD}%)")
                return

            # Only sync if we're a GPU node (training capable)
            if not self.self_info.is_gpu_node():
                return

            # Rate limit: only sync every 10 minutes
            last_sync = getattr(self, "_last_training_data_sync", 0)
            if time.time() - last_sync < 600:
                return

            # Load hosts config to get SSH details
            if not HAS_HOSTS_FOR_SYNC:
                return

            hosts = load_remote_hosts()
            if not hosts:
                return

            local_training_dir = Path(self.ringrift_path) / "ai-service" / "data" / "training"
            local_training_dir.mkdir(parents=True, exist_ok=True)

            # Calculate local training data size
            local_training_mb = sum(
                f.stat().st_size for f in local_training_dir.glob("*.npz")
            ) / (1024 * 1024) if local_training_dir.exists() else 0

            # Find peers with more training data
            synced_from = []
            for host_name, host_config in hosts.items():
                if host_name == self.node_id:
                    continue

                # Skip hosts without SSH access
                ssh_host = host_config.ssh_host
                if not ssh_host:
                    continue

                # Check if host is alive via P2P
                with self.peers_lock:
                    peer = self.peers.get(host_name)
                if not peer or not peer.is_alive():
                    continue

                # Get remote training data size via SSH
                try:
                    ssh_user = getattr(host_config, 'ssh_user', 'ubuntu')
                    remote_path = getattr(host_config, 'ringrift_path', '/home/ubuntu/ringrift')
                    if remote_path.startswith('~'):
                        remote_path = remote_path.replace('~', f'/home/{ssh_user}')

                    # Check remote training data size
                    cmd = [
                        "ssh", "-o", "ConnectTimeout=10", "-o", "StrictHostKeyChecking=no",
                        f"{ssh_user}@{ssh_host}",
                        f"du -sb {remote_path}/ai-service/data/training/*.npz 2>/dev/null | awk '{{sum+=$1}}END{{print sum}}'"
                    ]
                    result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
                    remote_size = int(result.stdout.strip() or 0)
                    remote_mb = remote_size / (1024 * 1024)

                    # Sync if remote has significantly more data (>20MB more)
                    if remote_mb > local_training_mb + 20:
                        logger.info(f"Syncing training data from {host_name}: {remote_mb:.1f}MB -> local {local_training_mb:.1f}MB")

                        # Use rsync to sync NPZ files
                        rsync_cmd = [
                            "rsync", "-avz", "--progress",
                            "-e", "ssh -o ConnectTimeout=30 -o StrictHostKeyChecking=no",
                            f"{ssh_user}@{ssh_host}:{remote_path}/ai-service/data/training/*.npz",
                            str(local_training_dir) + "/"
                        ]

                        sync_result = subprocess.run(
                            rsync_cmd, capture_output=True, text=True, timeout=300
                        )

                        if sync_result.returncode == 0:
                            synced_from.append(host_name)
                            logger.info(f"Successfully synced training data from {host_name}")
                        else:
                            logger.error(f"Failed to sync from {host_name}: {sync_result.stderr[:200]}")

                except subprocess.TimeoutExpired:
                    logger.info(f"Timeout checking training data on {host_name}")
                except Exception as e:
                    logger.error(f"syncing from {host_name}: {e}")

            if synced_from:
                # Recalculate local size after sync
                new_local_mb = sum(
                    f.stat().st_size for f in local_training_dir.glob("*.npz")
                ) / (1024 * 1024)
                logger.info(f"Training data sync complete: {local_training_mb:.1f}MB -> {new_local_mb:.1f}MB")

            self._last_training_data_sync = time.time()

        except Exception as e:
            logger.info(f"Data sync request error: {e}")
            import traceback
            traceback.print_exc()

    # ============================================
    # Git Auto-Update Methods
    # ============================================

    def _get_local_git_commit(self) -> str | None:
        """Get the current local git commit hash."""
        try:
            result = subprocess.run(
                self._git_cmd("rev-parse", "HEAD"),
                cwd=self.ringrift_path,
                capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0:
                return result.stdout.strip()
        except Exception as e:
            logger.error(f"Failed to get local git commit: {e}")
        return None

    def _get_local_git_branch(self) -> str | None:
        """Get the current local git branch name."""
        try:
            result = subprocess.run(
                self._git_cmd("rev-parse", "--abbrev-ref", "HEAD"),
                cwd=self.ringrift_path,
                capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0:
                return result.stdout.strip()
        except Exception as e:
            logger.error(f"Failed to get local git branch: {e}")
        return None

    def _get_remote_git_commit(self) -> str | None:
        """Fetch and get the remote branch's latest commit hash."""
        try:
            # First fetch to update remote refs
            fetch_result = subprocess.run(
                self._git_cmd("fetch", GIT_REMOTE_NAME, GIT_BRANCH_NAME),
                cwd=self.ringrift_path,
                capture_output=True, text=True, timeout=60
            )
            if fetch_result.returncode != 0:
                logger.info(f"Git fetch failed: {fetch_result.stderr}")
                return None

            # Get remote branch commit
            result = subprocess.run(
                self._git_cmd("rev-parse", f"{GIT_REMOTE_NAME}/{GIT_BRANCH_NAME}"),
                cwd=self.ringrift_path,
                capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0:
                return result.stdout.strip()
        except Exception as e:
            logger.error(f"Failed to get remote git commit: {e}")
        return None

    def _check_for_updates(self) -> tuple[bool, str | None, str | None]:
        """Check if there are updates available from GitHub.

        Returns: (has_updates, local_commit, remote_commit)
        """
        local_commit = self._get_local_git_commit()
        remote_commit = self._get_remote_git_commit()

        if not local_commit or not remote_commit:
            return False, local_commit, remote_commit

        has_updates = local_commit != remote_commit
        return has_updates, local_commit, remote_commit

    def _get_commits_behind(self, local_commit: str, remote_commit: str) -> int:
        """Get the number of commits the local branch is behind remote."""
        try:
            result = subprocess.run(
                self._git_cmd("rev-list", "--count", f"{local_commit}..{remote_commit}"),
                cwd=self.ringrift_path,
                capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0:
                return int(result.stdout.strip())
        except Exception as e:
            logger.error(f"Failed to count commits behind: {e}")
        return 0

    def _check_local_changes(self) -> bool:
        """Check if there are uncommitted local changes.

        Notes:
        - Ignore untracked files by default. Cluster nodes often accumulate local
          artifacts (logs, data, env backups) that should not block git updates.
        - Still blocks on tracked/staged modifications to avoid stomping on
          local hotfixes.
        """
        try:
            result = subprocess.run(
                self._git_cmd("status", "--porcelain", "--untracked-files=no"),
                cwd=self.ringrift_path,
                capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0:
                # If there's output, there are uncommitted changes
                return bool(result.stdout.strip())
        except Exception as e:
            logger.error(f"Failed to check local changes: {e}")
        return True  # Assume changes exist on error (safer)

    async def _stop_all_local_jobs(self) -> int:
        """Stop all local jobs gracefully before update.

        Returns: Number of jobs stopped
        """
        stopped = 0
        with self.jobs_lock:
            for job_id, job in list(self.local_jobs.items()):
                try:
                    if job.pid > 0:
                        os.kill(job.pid, signal.SIGTERM)
                        logger.info(f"Sent SIGTERM to job {job_id} (PID {job.pid})")
                        stopped += 1
                        job.status = "stopping"
                except ProcessLookupError:
                    # Process already gone
                    job.status = "stopped"
                except Exception as e:
                    logger.error(f"Failed to stop job {job_id}: {e}")

        # Wait for processes to terminate gracefully
        # GPU games can take 1-10 minutes, so use a longer timeout (Dec 2025 fix)
        grace_period = int(os.environ.get("RINGRIFT_JOB_GRACE_PERIOD", "60"))
        if stopped > 0:
            await asyncio.sleep(grace_period)

            # Force kill any remaining
            with self.jobs_lock:
                for job_id, job in list(self.local_jobs.items()):
                    if job.status == "stopping" and job.pid > 0:
                        try:
                            os.kill(job.pid, signal.SIGKILL)
                            logger.info(f"Force killed job {job_id}")
                        except OSError:
                            pass  # Process already dead
                        job.status = "stopped"

        return stopped

    async def _perform_git_update(self) -> tuple[bool, str]:
        """Perform git pull to update the codebase.

        Returns: (success, message)
        """
        # Check for local changes
        if self._check_local_changes():
            return False, "Local changes detected. Cannot auto-update. Please commit or stash changes."

        # Stop jobs if configured
        if GRACEFUL_SHUTDOWN_BEFORE_UPDATE:
            stopped = await self._stop_all_local_jobs()
            if stopped > 0:
                logger.info(f"Stopped {stopped} jobs before update")

        try:
            # Perform git pull
            result = subprocess.run(
                self._git_cmd("pull", GIT_REMOTE_NAME, GIT_BRANCH_NAME),
                cwd=self.ringrift_path,
                capture_output=True, text=True, timeout=120
            )

            if result.returncode != 0:
                return False, f"Git pull failed: {result.stderr}"

            logger.info(f"Git pull successful: {result.stdout}")
            return True, result.stdout

        except subprocess.TimeoutExpired:
            return False, "Git pull timed out"
        except Exception as e:
            return False, f"Git pull error: {e}"

    async def _restart_orchestrator(self):
        """Restart the orchestrator process after update."""
        logger.info("Restarting orchestrator to apply updates...")

        # Save state before restart
        self._save_state()

        # Get current script path and arguments
        script_path = Path(__file__).resolve()
        args = sys.argv[1:]

        # Schedule restart
        await asyncio.sleep(2)

        # Use exec to replace current process
        os.execv(sys.executable, [sys.executable, str(script_path), *args])

    async def _git_update_loop(self):
        """Background loop to periodically check for and apply updates."""
        if not AUTO_UPDATE_ENABLED:
            logger.info("Auto-update disabled")
            return

        logger.info(f"Git auto-update loop started (interval: {GIT_UPDATE_CHECK_INTERVAL}s)")

        while self.running:
            try:
                await asyncio.sleep(GIT_UPDATE_CHECK_INTERVAL)

                if not self.running:
                    break

                # Check for updates
                has_updates, local_commit, remote_commit = self._check_for_updates()

                if has_updates and local_commit and remote_commit:
                    commits_behind = self._get_commits_behind(local_commit, remote_commit)
                    logger.info(f"Update available: {commits_behind} commits behind")
                    logger.info(f"Local:  {local_commit[:8]}")
                    logger.info(f"Remote: {remote_commit[:8]}")

                    # Perform update
                    success, message = await self._perform_git_update()

                    if success:
                        logger.info("Update successful, restarting...")
                        await self._restart_orchestrator()
                    else:
                        logger.info(f"Update failed: {message}")

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.info(f"Git update loop error: {e}")
                await asyncio.sleep(60)  # Wait before retry on error

    # ============================================
    # HTTP API Handlers
    # ============================================

    async def handle_heartbeat(self, request: web.Request) -> web.Response:
        """Handle heartbeat from peer node."""
        try:
            data = await request.json()
            incoming_voters = data.get("voter_node_ids") or data.get("voters") or None
            if incoming_voters:
                voters_list: list[str] = []
                if isinstance(incoming_voters, list):
                    voters_list = [str(v).strip() for v in incoming_voters if str(v).strip()]
                elif isinstance(incoming_voters, str):
                    voters_list = [t.strip() for t in incoming_voters.split(",") if t.strip()]
                if voters_list:
                    self._maybe_adopt_voter_node_ids(voters_list, source="learned")
            peer_info = NodeInfo.from_dict(data)
            # Ignore self-heartbeats so NAT detection + leader election aren't
            # distorted when COORDINATOR_URL includes this node's own endpoint(s).
            if peer_info.node_id == self.node_id:
                self._update_self_info()
                payload = self.self_info.to_dict()
                voter_node_ids = list(getattr(self, "voter_node_ids", []) or [])
                if voter_node_ids:
                    payload["voter_node_ids"] = voter_node_ids
                    payload["voter_quorum_size"] = int(getattr(self, "voter_quorum_size", 0) or 0)
                    payload["voter_config_source"] = str(getattr(self, "voter_config_source", "") or "")
                return web.json_response(payload)
            # Receiving any inbound heartbeat implies we're reachable inbound.
            self.last_inbound_heartbeat = time.time()
            # Preserve the node's self-reported endpoint for multi-path retries.
            if not peer_info.reported_host:
                peer_info.reported_host = peer_info.host
            if not peer_info.reported_port:
                peer_info.reported_port = peer_info.port
            peer_info.last_heartbeat = time.time()
            # Prefer the remote socket address over self-reported host so that
            # nodes behind overlays (e.g., Tailscale) use a reachable address.
            forwarded_for = (
                request.headers.get("X-Forwarded-For")
                or request.headers.get("X-Real-IP")
                or request.headers.get("CF-Connecting-IP")
            )
            if forwarded_for:
                peer_info.host = forwarded_for.split(",")[0].strip()
            elif request.remote:
                peer_info.host = request.remote

            # Preserve local reachability diagnostics: a peer can be "alive" (it can
            # send us heartbeats) while still being unreachable for inbound HTTP
            # (e.g. NAT/firewall). Our outbound heartbeat failures track that.
            # Use AsyncLockWrapper to avoid blocking event loop under high load
            async with AsyncLockWrapper(self.peers_lock):
                existing = self.peers.get(peer_info.node_id)
                if existing:
                    peer_info.consecutive_failures = int(getattr(existing, "consecutive_failures", 0) or 0)
                    peer_info.last_failure_time = float(getattr(existing, "last_failure_time", 0.0) or 0.0)
                    # Sticky NAT/relay routing with recovery:
                    # - Receiving a direct heartbeat does NOT imply the peer is reachable inbound.
                    # - If a peer has ever registered via /relay/heartbeat, preserve nat_blocked
                    #   and relay_via so leaders can continue routing commands through the relay hub.
                    # - BUT: allow recovery after NAT_BLOCKED_RECOVERY_TIMEOUT if peer becomes reachable
                    existing_nat_blocked = getattr(existing, "nat_blocked", False)
                    existing_nat_blocked_since = float(getattr(existing, "nat_blocked_since", 0.0) or 0.0)
                    existing_last_nat_probe = float(getattr(existing, "last_nat_probe", 0.0) or 0.0)

                    if existing_nat_blocked and not getattr(peer_info, "nat_blocked", False):
                        # Peer was NAT-blocked, incoming says not blocked - preserve unless recovery triggered
                        peer_info.nat_blocked = True
                        peer_info.nat_blocked_since = existing_nat_blocked_since or time.time()
                        peer_info.last_nat_probe = existing_last_nat_probe
                    elif peer_info.nat_blocked and not existing_nat_blocked:
                        # Peer newly marked as NAT-blocked - record timestamp
                        peer_info.nat_blocked_since = time.time()
                    elif existing_nat_blocked and peer_info.nat_blocked:
                        # Both agree NAT-blocked - preserve original timestamp
                        peer_info.nat_blocked_since = existing_nat_blocked_since or time.time()
                        peer_info.last_nat_probe = existing_last_nat_probe

                    if (getattr(existing, "relay_via", "") or "") and not (getattr(peer_info, "relay_via", "") or ""):
                        peer_info.relay_via = str(getattr(existing, "relay_via", "") or "")
                    # Preserve retirement state across updates.
                    if getattr(existing, "retired", False):
                        peer_info.retired = True
                        peer_info.retired_at = float(getattr(existing, "retired_at", 0.0) or 0.0)

                # STABILITY FIX: Correct stale leader role claims
                # If a peer claims to be leader but we have an elected leader that's different,
                # downgrade their role to follower to prevent split-brain confusion.
                # This prevents stale leader info from propagating through gossip.
                if peer_info.role == NodeRole.LEADER and peer_info.node_id != self.node_id:
                    actual_leader = self.leader_id
                    if actual_leader and actual_leader != peer_info.node_id:
                        # Peer claims leader but we have a different elected leader
                        peer_info.role = NodeRole.FOLLOWER

                self.peers[peer_info.node_id] = peer_info

            # Return our info
            self._update_self_info()
            payload = self.self_info.to_dict()
            voter_node_ids = list(getattr(self, "voter_node_ids", []) or [])
            if voter_node_ids:
                payload["voter_node_ids"] = voter_node_ids
                payload["voter_quorum_size"] = int(getattr(self, "voter_quorum_size", 0) or 0)
                payload["voter_config_source"] = str(getattr(self, "voter_config_source", "") or "")
            return web.json_response(payload)
        except Exception as e:
            return web.json_response({"error": str(e)}, status=400)

    async def handle_status(self, request: web.Request) -> web.Response:
        """Return cluster status."""
        self._update_self_info()

        async with AsyncLockWrapper(self.peers_lock):
            peers_snapshot = list(self.peers.values())

        conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])
        effective_leader = self._get_leader_peer()

        peers: dict[str, Any] = {}
        for node_id, info in ((p.node_id, p) for p in peers_snapshot):
            d = info.to_dict()
            d["endpoint_conflict"] = self._endpoint_key(info) in conflict_keys
            d["leader_eligible"] = self._is_leader_eligible(info, conflict_keys, require_alive=False)
            peers[node_id] = d

        # Convenience diagnostics: reported leaders vs eligible leaders.
        leaders_reported = sorted(
            [p.node_id for p in peers_snapshot if p.role == NodeRole.LEADER and p.is_alive()]
        )
        leaders_eligible = sorted(
            [
                p.node_id
                for p in peers_snapshot
                if p.role == NodeRole.LEADER and self._is_leader_eligible(p, conflict_keys)
            ]
        )

        async with AsyncLockWrapper(self.jobs_lock):
            jobs = {k: v.to_dict() for k, v in self.local_jobs.items()}

        # Get improvement cycle manager status
        improvement_status = None
        if self.improvement_cycle_manager:
            try:
                improvement_status = self.improvement_cycle_manager.get_status()
            except Exception as e:
                improvement_status = {"error": str(e)}

        # Get diversity metrics
        diversity_metrics = self._get_diversity_metrics()

        voter_ids = list(getattr(self, "voter_node_ids", []) or [])
        voters_alive = 0
        if voter_ids:
            peer_map = {p.node_id: p for p in peers_snapshot}
            for vid in voter_ids:
                if vid == self.node_id:
                    voters_alive += 1
                    continue
                peer = peer_map.get(vid)
                if peer and peer.is_alive():
                    voters_alive += 1

        # Get P2P sync metrics (with error handling for new features)
        p2p_sync_metrics = getattr(self, "_p2p_sync_metrics", {})
        gossip_metrics = self._get_gossip_metrics_summary()
        distributed_training = self._get_distributed_training_summary()
        cluster_elo = self._get_cluster_elo_summary()
        node_recovery = self._get_node_recovery_metrics()
        try:
            leader_consensus = self._get_cluster_leader_consensus()
        except Exception as e:
            leader_consensus = {"error": str(e)}
        try:
            peer_reputation = self._get_cluster_peer_reputation()
        except Exception as e:
            peer_reputation = {"error": str(e)}
        try:
            sync_intervals = self._get_sync_interval_summary()  # ADAPTIVE SYNC INTERVALS
        except Exception as e:
            sync_intervals = {"error": str(e)}
        try:
            tournament_scheduling = self._get_distributed_tournament_summary()  # DISTRIBUTED TOURNAMENTS
        except Exception as e:
            tournament_scheduling = {"error": str(e)}
        try:
            data_dedup = self._get_dedup_summary()  # DATA DEDUPLICATION
        except Exception as e:
            data_dedup = {"error": str(e)}

        return web.json_response({
            "node_id": self.node_id,
            "role": self.role.value,
            "leader_id": self.leader_id,
            "effective_leader_id": (effective_leader.node_id if effective_leader else None),
            "leaders_reported": leaders_reported,
            "leaders_eligible": leaders_eligible,
            "voter_node_ids": voter_ids,
            "voter_quorum_size": int(getattr(self, "voter_quorum_size", 0) or 0),
            "voters_alive": voters_alive,
            "voter_quorum_ok": self._has_voter_quorum(),
            "self": self.self_info.to_dict(),
            "peers": peers,
            "local_jobs": jobs,
            "alive_peers": len([p for p in self.peers.values() if p.is_alive()]),
            "improvement_cycle_manager": improvement_status,
            "diversity_metrics": diversity_metrics,
            "gossip_metrics": gossip_metrics,
            "p2p_sync_metrics": p2p_sync_metrics,
            "distributed_training": distributed_training,
            "cluster_elo": cluster_elo,
            "node_recovery": node_recovery,
            "leader_consensus": leader_consensus,
            "peer_reputation": peer_reputation,
            "sync_intervals": sync_intervals,
            "tournament_scheduling": tournament_scheduling,
            "data_dedup": data_dedup,
        })

    async def handle_external_work(self, request: web.Request) -> web.Response:
        """Return external work status across the cluster.

        This endpoint shows work running outside P2P orchestrator tracking:
        - CMA-ES optimization jobs
        - Gauntlet runs
        - ELO tournaments
        - Data merge/aggregation

        Also identifies misrouted nodes (GPU nodes running CPU-bound work).
        """
        self._update_self_info()

        async with AsyncLockWrapper(self.peers_lock):
            peers_snapshot = list(self.peers.values())

        # Collect external work info
        nodes_with_external = []
        misrouted_nodes = []

        # Check self
        self.self_info.to_dict()
        if self.self_info.has_external_work():
            nodes_with_external.append({
                'node_id': self.node_id,
                'cmaes': self.self_info.cmaes_running,
                'gauntlet': self.self_info.gauntlet_running,
                'tournament': self.self_info.tournament_running,
                'data_merge': self.self_info.data_merge_running,
                'gpu_percent': self.self_info.gpu_percent,
                'cpu_percent': self.self_info.cpu_percent,
            })
        if self.self_info.is_misrouted():
            misrouted_nodes.append({
                'node_id': self.node_id,
                'gpu_name': self.self_info.gpu_name,
                'gpu_percent': self.self_info.gpu_percent,
                'cpu_percent': self.self_info.cpu_percent,
                'external_work': {
                    'cmaes': self.self_info.cmaes_running,
                    'gauntlet': self.self_info.gauntlet_running,
                    'tournament': self.self_info.tournament_running,
                }
            })

        # Check peers
        for peer in peers_snapshot:
            if peer.has_external_work():
                nodes_with_external.append({
                    'node_id': peer.node_id,
                    'cmaes': peer.cmaes_running,
                    'gauntlet': peer.gauntlet_running,
                    'tournament': peer.tournament_running,
                    'data_merge': peer.data_merge_running,
                    'gpu_percent': peer.gpu_percent,
                    'cpu_percent': peer.cpu_percent,
                })
            if peer.is_misrouted():
                misrouted_nodes.append({
                    'node_id': peer.node_id,
                    'gpu_name': peer.gpu_name,
                    'gpu_percent': peer.gpu_percent,
                    'cpu_percent': peer.cpu_percent,
                    'external_work': {
                        'cmaes': peer.cmaes_running,
                        'gauntlet': peer.gauntlet_running,
                        'tournament': peer.tournament_running,
                    }
                })

        return web.json_response({
            'nodes_with_external_work': nodes_with_external,
            'misrouted_nodes': misrouted_nodes,
            'total_external_work': len(nodes_with_external),
            'total_misrouted': len(misrouted_nodes),
        })

    # ========== Work Queue Handlers (Centralized Work Distribution) ==========

    async def handle_work_add(self, request: web.Request) -> web.Response:
        """Add work to the centralized queue (leader only)."""
        try:
            if not self.is_leader:
                return web.json_response({'error': 'not_leader', 'leader_id': self.leader_id}, status=403)

            wq = get_work_queue()
            if wq is None:
                return web.json_response({'error': 'work_queue_not_available'}, status=503)

            data = await request.json()
            work_type = data.get('work_type', 'selfplay')
            priority = data.get('priority', 50)
            config = data.get('config', {})
            timeout = data.get('timeout_seconds', 3600.0)
            depends_on = data.get('depends_on', [])

            from app.coordination.work_queue import WorkItem, WorkType
            item = WorkItem(
                work_type=WorkType(work_type),
                priority=priority,
                config=config,
                timeout_seconds=timeout,
                depends_on=depends_on,
            )
            work_id = wq.add_work(item)

            return web.json_response({
                'status': 'added',
                'work_id': work_id,
                'work_type': work_type,
                'priority': priority,
            })
        except Exception as e:
            logger.error(f"Error adding work: {e}")
            return web.json_response({'error': str(e)}, status=500)

    async def handle_work_add_batch(self, request: web.Request) -> web.Response:
        """Add multiple work items to the queue in a single request (leader only).

        Request body:
        {
            "items": [
                {"work_type": "selfplay", "priority": 50, "config": {...}},
                {"work_type": "training", "priority": 80, "config": {...}},
                ...
            ]
        }

        Response:
        {
            "status": "added",
            "count": 2,
            "work_ids": ["abc123", "def456"]
        }
        """
        try:
            if not self.is_leader:
                return web.json_response({'error': 'not_leader', 'leader_id': self.leader_id}, status=403)

            wq = get_work_queue()
            if wq is None:
                return web.json_response({'error': 'work_queue_not_available'}, status=503)

            data = await request.json()
            items = data.get('items', [])

            if not items:
                return web.json_response({'error': 'no_items_provided'}, status=400)

            if len(items) > 100:
                return web.json_response({'error': 'too_many_items', 'max': 100}, status=400)

            from app.coordination.work_queue import WorkItem, WorkType

            work_ids = []
            errors = []

            for i, item_data in enumerate(items):
                try:
                    work_type = item_data.get('work_type', 'selfplay')
                    priority = item_data.get('priority', 50)
                    config = item_data.get('config', {})
                    timeout = item_data.get('timeout_seconds', 3600.0)
                    depends_on = item_data.get('depends_on', [])

                    item = WorkItem(
                        work_type=WorkType(work_type),
                        priority=priority,
                        config=config,
                        timeout_seconds=timeout,
                        depends_on=depends_on,
                    )
                    work_id = wq.add_work(item)
                    work_ids.append(work_id)
                except Exception as e:
                    errors.append({'index': i, 'error': str(e)})

            return web.json_response({
                'status': 'added',
                'count': len(work_ids),
                'work_ids': work_ids,
                'errors': errors if errors else None,
            })
        except Exception as e:
            logger.error(f"Error adding batch work: {e}")
            return web.json_response({'error': str(e)}, status=500)

    async def handle_work_claim(self, request: web.Request) -> web.Response:
        """Claim available work from the queue."""
        try:
            if not self.is_leader:
                return web.json_response({'error': 'not_leader', 'leader_id': self.leader_id}, status=403)

            wq = get_work_queue()
            if wq is None:
                return web.json_response({'error': 'work_queue_not_available'}, status=503)

            node_id = request.query.get('node_id', '')
            capabilities_str = request.query.get('capabilities', '')
            capabilities = [c.strip() for c in capabilities_str.split(',') if c.strip()] or None

            if not node_id:
                return web.json_response({'error': 'node_id_required'}, status=400)

            item = wq.claim_work(node_id, capabilities)
            if item is None:
                return web.json_response({'status': 'no_work_available'})

            return web.json_response({
                'status': 'claimed',
                'work': item.to_dict(),
            })
        except Exception as e:
            logger.error(f"Error claiming work: {e}")
            return web.json_response({'error': str(e)}, status=500)

    async def handle_work_start(self, request: web.Request) -> web.Response:
        """Mark work as started (running)."""
        try:
            if not self.is_leader:
                return web.json_response({'error': 'not_leader', 'leader_id': self.leader_id}, status=403)

            wq = get_work_queue()
            if wq is None:
                return web.json_response({'error': 'work_queue_not_available'}, status=503)

            data = await request.json()
            work_id = data.get('work_id', '')
            if not work_id:
                return web.json_response({'error': 'work_id_required'}, status=400)

            success = wq.start_work(work_id)
            return web.json_response({'status': 'started' if success else 'failed', 'work_id': work_id})
        except Exception as e:
            logger.error(f"Error starting work: {e}")
            return web.json_response({'error': str(e)}, status=500)

    async def handle_work_complete(self, request: web.Request) -> web.Response:
        """Mark work as completed successfully."""
        try:
            from app.coordination.work_queue import WorkType

            if not self.is_leader:
                return web.json_response({'error': 'not_leader', 'leader_id': self.leader_id}, status=403)

            wq = get_work_queue()
            if wq is None:
                return web.json_response({'error': 'work_queue_not_available'}, status=503)

            data = await request.json()
            work_id = data.get('work_id', '')
            result = data.get('result', {})
            if not work_id:
                return web.json_response({'error': 'work_id_required'}, status=400)

            # Get work item before completing to track type
            work_item = wq.items.get(work_id)
            work_type = work_item.work_type if work_item else None
            config = work_item.config if work_item else {}

            success = wq.complete_work(work_id, result)

            # Update queue populator with Elo data if applicable
            if success and self._queue_populator is not None:
                board_type = config.get('board_type', '')
                num_players = config.get('num_players', 0)

                if work_type == WorkType.TOURNAMENT:
                    # Tournament results include Elo updates
                    elo = result.get('best_elo') or result.get('elo') or result.get('winner_elo')
                    model_id = result.get('best_model') or result.get('winner_model')
                    if elo and board_type and num_players:
                        self._queue_populator.update_target_elo(board_type, num_players, elo, model_id)
                        logger.info(f"Updated populator Elo: {board_type}_{num_players}p = {elo}")

                elif work_type == WorkType.SELFPLAY:
                    # Selfplay increments games count
                    games = result.get('games_generated', config.get('games', 0))
                    if games and board_type and num_players:
                        self._queue_populator.increment_games(board_type, num_players, games)

                elif work_type == WorkType.TRAINING:
                    # Training increments training runs
                    if board_type and num_players:
                        self._queue_populator.increment_training(board_type, num_players)

            return web.json_response({'status': 'completed' if success else 'failed', 'work_id': work_id})
        except Exception as e:
            logger.error(f"Error completing work: {e}")
            return web.json_response({'error': str(e)}, status=500)

    async def handle_work_fail(self, request: web.Request) -> web.Response:
        """Mark work as failed (may retry based on attempts)."""
        try:
            if not self.is_leader:
                return web.json_response({'error': 'not_leader', 'leader_id': self.leader_id}, status=403)

            wq = get_work_queue()
            if wq is None:
                return web.json_response({'error': 'work_queue_not_available'}, status=503)

            data = await request.json()
            work_id = data.get('work_id', '')
            error = data.get('error', 'unknown')
            if not work_id:
                return web.json_response({'error': 'work_id_required'}, status=400)

            success = wq.fail_work(work_id, error)
            return web.json_response({'status': 'failed' if success else 'not_found', 'work_id': work_id})
        except Exception as e:
            logger.error(f"Error failing work: {e}")
            return web.json_response({'error': str(e)}, status=500)

    async def handle_work_status(self, request: web.Request) -> web.Response:
        """Get work queue status."""
        try:
            wq = get_work_queue()
            if wq is None:
                return web.json_response({'error': 'work_queue_not_available'}, status=503)

            # Check for timeouts
            timed_out = wq.check_timeouts()

            status = wq.get_queue_status()
            status['is_leader'] = self.is_leader
            status['leader_id'] = self.leader_id
            status['timed_out_this_check'] = timed_out

            return web.json_response(status)
        except Exception as e:
            logger.error(f"Error getting work status: {e}")
            return web.json_response({'error': str(e)}, status=500)

    async def handle_populator_status(self, request: web.Request) -> web.Response:
        """Get queue populator status for monitoring."""
        try:
            if self._queue_populator is None:
                return web.json_response({
                    'enabled': False,
                    'message': 'Queue populator not initialized',
                })

            status = self._queue_populator.get_status()
            status['is_leader'] = self.is_leader
            return web.json_response(status)
        except Exception as e:
            logger.error(f"Error getting populator status: {e}")
            return web.json_response({'error': str(e)}, status=500)

    async def handle_work_for_node(self, request: web.Request) -> web.Response:
        """Get all work assigned to a specific node."""
        try:
            wq = get_work_queue()
            if wq is None:
                return web.json_response({'error': 'work_queue_not_available'}, status=503)

            node_id = request.match_info.get('node_id', '')
            if not node_id:
                return web.json_response({'error': 'node_id_required'}, status=400)

            work_items = wq.get_work_for_node(node_id)
            return web.json_response({
                'node_id': node_id,
                'work_items': work_items,
                'count': len(work_items),
            })
        except Exception as e:
            logger.error(f"Error getting work for node: {e}")
            return web.json_response({'error': str(e)}, status=500)

    async def handle_work_cancel(self, request: web.Request) -> web.Response:
        """Cancel a pending or claimed work item."""
        try:
            if not self.is_leader:
                return web.json_response({'error': 'not_leader', 'leader_id': self.leader_id}, status=403)

            wq = get_work_queue()
            if wq is None:
                return web.json_response({'error': 'work_queue_not_available'}, status=503)

            data = await request.json()
            work_id = data.get('work_id', '')
            if not work_id:
                return web.json_response({'error': 'work_id_required'}, status=400)

            success = wq.cancel_work(work_id)
            return web.json_response({
                'status': 'cancelled' if success else 'failed',
                'work_id': work_id,
            })
        except Exception as e:
            logger.error(f"Error cancelling work: {e}")
            return web.json_response({'error': str(e)}, status=500)

    async def handle_work_history(self, request: web.Request) -> web.Response:
        """Get work history from the database."""
        try:
            wq = get_work_queue()
            if wq is None:
                return web.json_response({'error': 'work_queue_not_available'}, status=503)

            limit = int(request.query.get('limit', '50'))
            status_filter = request.query.get('status', None)

            history = wq.get_history(limit=limit, status_filter=status_filter)
            return web.json_response({
                'history': history,
                'count': len(history),
                'limit': limit,
                'status_filter': status_filter,
            })
        except Exception as e:
            logger.error(f"Error getting work history: {e}")
            return web.json_response({'error': str(e)}, status=500)

    async def handle_election(self, request: web.Request) -> web.Response:
        """Handle election message from another node."""
        try:
            # Only "bully" lower-priority candidates when we're actually eligible
            # to act as a leader. Otherwise (e.g. NAT-blocked / ambiguous endpoint),
            # responding ALIVE can stall elections and leave the cluster leaderless.
            self._update_self_info()
            data = await request.json()
            candidate_id = str(data.get("candidate_id") or "")
            if not candidate_id:
                return web.json_response({"error": "missing_candidate_id"}, status=400)

            with self.peers_lock:
                peers_snapshot = [p for p in self.peers.values() if p.node_id != self.node_id]
            conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])
            eligible = self._is_leader_eligible(self.self_info, conflict_keys, require_alive=False)
            voter_node_ids = list(getattr(self, "voter_node_ids", []) or [])
            if eligible and voter_node_ids:
                # When quorum gating is enabled, only configured voters can participate
                # in bully elections. Non-voters responding "ALIVE" would stall the
                # election because their own `_start_election()` returns early.
                eligible = (self.node_id in voter_node_ids) and self._has_voter_quorum()

            # If our ID is higher, we respond with "ALIVE" (Bully algorithm)
            if self.node_id > candidate_id and eligible:
                # Start our own election
                asyncio.create_task(self._start_election())
                return web.json_response({"response": "ALIVE", "node_id": self.node_id, "eligible": True})
            else:
                return web.json_response({"response": "OK", "node_id": self.node_id, "eligible": bool(eligible)})
        except Exception as e:
            return web.json_response({"error": str(e)}, status=400)

    async def handle_lease_request(self, request: web.Request) -> web.Response:
        """Voter endpoint: grant/renew an exclusive leader lease.

        A leader candidate must obtain grants from a quorum of voters before it
        may act as leader. Voters only grant to one leader at a time until the
        lease expires (or is explicitly released by stepping down).
        """
        try:
            if self.auth_token and not self._is_request_authorized(request):
                return web.json_response({"error": "unauthorized"}, status=401)
            data = await request.json()
            leader_id = str(data.get("leader_id") or data.get("candidate_id") or "").strip()
            lease_id = str(data.get("lease_id") or "").strip()
            duration_raw = data.get("lease_duration", LEADER_LEASE_DURATION)
            try:
                duration = int(duration_raw)
            except Exception:
                duration = int(LEADER_LEASE_DURATION)
            duration = max(10, min(duration, int(LEADER_LEASE_DURATION * 2)))

            if not leader_id or not lease_id:
                return web.json_response({"granted": False, "reason": "missing_fields"}, status=400)

            voters = list(getattr(self, "voter_node_ids", []) or [])
            if voters:
                if self.node_id not in voters:
                    return web.json_response({"granted": False, "reason": "not_a_voter"}, status=403)
                if leader_id not in voters:
                    return web.json_response({"granted": False, "reason": "leader_not_voter"}, status=403)

            now = time.time()
            current_leader = str(getattr(self, "voter_grant_leader_id", "") or "")
            current_expires = float(getattr(self, "voter_grant_expires", 0.0) or 0.0)

            if current_leader and current_expires > now and current_leader != leader_id:
                return web.json_response(
                    {
                        "granted": False,
                        "reason": "lease_already_granted",
                        "current_leader_id": current_leader,
                        "current_lease_id": str(getattr(self, "voter_grant_lease_id", "") or ""),
                        "lease_expires": current_expires,
                    },
                    status=409,
                )

            self.voter_grant_leader_id = leader_id
            self.voter_grant_lease_id = lease_id
            self.voter_grant_expires = now + float(duration)
            self._save_state()

            lease_ttl_seconds = max(0.0, float(self.voter_grant_expires) - time.time())
            return web.json_response(
                {
                    "granted": True,
                    "leader_id": leader_id,
                    "lease_id": lease_id,
                    "lease_expires": self.voter_grant_expires,
                    # Use a relative TTL for robustness under clock skew (absolute
                    # timestamps from different machines are not directly comparable).
                    "lease_ttl_seconds": lease_ttl_seconds,
                    "voter_id": self.node_id,
                }
            )
        except Exception as e:
            return web.json_response({"granted": False, "error": str(e)}, status=400)

    async def handle_voter_grant_status(self, request: web.Request) -> web.Response:
        """Read-only voter endpoint: return our currently granted leader lease.

        This lets nodes resolve split-brain by consulting a quorum of voters for
        the active lease holder, without mutating lease state.
        """
        try:
            if self.auth_token and not self._is_request_authorized(request):
                return web.json_response({"error": "unauthorized"}, status=401)
            now = time.time()
            expires = float(getattr(self, "voter_grant_expires", 0.0) or 0.0)
            return web.json_response(
                {
                    "voter_id": self.node_id,
                    "now": now,
                    "leader_id": str(getattr(self, "voter_grant_leader_id", "") or ""),
                    "lease_id": str(getattr(self, "voter_grant_lease_id", "") or ""),
                    "lease_expires": expires,
                    "lease_ttl_seconds": max(0.0, expires - now),
                }
            )
        except Exception as e:
            return web.json_response({"error": str(e)}, status=400)

    async def handle_coordinator(self, request: web.Request) -> web.Response:
        """Handle coordinator announcement from new leader.

        LEARNED LESSONS - Only accept leadership from higher-priority nodes (Bully algorithm).
        Also handles lease-based leadership updates.
        """
        try:
            self._update_self_info()
            data = await request.json()
            new_leader_raw = data.get("leader_id")
            if not new_leader_raw:
                return web.json_response(
                    {"accepted": False, "reason": "missing_leader_id"},
                    status=400,
                )
            new_leader = str(new_leader_raw)
            lease_id = data.get("lease_id", "")
            lease_expires = data.get("lease_expires", 0)
            is_renewal = data.get("lease_renewal", False)
            incoming_voters = data.get("voter_node_ids") or data.get("voters") or None
            if incoming_voters:
                voters_list: list[str] = []
                if isinstance(incoming_voters, list):
                    voters_list = [str(v).strip() for v in incoming_voters if str(v).strip()]
                elif isinstance(incoming_voters, str):
                    voters_list = [t.strip() for t in incoming_voters.split(",") if t.strip()]
                if voters_list:
                    self._maybe_adopt_voter_node_ids(voters_list, source="learned")

            voters = list(getattr(self, "voter_node_ids", []) or [])
            if voters and new_leader not in voters:
                return web.json_response(
                    {"accepted": False, "reason": "leader_not_voter", "voters": voters},
                    status=403,
                )

            # Voter-side safety: if we've granted a still-valid lease to a different leader,
            # do not accept a conflicting coordinator announcement. This prevents a voter
            # from "following" a non-quorum leader during transient partitions.
            if voters and self.node_id in voters:
                grant_leader = str(getattr(self, "voter_grant_leader_id", "") or "")
                grant_expires = float(getattr(self, "voter_grant_expires", 0.0) or 0.0)
                if grant_leader and grant_expires > time.time() and grant_leader != new_leader:
                    return web.json_response(
                        {
                            "accepted": False,
                            "reason": "voter_lease_conflict",
                            "granted_to": grant_leader,
                            "granted_until": grant_expires,
                        },
                        status=409,
                    )

            # If quorum gating is not configured, fall back to bully ordering
            # (lexicographically highest node_id wins).
            if not voters and self.role == NodeRole.LEADER and new_leader < self.node_id:
                # Exception: accept if our lease has expired
                if self.leader_lease_expires > 0 and time.time() >= self.leader_lease_expires:
                    logger.info(f"Our lease expired, accepting leader: {new_leader}")
                else:
                    logger.info(f"Rejecting leader announcement from lower-priority node: {new_leader} < {self.node_id}")
                    return web.json_response({"accepted": False, "reason": "lower_priority"})

            # Reject leadership from nodes that are not directly reachable / uniquely addressable.
            if new_leader != self.node_id:
                with self.peers_lock:
                    peer = self.peers.get(new_leader)
                    peers_snapshot = [p for p in self.peers.values() if p.node_id != self.node_id]
                if peer:
                    conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])
                    if not self._is_leader_eligible(peer, conflict_keys, require_alive=False):
                        return web.json_response({"accepted": False, "reason": "leader_ineligible"})

            if is_renewal and new_leader == self.leader_id:
                self.leader_lease_expires = lease_expires
                self.leader_lease_id = lease_id
                return web.json_response({"accepted": True})

            logger.info(f"Accepting leader announcement: {new_leader}")
            self.leader_id = new_leader
            self.leader_lease_id = lease_id
            self.leader_lease_expires = lease_expires if lease_expires else time.time() + LEADER_LEASE_DURATION

            if new_leader == self.node_id:
                self.role = NodeRole.LEADER
            else:
                self.role = NodeRole.FOLLOWER

            self._save_state()
            return web.json_response({"accepted": True})
        except Exception as e:
            return web.json_response({"error": str(e)}, status=400)

    async def handle_start_job(self, request: web.Request) -> web.Response:
        """Handle request to start a job (from leader)."""
        try:
            data = await request.json()
            job_type = JobType(data.get("job_type", "selfplay"))
            board_type = data.get("board_type", "square8")
            num_players = data.get("num_players", 2)
            engine_mode = data.get("engine_mode", "gumbel-mcts")  # GPU-accelerated default
            job_id = data.get("job_id")
            cuda_visible_devices = data.get("cuda_visible_devices")

            # Extra params for DATA_EXPORT jobs
            export_params = None
            if job_type == JobType.DATA_EXPORT:
                export_params = {
                    "input_path": data.get("input_path"),
                    "output_path": data.get("output_path"),
                    "encoder_version": data.get("encoder_version", "v3"),
                    "max_games": data.get("max_games", 5000),
                    "is_jsonl": data.get("is_jsonl", False),
                }

            job = await self._start_local_job(
                job_type,
                board_type=board_type,
                num_players=num_players,
                engine_mode=engine_mode,
                job_id=job_id,
                cuda_visible_devices=cuda_visible_devices,
                export_params=export_params,
            )

            if job:
                return web.json_response({"success": True, "job": job.to_dict()})
            else:
                return web.json_response({"success": False, "error": "Failed to start job"}, status=500)
        except Exception as e:
            return web.json_response({"error": str(e)}, status=400)

    async def handle_stop_job(self, request: web.Request) -> web.Response:
        """Handle request to stop a job."""
        try:
            data = await request.json()
            job_id = data.get("job_id")

            async with AsyncLockWrapper(self.jobs_lock):
                if job_id in self.local_jobs:
                    job = self.local_jobs[job_id]
                    try:
                        os.kill(job.pid, signal.SIGTERM)
                        job.status = "stopped"
                    except OSError:
                        pass  # Process already dead
                    return web.json_response({"success": True})

            return web.json_response({"success": False, "error": "Job not found"}, status=404)
        except Exception as e:
            return web.json_response({"error": str(e)}, status=400)

    async def handle_job_kill(self, request: web.Request) -> web.Response:
        """Handle request to forcefully kill a stuck job (SIGKILL).

        Used by the leader's self-healing system to kill stuck jobs remotely.
        Supports killing by job_id or by job_type pattern.
        """
        try:
            data = await request.json()
            job_id = data.get("job_id")
            job_type = data.get("job_type")  # "training", "selfplay", etc.
            reason = data.get("reason", "unknown")

            killed = 0

            # Try to kill by job_id first
            if job_id:
                async with AsyncLockWrapper(self.jobs_lock):
                    if job_id in self.local_jobs:
                        job = self.local_jobs[job_id]
                        try:
                            os.kill(job.pid, signal.SIGKILL)
                            job.status = "killed"
                            killed += 1
                            logger.info(f"Killed job {job_id} (pid {job.pid}): {reason}")
                        except Exception as e:
                            logger.error(f"Failed to kill job {job_id}: {e}")

            # Kill by job_type pattern (for stuck training, etc.)
            if job_type and killed == 0:
                import subprocess
                patterns = {
                    "training": ["train_nnue", "train.*model"],
                    "selfplay": ["selfplay", "run_hybrid_selfplay"],
                }
                for pattern in patterns.get(job_type, [job_type]):
                    try:
                        result = subprocess.run(
                            ["pkill", "-9", "-f", pattern],
                            timeout=5,
                            capture_output=True,
                        )
                        if result.returncode == 0:
                            killed += 1
                            logger.info(f"Killed processes matching '{pattern}': {reason}")
                    except Exception as e:
                        logger.info(f"pkill error for {pattern}: {e}")

            return web.json_response({
                "success": killed > 0,
                "killed": killed,
                "reason": reason,
            })

        except Exception as e:
            return web.json_response({"error": str(e)}, status=400)

    async def handle_cleanup(self, request: web.Request) -> web.Response:
        """Handle cleanup request (from leader or manual).

        LEARNED LESSONS - This endpoint allows remote nodes to trigger disk cleanup
        when the leader detects disk usage approaching critical thresholds.
        """
        try:
            logger.info("Cleanup request received")

            # Run cleanup in background to avoid blocking the request
            asyncio.create_task(self._cleanup_local_disk())

            # Return current disk usage
            usage = self._get_resource_usage()
            return web.json_response({
                "success": True,
                "disk_percent_before": usage["disk_percent"],
                "message": "Cleanup initiated",
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_restart_stuck_jobs(self, request: web.Request) -> web.Response:
        """Handle request to restart stuck selfplay jobs.

        LEARNED LESSONS - Called by leader when it detects GPU idle with running processes.
        Kills all selfplay processes and clears job tracking so they restart.
        """
        try:
            logger.info("Restart stuck jobs request received")

            # Run in background to avoid blocking
            asyncio.create_task(self._restart_local_stuck_jobs())

            return web.json_response({
                "success": True,
                "message": "Stuck job restart initiated",
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_reduce_selfplay(self, request: web.Request) -> web.Response:
        """Stop excess selfplay jobs on this node (load shedding).

        Used by leaders when a node is under memory/disk pressure so the node
        can recover without requiring manual intervention.
        """
        try:
            data = await request.json()
            target_raw = data.get("target_selfplay_jobs", data.get("target", 0))
            reason = str(data.get("reason") or "remote_request")
            try:
                target = int(target_raw)
            except Exception:
                target = 0

            result = await self._reduce_local_selfplay_jobs(target, reason=reason)
            return web.json_response({"success": True, **result})
        except Exception as e:
            return web.json_response({"success": False, "error": str(e)}, status=400)

    async def handle_selfplay_start(self, request: web.Request) -> web.Response:
        """POST /selfplay/start - Start GPU selfplay job on this node.

        Called by leader to dispatch GPU selfplay work to worker nodes.
        Uses run_hybrid_selfplay.py for GPU-accelerated game generation.
        """
        try:
            data = await request.json()
            board_type = data.get("board_type", "square8")
            num_players = data.get("num_players", 2)
            num_games = data.get("num_games", 500)
            engine_mode = data.get("engine_mode", "gpu")
            data.get("auto_scaled", False)

            job_id = f"selfplay-{self.node_id}-{int(time.time())}"

            # Start the selfplay job in background
            asyncio.create_task(self._run_gpu_selfplay_job(
                job_id=job_id,
                board_type=board_type,
                num_players=num_players,
                num_games=num_games,
                engine_mode=engine_mode,
            ))

            logger.info(f"Started GPU selfplay job {job_id}: {board_type}/{num_players}p, {num_games} games")
            return web.json_response({
                "success": True,
                "job_id": job_id,
                "board_type": board_type,
                "num_players": num_players,
                "num_games": num_games,
                "node_id": self.node_id,
            })
        except Exception as e:
            logger.error(f"Failed to start selfplay: {e}")
            return web.json_response({"success": False, "error": str(e)}, status=500)

    async def _run_gpu_selfplay_job(
        self, job_id: str, board_type: str, num_players: int, num_games: int, engine_mode: str
    ):
        """Run selfplay job with appropriate script based on engine mode.

        For simple modes (random, heuristic, nnue-guided): use run_gpu_selfplay.py (GPU-optimized)
        For search modes (maxn, brs, mcts, gumbel-mcts): use run_hybrid_selfplay.py (supports search)
        """
        import sys
        from pathlib import Path

        # Engine modes that require search (need run_hybrid_selfplay.py)
        SEARCH_ENGINE_MODES = {"maxn", "brs", "mcts", "gumbel-mcts", "policy-only", "nn-descent", "nn-minimax"}

        board_norm = board_type.replace("hexagonal", "hex")
        output_dir = Path(self.ringrift_path) / "ai-service" / "data" / "selfplay" / "p2p_gpu" / f"{board_norm}_{num_players}p" / job_id
        output_dir.mkdir(parents=True, exist_ok=True)

        effective_mode = engine_mode or "heuristic-only"

        if effective_mode in SEARCH_ENGINE_MODES:
            # Use run_hybrid_selfplay.py for search-based modes (maxn, brs, mcts, etc.)
            script_path = os.path.join(self.ringrift_path, "ai-service", "scripts", "run_hybrid_selfplay.py")
            if not os.path.exists(script_path):
                logger.warning(f"Hybrid selfplay script not found: {script_path}")
                return

            cmd = [
                sys.executable,
                script_path,
                "--board-type", board_norm,
                "--num-players", str(num_players),
                "--num-games", str(num_games),
                "--output-dir", str(output_dir),
                "--record-db", str(output_dir / "games.db"),
                "--lean-db",
                "--engine-mode", effective_mode,
                "--seed", str(int(time.time() * 1000) % 2**31),
            ]
        else:
            # Use run_gpu_selfplay.py for GPU-optimized modes (random, heuristic, nnue-guided)
            script_path = os.path.join(self.ringrift_path, "ai-service", "scripts", "run_gpu_selfplay.py")
            if not os.path.exists(script_path):
                logger.warning(f"GPU selfplay script not found: {script_path}")
                return

            # Map engine modes: run_gpu_selfplay.py only supports: random-only, heuristic-only, nnue-guided
            mode_map = {
                "mixed": "heuristic-only",
                "gpu": "heuristic-only",
                "descent-only": "heuristic-only",
                "heuristic-only": "heuristic-only",
                "nnue-guided": "nnue-guided",
                "random-only": "random-only",
            }
            gpu_engine_mode = mode_map.get(effective_mode, "heuristic-only")

            cmd = [
                sys.executable,
                script_path,
                "--board", board_norm,  # Note: --board not --board-type
                "--num-players", str(num_players),
                "--num-games", str(num_games),
                "--output-dir", str(output_dir),
                "--output-db", str(output_dir / "games.db"),
                "--engine-mode", gpu_engine_mode,
                "--seed", str(int(time.time() * 1000) % 2**31),
            ]

        env = os.environ.copy()
        env["PYTHONPATH"] = os.path.join(self.ringrift_path, "ai-service")
        env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

        try:
            proc = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env,
            )

            # Track the job
            with self.jobs_lock:
                if "selfplay" not in self.active_jobs:
                    self.active_jobs["selfplay"] = {}
                self.active_jobs["selfplay"][job_id] = {
                    "job_id": job_id,
                    "status": "running",
                    "board_type": board_type,
                    "num_players": num_players,
                    "num_games": num_games,
                    "started_at": time.time(),
                    "pid": proc.pid,
                }

            _stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=7200)  # 2 hour max

            # Update job status
            with self.jobs_lock:
                if job_id in self.active_jobs.get("selfplay", {}):
                    if proc.returncode == 0:
                        self.active_jobs["selfplay"][job_id]["status"] = "completed"
                        logger.info(f"GPU selfplay job {job_id} completed successfully")
                    else:
                        self.active_jobs["selfplay"][job_id]["status"] = "failed"
                        logger.info(f"GPU selfplay job {job_id} failed: {stderr.decode()[:500]}")
                    del self.active_jobs["selfplay"][job_id]

        except asyncio.TimeoutError:
            logger.info(f"GPU selfplay job {job_id} timed out")
            with self.jobs_lock:
                if job_id in self.active_jobs.get("selfplay", {}):
                    del self.active_jobs["selfplay"][job_id]
        except Exception as e:
            logger.info(f"GPU selfplay job {job_id} error: {e}")
            with self.jobs_lock:
                if job_id in self.active_jobs.get("selfplay", {}):
                    del self.active_jobs["selfplay"][job_id]

    async def handle_cleanup_files(self, request: web.Request) -> web.Response:
        """Delete specific files from this node (for post-sync cleanup).

        Called by leader after successful sync to training nodes to free
        disk space on source nodes with high disk usage.
        """
        try:
            data = await request.json()
            files = data.get("files", [])
            reason = data.get("reason", "manual")

            if not files:
                return web.json_response({"success": False, "error": "No files specified"}, status=400)

            logger.info(f"Cleanup files request: {len(files)} files, reason={reason}")

            data_dir = self.get_data_directory()
            freed_bytes = 0
            deleted_count = 0

            for file_path in files:
                # Security: only allow deletion within data directory
                full_path = data_dir / (file_path or "").lstrip("/")
                try:
                    data_root = data_dir.resolve()
                    resolved = full_path.resolve()
                    resolved.relative_to(data_root)
                except Exception:
                    logger.info(f"Cleanup: skipping path outside data dir: {file_path}")
                    continue

                if resolved.exists():
                    try:
                        size = resolved.stat().st_size
                        resolved.unlink()
                        freed_bytes += size
                        deleted_count += 1
                    except Exception as e:
                        logger.error(f"Failed to delete {file_path}: {e}")

            logger.info(f"Cleanup complete: {deleted_count} files, {freed_bytes / 1e6:.1f}MB freed")

            return web.json_response({
                "success": True,
                "freed_bytes": freed_bytes,
                "deleted_count": deleted_count,
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_purge_retired_peers(self, request: web.Request) -> web.Response:
        """Purge retired peers from the cluster registry.

        Removes peers that have been marked as retired (dead/terminated instances)
        to clean up the peer list. This endpoint is unauthenticated for ease of
        admin access; it only cleans up stale entries, not active nodes.
        """
        # Skip auth check for this admin cleanup operation
        try:
            async with AsyncLockWrapper(self.peers_lock):
                retired_peers = [
                    node_id for node_id, info in self.peers.items()
                    if getattr(info, "retired", False)
                ]

                if not retired_peers:
                    return web.json_response({
                        "success": True,
                        "purged_count": 0,
                        "message": "No retired peers to purge",
                    })

                for node_id in retired_peers:
                    del self.peers[node_id]
                    logger.info(f"Purged retired peer: {node_id}")

                logger.info(f"Purged {len(retired_peers)} retired peers")

            return web.json_response({
                "success": True,
                "purged_count": len(retired_peers),
                "purged_peers": retired_peers,
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_purge_stale_peers(self, request: web.Request) -> web.Response:
        """Purge stale peers based on heartbeat age.

        This is more aggressive than purge_retired - it removes any peer
        that hasn't sent a heartbeat in the specified threshold (default 1 hour).

        Query params:
            max_age: Maximum heartbeat age in seconds (default: 3600)
            dry_run: If 1, just report what would be purged without deleting
        """
        try:
            max_age = int(request.query.get("max_age", "3600"))
            dry_run = request.query.get("dry_run", "0") == "1"
            now = time.time()

            stale_peers = []
            async with AsyncLockWrapper(self.peers_lock):
                for node_id, info in self.peers.items():
                    if node_id == self.node_id:
                        continue  # Don't purge self
                    last_hb = getattr(info, "last_heartbeat", 0.0) or 0.0
                    age = now - last_hb
                    if age >= max_age:
                        stale_peers.append({
                            "node_id": node_id,
                            "age_seconds": int(age),
                            "last_heartbeat": last_hb,
                            "role": str(getattr(info, "role", "unknown")),
                            "nat_blocked": getattr(info, "nat_blocked", False),
                        })

            if not stale_peers:
                return web.json_response({
                    "success": True,
                    "purged_count": 0,
                    "message": f"No peers older than {max_age}s found",
                })

            purged_ids = []
            if not dry_run:
                async with AsyncLockWrapper(self.peers_lock):
                    for peer in stale_peers:
                        node_id = peer["node_id"]
                        if node_id in self.peers:
                            del self.peers[node_id]
                            purged_ids.append(node_id)
                            logger.info(f"Purged stale peer: {node_id} (no heartbeat for {peer['age_seconds']}s)")

            return web.json_response({
                "success": True,
                "purged_count": len(purged_ids) if not dry_run else 0,
                "would_purge_count": len(stale_peers),
                "dry_run": dry_run,
                "max_age_seconds": max_age,
                "stale_peers": stale_peers,
                "purged_peers": purged_ids,
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_admin_unretire(self, request: web.Request) -> web.Response:
        """Unretire a specific peer node.

        This endpoint allows external systems (like vast_p2p_sync.py) to
        programmatically unretire nodes that are known to be active but were
        marked as retired due to temporary connectivity issues.

        Query params:
            node_id: The node ID to unretire (required)

        Returns:
            JSON with success status and node info
        """
        try:
            node_id = request.query.get("node_id", "").strip()
            if not node_id:
                return web.json_response({
                    "error": "node_id parameter is required"
                }, status=400)

            async with AsyncLockWrapper(self.peers_lock):
                if node_id not in self.peers:
                    # List available nodes for debugging
                    available = list(self.peers.keys())
                    return web.json_response({
                        "error": f"Node '{node_id}' not found in peer registry",
                        "available_nodes": available[:20],  # Limit to first 20
                        "total_nodes": len(available),
                    }, status=404)

                peer_info = self.peers[node_id]
                was_retired = getattr(peer_info, "retired", False)

                if not was_retired:
                    return web.json_response({
                        "success": True,
                        "message": f"Node '{node_id}' was not retired",
                        "already_active": True,
                    })

                # Unretire the node
                peer_info.retired = False
                peer_info.retired_at = 0.0

                # Also reset failure counters to give it a fresh start
                peer_info.consecutive_failures = 0
                peer_info.last_failure_time = 0.0

                logger.info(f"Unretired peer: {node_id} (admin request)")

            return web.json_response({
                "success": True,
                "message": f"Node '{node_id}' has been unretired",
                "node_id": node_id,
                "host": getattr(peer_info, "host", ""),
                "gpu_name": getattr(peer_info, "gpu_name", ""),
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_training_sync(self, request: web.Request) -> web.Response:
        """Manually trigger sync of selfplay data to training nodes.

        Leader-only: Syncs selfplay data to the top GPU nodes for training.
        """
        try:
            result = await self._sync_selfplay_to_training_nodes()
            return web.json_response(result)
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_gpu_rankings(self, request: web.Request) -> web.Response:
        """Get GPU power rankings for all nodes in the cluster.

        Returns nodes sorted by GPU processing power for training priority.
        """
        try:
            rankings = self._get_training_nodes_ranked()
            training_nodes = self._get_training_primary_nodes()

            return web.json_response({
                "rankings": rankings,
                "training_primary_nodes": [n.node_id for n in training_nodes],
                "training_node_count": TRAINING_NODE_COUNT,
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_health(self, request: web.Request) -> web.Response:
        """Handle health check request.

        LEARNED LESSONS - Simple health endpoint for monitoring and load balancers.
        Returns node health status without full cluster state.
        Includes utilization status from resource_optimizer for cluster coordination.
        """
        try:
            self._update_self_info()
            is_healthy = self.self_info.is_healthy()

            # Calculate uptime and leader status
            uptime_seconds = time.time() - getattr(self, "start_time", time.time())
            leader_last_seen = time.time() - getattr(self, "last_leader_seen", time.time())
            active_peers = sum(1 for p in self.peers.values()
                             if time.time() - p.last_heartbeat < 120)

            response = {
                "healthy": is_healthy,
                "node_id": self.node_id,
                "role": self.role.value,
                "disk_percent": self.self_info.disk_percent,
                "memory_percent": self.self_info.memory_percent,
                "cpu_percent": self.self_info.cpu_percent,
                "selfplay_jobs": self.self_info.selfplay_jobs,
                "training_jobs": self.self_info.training_jobs,
                # Cluster health for alerting
                "leader_id": self.leader_id,
                "leader_last_seen_seconds": leader_last_seen if self.leader_id else None,
                "active_peers": active_peers,
                "total_peers": len(self.peers),
                "uptime_seconds": uptime_seconds,
                "timestamp": datetime.utcnow().isoformat(),
            }

            # Add cluster utilization status for cooperative 60-80% targeting
            if HAS_RATE_NEGOTIATION and get_utilization_status is not None:
                try:
                    util_status = get_utilization_status()
                    response["cluster_utilization"] = {
                        "cpu_util": util_status.get("cpu_util", 0),
                        "gpu_util": util_status.get("gpu_util", 0),
                        "selfplay_rate": util_status.get("current_rate", 1000),
                        "target_range": "60-80%",
                        "status": util_status.get("status", "unknown"),
                    }
                except Exception:
                    pass

            return web.json_response(response)
        except Exception as e:
            return web.json_response({"error": str(e), "healthy": False}, status=500)

    async def handle_cluster_health(self, request: web.Request) -> web.Response:
        """Aggregate health status from all cluster nodes (leader-only).

        Phase 6: Health broadcasting - leader aggregates peer health data
        and reports unhealthy nodes for monitoring and alerting.
        """
        try:
            if not self._is_leader():
                return await self._proxy_to_leader(request)

            self._update_self_info()

            # Collect health from all peers
            unhealthy_nodes = []
            code_version_mismatches = []
            disk_warnings = []
            memory_warnings = []
            nfs_issues = []

            my_version = self.build_version

            with self.peers_lock:
                peers_snapshot = list(self.peers.values())

            for peer in peers_snapshot:
                # Check for health issues
                issues = peer.get_health_issues()
                if issues:
                    unhealthy_nodes.append({
                        "node_id": peer.node_id,
                        "issues": [{"code": code, "description": desc} for code, desc in issues],
                    })

                # Check code version mismatch
                if peer.code_version and peer.code_version != my_version:
                    code_version_mismatches.append({
                        "node_id": peer.node_id,
                        "version": peer.code_version,
                        "leader_version": my_version,
                    })

                # Check disk warnings
                if peer.disk_percent >= 85:
                    disk_warnings.append({
                        "node_id": peer.node_id,
                        "disk_percent": peer.disk_percent,
                        "disk_free_gb": peer.disk_free_gb,
                    })

                # Check memory warnings
                if peer.memory_percent >= 85:
                    memory_warnings.append({
                        "node_id": peer.node_id,
                        "memory_percent": peer.memory_percent,
                    })

                # Check NFS issues
                if not peer.nfs_accessible:
                    nfs_issues.append({
                        "node_id": peer.node_id,
                    })

            # Calculate cluster health summary
            total_nodes = len(peers_snapshot) + 1  # Include self
            healthy_nodes = total_nodes - len(unhealthy_nodes)
            cluster_health_pct = (healthy_nodes / total_nodes * 100) if total_nodes > 0 else 0

            return web.json_response({
                "success": True,
                "timestamp": time.time(),
                "leader_id": self.node_id,
                "leader_version": my_version,
                "cluster_health": {
                    "total_nodes": total_nodes,
                    "healthy_nodes": healthy_nodes,
                    "unhealthy_nodes": len(unhealthy_nodes),
                    "health_percent": cluster_health_pct,
                },
                "issues": {
                    "unhealthy_nodes": unhealthy_nodes,
                    "code_version_mismatches": code_version_mismatches,
                    "disk_warnings": disk_warnings,
                    "memory_warnings": memory_warnings,
                    "nfs_issues": nfs_issues,
                },
            })
        except Exception as e:
            logger.error(f"Cluster health check error: {e}")
            return web.json_response({"error": str(e)}, status=500)

    # ============================================
    # Relay/Hub Handlers for NAT-blocked nodes
    # ============================================

    async def handle_relay_heartbeat(self, request: web.Request) -> web.Response:
        """POST /relay/heartbeat - Accept heartbeat from NAT-blocked node.

        NAT-blocked nodes (e.g., Vast.ai behind carrier NAT) can't receive
        incoming connections. They use this endpoint to:
        1. Send their status to the leader
        2. Get back the full cluster peer list
        3. Mark themselves as nat_blocked so leader doesn't try to reach them

        Request body: Same as regular heartbeat (NodeInfo dict)
        Response: {
            "self": NodeInfo,  # Leader's info
            "peers": {node_id: NodeInfo},  # All known peers including NAT-blocked
            "leader_id": str
        }
        """
        try:
            data = await request.json()
            relay_ack = data.get("relay_ack") or []
            relay_results = data.get("relay_results") or []
            peer_info = NodeInfo.from_dict(data)
            if not peer_info.reported_host:
                peer_info.reported_host = peer_info.host
            if not peer_info.reported_port:
                peer_info.reported_port = peer_info.port
            peer_info.last_heartbeat = time.time()
            peer_info.nat_blocked = True  # Mark as NAT-blocked
            peer_info.nat_blocked_since = peer_info.nat_blocked_since or time.time()  # Track when blocked
            peer_info.relay_via = self.node_id  # This node is their relay

            # Get their real IP from the request (for logging/debugging)
            forwarded_for = (
                request.headers.get("X-Forwarded-For")
                or request.headers.get("X-Real-IP")
                or request.headers.get("CF-Connecting-IP")
            )
            real_ip = forwarded_for.split(",")[0].strip() if forwarded_for else request.remote
            if real_ip:
                peer_info.host = real_ip

            # STABILITY FIX: Correct stale leader role claims
            if peer_info.role == NodeRole.LEADER and peer_info.node_id != self.node_id:
                actual_leader = self.leader_id
                if actual_leader and actual_leader != peer_info.node_id:
                    peer_info.role = NodeRole.FOLLOWER

            # Store in peers list (they're part of the cluster even if not directly reachable)
            async with AsyncLockWrapper(self.peers_lock):
                self.peers[peer_info.node_id] = peer_info

            logger.info(f"Relay heartbeat from {peer_info.node_id} (real IP: {real_ip})")

            # Apply relay ACKs/results and return any queued commands.
            commands_to_send: list[dict[str, Any]] = []
            async with AsyncLockWrapper(self.relay_lock):
                queue = list(self.relay_command_queue.get(peer_info.node_id, []))
                now = time.time()
                queue = [
                    cmd for cmd in queue
                    if float(cmd.get("expires_at", 0.0) or 0.0) > now
                ]

                if relay_ack:
                    ack_set = {str(c) for c in relay_ack if c}
                    queue = [cmd for cmd in queue if str(cmd.get("id", "")) not in ack_set]

                if relay_results:
                    for item in relay_results:
                        try:
                            cmd_id = str(item.get("id") or "")
                            ok = bool(item.get("ok", False))
                            err = str(item.get("error") or "")
                            if not cmd_id:
                                continue
                            if ok:
                                logger.info(f"Relay command {cmd_id} on {peer_info.node_id}: ok")
                            else:
                                logger.info(f"Relay command {cmd_id} on {peer_info.node_id}: failed {err[:200]}")
                        except Exception:
                            continue

                self.relay_command_queue[peer_info.node_id] = queue
                commands_to_send = queue[:RELAY_COMMAND_MAX_BATCH]

            # Return cluster state so they can see all peers
            self._update_self_info()
            async with AsyncLockWrapper(self.peers_lock):
                peers = {k: v.to_dict() for k, v in self.peers.items()}

            effective_leader = self._get_leader_peer()
            effective_leader_id = effective_leader.node_id if effective_leader else None
            return web.json_response({
                "success": True,
                "self": self.self_info.to_dict(),
                "peers": peers,
                # IMPORTANT: only advertise a leader_id when it is actually reachable
                # and currently reporting itself as leader. Persisted/stale leader_id
                # values are surfaced separately so bootstrapping nodes don't get
                # stuck pointing at a non-leader.
                "leader_id": effective_leader_id,
                "effective_leader_id": effective_leader_id,
                "last_known_leader_id": self.leader_id,
                "relay_node": self.node_id,
                # Propagate the stable voter set so nodes that boot without local
                # config still enable quorum gating and avoid split-brain.
                "voter_node_ids": list(getattr(self, "voter_node_ids", []) or []),
                "voter_quorum_size": int(getattr(self, "voter_quorum_size", 0) or 0),
                "voter_quorum_ok": self._has_voter_quorum(),
                "voter_config_source": str(getattr(self, "voter_config_source", "") or ""),
                "commands": commands_to_send,
            })

        except Exception as e:
            return web.json_response({"error": str(e)}, status=400)

    async def handle_relay_enqueue(self, request: web.Request) -> web.Response:
        """POST /relay/enqueue - Enqueue a command for a NAT-blocked node on this relay.

        This enables multi-hop operation when NAT-blocked nodes can reach a
        public relay hub (e.g., AWS) but cannot reach the cluster leader
        directly (e.g., TUN-less Tailscale inside some containers).

        Request body:
          {
            "target_node_id": "node-id",
            "type": "start_job" | "cleanup" | ...,
            "payload": { ... }
          }

        Response:
          { "success": true, "id": "<cmd_id>" }
        """
        try:
            data = await request.json()
        except Exception:
            data = {}

        try:
            target_node_id = str(data.get("target_node_id") or data.get("node_id") or "").strip()
            cmd_type = str(data.get("type") or data.get("cmd_type") or "").strip()
            payload = data.get("payload") or {}
            if not isinstance(payload, dict):
                payload = {}
        except Exception:
            target_node_id = ""
            cmd_type = ""
            payload = {}

        if not target_node_id or not cmd_type:
            return web.json_response(
                {"success": False, "error": "invalid_request", "message": "target_node_id and type are required"},
                status=400,
            )

        cmd_id = self._enqueue_relay_command(target_node_id, cmd_type, payload)
        if not cmd_id:
            return web.json_response({"success": False, "error": "queue_full"}, status=429)

        return web.json_response({"success": True, "id": cmd_id})

    async def handle_relay_peers(self, request: web.Request) -> web.Response:
        """GET /relay/peers - Get list of all peers including NAT-blocked ones.

        Used by nodes to discover the full cluster including NAT-blocked members.
        """
        try:
            if self.auth_token and not self._is_request_authorized(request):
                return web.json_response({"error": "unauthorized"}, status=401)
            self._update_self_info()
            effective_leader = self._get_leader_peer()
            async with AsyncLockWrapper(self.peers_lock):
                all_peers = {k: v.to_dict() for k, v in self.peers.items()}

            # Separate NAT-blocked and directly reachable
            nat_blocked = {k: v for k, v in all_peers.items() if v.get('nat_blocked')}
            direct = {k: v for k, v in all_peers.items() if not v.get('nat_blocked')}

            return web.json_response({
                "success": True,
                "leader_id": (effective_leader.node_id if effective_leader else self.leader_id),
                "effective_leader_id": (effective_leader.node_id if effective_leader else None),
                "total_peers": len(all_peers),
                "direct_peers": len(direct),
                "nat_blocked_peers": len(nat_blocked),
                "voter_node_ids": list(getattr(self, "voter_node_ids", []) or []),
                "voter_quorum_size": int(getattr(self, "voter_quorum_size", 0) or 0),
                "voter_quorum_ok": self._has_voter_quorum(),
                "voter_config_source": str(getattr(self, "voter_config_source", "") or ""),
                "peers": all_peers,
            })

        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_relay_status(self, request: web.Request) -> web.Response:
        """GET /relay/status - Get relay queue status for debugging.

        Shows pending commands per NAT-blocked node including command ages.
        Useful for diagnosing relay delivery issues.
        """
        try:
            if self.auth_token and not self._is_request_authorized(request):
                return web.json_response({"error": "unauthorized"}, status=401)

            now = time.time()
            queue_status = {}
            total_pending = 0

            for node_id, commands in self.relay_command_queue.items():
                if not commands:
                    continue
                cmd_info = []
                for cmd in commands:
                    age_secs = now - cmd.get("ts", now)
                    cmd_info.append({
                        "id": cmd.get("id", ""),
                        "type": cmd.get("cmd", ""),
                        "age_secs": round(age_secs, 1),
                        "stale": age_secs > 300,  # >5 min is stale
                    })
                queue_status[node_id] = {
                    "pending_count": len(commands),
                    "commands": cmd_info,
                    "oldest_age_secs": round(max((now - c.get("ts", now)) for c in commands), 1) if commands else 0,
                }
                total_pending += len(commands)

            # Get NAT-blocked nodes for context
            with self.peers_lock:
                nat_blocked_nodes = [nid for nid, p in self.peers.items() if getattr(p, 'nat_blocked', False)]

            return web.json_response({
                "success": True,
                "total_pending_commands": total_pending,
                "nat_blocked_nodes": nat_blocked_nodes,
                "nodes_with_pending": list(queue_status.keys()),
                "queues": queue_status,
            })

        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_gossip(self, request: web.Request) -> web.Response:
        """POST /gossip - Receive gossip from peer and respond with our state.

        GOSSIP PROTOCOL: Decentralized state sharing between nodes.
        Each node shares its state with random peers, and information
        propagates through the cluster without leader coordination.

        GOSSIP COMPRESSION: Supports gzip-compressed requests and responses
        to reduce network bandwidth. Check Content-Encoding header.

        Request body:
        {
            "sender": "node-id",
            "sender_state": { state dict },
            "known_states": { node_id -> state dict }
        }

        Response:
        {
            "sender_state": { our state },
            "known_states": { our known states },
            "peer_manifests": { node_id -> manifest summary }
        }
        """
        try:
            if self.auth_token and not self._is_request_authorized(request):
                return web.json_response({"error": "unauthorized"}, status=401)

            # GOSSIP COMPRESSION: Handle gzip-compressed requests
            content_encoding = request.headers.get("Content-Encoding", "")
            if content_encoding == "gzip":
                compressed_body = await request.read()
                decompressed = gzip.decompress(compressed_body)
                data = json.loads(decompressed.decode("utf-8"))
            else:
                data = await request.json()
        except Exception:
            data = {}

        try:
            # Process incoming gossip
            self._process_gossip_response(data)
            self._record_gossip_metrics("received")

            # Prepare our response
            now = time.time()
            self._update_self_info()

            our_state = {
                "node_id": self.node_id,
                "timestamp": now,
                "version": int(now * 1000),
                "role": self.role.value if hasattr(self.role, "value") else str(self.role),
                "leader_id": self.leader_id,
                "leader_lease_expires": getattr(self, "leader_lease_expires", 0),
                "selfplay_jobs": getattr(self.self_info, "selfplay_jobs", 0),
                "training_jobs": getattr(self.self_info, "training_jobs", 0),
                "gpu_percent": getattr(self.self_info, "gpu_percent", 0),
                "cpu_percent": getattr(self.self_info, "cpu_percent", 0),
                "memory_percent": getattr(self.self_info, "memory_percent", 0),
                "disk_percent": getattr(self.self_info, "disk_percent", 0),
                "has_gpu": getattr(self.self_info, "has_gpu", False),
                "gpu_name": getattr(self.self_info, "gpu_name", ""),
                "voter_quorum_ok": self._has_voter_quorum(),
            }

            # Include manifest summary
            local_manifest = getattr(self, "local_data_manifest", None)
            if local_manifest:
                our_state["manifest_summary"] = {
                    "total_files": getattr(local_manifest, "total_files", 0),
                    "selfplay_games": getattr(local_manifest, "selfplay_games", 0),
                    "collected_at": getattr(local_manifest, "collected_at", 0),
                }

            # Get known states to propagate
            known_states = self._get_gossip_known_states()

            # Include peer manifests for P2P sync
            peer_manifests = {}
            local_manifest = getattr(self, "local_data_manifest", None)
            if local_manifest and hasattr(local_manifest, "to_dict"):
                peer_manifests[self.node_id] = local_manifest.to_dict()

            response_data = {
                "sender_state": our_state,
                "known_states": known_states,
                "peer_manifests": peer_manifests,
            }

            # GOSSIP COMPRESSION: Send compressed response if client accepts it
            # Always compress responses for efficiency
            response_json = json.dumps(response_data).encode("utf-8")
            compressed_response = gzip.compress(response_json, compresslevel=6)
            return web.Response(
                body=compressed_response,
                content_type="application/json",
                headers={"Content-Encoding": "gzip"},
            )

        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_gossip_anti_entropy(self, request: web.Request) -> web.Response:
        """POST /gossip/anti-entropy - Full state exchange for consistency repair.

        ANTI-ENTROPY: Unlike regular gossip (which shares recent state only),
        this endpoint exchanges ALL known states to ensure eventual consistency.
        Used periodically to catch any missed updates from network issues.

        Request body:
        {
            "anti_entropy": true,
            "sender": "node-id",
            "timestamp": <float>,
            "all_known_states": { node_id -> state dict }
        }

        Response: Same format with our full state knowledge.
        """
        try:
            if self.auth_token and not self._is_request_authorized(request):
                return web.json_response({"error": "unauthorized"}, status=401)

            data = await request.json()
        except Exception:
            data = {}

        try:
            self._record_gossip_metrics("received")

            # Initialize gossip state storage if needed
            if not hasattr(self, "_gossip_peer_states"):
                self._gossip_peer_states = {}

            # Process ALL states from peer
            peer_states = data.get("all_known_states", {})
            updates = 0
            for node_id, state in peer_states.items():
                if node_id == self.node_id:
                    continue
                existing = self._gossip_peer_states.get(node_id, {})
                if state.get("version", 0) > existing.get("version", 0):
                    self._gossip_peer_states[node_id] = state
                    updates += 1
                    self._record_gossip_metrics("update", node_id)

            if updates > 0:
                self._record_gossip_metrics("anti_entropy")

            # Prepare our full state response
            now = time.time()
            self._update_self_info()

            all_known_states = {}

            # Include all our known peer states
            for node_id, state in self._gossip_peer_states.items():
                all_known_states[node_id] = state

            # Include our own state
            all_known_states[self.node_id] = {
                "node_id": self.node_id,
                "timestamp": now,
                "version": int(now * 1000),
                "role": self.role.value if hasattr(self.role, "value") else str(self.role),
                "leader_id": self.leader_id,
                "selfplay_jobs": getattr(self.self_info, "selfplay_jobs", 0),
                "training_jobs": getattr(self.self_info, "training_jobs", 0),
                "gpu_percent": getattr(self.self_info, "gpu_percent", 0),
                "cpu_percent": getattr(self.self_info, "cpu_percent", 0),
                "memory_percent": getattr(self.self_info, "memory_percent", 0),
                "disk_percent": getattr(self.self_info, "disk_percent", 0),
            }

            return web.json_response({
                "anti_entropy": True,
                "sender": self.node_id,
                "timestamp": now,
                "all_known_states": all_known_states,
                "updates_applied": updates,
            })

        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_register(self, request: web.Request) -> web.Response:
        """POST /register - Node self-registration for dynamic IP updates.

        Nodes call this endpoint to announce their current IP address.
        Useful when Vast.ai instances restart and get new IPs.

        Request body:
        {
            "node_id": "vast-5090-quad",
            "host": "211.72.13.202",
            "port": 45875,
            "vast_instance_id": "28654132"  // optional
        }
        """
        if not HAS_DYNAMIC_REGISTRY:
            return web.json_response({
                "error": "Dynamic registry not available"
            }, status=501)

        try:
            data = await request.json()
            node_id = data.get("node_id")
            host = data.get("host")
            port = data.get("port", 22)
            vast_instance_id = data.get("vast_instance_id")
            tailscale_ip = data.get("tailscale_ip")

            if not node_id or not host:
                return web.json_response({
                    "error": "Missing required fields: node_id, host"
                }, status=400)

            registry = get_registry()
            success = registry.register_node(node_id, host, port, vast_instance_id, tailscale_ip=tailscale_ip)

            if success:
                logger.info(f"Node registered: {node_id} at {host}:{port}")
                return web.json_response({
                    "success": True,
                    "node_id": node_id,
                    "registered_host": host,
                    "registered_port": port,
                })
            else:
                return web.json_response({
                    "error": "Registration failed"
                }, status=500)

        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_registry_status(self, request: web.Request) -> web.Response:
        """GET /registry/status - Get dynamic registry status for all nodes.

        Returns current state of all nodes including:
        - Effective IP addresses (dynamic if registered)
        - Health state (online/degraded/offline)
        - Failure counters
        """
        if not HAS_DYNAMIC_REGISTRY:
            return web.json_response({
                "error": "Dynamic registry not available"
            }, status=501)

        try:
            registry = get_registry()
            nodes_status = registry.get_all_nodes_status()
            online_nodes = registry.get_online_nodes()

            return web.json_response({
                "total_nodes": len(nodes_status),
                "online_nodes": len(online_nodes),
                "online_node_ids": online_nodes,
                "nodes": nodes_status,
            })

        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_registry_update_vast(self, request: web.Request) -> web.Response:
        """POST /registry/update_vast - Refresh Vast instance IPs in the dynamic registry.

        Uses VAST_API_KEY when available, otherwise attempts the `vastai` CLI.
        """
        if not HAS_DYNAMIC_REGISTRY:
            return web.json_response({
                "error": "Dynamic registry not available"
            }, status=501)

        try:
            registry = get_registry()
            updated = await registry.update_vast_ips()

            return web.json_response({
                "success": True,
                "nodes_updated": updated,
            })

        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_registry_update_aws(self, request: web.Request) -> web.Response:
        """POST /registry/update_aws - Refresh AWS instance IPs in the dynamic registry.

        Uses the `aws` CLI and requires nodes to define `aws_instance_id` in
        distributed_hosts.yaml properties.
        """
        if not HAS_DYNAMIC_REGISTRY:
            return web.json_response({"error": "Dynamic registry not available"}, status=501)

        try:
            registry = get_registry()
            updated = await registry.update_aws_ips()
            return web.json_response({"success": True, "nodes_updated": updated})
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_registry_update_tailscale(self, request: web.Request) -> web.Response:
        """POST /registry/update_tailscale - Discover Tailscale IPs in the dynamic registry.

        Uses `tailscale status --json` when available. No-op if `tailscale` is
        not installed or the node is not part of a Tailscale network.
        """
        if not HAS_DYNAMIC_REGISTRY:
            return web.json_response({"error": "Dynamic registry not available"}, status=501)

        try:
            registry = get_registry()
            updated = await registry.update_tailscale_ips()
            return web.json_response({"success": True, "nodes_updated": updated})
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_registry_save_yaml(self, request: web.Request) -> web.Response:
        """POST /registry/save_yaml - Write dynamic IPs back to YAML config.

        Creates a backup before modifying. Only updates hosts where
        dynamic IP differs from static IP.
        """
        if not HAS_DYNAMIC_REGISTRY:
            return web.json_response({
                "error": "Dynamic registry not available"
            }, status=501)

        try:
            registry = get_registry()
            updated = registry.update_yaml_config()

            return web.json_response({
                "success": True,
                "config_updated": updated,
            })

        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    # ============================================
    # Connectivity Diagnosis Handlers (SSH/HTTP fallback)
    # ============================================

    async def handle_connectivity_diagnose(self, request: web.Request) -> web.Response:
        """GET /connectivity/diagnose/{node_id} - Diagnose connectivity to a specific node.

        Probes HTTP, Tailscale, and SSH transports and returns latency/reachability
        for each. Helps identify the best transport for communicating with a node.
        """
        node_id = request.match_info.get("node_id", "")
        if not node_id:
            return web.json_response({"error": "node_id required"}, status=400)

        # Find the node's address
        with self.peers_lock:
            peer = self.peers.get(node_id)

        if not peer:
            return web.json_response({
                "error": f"Node {node_id} not found in peers",
                "known_peers": list(self.peers.keys()),
            }, status=404)

        if not HAS_HYBRID_TRANSPORT or not self.hybrid_transport:
            # Fallback: just check if we can reach the node via HTTP
            try:
                info = await self._send_heartbeat_to_peer(peer.host, peer.port)
                return web.json_response({
                    "node_id": node_id,
                    "http_reachable": info is not None,
                    "hybrid_transport_available": False,
                })
            except Exception as e:
                return web.json_response({
                    "node_id": node_id,
                    "http_reachable": False,
                    "error": str(e),
                    "hybrid_transport_available": False,
                })

        try:
            diagnosis = await diagnose_node_connectivity(node_id, peer.host, peer.port)
            return web.json_response(diagnosis)
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_transport_stats(self, request: web.Request) -> web.Response:
        """GET /connectivity/transport_stats - Get transport statistics for all nodes.

        Returns per-node transport preferences and success rates.
        """
        if not HAS_HYBRID_TRANSPORT or not self.hybrid_transport:
            return web.json_response({
                "available": False,
                "message": "Hybrid transport not available",
            })

        try:
            stats = self.hybrid_transport.get_transport_stats()
            return web.json_response({
                "available": True,
                "node_count": len(stats),
                "nodes": stats,
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_probe_vast_nodes(self, request: web.Request) -> web.Response:
        """POST /connectivity/probe_vast - Probe all Vast nodes via SSH.

        Tests SSH connectivity to all vast-* nodes in the registry.
        Useful for diagnosing networking issues with Vast instances.
        """
        if not HAS_HYBRID_TRANSPORT:
            return web.json_response({
                "error": "Hybrid transport not available"
            }, status=501)

        try:
            results = await probe_vast_nodes_via_ssh()
            reachable = sum(1 for r, _ in results.values() if r)

            return web.json_response({
                "total_nodes": len(results),
                "reachable": reachable,
                "unreachable": len(results) - reachable,
                "nodes": {
                    node_id: {"reachable": r, "message": msg}
                    for node_id, (r, msg) in results.items()
                },
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    # ============================================
    # Gauntlet Evaluation Handlers
    # ============================================

    async def handle_gauntlet_execute(self, request: web.Request) -> web.Response:
        """POST /gauntlet/execute - Execute a batch of gauntlet games.

        Workers receive batches of games to play and return results.
        This endpoint allows distributed gauntlet evaluation across the cluster.

        Request body:
            {
                "run_id": "abc123",
                "config_key": "square8_2p",
                "tasks": [
                    {"task_id": "...", "model_id": "...", "baseline_id": "...", "game_num": 0},
                    ...
                ]
            }

        Response:
            {
                "success": true,
                "results": [
                    {"task_id": "...", "model_id": "...", "model_won": true, ...},
                    ...
                ]
            }
        """
        try:
            data = await request.json()
        except Exception:
            return web.json_response({"error": "Invalid JSON"}, status=400)

        run_id = data.get("run_id", "unknown")
        config_key = data.get("config_key", "")
        tasks = data.get("tasks", [])

        if not config_key or not tasks:
            return web.json_response({
                "error": "config_key and tasks required"
            }, status=400)

        logger.info(f"Gauntlet: Executing {len(tasks)} games for {config_key} (run {run_id})")

        try:
            results = await self._execute_gauntlet_batch(config_key, tasks)

            return web.json_response({
                "success": True,
                "node_id": self.node_id,
                "run_id": run_id,
                "games_completed": len(results),
                "results": results,
            })

        except Exception as e:
            logger.info(f"Gauntlet execution error: {e}")
            return web.json_response({
                "success": False,
                "error": str(e),
            }, status=500)

    async def _execute_gauntlet_batch(
        self,
        config_key: str,
        tasks: list[dict[str, Any]],
    ) -> list[dict[str, Any]]:
        """Execute a batch of gauntlet games.

        Args:
            config_key: Config like "square8_2p"
            tasks: List of task dicts

        Returns:
            List of result dicts
        """
        results = []

        # Parse config
        parts = config_key.split("_")
        board_type = parts[0]
        num_players = int(parts[1].replace("p", ""))

        # Import game execution modules
        try:
            pass
        except ImportError as e:
            logger.info(f"Gauntlet: Import error: {e}")
            # Return simulated results if modules not available
            for task in tasks:
                results.append({
                    "task_id": task["task_id"],
                    "model_id": task["model_id"],
                    "baseline_id": task["baseline_id"],
                    "model_won": False,
                    "baseline_won": True,
                    "draw": False,
                    "game_length": 0,
                    "duration_sec": 0.0,
                    "error": "Game modules not available",
                })
            return results

        # Load model paths
        model_dir = Path(self.ringrift_path) / "ai-service" / "data" / "models"

        # Execute games concurrently in small batches
        batch_size = 4  # Run 4 games at a time
        for batch_start in range(0, len(tasks), batch_size):
            batch = tasks[batch_start:batch_start + batch_size]

            batch_coros = [
                self._execute_single_gauntlet_game(
                    task, board_type, num_players, model_dir
                )
                for task in batch
            ]

            batch_results = await asyncio.gather(*batch_coros, return_exceptions=True)

            for task, result in zip(batch, batch_results, strict=False):
                if isinstance(result, Exception):
                    results.append({
                        "task_id": task["task_id"],
                        "model_id": task["model_id"],
                        "baseline_id": task["baseline_id"],
                        "model_won": False,
                        "baseline_won": False,
                        "draw": True,
                        "game_length": 0,
                        "duration_sec": 0.0,
                        "error": str(result),
                    })
                else:
                    results.append(result)

            # Progress update
            logger.info(f"Gauntlet: Completed {len(results)}/{len(tasks)} games")

        return results

    async def _execute_single_gauntlet_game(
        self,
        task: dict[str, Any],
        board_type: str,
        num_players: int,
        model_dir: Path,
    ) -> dict[str, Any]:
        """Execute a single gauntlet game.

        Args:
            task: Task dict with model_id, baseline_id, etc.
            board_type: Board type (square8, etc.)
            num_players: Number of players
            model_dir: Path to model files

        Returns:
            Result dict
        """
        start_time = time.time()
        task_id = task["task_id"]
        model_id = task["model_id"]
        baseline_id = task["baseline_id"]

        try:
            # Run game in thread pool to avoid blocking
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None,
                self._run_gauntlet_game_sync,
                task_id, model_id, baseline_id,
                board_type, num_players, model_dir,
            )
            result["duration_sec"] = time.time() - start_time
            return result

        except Exception as e:
            return {
                "task_id": task_id,
                "model_id": model_id,
                "baseline_id": baseline_id,
                "model_won": False,
                "baseline_won": False,
                "draw": True,
                "game_length": 0,
                "duration_sec": time.time() - start_time,
                "error": str(e),
            }

    def _run_gauntlet_game_sync(
        self,
        task_id: str,
        model_id: str,
        baseline_id: str,
        board_type: str,
        num_players: int,
        model_dir: Path,
    ) -> dict[str, Any]:
        """Synchronously run a single gauntlet game.

        This runs in a thread pool executor. Uses GameExecutor for consistent
        game execution across all gauntlet modes.
        """
        try:
            from app.execution.game_executor import GameExecutor

            # Map model IDs to player configs
            player_configs = []

            # Model agent (player 0)
            # Use mcts_25 for fast gauntlet evaluation (25 simulations = ~0.15s/move)
            if model_id == "random_ai":
                player_configs.append({"ai_type": "random", "difficulty": 1})
            else:
                model_path = model_dir / f"{model_id}.pth"
                if model_path.exists():
                    # Use MCTS with neural guidance - 25 sims for speed
                    player_configs.append({
                        "ai_type": "mcts_25",
                        "difficulty": 5,
                        "nn_model_id": model_id,
                    })
                else:
                    # Model file not found, use MCTS fallback
                    player_configs.append({"ai_type": "mcts_25", "difficulty": 4})

            # Baseline agent (player 1)
            if baseline_id == "random_ai":
                player_configs.append({"ai_type": "random", "difficulty": 1})
            else:
                baseline_path = model_dir / f"{baseline_id}.pth"
                if baseline_path.exists():
                    player_configs.append({
                        "ai_type": "mcts_25",
                        "difficulty": 5,
                        "nn_model_id": baseline_id,
                    })
                else:
                    player_configs.append({"ai_type": "mcts_25", "difficulty": 4})

            # Add random players for 3p/4p games
            while len(player_configs) < num_players:
                player_configs.append({"ai_type": "random", "difficulty": 1})

            # Run game using GameExecutor
            max_moves = 2000 if "19" in board_type else 500
            executor = GameExecutor(board_type=board_type, num_players=num_players)
            result = executor.run_game(
                player_configs=player_configs,
                max_moves=max_moves,
            )

            game_length = result.move_count

            # Convert executor result to gauntlet result format
            # GameExecutor uses 1-indexed winner (1 = player 1 = model)
            if result.winner is None or result.outcome.value == "draw":
                return {
                    "task_id": task_id,
                    "model_id": model_id,
                    "baseline_id": baseline_id,
                    "model_won": False,
                    "baseline_won": False,
                    "draw": True,
                    "game_length": game_length,
                    "duration_sec": 0.0,
                }
            elif result.winner == 1:  # Player 1 (model) won
                return {
                    "task_id": task_id,
                    "model_id": model_id,
                    "baseline_id": baseline_id,
                    "model_won": True,
                    "baseline_won": False,
                    "draw": False,
                    "game_length": game_length,
                    "duration_sec": 0.0,
                }
            else:  # Player 2+ (baseline or other) won
                return {
                    "task_id": task_id,
                    "model_id": model_id,
                    "baseline_id": baseline_id,
                    "model_won": False,
                    "baseline_won": True,
                    "draw": False,
                    "game_length": game_length,
                    "duration_sec": 0.0,
                }

        except Exception as e:
            return {
                "task_id": task_id,
                "model_id": model_id,
                "baseline_id": baseline_id,
                "model_won": False,
                "baseline_won": False,
                "draw": True,
                "game_length": 0,
                "duration_sec": 0.0,
                "error": str(e),
            }

    async def handle_gauntlet_status(self, request: web.Request) -> web.Response:
        """GET /gauntlet/status - Get current gauntlet execution status.

        Returns information about this node's gauntlet capabilities.
        """
        return web.json_response({
            "node_id": self.node_id,
            "available": True,
            "has_gpu": self.self_info.has_gpu if hasattr(self.self_info, "has_gpu") else False,
            "gpu_name": self.self_info.gpu_name if hasattr(self.self_info, "gpu_name") else "",
            "cpu_count": self.self_info.cpu_count if hasattr(self.self_info, "cpu_count") else 0,
        })

    async def handle_gauntlet_quick_eval(self, request: web.Request) -> web.Response:
        """POST /gauntlet/quick-eval - Run quick gauntlet evaluation.

        This endpoint is called by GPU nodes to offload gauntlet work to
        CPU-rich nodes (like Vast instances). Returns win rate and pass status.
        """
        try:
            data = await request.json()
            config_key = data.get("config_key")
            model_id = data.get("model_id")
            baseline_id = data.get("baseline_id")
            games_per_side = data.get("games_per_side", 4)

            if not all([config_key, model_id, baseline_id]):
                return web.json_response(
                    {"error": "Missing required fields"},
                    status=400
                )

            # Parse config
            parts = config_key.rsplit("_", 1)
            if len(parts) != 2:
                return web.json_response({"error": "Invalid config_key"}, status=400)
            board_type = parts[0]
            num_players = int(parts[1].rstrip("p"))

            model_dir = Path(self.ringrift_path) / "ai-service" / "models"

            # Run games: model vs baseline from both sides
            wins = 0
            total_games = 0
            loop = asyncio.get_event_loop()

            for game_num in range(games_per_side * 2):
                try:
                    if game_num < games_per_side:
                        # Model plays first
                        result = await loop.run_in_executor(
                            None,
                            self._run_gauntlet_game_sync,
                            f"quick_eval_{game_num}", model_id, baseline_id,
                            board_type, num_players, model_dir
                        )
                        if result.get("model_won"):
                            wins += 1
                    else:
                        # Baseline plays first
                        result = await loop.run_in_executor(
                            None,
                            self._run_gauntlet_game_sync,
                            f"quick_eval_{game_num}", baseline_id, model_id,
                            board_type, num_players, model_dir
                        )
                        if result.get("baseline_won"):
                            wins += 1
                    total_games += 1
                except Exception as e:
                    logger.info(f"Quick eval game {game_num} error: {e}")
                    total_games += 1

            win_rate = wins / total_games if total_games > 0 else 0
            passed = win_rate >= 0.50

            return web.json_response({
                "success": True,
                "node_id": self.node_id,
                "model_id": model_id,
                "baseline_id": baseline_id,
                "wins": wins,
                "total_games": total_games,
                "win_rate": win_rate,
                "passed": passed,
            })

        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_git_status(self, request: web.Request) -> web.Response:
        """Get git status for this node.

        Returns local/remote commit info and whether updates are available.
        """
        try:
            local_commit = self._get_local_git_commit()
            local_branch = self._get_local_git_branch()
            has_local_changes = self._check_local_changes()

            # Check for remote updates (this does a git fetch)
            has_updates, _, remote_commit = self._check_for_updates()
            commits_behind = 0
            if has_updates and local_commit and remote_commit:
                commits_behind = self._get_commits_behind(local_commit, remote_commit)

            return web.json_response({
                "local_commit": local_commit[:8] if local_commit else None,
                "local_commit_full": local_commit,
                "local_branch": local_branch,
                "remote_commit": remote_commit[:8] if remote_commit else None,
                "remote_commit_full": remote_commit,
                "has_updates": has_updates,
                "commits_behind": commits_behind,
                "has_local_changes": has_local_changes,
                "auto_update_enabled": AUTO_UPDATE_ENABLED,
                "ringrift_path": self.ringrift_path,
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_git_update(self, request: web.Request) -> web.Response:
        """Manually trigger a git update on this node.

        This will stop jobs, pull updates, and restart the orchestrator.
        """
        try:
            # Check for updates first
            has_updates, local_commit, remote_commit = self._check_for_updates()

            if not has_updates:
                return web.json_response({
                    "success": True,
                    "message": "Already up to date",
                    "local_commit": local_commit[:8] if local_commit else None,
                })

            # Perform the update
            success, message = await self._perform_git_update()

            if success:
                # Schedule restart
                asyncio.create_task(self._restart_orchestrator())
                return web.json_response({
                    "success": True,
                    "message": "Update successful, restarting...",
                    "old_commit": local_commit[:8] if local_commit else None,
                    "new_commit": remote_commit[:8] if remote_commit else None,
                })
            else:
                return web.json_response({
                    "success": False,
                    "message": message,
                }, status=400)

        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_admin_restart(self, request: web.Request) -> web.Response:
        """Force restart the orchestrator process.

        Useful after code updates when /git/update shows "already up to date"
        but the running process hasn't picked up the changes.
        """
        try:
            logger.info("Admin restart requested via API")
            # Schedule restart (gives time to return response)
            asyncio.create_task(self._restart_orchestrator())
            return web.json_response({
                "success": True,
                "message": "Restart scheduled, process will restart in 2 seconds",
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    # ============================================
    # Phase 2: Distributed Data Manifest Handlers
    # ============================================

    async def handle_data_manifest(self, request: web.Request) -> web.Response:
        """Return this node's local data manifest.

        Used by leader to collect data inventory from all nodes.
        """
        try:
            local_manifest = await asyncio.to_thread(self._collect_local_data_manifest)
            with self.manifest_lock:
                self.local_data_manifest = local_manifest

            return web.json_response({
                "node_id": self.node_id,
                "manifest": local_manifest.to_dict(),
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_cluster_data_manifest(self, request: web.Request) -> web.Response:
        """Leader-only: Return cluster-wide data manifest.

        Aggregates data manifests from all nodes to show:
        - Total files across cluster
        - Total selfplay games
        - Files missing from specific nodes (for sync planning)
        """
        try:
            if self.role != NodeRole.LEADER:
                return web.json_response({
                    "error": "Not leader",
                    "leader_id": self.leader_id,
                }, status=400)

            refresh_raw = str(request.query.get("refresh", "") or "").strip().lower()
            refresh = refresh_raw in {"1", "true", "yes", "y"}

            # Default to returning the cached manifest to keep this endpoint
            # fast and usable by daemons with tight timeouts.
            if not refresh:
                with self.manifest_lock:
                    cached = self.cluster_data_manifest
                if cached:
                    return web.json_response({
                        "cluster_manifest": cached.to_dict(),
                        "cached": True,
                    })
                # Manifest collection loop runs shortly after startup; callers
                # can retry or pass ?refresh=1 to force.
                return web.json_response({
                    "cluster_manifest": None,
                    "cached": True,
                    "error": "manifest_not_ready",
                })

            # Forced refresh: collect and update cache.
            cluster_manifest = await self._collect_cluster_manifest()
            with self.manifest_lock:
                self.cluster_data_manifest = cluster_manifest

            return web.json_response({
                "cluster_manifest": cluster_manifest.to_dict(),
                "cached": False,
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_refresh_manifest(self, request: web.Request) -> web.Response:
        """Force refresh of local data manifest."""
        try:
            local_manifest = await asyncio.to_thread(self._collect_local_data_manifest)
            with self.manifest_lock:
                self.local_data_manifest = local_manifest

            return web.json_response({
                "success": True,
                "node_id": self.node_id,
                "total_files": local_manifest.total_files,
                "total_size_bytes": local_manifest.total_size_bytes,
                "selfplay_games": local_manifest.selfplay_games,
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    # ============================================
    # Distributed CMA-ES Handlers
    # ============================================

    async def handle_cmaes_start(self, request: web.Request) -> web.Response:
        """Start a distributed CMA-ES optimization job.

        Only the leader can start distributed CMA-ES jobs.
        Request body:
        {
            "board_type": "square8",
            "num_players": 2,
            "generations": 100,
            "population_size": 20,
            "games_per_eval": 50
        }
        """
        try:
            if self.role != NodeRole.LEADER:
                return web.json_response({
                    "error": "Only the leader can start distributed CMA-ES",
                    "leader_id": self.leader_id,
                }, status=403)

            data = await request.json()
            job_id = f"cmaes_{uuid.uuid4().hex[:8]}"

            # Create state for this job
            state = DistributedCMAESState(
                job_id=job_id,
                board_type=data.get("board_type", "square8"),
                num_players=data.get("num_players", 2),
                generations=data.get("generations", 100),
                population_size=data.get("population_size", 20),
                games_per_eval=data.get("games_per_eval", 50),
                status="starting",
                started_at=time.time(),
                last_update=time.time(),
            )

            # Find available GPU workers
            with self.peers_lock:
                gpu_nodes = [
                    p.node_id for p in self.peers.values()
                    if p.is_healthy() and p.has_gpu
                ]
            state.worker_nodes = gpu_nodes

            if not state.worker_nodes:
                return web.json_response({
                    "error": "No GPU workers available for CMA-ES",
                }, status=503)

            self.distributed_cmaes_state[job_id] = state
            state.status = "running"

            logger.info(f"Started distributed CMA-ES job {job_id} with {len(state.worker_nodes)} workers")

            # Launch coordinator task
            asyncio.create_task(self._run_distributed_cmaes(job_id))

            return web.json_response({
                "success": True,
                "job_id": job_id,
                "workers": state.worker_nodes,
                "config": {
                    "board_type": state.board_type,
                    "num_players": state.num_players,
                    "generations": state.generations,
                    "population_size": state.population_size,
                    "games_per_eval": state.games_per_eval,
                },
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_cmaes_evaluate(self, request: web.Request) -> web.Response:
        """Request evaluation of weights from workers.

        Called by the coordinator to distribute weight evaluation tasks.
        Workers respond via /cmaes/result endpoint.
        """
        try:
            data = await request.json()
            job_id = data.get("job_id")
            weights = data.get("weights", {})
            generation = data.get("generation", 0)
            individual_idx = data.get("individual_idx", 0)

            if not job_id:
                return web.json_response({"error": "job_id required"}, status=400)

            # Extract evaluation parameters from request
            games_per_eval = data.get("games_per_eval", 5)
            board_type = data.get("board_type", "square8")
            num_players = data.get("num_players", 2)

            # Store evaluation task for local processing
            logger.info(f"Received CMA-ES evaluation request: job={job_id}, gen={generation}, idx={individual_idx}")

            # Start evaluation in background
            asyncio.create_task(self._evaluate_cmaes_weights(
                job_id, weights, generation, individual_idx,
                games_per_eval=games_per_eval, board_type=board_type, num_players=num_players
            ))

            return web.json_response({
                "success": True,
                "job_id": job_id,
                "status": "evaluation_started",
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_cmaes_status(self, request: web.Request) -> web.Response:
        """Get status of distributed CMA-ES jobs."""
        try:
            job_id = request.query.get("job_id")

            if job_id:
                if job_id not in self.distributed_cmaes_state:
                    return web.json_response({"error": "Job not found"}, status=404)
                state = self.distributed_cmaes_state[job_id]
                return web.json_response(state.to_dict())

            # Return all jobs
            return web.json_response({
                job_id: state.to_dict()
                for job_id, state in self.distributed_cmaes_state.items()
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_cmaes_result(self, request: web.Request) -> web.Response:
        """Receive evaluation result from a worker."""
        try:
            data = await request.json()
            job_id = data.get("job_id")
            generation = data.get("generation", 0)
            individual_idx = data.get("individual_idx", 0)
            fitness = data.get("fitness", 0.0)
            worker_id = data.get("worker_id", "unknown")

            if job_id not in self.distributed_cmaes_state:
                return web.json_response({"error": "Job not found"}, status=404)

            logger.info(f"CMA-ES result: job={job_id}, gen={generation}, idx={individual_idx}, fitness={fitness:.4f} from {worker_id}")

            # Store result - the coordinator loop will process it
            state = self.distributed_cmaes_state[job_id]
            state.last_update = time.time()

            # LEARNED LESSONS - Store result keyed by generation and index for coordinator to collect
            result_key = f"{generation}_{individual_idx}"
            state.pending_results[result_key] = fitness

            # Update best if applicable
            if fitness > state.best_fitness:
                state.best_fitness = fitness
                state.best_weights = data.get("weights", {})

            return web.json_response({
                "success": True,
                "job_id": job_id,
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def _run_distributed_cmaes(self, job_id: str):
        """Main coordinator loop for distributed CMA-ES.

        Integrates with CMA-ES algorithm to optimize heuristic weights.
        Distributes candidate evaluation across GPU workers in the cluster.
        """
        try:
            state = self.distributed_cmaes_state.get(job_id)
            if not state:
                return

            logger.info(f"CMA-ES coordinator started for job {job_id}")
            logger.info(f"Config: {state.generations} gens, pop={state.population_size}, {state.games_per_eval} games/eval")

            # Try to import CMA-ES library
            try:
                import cma
                import numpy as np
            except ImportError:
                logger.info("CMA-ES requires: pip install cma numpy")
                state.status = "error: cma not installed"
                return

            # Default heuristic weights to optimize
            weight_names = [
                "material_weight", "ring_count_weight", "stack_height_weight",
                "center_control_weight", "territory_weight", "mobility_weight",
                "line_potential_weight", "defensive_weight",
            ]
            default_weights = {
                "material_weight": 1.0, "ring_count_weight": 0.5,
                "stack_height_weight": 0.3, "center_control_weight": 0.4,
                "territory_weight": 0.8, "mobility_weight": 0.2,
                "line_potential_weight": 0.6, "defensive_weight": 0.3,
            }

            # Convert to vector for CMA-ES
            x0 = np.array([default_weights[n] for n in weight_names])

            # Initialize CMA-ES
            es = cma.CMAEvolutionStrategy(x0, 0.5, {
                'popsize': state.population_size,
                'maxiter': state.generations,
                'bounds': [0, 2],  # Weights between 0 and 2
            })

            state.current_generation = 0

            while not es.stop() and state.status == "running":
                state.current_generation += 1
                state.last_update = time.time()

                # Get candidate solutions
                solutions = es.ask()

                # Distribute evaluations across workers
                fitness_results = {}
                pending_evals = {}

                for idx, sol in enumerate(solutions):
                    weights = {name: float(sol[i]) for i, name in enumerate(weight_names)}

                    # Round-robin assign to workers
                    if state.worker_nodes:
                        worker_idx = idx % len(state.worker_nodes)
                        worker_id = state.worker_nodes[worker_idx]

                        # Send evaluation request to worker
                        eval_id = f"{job_id}_gen{state.current_generation}_idx{idx}"
                        pending_evals[eval_id] = idx

                        try:
                            with self.peers_lock:
                                worker = self.peers.get(worker_id)
                            if worker:
                                timeout = ClientTimeout(total=300)
                                async with get_client_session(timeout) as session:
                                    url = self._url_for_peer(worker, "/cmaes/evaluate")
                                    await session.post(url, json={
                                        "job_id": job_id,
                                        "weights": weights,
                                        "generation": state.current_generation,
                                        "individual_idx": idx,
                                        "games_per_eval": state.games_per_eval,
                                        "board_type": state.board_type,
                                        "num_players": state.num_players,
                                    }, headers=self._auth_headers())
                        except Exception as e:
                            logger.error(f"Failed to send eval to {worker_id}: {e}")
                            # Fall back to local evaluation
                            fitness = await self._evaluate_cmaes_weights_local(
                                weights, state.games_per_eval, state.board_type, state.num_players
                            )
                            fitness_results[idx] = fitness

                # Wait for results with timeout
                wait_start = time.time()
                len(solutions) - len(fitness_results)
                while len(fitness_results) < len(solutions) and (time.time() - wait_start) < 300:
                    await asyncio.sleep(1)
                    state.last_update = time.time()

                    # Check for results that came in via /cmaes/result endpoint
                    # Results are stored in state.pending_results by handle_cmaes_result
                    for idx in range(len(solutions)):
                        if idx in fitness_results:
                            continue
                        result_key = f"{state.current_generation}_{idx}"
                        if result_key in state.pending_results:
                            fitness_results[idx] = state.pending_results[result_key]
                            del state.pending_results[result_key]  # Clean up

                    # Progress logging every 30 seconds
                    elapsed = time.time() - wait_start
                    if int(elapsed) % 30 == 0 and elapsed > 1:
                        received = len(fitness_results)
                        logger.info(f"Gen {state.current_generation}: {received}/{len(solutions)} results received ({elapsed:.0f}s elapsed)")

                # Fill in any missing results with default fitness
                fitnesses = []
                for idx in range(len(solutions)):
                    fitness = fitness_results.get(idx, 0.5)  # Default to 0.5 if no result
                    fitnesses.append(-fitness)  # CMA-ES minimizes, so negate

                # Update CMA-ES
                es.tell(solutions, fitnesses)

                # Track best
                best_idx = np.argmin(fitnesses)
                if -fitnesses[best_idx] > state.best_fitness:
                    state.best_fitness = -fitnesses[best_idx]
                    state.best_weights = {name: float(solutions[best_idx][i]) for i, name in enumerate(weight_names)}

                logger.info(f"Gen {state.current_generation}: best_fitness={state.best_fitness:.4f}")

            state.status = "completed"
            logger.info(f"CMA-ES job {job_id} completed: best_fitness={state.best_fitness:.4f}")
            logger.info(f"Best weights: {state.best_weights}")

            # Feed CMA-ES results back to improvement cycle manager
            if self.improvement_cycle_manager and state.best_weights:
                try:
                    agent_id = self.improvement_cycle_manager.handle_cmaes_complete(
                        state.board_type, state.num_players, state.best_weights
                    )
                    logger.info(f"CMA-ES weights registered as agent: {agent_id}")
                    self.diversity_metrics["cmaes_triggers"] += 1

                    # Save weights to file for future use
                    weights_file = Path(self.ringrift_path) / "ai-service" / "data" / "cmaes" / f"best_weights_{state.board_type}_{state.num_players}p.json"
                    weights_file.parent.mkdir(parents=True, exist_ok=True)
                    import json as json_mod
                    with open(weights_file, "w") as f:
                        json_mod.dump({
                            "weights": state.best_weights,
                            "fitness": state.best_fitness,
                            "job_id": job_id,
                            "generation": state.current_generation,
                            "timestamp": time.time(),
                        }, f, indent=2)
                    logger.info(f"Saved CMA-ES weights to {weights_file}")

                    # Propagate new weights to selfplay jobs
                    asyncio.create_task(self._propagate_cmaes_weights(
                        state.board_type, state.num_players, state.best_weights
                    ))
                except Exception as e:
                    logger.error(f"Failed to register CMA-ES weights: {e}")

        except Exception as e:
            import traceback
            logger.info(f"CMA-ES coordinator error: {e}")
            traceback.print_exc()
            if job_id in self.distributed_cmaes_state:
                self.distributed_cmaes_state[job_id].status = f"error: {e}"

    async def _evaluate_cmaes_weights_local(
        self, weights: dict, num_games: int, board_type: str, num_players: int
    ) -> float:
        """Evaluate weights locally by running selfplay games."""
        try:
            sem = getattr(self, "_cmaes_eval_semaphore", None)
            if sem is None:
                sem = asyncio.Semaphore(1)

            async with sem:
                # Run selfplay subprocess to evaluate weights
                import json as json_mod
                import tempfile

                with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                    json_mod.dump(weights, f)
                    weights_file = f.name

                ai_service_path = str(Path(self.ringrift_path) / "ai-service")
                cmd = [
                    sys.executable, "-c", f"""
import sys
sys.path.insert(0, '{ai_service_path}')
from app.game_engine import GameEngine
from app.ai.heuristic_ai import HeuristicAI
from app.models import AIConfig, BoardType, GameStatus
from app.training.generate_data import create_initial_state
import json

weights = json.load(open('{weights_file}'))
board_type = BoardType('{board_type}')
wins = 0
total = {num_games}

for i in range(total):
    state = create_initial_state(board_type, num_players={num_players})
    engine = GameEngine()

    # Candidate with custom weights vs baseline
    config_candidate = AIConfig(difficulty=5, randomness=0.1, think_time=500, custom_weights=weights)
    config_baseline = AIConfig(difficulty=5, randomness=0.1, think_time=500)

    ai_candidate = HeuristicAI(1, config_candidate)
    ai_baseline = HeuristicAI(2, config_baseline)

    move_count = 0
    while state.game_status == GameStatus.ACTIVE and move_count < 300:
        current_ai = ai_candidate if state.current_player == 1 else ai_baseline
        move = current_ai.select_move(state)
        if move is None:
            break
        state = engine.apply_move(state, move)
        move_count += 1

    if state.winner == 1:
        wins += 1
    elif state.winner is None:
        wins += 0.5  # Draw counts as half

print(wins / total)
"""
                ]

                proc = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                    env={**os.environ, "PYTHONPATH": ai_service_path},
                )
                stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=300)

                # Clean up temp file
                os.unlink(weights_file)

                if proc.returncode == 0:
                    return float(stdout.decode().strip())
                else:
                    logger.info(f"Local eval error: {stderr.decode()}")
                    return 0.5

        except Exception as e:
            logger.info(f"Local CMA-ES evaluation error: {e}")
            return 0.5

    async def _evaluate_cmaes_weights(
        self, job_id: str, weights: dict, generation: int, individual_idx: int,
        games_per_eval: int = 5, board_type: str = "square8", num_players: int = 2
    ):
        """Evaluate weights locally and report result to coordinator."""
        try:
            # Run local evaluation using passed parameters (workers don't have state)
            fitness = await self._evaluate_cmaes_weights_local(
                weights, games_per_eval, board_type, num_players
            )

            logger.info(f"Completed local CMA-ES evaluation: job={job_id}, gen={generation}, idx={individual_idx}, fitness={fitness:.4f}")

            # If we're not the coordinator, report result back
            if self.role != NodeRole.LEADER and self.leader_id:
                with self.peers_lock:
                    leader = self.peers.get(self.leader_id)
                if leader:
                    try:
                        timeout = ClientTimeout(total=30)
                        async with get_client_session(timeout) as session:
                            url = self._url_for_peer(leader, "/cmaes/result")
                            await session.post(url, json={
                                "job_id": job_id,
                                "generation": generation,
                                "individual_idx": individual_idx,
                                "fitness": fitness,
                                "weights": weights,
                                "worker_id": self.node_id,
                            }, headers=self._auth_headers())
                    except Exception as e:
                        logger.error(f"Failed to report CMA-ES result to leader: {e}")

        except Exception as e:
            logger.info(f"CMA-ES evaluation error: {e}")

    # ============================================
    # Distributed Tournament Handlers
    # ============================================

    async def handle_tournament_start(self, request: web.Request) -> web.Response:
        """Start or propose a distributed tournament.

        DISTRIBUTED TOURNAMENT SCHEDULING:
        - Leaders can start tournaments directly (immediate)
        - Non-leaders can propose tournaments (gossip-based consensus)

        Request body:
        {
            "board_type": "square8",
            "num_players": 2,
            "agent_ids": ["agent1", "agent2", "agent3"],
            "games_per_pairing": 2
        }
        """
        try:
            data = await request.json()

            # Non-leaders propose tournaments via gossip consensus
            if self.role != NodeRole.LEADER:
                agent_ids = data.get("agent_ids", [])
                if len(agent_ids) < 2:
                    return web.json_response({"error": "At least 2 agents required"}, status=400)

                proposal = self._propose_tournament(
                    board_type=data.get("board_type", "square8"),
                    num_players=data.get("num_players", 2),
                    agent_ids=agent_ids,
                    games_per_pairing=data.get("games_per_pairing", 2),
                )

                return web.json_response({
                    "success": True,
                    "mode": "proposal",
                    "proposal_id": proposal["proposal_id"],
                    "status": "Proposal created, awaiting gossip consensus",
                    "agents": agent_ids,
                })

            # Leader can start tournaments directly
            job_id = f"tournament_{uuid.uuid4().hex[:8]}"

            agent_ids = data.get("agent_ids", [])
            if len(agent_ids) < 2:
                return web.json_response({"error": "At least 2 agents required"}, status=400)

            # Create round-robin pairings
            pairings = []
            for i, a1 in enumerate(agent_ids):
                for a2 in agent_ids[i+1:]:
                    for game_num in range(data.get("games_per_pairing", 2)):
                        pairings.append({
                            "agent1": a1,
                            "agent2": a2,
                            "game_num": game_num,
                            "status": "pending",
                        })

            state = DistributedTournamentState(
                job_id=job_id,
                board_type=data.get("board_type", "square8"),
                num_players=data.get("num_players", 2),
                agent_ids=agent_ids,
                games_per_pairing=data.get("games_per_pairing", 2),
                total_matches=len(pairings),
                pending_matches=pairings,
                status="running",
                started_at=time.time(),
                last_update=time.time(),
            )

            # Find available workers
            with self.peers_lock:
                workers = [p.node_id for p in self.peers.values() if p.is_healthy()]
            state.worker_nodes = workers

            if not state.worker_nodes:
                return web.json_response({"error": "No workers available"}, status=503)

            self.distributed_tournament_state[job_id] = state

            logger.info(f"Started tournament {job_id}: {len(agent_ids)} agents, {len(pairings)} matches, {len(workers)} workers")

            # Launch coordinator task
            asyncio.create_task(self._run_distributed_tournament(job_id))

            return web.json_response({
                "success": True,
                "job_id": job_id,
                "agents": agent_ids,
                "total_matches": len(pairings),
                "workers": workers,
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_tournament_match(self, request: web.Request) -> web.Response:
        """Request a tournament match to be played by a worker."""
        try:
            data = await request.json()
            job_id = data.get("job_id")
            match_info = data.get("match")

            if not job_id or not match_info:
                return web.json_response({"error": "job_id and match required"}, status=400)

            logger.info(f"Received tournament match request: {match_info}")

            # Start match in background
            asyncio.create_task(self._play_tournament_match(job_id, match_info))

            return web.json_response({
                "success": True,
                "job_id": job_id,
                "status": "match_started",
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_tournament_status(self, request: web.Request) -> web.Response:
        """Get status of distributed tournaments."""
        try:
            job_id = request.query.get("job_id")

            if job_id:
                if job_id not in self.distributed_tournament_state:
                    return web.json_response({"error": "Tournament not found"}, status=404)
                state = self.distributed_tournament_state[job_id]
                return web.json_response(state.to_dict())

            return web.json_response({
                job_id: state.to_dict()
                for job_id, state in self.distributed_tournament_state.items()
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_tournament_result(self, request: web.Request) -> web.Response:
        """Receive match result from a worker."""
        try:
            data = await request.json()
            job_id = data.get("job_id")
            match_result = data.get("result", {})
            worker_id = data.get("worker_id", "unknown")

            if job_id not in self.distributed_tournament_state:
                return web.json_response({"error": "Tournament not found"}, status=404)

            state = self.distributed_tournament_state[job_id]
            state.results.append(match_result)
            state.completed_matches += 1
            state.last_update = time.time()

            logger.info(f"Tournament result: {state.completed_matches}/{state.total_matches} matches from {worker_id}")

            return web.json_response({
                "success": True,
                "job_id": job_id,
                "completed": state.completed_matches,
                "total": state.total_matches,
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_play_elo_match(self, request: web.Request) -> web.Response:
        """Play a single Elo calibration match between AI configurations.

        This endpoint supports playing games between different AI types
        (random, heuristic, minimax, mcts, policy_only, gumbel_mcts, descent)
        for Elo calibration purposes. Supports 2-4 player games.

        Request body:
            match_id: Unique match identifier
            agent_a: Agent A identifier (e.g., "random", "mcts_neural")
            agent_b: Agent B identifier
            agent_a_config: Full AI configuration for agent A
            agent_b_config: Full AI configuration for agent B
            agents: List of agent identifiers for multiplayer (optional)
            agent_configs: List of AI configs for multiplayer (optional)
            board_type: Board type (default: square8)
            num_players: Number of players (default: 2)

        Returns:
            success: True if match completed
            winner: "agent_a", "agent_b", "agent_c", "agent_d", or "draw"
            game_length: Number of moves
            duration_sec: Game duration in seconds
        """
        try:
            logger.info("Tournament endpoint called, parsing request...")
            data = await request.json()
            logger.info(f"Request parsed: {data}")

            match_id = data.get("match_id", str(uuid.uuid4())[:8])
            board_type_str = data.get("board_type", "square8")
            num_players = data.get("num_players", 2)

            # Support both legacy 2-player (agent_a/agent_b) and multiplayer (agents list)
            agents_list = data.get("agents")
            agent_configs_list = data.get("agent_configs")

            if agents_list and len(agents_list) >= 2:
                # Multiplayer mode: use agents list
                agents = agents_list[:num_players]
                if agent_configs_list and len(agent_configs_list) >= num_players:
                    agent_configs = agent_configs_list[:num_players]
                else:
                    # Build configs from agent names
                    agent_configs = [{"ai_type": a} for a in agents]
            else:
                # Legacy 2-player mode
                agent_a = data.get("agent_a", "random")
                agent_b = data.get("agent_b", "heuristic")
                agent_a_config = data.get("agent_a_config", {"ai_type": agent_a})
                agent_b_config = data.get("agent_b_config", {"ai_type": agent_b})
                agents = [agent_a, agent_b]
                agent_configs = [agent_a_config, agent_b_config]

            # Pad with random if fewer agents than players
            while len(agents) < num_players:
                agents.append("random")
                agent_configs.append({"ai_type": "random"})

            agents_desc = " vs ".join(agents)
            logger.info(f"Playing Elo match {match_id}: {agents_desc}")
            start_time = time.time()

            # Acquire semaphore to prevent concurrent matches (OOM protection)
            # Create lazily in async context to avoid event loop issues
            if self._tournament_match_semaphore is None:
                logger.info("Creating tournament semaphore...")
                self._tournament_match_semaphore = asyncio.Semaphore(1)

            # Try to acquire semaphore with timeout to avoid deadlocks
            # If we can't get the semaphore within 30 seconds, fail fast
            logger.info(f"Acquiring semaphore (current holder: {getattr(self, '_current_match_holder', 'none')})...")
            try:
                await asyncio.wait_for(
                    self._tournament_match_semaphore.acquire(),
                    timeout=30.0
                )
            except asyncio.TimeoutError:
                logger.info(f"Semaphore acquisition timed out after 30s (holder: {getattr(self, '_current_match_holder', 'unknown')})")
                return web.json_response({
                    "success": False,
                    "error": f"Server busy - another match in progress (holder: {getattr(self, '_current_match_holder', 'unknown')})",
                    "match_id": match_id,
                }, status=503)

            # Track who holds the semaphore for debugging
            self._current_match_holder = f"{match_id} ({agents_desc})"
            try:
                logger.info(f"Semaphore acquired, running match {match_id}...")
                # Run the match in a thread pool to avoid blocking
                # Add 5-minute timeout to prevent hung matches
                loop = asyncio.get_event_loop()
                try:
                    result = await asyncio.wait_for(
                        loop.run_in_executor(
                            None,
                            self._play_elo_match_sync,
                            agent_configs,
                            board_type_str,
                            num_players,
                            match_id,
                            agents,
                        ),
                        timeout=300.0,  # 5 minute timeout for tournament matches
                    )
                except asyncio.TimeoutError:
                    logger.info(f"Elo match {match_id} timed out after 5 minutes")
                    return web.json_response({
                        "success": False,
                        "error": "Match timed out after 5 minutes",
                        "match_id": match_id,
                    }, status=504)
            finally:
                # Always release semaphore and clear holder
                self._current_match_holder = None
                self._tournament_match_semaphore.release()
                logger.info(f"Semaphore released for match {match_id}")

            duration = time.time() - start_time

            if result is None:
                return web.json_response({
                    "success": False,
                    "error": "Match failed to complete",
                    "match_id": match_id,
                }, status=500)

            # Map winner player number to agent label
            winner_player = result.get("winner_player")
            if winner_player is not None and winner_player > 0:
                agent_labels = ["agent_a", "agent_b", "agent_c", "agent_d"]
                winner = agent_labels[winner_player - 1] if winner_player <= len(agent_labels) else "draw"
            else:
                # Legacy format support
                winner_map = {"model_a": "agent_a", "model_b": "agent_b", "draw": "draw"}
                winner = winner_map.get(result.get("winner", "draw"), "draw")

            response = {
                "success": True,
                "match_id": match_id,
                "agents": agents,
                "agent_a": agents[0] if len(agents) > 0 else "unknown",
                "agent_b": agents[1] if len(agents) > 1 else "unknown",
                "winner": winner,
                "winner_player": winner_player,
                "game_length": result.get("game_length", 0),
                "duration_sec": duration,
                "worker_node": self.node_id,
            }

            logger.info(f"Elo match {match_id} complete: {agents_desc} -> {response['winner']} ({result.get('game_length', 0)} moves)")
            return web.json_response(response)

        except Exception as e:
            import traceback
            logger.info(f"Elo match error: {e}")
            traceback.print_exc()
            return web.json_response({"error": str(e)}, status=500)

    def _play_elo_match_sync(
        self,
        agent_configs: list[dict],
        board_type_str: str,
        num_players: int,
        match_id: str,
        agent_ids: list[str] | None = None,
    ) -> dict | None:
        """Synchronous wrapper for playing an Elo match.

        Uses a lightweight implementation for simple AI types (random, heuristic, minimax)
        to avoid loading heavy neural network dependencies that cause OOM.

        Args:
            agent_configs: List of AI configurations for each player
            board_type_str: Board type string
            num_players: Number of players in the game
        """
        try:
            import time as time_mod

            from app.db.unified_recording import (
                RecordingConfig,
                RecordSource,
                UnifiedGameRecorder,
                is_recording_enabled,
            )
            from app.game_engine import GameEngine
            from app.models import AIConfig, AIType, BoardType, GameStatus
            from app.training.initial_state import create_initial_state

            board_type = BoardType(board_type_str)
            start_time = time_mod.time()

            # Generate unique random seeds for each player
            import random as rand_mod
            match_seed = int(time_mod.time() * 1000000) % (2**31)
            rand_mod.seed(match_seed)
            seeds = [rand_mod.randint(0, 2**31 - 1) for _ in range(num_players)]

            # Create initial state
            state = create_initial_state(board_type, num_players)
            engine = GameEngine()

            # Map agent names to AI types
            def get_ai_type(agent_config: dict) -> str:
                ai_type = agent_config.get("ai_type", "random")
                if isinstance(ai_type, str):
                    return ai_type.lower()
                return str(ai_type).lower()

            def create_lightweight_ai(agent_config: dict, player_num: int, rng_seed: int):
                """Create AI without loading heavy dependencies."""
                ai_type = get_ai_type(agent_config)

                if ai_type in ("random", "aitype.random"):
                    from app.ai.random_ai import RandomAI
                    config = AIConfig(ai_type=AIType.RANDOM, board_type=board_type, difficulty=1, rng_seed=rng_seed)
                    return RandomAI(player_num, config)

                elif ai_type in ("heuristic", "aitype.heuristic"):
                    from app.ai.heuristic_ai import HeuristicAI
                    config = AIConfig(ai_type=AIType.HEURISTIC, board_type=board_type, difficulty=3, rng_seed=rng_seed)
                    return HeuristicAI(player_num, config)

                elif ai_type in ("minimax", "minimax_heuristic", "aitype.minimax"):
                    from app.ai.minimax_ai import MinimaxAI
                    use_nn = agent_config.get("use_neural_net", False)
                    max_depth = agent_config.get("max_depth", 3)
                    config = AIConfig(
                        ai_type=AIType.MINIMAX,
                        board_type=board_type,
                        difficulty=agent_config.get("difficulty", 3),
                        use_neural_net=use_nn,
                        max_depth=max_depth,
                        rng_seed=rng_seed,
                    )
                    return MinimaxAI(player_num, config)

                elif ai_type in ("mcts", "mcts_heuristic", "aitype.mcts"):
                    from app.ai.mcts_ai import MCTSAI
                    use_nn = agent_config.get("use_neural_net", False)
                    iters = agent_config.get("mcts_iterations", 100)
                    config = AIConfig(
                        ai_type=AIType.MCTS,
                        board_type=board_type,
                        difficulty=agent_config.get("difficulty", 5),
                        use_neural_net=use_nn,
                        mcts_iterations=iters,
                        rng_seed=rng_seed,
                    )
                    return MCTSAI(player_num, config)

                elif ai_type in ("descent", "aitype.descent"):
                    # Descent AI is CPU-based but can be slow at high difficulty
                    # Cap at difficulty 5 for tournament matches (~1.1s/move)
                    from app.ai.descent_ai import DescentAI
                    requested_diff = agent_config.get("difficulty", 5)
                    capped_diff = min(requested_diff, 5)  # Cap at 5 for tournaments
                    if capped_diff < requested_diff:
                        logger.info(f"Descent AI difficulty capped from {requested_diff} to {capped_diff} for tournament")
                    config = AIConfig(
                        ai_type=AIType.DESCENT,
                        board_type=board_type,
                        difficulty=capped_diff,
                        rng_seed=rng_seed,
                    )
                    return DescentAI(player_num, config)

                else:
                    # For neural-net based types (policy_only, gumbel_mcts, mcts_neural),
                    # check available memory before loading
                    import psutil
                    mem = psutil.virtual_memory()
                    available_gb = mem.available / (1024**3)

                    # Require at least 8GB free for neural network loading (conservative to prevent OOM)
                    if available_gb < 8.0:
                        logger.info(f"Skipping NN-based AI {ai_type}: only {available_gb:.1f}GB available (need 8GB)")
                        # Fall back to descent AI (CPU-based, no NN)
                        from app.ai.descent_ai import DescentAI
                        config = AIConfig(ai_type=AIType.DESCENT, board_type=board_type, difficulty=7, rng_seed=rng_seed)
                        return DescentAI(player_num, config)

                    # Safe to load neural network AI
                    try:
                        from scripts.run_model_elo_tournament import create_ai_from_model
                        return create_ai_from_model(agent_config, player_num, board_type)
                    except Exception as e:
                        logger.error(f"Failed to create NN AI {ai_type}: {e}, falling back to heuristic")
                        from app.ai.heuristic_ai import HeuristicAI
                        config = AIConfig(ai_type=AIType.HEURISTIC, board_type=board_type, difficulty=7, rng_seed=rng_seed)
                        return HeuristicAI(player_num, config)

            # Create AIs for all players with unique seeds
            ais = {}
            for i in range(num_players):
                player_num = i + 1
                config = agent_configs[i] if i < len(agent_configs) else {"ai_type": "random"}
                seed = seeds[i] if i < len(seeds) else 0
                ais[player_num] = create_lightweight_ai(config, player_num, seed)

            # Keep initial state for training record
            initial_state = state

            if not agent_ids:
                agent_ids = [
                    str(cfg.get("agent_id") or cfg.get("ai_type") or f"player_{idx + 1}")
                    for idx, cfg in enumerate(agent_configs[:num_players])
                ]
            while len(agent_ids) < num_players:
                agent_ids.append(f"player_{len(agent_ids) + 1}")

            # Play game and record actual Move objects for training
            move_count = 0
            max_moves = 500
            recorded_moves = []  # List of actual Move objects for training
            termination_reason = "completed"
            recording_enabled = is_recording_enabled()
            tags = [
                "elo_tournament",
                f"node_{self.node_id}",
                f"board_{board_type.value}",
                f"players_{num_players}",
            ]
            for idx, cfg in enumerate(agent_configs[:num_players]):
                ai_type = cfg.get("ai_type", "unknown")
                tags.append(f"p{idx + 1}_{ai_type}")

            recording_config = RecordingConfig(
                board_type=board_type.value,
                num_players=num_players,
                source=RecordSource.TOURNAMENT,
                engine_mode="p2p_elo",
                db_prefix="tournament",
                db_dir="data/games",
                store_history_entries=True,
                fsm_validation=True,
                tags=tags,
            )
            recorder = UnifiedGameRecorder(recording_config, state, game_id=match_id) if recording_enabled else None

            try:
                if recorder is not None:
                    recorder.__enter__()

                while state.game_status == GameStatus.ACTIVE and move_count < max_moves:
                    current_player = state.current_player

                    requirement = GameEngine.get_phase_requirement(state, current_player)
                    if requirement is not None:
                        move = GameEngine.synthesize_bookkeeping_move(requirement, state)
                    else:
                        current_ai = ais.get(current_player)
                        if current_ai is None:
                            logger.warning(f"No AI for player {current_player}, using random fallback")
                            from app.ai.random_ai import RandomAI
                            config = AIConfig(ai_type=AIType.RANDOM, board_type=board_type, difficulty=1)
                            current_ai = RandomAI(current_player, config)
                            ais[current_player] = current_ai
                        move = current_ai.select_move(state)

                    if move is None:
                        termination_reason = "no_move"
                        break

                    # Get soft policy targets for training data
                    move_probs = None
                    if hasattr(current_ai, 'get_visit_distribution'):
                        try:
                            moves_dist, probs_dist = current_ai.get_visit_distribution()
                            if moves_dist and probs_dist:
                                move_probs = {}
                                for mv, prob in zip(moves_dist, probs_dist, strict=False):
                                    # Create move key in format: "{from_x},{from_y}->{to_x},{to_y}"
                                    if hasattr(mv, 'to') and mv.to is not None:
                                        move_key = f"{mv.to.x},{mv.to.y}"
                                        if hasattr(mv, 'from_pos') and mv.from_pos is not None:
                                            move_key = f"{mv.from_pos.x},{mv.from_pos.y}->{move_key}"
                                        move_probs[move_key] = float(prob)
                        except Exception:
                            pass  # Silently ignore if visit distribution fails

                    # Record actual Move object for training
                    recorded_moves.append(move)

                    state_before = state
                    state = engine.apply_move(state, move, trace_mode=True)
                    move_count += 1
                    if recorder is not None:
                        recorder.add_move(
                            move,
                            state_after=state,
                            state_before=state_before,
                            available_moves_count=None,
                            move_probs=move_probs,
                        )
            finally:
                if move_count >= max_moves and state.game_status == GameStatus.ACTIVE:
                    termination_reason = "max_moves"

                if recorder is not None:
                    winner_player = state.winner if state.winner else 0
                    winner_agent = None
                    if isinstance(winner_player, int) and winner_player > 0 and winner_player <= len(agent_ids):
                        winner_agent = agent_ids[winner_player - 1]

                    extra_metadata = {
                        "match_id": match_id,
                        "tournament_id": f"p2p_elo_{match_id}",
                        "node_id": self.node_id,
                        "match_seed": match_seed,
                        "agent_ids": agent_ids,
                        "agent_configs": agent_configs,
                        "winner_player": winner_player,
                        "winner_agent": winner_agent,
                        "game_length": move_count,
                        "duration_sec": time_mod.time() - start_time,
                        "termination_reason": termination_reason,
                    }
                    try:
                        recorder.finalize(state, extra_metadata=extra_metadata)
                    finally:
                        recorder.__exit__(None, None, None)

            duration = time_mod.time() - start_time

            # Determine winner (as player number)
            winner_player = state.winner if state.winner else 0

            # Legacy format for 2-player backward compatibility
            winner = "draw"
            if winner_player == 1:
                winner = "model_a"
            elif winner_player == 2:
                winner = "model_b"

            # Save game for training using proper GameRecord format
            if recorded_moves and winner_player > 0:
                try:
                    self._save_tournament_game_for_training(
                        initial_state=initial_state,
                        final_state=state,
                        moves=recorded_moves,
                        match_seed=match_seed,
                        agent_configs=agent_configs,
                    )
                except Exception as e:
                    logger.warning(f"Failed to save tournament game for training: {e}")

            return {
                "winner": winner,
                "winner_player": winner_player,
                "game_length": move_count,
                "duration_sec": duration,
            }

        except Exception as e:
            import traceback
            logger.info(f"_play_elo_match_sync error: {e}")
            traceback.print_exc()
            return None

    def _save_tournament_game_for_training(
        self,
        initial_state,  # GameState
        final_state,  # GameState
        moves: list,  # List of Move objects
        match_seed: int,
        agent_configs: list[dict] | None = None,  # Optional AI configs for metadata
    ) -> None:
        """Save a tournament game to JSONL format for training.

        Uses build_training_game_record to create proper GameRecord format
        compatible with the training pipeline. The saved games can be ingested
        by the training system alongside selfplay games.

        Saves games to data/tournament_games/{board_type}_{num_players}p/ directory.

        Args:
            initial_state: The initial game state
            final_state: The final game state after all moves
            moves: List of Move objects representing the game
            match_seed: RNG seed used for this match
            agent_configs: Optional list of AI configs for each player
        """
        import json
        import sys
        from datetime import datetime, timezone
        from pathlib import Path

        # Ensure app module is importable
        ai_service_path = str(Path(self.ringrift_path) / "ai-service")
        if ai_service_path not in sys.path:
            sys.path.insert(0, ai_service_path)

        try:
            from app.models.game_record import RecordSource
            from app.training.game_record_export import build_training_game_record
        except ImportError as e:
            logger.warning(f"Cannot import game record modules: {e}")
            return

        board_type_str = initial_state.board_type.value if hasattr(initial_state.board_type, 'value') else str(initial_state.board_type)
        num_players = len(initial_state.players)

        # Create output directory
        data_dir = Path(self.ringrift_path) / "ai-service" / "data" / "tournament_games"
        config_dir = data_dir / f"{board_type_str}_{num_players}p"
        config_dir.mkdir(parents=True, exist_ok=True)

        # Create game ID with full metadata
        timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
        game_id = f"tournament_{self.node_id}_{timestamp}_{uuid.uuid4().hex[:8]}"

        # Build tags with detailed metadata for training filtering
        tags = [
            "elo_tournament",
            f"node_{self.node_id}",
            f"board_{board_type_str}",
            f"players_{num_players}",
        ]
        if agent_configs:
            for i, cfg in enumerate(agent_configs[:num_players]):
                ai_type = cfg.get("ai_type", "unknown")
                tags.append(f"player{i+1}_{ai_type}")

        # Build proper GameRecord using the training export function
        try:
            game_record = build_training_game_record(
                game_id=game_id,
                initial_state=initial_state,
                final_state=final_state,
                moves=moves,
                source=RecordSource.TOURNAMENT,
                rng_seed=match_seed,
                terminated_by_budget_only=False,
                created_at=datetime.now(timezone.utc),
                tags=tags,
                fsm_validated=None,  # Not FSM validated in tournament context
            )

            # Use the canonical to_jsonl_line() method for proper serialization
            jsonl_line = game_record.to_jsonl_line()

            # Append to daily file for this config
            daily_file = config_dir / f"tournament_{timestamp[:8]}.jsonl"
            with open(daily_file, "a", encoding="utf-8") as f:
                f.write(jsonl_line + "\n")

            logger.info(f"Saved tournament game {game_id} to {daily_file} ({len(moves)} moves, winner={final_state.winner})")

        except Exception as e:
            import traceback
            logger.warning(f"Failed to build/save tournament game record: {e}")
            traceback.print_exc()

    async def handle_ssh_tournament_start(self, request: web.Request) -> web.Response:
        """Start an SSH-distributed difficulty-tier tournament (leader only).

        This is a thin wrapper that runs `scripts/run_ssh_distributed_tournament.py`
        as a subprocess and tracks its status locally on the leader node.
        """
        try:
            if self.role != NodeRole.LEADER:
                return web.json_response({
                    "error": "Only the leader can start SSH tournaments",
                    "leader_id": self.leader_id,
                }, status=403)

            data = await request.json()

            tiers = str(data.get("tiers") or "D1-D10")
            board = str(data.get("board") or data.get("board_type") or "square8").strip().lower()
            if board == "hexagonal":
                board = "hex"
            if board not in ("square8", "square19", "hex8", "hex"):
                return web.json_response({"error": f"Invalid board: {board!r}"}, status=400)

            games_per_matchup = int(data.get("games_per_matchup", 50) or 50)
            seed = int(data.get("seed", 1) or 1)
            think_time_scale = float(data.get("think_time_scale", 1.0) or 1.0)
            max_moves = int(data.get("max_moves", 10000) or 10000)
            wilson_confidence = float(data.get("wilson_confidence", 0.95) or 0.95)
            nn_model_id = data.get("nn_model_id") or None
            config_path = data.get("config") or None
            include_nonready = bool(data.get("include_nonready", False))
            max_parallel_per_host = data.get("max_parallel_per_host")
            remote_output_dir = str(data.get("remote_output_dir") or "results/tournaments/ssh_shards")
            job_timeout_sec = int(data.get("job_timeout_sec", 6 * 60 * 60) or (6 * 60 * 60))
            retries = int(data.get("retries", 1) or 1)
            dry_run = bool(data.get("dry_run", False))

            requested_run_id = str(data.get("run_id") or "").strip()
            job_id = requested_run_id or f"ssh_tournament_{uuid.uuid4().hex[:8]}"
            run_id = job_id

            hosts = data.get("hosts")
            hosts_spec: str | None = None
            if isinstance(hosts, list):
                hosts_spec = ",".join(str(h).strip() for h in hosts if str(h).strip())
            elif isinstance(hosts, str) and hosts.strip():
                hosts_spec = hosts.strip()

            output_root = str(
                data.get("output_root") or f"results/tournaments/p2p_orchestrator/{run_id}"
            )

            report_path = str(Path(output_root) / f"report_{run_id}.json")
            checkpoint_path = str(Path(output_root) / f"tournament_{run_id}.json")
            manifest_path = str(Path(output_root) / "manifest.json")

            log_dir = STATE_DIR / "ssh_tournaments"
            log_dir.mkdir(parents=True, exist_ok=True)
            log_path = str(log_dir / f"{run_id}.log")

            cmd: list[str] = [
                sys.executable,
                "scripts/run_ssh_distributed_tournament.py",
                "--tiers", tiers,
                "--board", board,
                "--games-per-matchup", str(games_per_matchup),
                "--seed", str(seed),
                "--think-time-scale", str(think_time_scale),
                "--max-moves", str(max_moves),
                "--wilson-confidence", str(wilson_confidence),
                "--remote-output-dir", remote_output_dir,
                "--job-timeout-sec", str(job_timeout_sec),
                "--retries", str(retries),
                "--run-id", run_id,
                "--output-root", output_root,
            ]
            if nn_model_id:
                cmd.extend(["--nn-model-id", str(nn_model_id)])
            if config_path:
                cmd.extend(["--config", str(config_path)])
            if hosts_spec:
                cmd.extend(["--hosts", hosts_spec])
            if include_nonready:
                cmd.append("--include-nonready")
            if max_parallel_per_host is not None:
                cmd.extend(["--max-parallel-per-host", str(int(max_parallel_per_host))])
            if dry_run:
                cmd.append("--dry-run")

            env = os.environ.copy()
            env["PYTHONPATH"] = os.path.join(self.ringrift_path, "ai-service")

            cwd = os.path.join(self.ringrift_path, "ai-service")
            with open(log_path, "ab") as log_file:
                proc = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=log_file,
                    stderr=asyncio.subprocess.STDOUT,
                    env=env,
                    cwd=cwd,
                )

            run_state = SSHTournamentRun(
                job_id=job_id,
                run_id=run_id,
                tiers=tiers,
                board=board,
                games_per_matchup=games_per_matchup,
                pid=proc.pid,
                status="running",
                started_at=time.time(),
                output_root=output_root,
                manifest_path=manifest_path,
                checkpoint_path=checkpoint_path,
                report_path=report_path,
                log_path=log_path,
                command=cmd,
            )

            with self.ssh_tournament_lock:
                self.ssh_tournament_runs[job_id] = run_state

            asyncio.create_task(self._monitor_ssh_tournament_process(job_id, proc))

            return web.json_response({"success": True, "job": run_state.to_dict()})
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_ssh_tournament_status(self, request: web.Request) -> web.Response:
        """Get status of SSH-distributed tournaments."""
        try:
            job_id = request.query.get("job_id")

            with self.ssh_tournament_lock:
                if job_id:
                    job = self.ssh_tournament_runs.get(job_id)
                    if not job:
                        return web.json_response({"error": "Tournament not found"}, status=404)
                    return web.json_response(job.to_dict())

                return web.json_response({
                    jid: job.to_dict() for jid, job in self.ssh_tournament_runs.items()
                })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_ssh_tournament_cancel(self, request: web.Request) -> web.Response:
        """Cancel a running SSH tournament (best-effort)."""
        try:
            if self.role != NodeRole.LEADER:
                return web.json_response({
                    "error": "Only the leader can cancel SSH tournaments",
                    "leader_id": self.leader_id,
                }, status=403)

            data = await request.json()
            job_id = data.get("job_id")
            if not job_id:
                return web.json_response({"error": "job_id is required"}, status=400)

            with self.ssh_tournament_lock:
                job = self.ssh_tournament_runs.get(job_id)
            if not job:
                return web.json_response({"error": "Tournament not found"}, status=404)

            if job.status != "running":
                return web.json_response({
                    "success": False,
                    "error": f"Cannot cancel tournament in status: {job.status}",
                }, status=400)

            try:
                os.kill(job.pid, signal.SIGTERM)
            except Exception as e:
                return web.json_response({
                    "success": False,
                    "error": f"Failed to signal process: {e}",
                }, status=500)

            with self.ssh_tournament_lock:
                job.status = "cancelled"
                job.completed_at = time.time()

            return web.json_response({"success": True, "job_id": job_id})
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def _monitor_ssh_tournament_process(self, job_id: str, proc) -> None:
        """Monitor a tournament subprocess and update status."""
        try:
            return_code = await proc.wait()
            with self.ssh_tournament_lock:
                job = self.ssh_tournament_runs.get(job_id)
                if not job:
                    return
                job.return_code = return_code
                job.completed_at = time.time()
                if job.status != "cancelled":
                    job.status = "completed" if return_code == 0 else "failed"
                    if return_code != 0:
                        job.error_message = f"Process exited with code {return_code}"
        except Exception as e:
            with self.ssh_tournament_lock:
                job = self.ssh_tournament_runs.get(job_id)
                if job and job.status != "cancelled":
                    job.status = "failed"
                    job.completed_at = time.time()
                    job.error_message = str(e)

    async def _run_distributed_tournament(self, job_id: str):
        """Main coordinator loop for distributed tournament."""
        try:
            state = self.distributed_tournament_state.get(job_id)
            if not state:
                return

            logger.info(f"Tournament coordinator started for job {job_id}")

            # Distribute matches to workers
            while state.pending_matches and state.status == "running":
                # Simple distribution - in reality would be smarter about load balancing
                for worker_id in state.worker_nodes:
                    if not state.pending_matches:
                        break
                    match = state.pending_matches.pop(0)
                    match["status"] = "in_progress"

                    # Send match to worker
                    await self._send_match_to_worker(job_id, worker_id, match)

                await asyncio.sleep(1)

            # Wait for all results
            while state.completed_matches < state.total_matches and state.status == "running":
                state.last_update = time.time()
                await asyncio.sleep(1)

            # Calculate final ratings
            self._calculate_tournament_ratings(state)
            state.status = "completed"

            logger.info(f"Tournament {job_id} completed: {state.completed_matches} matches, ratings={state.final_ratings}")

        except Exception as e:
            logger.info(f"Tournament coordinator error: {e}")
            if job_id in self.distributed_tournament_state:
                self.distributed_tournament_state[job_id].status = f"error: {e}"

    async def _send_match_to_worker(self, job_id: str, worker_id: str, match: dict):
        """Send a match to a worker node."""
        try:
            with self.peers_lock:
                worker = self.peers.get(worker_id)
            if not worker:
                return

            timeout = ClientTimeout(total=10)
            async with get_client_session(timeout) as session:
                url = self._url_for_peer(worker, "/tournament/match")
                await session.post(url, json={"job_id": job_id, "match": match}, headers=self._auth_headers())
        except Exception as e:
            logger.error(f"Failed to send match to worker {worker_id}: {e}")

    async def _play_tournament_match(self, job_id: str, match_info: dict):
        """Play a tournament match locally using subprocess selfplay."""
        try:
            import json as json_module
            import sys

            agent1 = match_info["agent1"]
            agent2 = match_info["agent2"]
            game_num = match_info.get("game_num", 0)
            board_type = match_info.get("board_type", "square8")
            num_players = match_info.get("num_players", 2)

            logger.info(f"Playing tournament match: {agent1} vs {agent2} (game {game_num})")

            # Build the subprocess command to run a single game
            # Agent IDs map to model paths or heuristic configurations
            game_script = f"""
import sys
sys.path.insert(0, '{self.ringrift_path}/ai-service')
from app.game_engine import GameEngine
from app.agents.heuristic_agent import HeuristicAgent
import json
import random

def load_agent(agent_id: str, player_idx: int):
    '''Load agent by ID - supports heuristic weights or model paths.'''
    if agent_id.startswith('heuristic:'):
        # Parse weights from agent ID: "heuristic:w1,w2,w3,..."
        weight_str = agent_id.split(':')[1]
        weights = [float(w) for w in weight_str.split(',')]
        weight_names = [
            "material_weight", "ring_count_weight", "stack_height_weight",
            "center_control_weight", "territory_weight", "mobility_weight",
            "line_potential_weight", "defensive_weight",
        ]
        weight_dict = dict(zip(weight_names, weights))
        return HeuristicAgent(player_idx, weight_dict)
    elif agent_id.startswith('model:'):
        # Neural network model - would load from path
        # For now, fall back to heuristic
        return HeuristicAgent(player_idx)
    else:
        # Default heuristic agent
        return HeuristicAgent(player_idx)

# Initialize game
engine = GameEngine(board_type='{board_type}', num_players={num_players})
agents = [
    load_agent('{agent1}', 0),
    load_agent('{agent2}', 1),
]

# Play until completion
max_moves = 10000
move_count = 0
while not engine.is_game_over() and move_count < max_moves:
    current_player = engine.current_player
    agent = agents[current_player]
    legal_moves = engine.get_legal_moves()
    if not legal_moves:
        break
    move = agent.select_move(engine.get_state(), legal_moves)
    engine.apply_move(move)
    move_count += 1

# Get result
outcome = engine.get_outcome()
winner_idx = outcome.get('winner')
victory_type = outcome.get('victory_type', 'unknown')

# Map winner index to agent ID
winner_agent = None
if winner_idx == 0:
    winner_agent = '{agent1}'
elif winner_idx == 1:
    winner_agent = '{agent2}'

result = {{
    'agent1': '{agent1}',
    'agent2': '{agent2}',
    'winner': winner_agent,
    'winner_idx': winner_idx,
    'victory_type': victory_type,
    'move_count': move_count,
    'game_num': {game_num},
}}
print(json.dumps(result))
"""
            # Run the game in subprocess
            cmd = [sys.executable, "-c", game_script]
            env = os.environ.copy()
            env["PYTHONPATH"] = f"{self.ringrift_path}/ai-service"
            env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

            proc = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env,
            )

            stdout, stderr = await asyncio.wait_for(
                proc.communicate(),
                timeout=300  # 5 minute timeout per game
            )

            if proc.returncode != 0:
                logger.info(f"Tournament match subprocess error: {stderr.decode()}")
                result = {
                    "agent1": agent1,
                    "agent2": agent2,
                    "winner": None,
                    "error": stderr.decode()[:200],
                    "game_num": game_num,
                }
            else:
                # Parse result from stdout
                output_lines = stdout.decode().strip().split('\n')
                result_line = output_lines[-1] if output_lines else '{}'
                result = json_module.loads(result_line)

            logger.info(f"Match result: {agent1} vs {agent2} -> winner={result.get('winner')}")

            # Report result back to coordinator (leader)
            if self.role != NodeRole.LEADER and self.leader_id:
                with self.peers_lock:
                    leader = self.peers.get(self.leader_id)
                if leader:
                    try:
                        timeout = ClientTimeout(total=10)
                        async with get_client_session(timeout) as session:
                            url = self._url_for_peer(leader, "/tournament/result")
                            await session.post(url, json={
                                "job_id": job_id,
                                "result": result,
                                "worker_id": self.node_id,
                            }, headers=self._auth_headers())
                    except Exception as e:
                        logger.error(f"Failed to report tournament result to leader: {e}")
            else:
                # We are the leader, update state directly
                if job_id in self.distributed_tournament_state:
                    state = self.distributed_tournament_state[job_id]
                    state.results.append(result)
                    state.completed_matches += 1
                    state.last_update = time.time()

        except asyncio.TimeoutError:
            logger.info(f"Tournament match timed out: {match_info}")
        except Exception as e:
            logger.info(f"Tournament match error: {e}")

    def _calculate_tournament_ratings(self, state: DistributedTournamentState):
        """Calculate final Elo ratings from tournament results.

        Uses standard Elo rating system with K-factor from app.config.thresholds.
        Random is pinned at 400 Elo as the anchor point.
        """
        # Use canonical constant - Random AI is ALWAYS pinned at 400 Elo
        RANDOM_ANCHOR = BASELINE_ELO_RANDOM

        # Initialize ratings (using canonical constants from app.config.thresholds)
        ratings = {agent: float(INITIAL_ELO_RATING) for agent in state.agent_ids}
        wins = dict.fromkeys(state.agent_ids, 0)
        losses = dict.fromkeys(state.agent_ids, 0)
        draws = dict.fromkeys(state.agent_ids, 0)

        def expected_score(rating_a: float, rating_b: float) -> float:
            """Calculate expected score for player A against player B."""
            return 1.0 / (1.0 + 10 ** ((rating_b - rating_a) / 400.0))

        def update_elo(rating: float, expected: float, actual: float) -> float:
            """Update Elo rating based on game outcome."""
            return rating + ELO_K_FACTOR * (actual - expected)

        # Process all results
        for result in state.results:
            agent1 = result.get("agent1")
            agent2 = result.get("agent2")
            winner = result.get("winner")

            if not agent1 or not agent2:
                continue
            if agent1 not in ratings or agent2 not in ratings:
                continue

            # Determine actual scores
            if winner == agent1:
                score1, score2 = 1.0, 0.0
                wins[agent1] += 1
                losses[agent2] += 1
            elif winner == agent2:
                score1, score2 = 0.0, 1.0
                wins[agent2] += 1
                losses[agent1] += 1
            elif winner is None:
                # Draw
                score1, score2 = 0.5, 0.5
                draws[agent1] += 1
                draws[agent2] += 1
            else:
                # Unknown winner, skip
                continue

            # Calculate expected scores
            expected1 = expected_score(ratings[agent1], ratings[agent2])
            expected2 = expected_score(ratings[agent2], ratings[agent1])

            # Update ratings
            ratings[agent1] = update_elo(ratings[agent1], expected1, score1)
            ratings[agent2] = update_elo(ratings[agent2], expected2, score2)

        # Normalize ratings so random is pinned at 400
        if "random" in ratings:
            offset = RANDOM_ANCHOR - ratings["random"]
            ratings = {agent: rating + offset for agent, rating in ratings.items()}

        # Store final ratings and stats
        state.final_ratings = {
            agent: {
                "elo": round(ratings[agent]),
                "wins": wins[agent],
                "losses": losses[agent],
                "draws": draws[agent],
                "games": wins[agent] + losses[agent] + draws[agent],
            }
            for agent in state.agent_ids
        }

        # Log rankings
        ranked = sorted(state.final_ratings.items(), key=lambda x: x[1]["elo"], reverse=True)
        logger.info("Tournament final rankings:")
        for rank, (agent, stats) in enumerate(ranked, 1):
            logger.info(f"  {rank}. {agent}: Elo={stats['elo']}, W/L/D={stats['wins']}/{stats['losses']}/{stats['draws']}")

        # Persist results to unified Elo database
        try:
            from app.tournament import get_elo_database
            db = get_elo_database()

            for result in state.results:
                agent1 = result.get("agent1")
                agent2 = result.get("agent2")
                winner = result.get("winner")

                if not agent1 or not agent2:
                    continue

                # Determine rankings
                if winner == agent1:
                    rankings = [0, 1]
                elif winner == agent2:
                    rankings = [1, 0]
                else:
                    rankings = [0, 0]

                db.record_match_and_update(
                    participant_ids=[agent1, agent2],
                    rankings=rankings,
                    board_type=state.board_type,
                    num_players=state.num_players,
                    tournament_id=state.job_id,
                    game_length=result.get("game_length", 0),
                    duration_sec=result.get("duration_sec", 0.0),
                )

            logger.info(f"Persisted {len(state.results)} matches to unified Elo database")

            # Trigger Elo sync to propagate matches to cluster
            if HAS_ELO_SYNC and self.elo_sync_manager:
                asyncio.create_task(self._trigger_elo_sync_after_matches(len(state.results)))
        except Exception as e:
            logger.warning(f"Failed to persist to unified Elo database: {e}")

    # ============================================
    # Improvement Loop Handlers
    # ============================================

    async def handle_improvement_start(self, request: web.Request) -> web.Response:
        """Start an improvement loop (AlphaZero-style training cycle).

        Only the leader can start improvement loops.
        Request body:
        {
            "board_type": "square8",
            "num_players": 2,
            "max_iterations": 50,
            "games_per_iteration": 1000
        }
        """
        try:
            if self.role != NodeRole.LEADER:
                return web.json_response({
                    "error": "Only the leader can start improvement loops",
                    "leader_id": self.leader_id,
                }, status=403)

            data = await request.json()
            job_id = f"improve_{uuid.uuid4().hex[:8]}"

            # Query negotiated rate from resource_optimizer for cooperative utilization
            # This ensures selfplay rate respects cluster-wide 60-80% utilization target
            requested_games = data.get("games_per_iteration", 1000)
            if HAS_RATE_NEGOTIATION and negotiate_selfplay_rate is not None:
                try:
                    # Negotiate rate with resource_optimizer (60-80% target)
                    approved_rate = negotiate_selfplay_rate(
                        requested_rate=requested_games,
                        reason=f"p2p_improvement_loop:{job_id}",
                        requestor=f"p2p_{self.node_id}",
                    )
                    if approved_rate != requested_games:
                        logger.info(f"games_per_iteration adjusted: {requested_games} -> {approved_rate} (utilization-based)")
                    requested_games = approved_rate
                except Exception as e:
                    logger.info(f"Rate negotiation failed, using default: {e}")

            state = ImprovementLoopState(
                job_id=job_id,
                board_type=data.get("board_type", "square8"),
                num_players=data.get("num_players", 2),
                max_iterations=data.get("max_iterations", 50),
                games_per_iteration=requested_games,
                phase="selfplay",
                status="running",
                started_at=time.time(),
                last_update=time.time(),
            )

            # Find available workers
            with self.peers_lock:
                workers = [p.node_id for p in self.peers.values() if p.is_healthy()]
                gpu_workers = [p.node_id for p in self.peers.values() if p.is_healthy() and p.has_gpu]
            state.worker_nodes = workers

            if not gpu_workers:
                return web.json_response({"error": "No GPU workers available for training"}, status=503)

            self.improvement_loop_state[job_id] = state

            logger.info(f"Started improvement loop {job_id}: {len(workers)} workers, {len(gpu_workers)} GPU workers")

            # Launch improvement loop
            asyncio.create_task(self._run_improvement_loop(job_id))

            return web.json_response({
                "success": True,
                "job_id": job_id,
                "workers": workers,
                "gpu_workers": gpu_workers,
                "config": {
                    "board_type": state.board_type,
                    "num_players": state.num_players,
                    "max_iterations": state.max_iterations,
                    "games_per_iteration": state.games_per_iteration,
                },
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_improvement_status(self, request: web.Request) -> web.Response:
        """Get status of improvement loops."""
        try:
            job_id = request.query.get("job_id")

            if job_id:
                if job_id not in self.improvement_loop_state:
                    return web.json_response({"error": "Improvement loop not found"}, status=404)
                state = self.improvement_loop_state[job_id]
                return web.json_response(state.to_dict())

            return web.json_response({
                job_id: state.to_dict()
                for job_id, state in self.improvement_loop_state.items()
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_improvement_phase_complete(self, request: web.Request) -> web.Response:
        """Notify that a phase of the improvement loop is complete."""
        try:
            data = await request.json()
            job_id = data.get("job_id")
            phase = data.get("phase")
            worker_id = data.get("worker_id", "unknown")
            result = data.get("result", {})

            if job_id not in self.improvement_loop_state:
                return web.json_response({"error": "Improvement loop not found"}, status=404)

            state = self.improvement_loop_state[job_id]
            state.last_update = time.time()

            # Track progress by phase
            if phase == "selfplay":
                games_done = result.get("games_done", 0)
                state.selfplay_progress[worker_id] = games_done
                total_done = sum(state.selfplay_progress.values())
                logger.info(f"Improvement loop selfplay: {total_done}/{state.games_per_iteration} games")
            elif phase == "train":
                state.best_model_path = result.get("model_path", state.best_model_path)
            elif phase == "evaluate":
                winrate = result.get("winrate", 0.0)
                if winrate > state.best_winrate:
                    state.best_winrate = winrate
                    logger.info(f"New best model: winrate={winrate:.2%}")

            return web.json_response({
                "success": True,
                "job_id": job_id,
                "phase": state.phase,
                "iteration": state.current_iteration,
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    # =========================================================================
    # Phase 2: P2P Data Sync HTTP Handlers
    # =========================================================================

    async def handle_sync_start(self, request: web.Request) -> web.Response:
        """POST /sync/start - Leader initiates a cluster-wide data sync.

        Only the leader can start a sync. This collects manifests from all nodes,
        generates a sync plan, and dispatches rsync jobs to nodes.
        """
        try:
            if not self._is_leader() and request.query.get("local") != "1":
                return await self._proxy_to_leader(request)
            if not self._is_leader():
                return web.json_response({
                    "error": "Not the leader. Only leader can start cluster sync.",
                    "leader_id": self.leader_id,
                }, status=403)

            result = await self.start_cluster_sync()
            return web.json_response(result)
        except Exception as e:
            logger.error(f"in handle_sync_start: {e}")
            import traceback
            traceback.print_exc()
            return web.json_response({"error": str(e)}, status=500)

    async def handle_sync_status(self, request: web.Request) -> web.Response:
        """GET /sync/status - Get current sync status.

        Returns the current sync plan (if any), active sync jobs, and overall status.
        """
        try:
            if not self._is_leader() and request.query.get("local") != "1":
                proxied = await self._proxy_to_leader(request)
                if proxied.status not in (502, 503):
                    return proxied

            with self.sync_lock:
                sync_plan_dict = self.current_sync_plan.to_dict() if self.current_sync_plan else None
                active_jobs_dict = {
                    job_id: job.to_dict()
                    for job_id, job in self.active_sync_jobs.items()
                }

            return web.json_response({
                "node_id": self.node_id,
                "is_leader": self._is_leader(),
                "sync_in_progress": self.sync_in_progress,
                "last_sync_time": self.last_sync_time,
                "auto_sync_interval": self.auto_sync_interval,
                "current_sync_plan": sync_plan_dict,
                "active_sync_jobs": active_jobs_dict,
                "pending_sync_requests": len(self.pending_sync_requests),
            })
        except Exception as e:
            logger.error(f"in handle_sync_status: {e}")
            return web.json_response({"error": str(e)}, status=500)

    async def handle_subscriptions(self, request: web.Request) -> web.Response:
        """GET /subscriptions - Get event subscription dashboard.

        Phase 5 (December 2025): Visibility into feedback loop event wiring.

        Returns list of events and their subscribers to verify the feedback
        loop is properly wired. Critical for debugging dead-end events.

        Returns:
            JSON with event types and their subscriber counts/names
        """
        try:
            from app.distributed.data_events import DataEventType

            subscriptions: dict[str, dict] = {}
            router_info: dict = {"available": False, "type": "unknown"}

            try:
                from app.coordination.event_router import get_router

                router = get_router()
                if router is not None:
                    router_info["available"] = True
                    router_info["type"] = type(router).__name__

                    # Get all subscriptions from router
                    if hasattr(router, '_subscribers'):
                        for event_key, handlers in router._subscribers.items():
                            handler_names = []
                            for handler in handlers:
                                if hasattr(handler, '__name__'):
                                    handler_names.append(handler.__name__)
                                elif hasattr(handler, '__class__'):
                                    handler_names.append(handler.__class__.__name__)
                                else:
                                    handler_names.append(str(type(handler)))

                            subscriptions[event_key] = {
                                "count": len(handlers),
                                "handlers": handler_names[:10],  # Limit to first 10
                            }
            except Exception as e:
                router_info["error"] = str(e)

            # Define critical events for feedback loop
            critical_events = [
                "hyperparameter_updated",
                "curriculum_advanced",
                "adaptive_params_changed",
                "regression_critical",
                "evaluation_completed",
                "model_promoted",
                "training_complete",
                "selfplay_complete",
            ]

            critical_status: dict[str, dict] = {}
            for event in critical_events:
                if event in subscriptions:
                    critical_status[event] = {
                        "status": "active",
                        "subscribers": subscriptions[event]["count"],
                    }
                else:
                    critical_status[event] = {
                        "status": "missing",
                        "subscribers": 0,
                    }

            missing_count = sum(1 for e in critical_status.values() if e["status"] == "missing")

            return web.json_response({
                "node_id": self.node_id,
                "router": router_info,
                "feedback_loop_health": "healthy" if missing_count == 0 else f"{missing_count} missing",
                "critical_events": critical_status,
                "all_subscriptions": subscriptions,
                "total_event_types": len(subscriptions),
                "phase": "Phase 5 - December 2025",
            })

        except Exception as e:
            logger.error(f"in handle_subscriptions: {e}")
            return web.json_response({"error": str(e)}, status=500)

    async def handle_sync_pull(self, request: web.Request) -> web.Response:
        """POST /sync/pull - Handle incoming request to pull files from a source node.

        This is called by the leader to tell this node to pull files from another node.

        Request body:
        {
            "source_host": "192.168.1.100",
            "source_port": 8770,
            "source_node_id": "lambda-h100",
            "files": ["data/selfplay/sq8_2p/games_001.jsonl", ...]
        }
        """
        try:
            # Check disk capacity before accepting sync request
            has_capacity, disk_percent = check_disk_has_capacity()
            if not has_capacity:
                return web.json_response({
                    "error": f"Disk full ({disk_percent:.1f}% >= {MAX_DISK_USAGE_PERCENT}%)",
                    "disk_percent": disk_percent,
                    "threshold": MAX_DISK_USAGE_PERCENT
                }, status=507)  # 507 Insufficient Storage

            data = await request.json()
            source_node_id = data.get("source_node_id")
            files = data.get("files", [])

            if not source_node_id or not files:
                return web.json_response({
                    "error": "Missing required fields: source_node_id, files"
                }, status=400)

            # Prefer the local peer table for reachability (avoids leader guessing our routes).
            source_host = data.get("source_host")
            source_port = int(data.get("source_port", DEFAULT_PORT) or DEFAULT_PORT)
            with self.peers_lock:
                peer = self.peers.get(source_node_id)
            if source_node_id == self.node_id:
                peer = self.self_info
            if peer:
                source_host = peer.host
                source_port = peer.port

            if not source_host:
                return web.json_response({
                    "error": "Missing required fields: source_host (or unknown source_node_id)"
                }, status=400)

            logger.info(f"Received sync pull request: {len(files)} files from {source_node_id}")

            result = await self._handle_sync_pull_request(
                source_host=source_host,
                source_port=source_port,
                source_reported_host=(data.get("source_reported_host") or getattr(peer, "reported_host", "") or None),
                source_reported_port=(data.get("source_reported_port") or getattr(peer, "reported_port", 0) or None),
                source_node_id=source_node_id,
                files=files,
            )

            return web.json_response(result)
        except Exception as e:
            logger.error(f"in handle_sync_pull: {e}")
            import traceback
            traceback.print_exc()
            return web.json_response({"error": str(e)}, status=500)

    async def handle_sync_file(self, request: web.Request) -> web.StreamResponse:
        """GET /sync/file?path=<relative_path> - Stream a data file to a peer.

        Security:
        - Only serves files within `ai-service/data/**`.
        - Requires auth when RINGRIFT_CLUSTER_AUTH_TOKEN is set (even though it's a GET).
        """
        try:
            if self.auth_token and not self._is_request_authorized(request):
                return web.json_response({"error": "unauthorized"}, status=401)

            rel_path = (request.query.get("path") or "").lstrip("/")
            if not rel_path:
                return web.json_response({"error": "Missing required query param: path"}, status=400)

            data_dir = self.get_data_directory()
            data_dir.mkdir(parents=True, exist_ok=True)
            data_root = data_dir.resolve()
            full_path = (data_dir / rel_path)
            try:
                resolved = full_path.resolve()
                resolved.relative_to(data_root)
            except Exception:
                return web.json_response({"error": "Invalid path"}, status=400)

            if not resolved.exists() or not resolved.is_file():
                return web.json_response({"error": "Not found"}, status=404)

            stat = resolved.stat()
            resp = web.StreamResponse(
                status=200,
                headers={
                    "Content-Type": "application/octet-stream",
                    "Content-Length": str(stat.st_size),
                },
            )
            await resp.prepare(request)
            with open(resolved, "rb") as f:
                while True:
                    chunk = f.read(1024 * 1024)
                    if not chunk:
                        break
                    await resp.write(chunk)
            await resp.write_eof()
            return resp
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_sync_job_update(self, request: web.Request) -> web.Response:
        """POST /sync/job_update - Worker reports sync job status back to leader.

        Request body:
        {
            "job_id": "sync-123",
            "status": "completed|failed",
            "files_completed": 10,
            "bytes_transferred": 1048576,
            "error_message": "optional error message"
        }
        """
        try:
            data = await request.json()
            job_id = data.get("job_id")
            status = data.get("status")
            files_completed = data.get("files_completed", data.get("files_synced", 0))
            bytes_transferred = data.get("bytes_transferred", 0)
            error_message = data.get("error_message", data.get("error"))

            if not job_id or not status:
                return web.json_response({
                    "error": "Missing required fields: job_id, status"
                }, status=400)

            with self.sync_lock:
                if job_id in self.active_sync_jobs:
                    job = self.active_sync_jobs[job_id]
                    job.status = status
                    job.files_completed = int(files_completed or 0)
                    job.bytes_transferred = int(bytes_transferred or 0)
                    job.completed_at = time.time()
                    if error_message:
                        job.error_message = str(error_message)

                    logger.info(f"Sync job {job_id} {status}: {job.files_completed} files, {job.bytes_transferred} bytes")

                    # Update sync plan status if all jobs are done
                    if self.current_sync_plan:
                        all_done = all(
                            j.status in ("completed", "failed")
                            for j in self.current_sync_plan.sync_jobs
                        )
                        if all_done:
                            completed = sum(1 for j in self.current_sync_plan.sync_jobs if j.status == "completed")
                            failed = sum(1 for j in self.current_sync_plan.sync_jobs if j.status == "failed")
                            self.current_sync_plan.status = "completed" if failed == 0 else "partial"
                            self.current_sync_plan.completed_at = time.time()
                            self.sync_in_progress = False
                            self.last_sync_time = time.time()
                            logger.info(f"Cluster sync plan completed: {completed} succeeded, {failed} failed")

            return web.json_response({
                "success": True,
                "job_id": job_id,
                "status": status,
            })
        except Exception as e:
            logger.error(f"in handle_sync_job_update: {e}")
            return web.json_response({"error": str(e)}, status=500)

    async def _run_improvement_loop(self, job_id: str):
        """Main coordinator loop for AlphaZero-style improvement."""
        try:
            state = self.improvement_loop_state.get(job_id)
            if not state:
                return

            logger.info(f"Improvement loop coordinator started for job {job_id}")

            while state.current_iteration < state.max_iterations and state.status == "running":
                state.current_iteration += 1
                logger.info(f"Improvement iteration {state.current_iteration}/{state.max_iterations}")

                # Phase 1: Selfplay
                state.phase = "selfplay"
                state.selfplay_progress = {}
                await self._run_distributed_selfplay(job_id)

                # Phase 2: Export training data
                state.phase = "export"
                await self._export_training_data(job_id)

                # Phase 3: Training
                state.phase = "train"
                await self._run_training(job_id)

                # Phase 4: Evaluation
                state.phase = "evaluate"
                await self._run_evaluation(job_id)

                # Phase 5: Promote if better
                state.phase = "promote"
                await self._promote_model_if_better(job_id)

                state.last_update = time.time()

            state.status = "completed"
            state.phase = "idle"
            logger.info(f"Improvement loop {job_id} completed after {state.current_iteration} iterations")

        except Exception as e:
            logger.info(f"Improvement loop error: {e}")
            if job_id in self.improvement_loop_state:
                self.improvement_loop_state[job_id].status = f"error: {e}"

    async def _run_distributed_selfplay(self, job_id: str):
        """Coordinate distributed selfplay for improvement loop.

        Distributes selfplay games across all available workers.
        Each worker runs selfplay using the current best model and reports
        progress back to the coordinator.
        """

        state = self.improvement_loop_state.get(job_id)
        if not state:
            return

        # Distribute selfplay across workers
        num_workers = max(len(state.worker_nodes), 1)
        games_per_worker = state.games_per_iteration // num_workers
        remainder = state.games_per_iteration % num_workers

        logger.info(f"Starting distributed selfplay: {games_per_worker} games/worker, {num_workers} workers")

        # Create output directory for this iteration
        iteration_dir = os.path.join(
            self.ringrift_path, "ai-service", "data", "selfplay",
            f"improve_{job_id}", f"iter_{state.current_iteration}"
        )
        os.makedirs(iteration_dir, exist_ok=True)

        # Send selfplay tasks to workers
        tasks_sent = 0
        for idx, worker_id in enumerate(state.worker_nodes):
            with self.peers_lock:
                worker = self.peers.get(worker_id)
            if not worker or not worker.is_healthy():
                continue

            # Give first worker(s) the remainder games
            worker_games = games_per_worker + (1 if idx < remainder else 0)

            try:
                timeout = ClientTimeout(total=10)
                async with get_client_session(timeout) as session:
                    url = self._url_for_peer(worker, "/improvement/selfplay")
                    await session.post(url, json={
                        "job_id": job_id,
                        "iteration": state.current_iteration,
                        "num_games": worker_games,
                        "board_type": state.board_type,
                        "num_players": state.num_players,
                        "model_path": state.best_model_path,
                        "output_dir": iteration_dir,
                    }, headers=self._auth_headers())
                    tasks_sent += 1
            except Exception as e:
                logger.error(f"Failed to send selfplay task to {worker_id}: {e}")

        if tasks_sent == 0:
            # No workers available, run locally
            logger.info("No workers available, running selfplay locally")
            await self._run_local_selfplay(
                job_id, state.games_per_iteration,
                state.board_type, state.num_players,
                state.best_model_path, iteration_dir
            )
        else:
            # Wait for all workers to complete
            target_games = state.games_per_iteration
            check_interval = 5  # seconds
            timeout_seconds = 3600  # 1 hour max for selfplay phase
            elapsed = 0

            while elapsed < timeout_seconds and state.status == "running":
                total_done = sum(state.selfplay_progress.values())
                if total_done >= target_games:
                    break
                await asyncio.sleep(check_interval)
                elapsed += check_interval

            logger.info(f"Selfplay phase completed: {sum(state.selfplay_progress.values())} games")

    async def _run_local_selfplay(
        self, job_id: str, num_games: int, board_type: str,
        num_players: int, model_path: str | None, output_dir: str
    ):
        """Run selfplay locally using subprocess."""
        import sys

        output_file = os.path.join(output_dir, f"{self.node_id}_games.jsonl")

        # Build selfplay command
        cmd = [
            sys.executable,
            os.path.join(self.ringrift_path, "ai-service", "scripts", "run_self_play_soak.py"),
            "--num-games", str(num_games),
            "--board-type", board_type,
            "--num-players", str(num_players),
            "--engine-mode", "descent-only" if model_path else "heuristic-only",
            "--max-moves", "10000",  # LEARNED LESSONS - Avoid draws due to move limit
            "--log-jsonl", output_file,
        ]

        env = os.environ.copy()
        env["PYTHONPATH"] = os.path.join(self.ringrift_path, "ai-service")
        env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

        try:
            proc = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env,
            )

            _stdout, stderr = await asyncio.wait_for(
                proc.communicate(),
                timeout=3600  # 1 hour max
            )

            if proc.returncode == 0:
                logger.info(f"Local selfplay completed: {num_games} games")
                # Update progress
                if job_id in self.improvement_loop_state:
                    self.improvement_loop_state[job_id].selfplay_progress[self.node_id] = num_games
            else:
                logger.info(f"Local selfplay failed: {stderr.decode()[:500]}")

        except asyncio.TimeoutError:
            logger.info("Local selfplay timed out")
        except Exception as e:
            logger.info(f"Local selfplay error: {e}")

    async def _export_training_data(self, job_id: str):
        """Export training data from selfplay games.

        Converts JSONL game records to training format (HDF5 or NPZ).
        """
        import sys

        state = self.improvement_loop_state.get(job_id)
        if not state:
            return

        logger.info(f"Exporting training data for job {job_id}, iteration {state.current_iteration}")

        iteration_dir = os.path.join(
            self.ringrift_path, "ai-service", "data", "selfplay",
            f"improve_{job_id}", f"iter_{state.current_iteration}"
        )
        output_file = os.path.join(
            self.ringrift_path, "ai-service", "data", "training",
            f"improve_{job_id}", f"iter_{state.current_iteration}.npz"
        )

        # Ensure output directory exists
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        # Run export script
        export_script = f"""
import sys
sys.path.insert(0, '{self.ringrift_path}/ai-service')
import glob
import json
import numpy as np
from app.training.data_export import export_games_to_training_format

# Find all JSONL files from this iteration
jsonl_files = glob.glob('{iteration_dir}/*.jsonl')
print(f"Found {{len(jsonl_files)}} JSONL files")

games = []
for f in jsonl_files:
    with open(f) as fp:
        for line in fp:
            if line.strip():
                try:
                    games.append(json.loads(line))
                except (json.JSONDecodeError, ValueError):
                    pass

print(f"Loaded {{len(games)}} games")

if games:
    # Export to training format
    try:
        export_games_to_training_format(games, '{output_file}', '{state.board_type}')
        print(f"Exported to {output_file}")
    except Exception as e:
        # Fallback: save raw game data
        np.savez_compressed('{output_file}', games=games)
        print(f"Saved raw games to {output_file}")
else:
    print("No games to export")
"""

        cmd = [sys.executable, "-c", export_script]
        env = os.environ.copy()
        env["PYTHONPATH"] = os.path.join(self.ringrift_path, "ai-service")

        try:
            proc = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env,
            )

            _stdout, stderr = await asyncio.wait_for(
                proc.communicate(),
                timeout=600  # 10 minutes max
            )

            if proc.returncode == 0:
                logger.info("Training data export completed")
                state.training_data_path = output_file
            else:
                logger.info(f"Training data export failed: {stderr.decode()[:500]}")

        except asyncio.TimeoutError:
            logger.info("Training data export timed out")
        except Exception as e:
            logger.info(f"Training data export error: {e}")

    async def _run_training(self, job_id: str):
        """Run neural network training on GPU node.

        Finds a GPU worker and delegates training to it, or runs locally
        if this node has a GPU.
        """

        state = self.improvement_loop_state.get(job_id)
        if not state:
            return

        logger.info(f"Running training for job {job_id}, iteration {state.current_iteration}")

        # Find GPU worker
        gpu_worker = None
        with self.peers_lock:
            for peer in self.peers.values():
                if peer.has_gpu and peer.is_healthy():
                    gpu_worker = peer
                    break

        # Model output path
        new_model_path = os.path.join(
            self.ringrift_path, "ai-service", "models",
            f"improve_{job_id}", f"iter_{state.current_iteration}.pt"
        )
        os.makedirs(os.path.dirname(new_model_path), exist_ok=True)

        training_config = {
            "job_id": job_id,
            "iteration": state.current_iteration,
            "training_data": getattr(state, 'training_data_path', ''),
            "output_model": new_model_path,
            "board_type": state.board_type,
            "num_players": state.num_players,
            "epochs": 10,
            "batch_size": 256,
            "learning_rate": 0.001,
        }

        if gpu_worker and gpu_worker.node_id != self.node_id:
            # Delegate to GPU worker
            try:
                timeout = ClientTimeout(total=3600)  # 1 hour for training
                async with get_client_session(timeout) as session:
                    url = self._url_for_peer(gpu_worker, "/improvement/train")
                    async with session.post(url, json=training_config, headers=self._auth_headers()) as resp:
                        if resp.status == 200:
                            result = await resp.json()
                            if result.get("success"):
                                state.candidate_model_path = result.get("model_path", new_model_path)
                                logger.info(f"Training completed on {gpu_worker.node_id}")
                                return
            except Exception as e:
                logger.error(f"Failed to delegate training to {gpu_worker.node_id}: {e}")

        # Run training locally
        await self._run_local_training(training_config)
        state.candidate_model_path = new_model_path

    async def _run_local_training(self, config: dict):
        """Run training locally using subprocess."""
        import sys

        logger.info("Running local training")

        training_script = f"""
import sys
sys.path.insert(0, '{self.ringrift_path}/ai-service')
import numpy as np
import torch

# Load training data
try:
    data = np.load('{config.get("training_data", "")}', allow_pickle=True)
    print(f"Loaded training data")
except Exception as e:
    print(f"No training data available: {{e}}")
    # Create minimal model anyway
    data = None

# Import or create model architecture
try:
    from app.models.policy_value_net import PolicyValueNet
    model = PolicyValueNet(
        board_type='{config.get("board_type", "square8")}',
        num_players={config.get("num_players", 2)}
    )
except ImportError:
    # Fallback to simple model
    import torch.nn as nn
    model = nn.Sequential(
        nn.Linear(64, 256),
        nn.ReLU(),
        nn.Linear(256, 128),
        nn.ReLU(),
        nn.Linear(128, 64)
    )

# Save model
torch.save(model.state_dict(), '{config.get("output_model", "/tmp/model.pt")}')
print(f"Saved model to {config.get('output_model', '/tmp/model.pt')}")
"""

        cmd = [sys.executable, "-c", training_script]
        env = os.environ.copy()
        env["PYTHONPATH"] = os.path.join(self.ringrift_path, "ai-service")

        try:
            proc = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env,
            )

            stdout, stderr = await asyncio.wait_for(
                proc.communicate(),
                timeout=3600  # 1 hour max
            )

            logger.info(f"Training output: {stdout.decode()}")
            if proc.returncode != 0:
                logger.info(f"Training stderr: {stderr.decode()[:500]}")

        except asyncio.TimeoutError:
            logger.info("Local training timed out")
        except Exception as e:
            logger.info(f"Local training error: {e}")

    # ============================================
    # Phase 3: Training Pipeline Integration Methods
    # ============================================

    def _check_training_readiness(self) -> list[dict[str, Any]]:
        """Check cluster data manifest for training readiness.

        Returns list of training jobs that should be triggered based on
        accumulated selfplay data.

        Called periodically by leader to check if automatic training should start.
        """
        jobs_to_start = []

        if not self.cluster_data_manifest:
            return jobs_to_start

        current_time = time.time()
        thresholds = self.training_thresholds

        # Update adaptive thresholds based on current cluster state
        gpu_node_count = len([p for p in self.peers.values()
                              if getattr(p, 'has_gpu', False) and getattr(p, 'gpu_name', '')]
                             ) + (1 if getattr(self.self_info, 'has_gpu', False) else 0)
        thresholds.update_from_cluster_state(gpu_node_count)

        def _cooldown_ok(job_type: str, config_key: str) -> bool:
            cooldown = thresholds.get_effective_cooldown()
            if cooldown <= 0:
                return True
            last_seen = 0.0
            with self.training_lock:
                for job in self.training_jobs.values():
                    if str(getattr(job, "job_type", "")) != job_type:
                        continue
                    job_key = f"{job.board_type}_{job.num_players}p"
                    if job_key != config_key:
                        continue
                    last_seen = max(
                        last_seen,
                        float(getattr(job, "completed_at", 0.0) or 0.0),
                        float(getattr(job, "started_at", 0.0) or 0.0),
                        float(getattr(job, "created_at", 0.0) or 0.0),
                    )
            if last_seen <= 0:
                return True
            return (current_time - last_seen) >= cooldown

        # Check each board type / player count combination
        for config_key, config_data in self.cluster_data_manifest.by_board_type.items():
            parts = config_key.split("_")
            if len(parts) < 2:
                continue
            board_type = parts[0]
            num_players = int(parts[1].replace("p", ""))
            total_games = config_data.get("total_games", 0)

            # Check NNUE training threshold (using adaptive thresholds)
            if thresholds.auto_nnue_enabled:
                last_nnue_games = self.games_at_last_nnue_train.get(config_key, 0)
                min_games = thresholds.get_effective_min_games("nnue")
                incremental = thresholds.get_effective_incremental("nnue")
                if total_games >= min_games:
                    new_games = total_games - last_nnue_games
                    if new_games >= incremental or last_nnue_games == 0:
                        # Check cooldown
                        if not _cooldown_ok("nnue", config_key):
                            continue
                        existing_job = self._find_running_training_job("nnue", config_key)
                        if not existing_job:
                            jobs_to_start.append({
                                "job_type": "nnue",
                                "board_type": board_type,
                                "num_players": num_players,
                                "config_key": config_key,
                                "total_games": total_games,
                            })

            # Check CMA-ES optimization threshold (using adaptive thresholds)
            if thresholds.auto_cmaes_enabled:
                last_cmaes_games = self.games_at_last_cmaes_train.get(config_key, 0)
                min_games = thresholds.get_effective_min_games("cmaes")
                incremental = thresholds.get_effective_incremental("cmaes")
                if total_games >= min_games:
                    new_games = total_games - last_cmaes_games
                    if new_games >= incremental or last_cmaes_games == 0:
                        if not _cooldown_ok("cmaes", config_key):
                            continue
                        existing_job = self._find_running_training_job("cmaes", config_key)
                        if not existing_job:
                            jobs_to_start.append({
                                "job_type": "cmaes",
                                "board_type": board_type,
                                "num_players": num_players,
                                "config_key": config_key,
                                "total_games": total_games,
                            })

        return jobs_to_start

    def _find_running_training_job(self, job_type: str, config_key: str) -> TrainingJob | None:
        """Find a running training job of the given type for the config."""
        with self.training_lock:
            for job in self.training_jobs.values():
                if (job.job_type == job_type and
                    f"{job.board_type}_{job.num_players}p" == config_key and
                    job.status in ("pending", "queued", "running")):
                    return job
        return None

    def _find_resumable_training_job(self, job_type: str, config_key: str) -> TrainingJob | None:
        """Find a failed/interrupted training job with a valid checkpoint.

        TRAINING CHECKPOINTING: When a training job fails or is interrupted,
        this function finds it if it has a valid checkpoint that can be resumed.

        Returns:
            TrainingJob with valid checkpoint, or None
        """
        with self.training_lock:
            for job in self.training_jobs.values():
                if (job.job_type == job_type and
                    f"{job.board_type}_{job.num_players}p" == config_key and
                    job.status == "failed" and
                    job.checkpoint_path and
                    job.checkpoint_epoch > 0):
                    # Found a failed job with checkpoint
                    return job
        return None

    async def _dispatch_training_job(self, job_config: dict[str, Any]) -> TrainingJob | None:
        """Dispatch a training job to an appropriate worker.

        Finds a GPU node for NNUE training, or any available node for CMA-ES.
        Creates a TrainingJob and sends it to the worker.

        TRAINING CHECKPOINTING: If a failed job with checkpoint exists for this
        config, includes resume info in the dispatch.
        """
        job_type = job_config["job_type"]
        board_type = job_config["board_type"]
        num_players = job_config["num_players"]
        config_key = job_config["config_key"]

        # TRAINING CHECKPOINTING: Check for resumable failed job
        resumable = self._find_resumable_training_job(job_type, config_key)
        if resumable and not job_config.get("resume_checkpoint_path"):
            # Found a failed job with checkpoint - add resume info
            job_config["resume_checkpoint_path"] = resumable.checkpoint_path
            job_config["resume_epoch"] = resumable.checkpoint_epoch
            logger.info(f"Found resumable job {resumable.job_id} with checkpoint at epoch {resumable.checkpoint_epoch}")

        # Generate job ID
        job_id = f"{job_type}_{config_key}_{int(time.time())}"

        # Create TrainingJob
        job = TrainingJob(
            job_id=job_id,
            job_type=job_type,
            board_type=board_type,
            num_players=num_players,
            status="pending",
            data_games_count=job_config.get("total_games", 0),
        )

        # Find suitable worker (CPU/GPU-aware + load-balanced)
        self._update_self_info()
        with self.peers_lock:
            all_nodes = list(self.peers.values())
        all_nodes.append(self.self_info)
        # Filter for healthy nodes with sufficient memory
        healthy_nodes = [
            n for n in all_nodes
            if n.is_healthy() and int(getattr(n, "memory_gb", 0) or 0) >= MIN_MEMORY_GB_FOR_TASKS
        ]

        # Policy-based filtering: check if work type is allowed on each node
        policy_manager = None
        try:
            from app.coordination.node_policies import get_policy_manager
            policy_manager = get_policy_manager()
        except ImportError:
            pass

        # Determine work type for policy check
        policy_work_type = "training" if job_type == "nnue" else "cpu_cmaes"

        if policy_manager:
            # Filter nodes that allow this work type
            healthy_nodes = [
                n for n in healthy_nodes
                if policy_manager.is_work_allowed(n.node_id, policy_work_type)
            ]

        # Get set of nodes already running training jobs (for parallel training across configs)
        with self.training_lock:
            nodes_with_training = {
                job.worker_node for job in self.training_jobs.values()
                if job.status in ("pending", "queued", "running") and job.worker_node
            }

        worker_node: NodeInfo | None = None
        if job_type == "nnue":
            # NNUE training prefers accelerator nodes (CUDA/MPS).
            # Exclude nodes already running training to enable parallel training across configs
            gpu_nodes = [n for n in healthy_nodes if n.has_gpu and n.node_id not in nodes_with_training]
            if not gpu_nodes:
                # Fall back to allowing nodes with training if no free GPU nodes
                gpu_nodes = [n for n in healthy_nodes if n.has_gpu]
            gpu_nodes.sort(key=lambda n: (-n.gpu_power_score(), n.get_load_score()))
            worker_node = gpu_nodes[0] if gpu_nodes else None
        else:
            # CMA-ES is CPU-heavy. Prefer high-CPU nodes (vast nodes have 256-512 CPUs).
            # Use cpu_power_score() to prioritize vast nodes over lambda nodes.
            cpu_nodes = [n for n in healthy_nodes if n.is_cpu_only_node() and n.node_id not in nodes_with_training]
            if not cpu_nodes:
                cpu_nodes = [n for n in healthy_nodes if n.is_cpu_only_node()]
            candidates = cpu_nodes if cpu_nodes else healthy_nodes
            # Sort by CPU power (descending) then load score (ascending)
            candidates.sort(key=lambda n: (-n.cpu_power_score(), n.get_load_score()))
            worker_node = candidates[0] if candidates else None

        if not worker_node:
            logger.info(f"No suitable worker for {job_type} training job")
            return None

        job.worker_node = worker_node.node_id
        job.status = "queued"

        # Store job
        with self.training_lock:
            self.training_jobs[job_id] = job

        # Update games count at training start
        if job_type == "nnue":
            self.games_at_last_nnue_train[config_key] = job_config.get("total_games", 0)
        else:
            self.games_at_last_cmaes_train[config_key] = job_config.get("total_games", 0)

        # TRAINING CHECKPOINTING: Check for resumable job with checkpoint
        resume_checkpoint = job_config.get("resume_checkpoint_path", "")
        resume_epoch = job_config.get("resume_epoch", 0)
        if resume_checkpoint:
            job.checkpoint_path = resume_checkpoint
            job.checkpoint_epoch = resume_epoch
            job.resume_from_checkpoint = True
            logger.info(f"Resuming training from checkpoint: {resume_checkpoint} (epoch {resume_epoch})")

        # Send to worker
        try:
            endpoint = f"/training/{job_type}/start"
            timeout = ClientTimeout(total=30)
            async with get_client_session(timeout) as session:
                payload = {
                    "job_id": job_id,
                    "board_type": board_type,
                    "num_players": num_players,
                    "epochs": job.epochs,
                    "batch_size": job.batch_size,
                    "learning_rate": job.learning_rate,
                    # TRAINING CHECKPOINTING: Include resume info
                    "resume_checkpoint": resume_checkpoint,
                    "resume_epoch": resume_epoch,
                }
                last_err: str | None = None
                for url in self._urls_for_peer(worker_node, endpoint):
                    try:
                        async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                            if resp.status != 200:
                                last_err = f"http_{resp.status}"
                                continue
                            result = await resp.json()
                        if result.get("success"):
                            job.status = "running"
                            job.started_at = time.time()
                            logger.info(f"Started {job_type} training job {job_id} on {worker_node.node_id}")
                            self._save_state()
                            return job
                        job.status = "failed"
                        job.error_message = str(result.get("error") or "Unknown error")
                        return job
                    except Exception as e:
                        last_err = str(e)
                        continue
                job.status = "failed"
                job.error_message = last_err or "dispatch_failed"
        except Exception as e:
            job.status = "failed"
            job.error_message = str(e)
            logger.error(f"Failed to dispatch {job_type} training to {worker_node.node_id}: {e}")

        return job

    async def _check_and_trigger_training(self):
        """Periodic check for training readiness (leader only)."""
        if self.role != NodeRole.LEADER:
            return

        current_time = time.time()
        if current_time - self.last_training_check < self.training_check_interval:
            return

        self.last_training_check = current_time

        # Get jobs that should be started
        jobs_to_start = self._check_training_readiness()

        for job_config in jobs_to_start:
            logger.info(f"Auto-triggering {job_config['job_type']} training for {job_config['config_key']} ({job_config['total_games']} games)")
            await self._dispatch_training_job(job_config)

    async def _check_local_training_fallback(self):
        """DECENTRALIZED training trigger when cluster has no leader.

        LEADERLESS RESILIENCE: When the cluster has been without a leader for too long
        (LEADERLESS_TRAINING_TIMEOUT = 3 minutes), individual nodes can trigger local
        training to prevent data accumulation without progress.

        This makes the system more resilient to leader election failures while avoiding
        duplicate training by:
        1. Only triggering after a brief leaderless period (3 minutes)
        2. Using random jitter so nodes don't all train simultaneously
        3. Only training on local data (no cluster-wide coordination needed)
        4. Using reasonable cooldowns between fallback training runs
        """
        # Skip if we ARE the leader or have a known leader
        if self.role == NodeRole.LEADER or self.leader_id:
            self.last_leader_seen = time.time()  # Update leader seen time
            return

        current_time = time.time()
        leaderless_duration = current_time - self.last_leader_seen

        # Only trigger fallback if leaderless for the timeout period
        if leaderless_duration < LEADERLESS_TRAINING_TIMEOUT:
            return

        # Rate limit fallback training (10 minute cooldown - more aggressive than before)
        fallback_cooldown = 600  # 10 minutes between fallback triggers
        if current_time - self.last_local_training_fallback < fallback_cooldown:
            return

        # Random jitter: 40% probability per check (more aggressive than 20%)
        # This distributes training across nodes over time
        import random
        if random.random() > 0.4:
            return

        # Check if we have a GPU (training needs GPU)
        if not getattr(self.self_info, "has_gpu", False):
            return

        # Check local data manifest (use cached version for speed)
        local_manifest = getattr(self, "local_data_manifest", None)
        if not local_manifest:
            # Try to load from cache or collect if we don't have one
            try:
                local_manifest = self._collect_local_data_manifest_cached(max_cache_age=600)
                with self.manifest_lock:
                    self.local_data_manifest = local_manifest
            except Exception:
                return

        # Check for sufficient local data (lower threshold for faster training)
        min_games_fallback = 2000  # Lower threshold for faster response
        total_local_games = getattr(local_manifest, "selfplay_games", 0)
        if total_local_games < min_games_fallback:
            return

        # Find board types with enough local data
        game_counts_by_type: dict[str, int] = {}
        for file_info in getattr(local_manifest, "files", []) or []:
            board_type = getattr(file_info, "board_type", "")
            num_players = getattr(file_info, "num_players", 2)
            game_count = getattr(file_info, "game_count", 0)
            if board_type and game_count > 0:
                key = f"{board_type}_{num_players}p"
                game_counts_by_type[key] = game_counts_by_type.get(key, 0) + game_count

        # Sort by game count (descending) to train on richest data first
        sorted_configs = sorted(game_counts_by_type.items(), key=lambda x: x[1], reverse=True)

        # Trigger local training for configurations with enough data
        triggered_count = 0
        max_concurrent_fallback = 2  # Can trigger up to 2 training jobs per fallback
        for config_key, game_count in sorted_configs:
            if triggered_count >= max_concurrent_fallback:
                break
            if game_count < 1000:  # Minimum threshold (lowered)
                continue

            # Check if we already have a running training job for this config
            existing_job = self._find_running_training_job("nnue", config_key)
            if existing_job:
                continue

            # DISTRIBUTED TRAINING COORDINATION: Check cluster-wide before starting
            is_training, _training_nodes = self._is_config_being_trained_cluster_wide(config_key)
            if is_training:
                # Someone else is already training this config
                continue

            # Use distributed slot claiming to avoid race conditions
            if not self._should_claim_training_slot(config_key):
                continue

            # Parse board type and player count
            parts = config_key.split("_")
            if len(parts) < 2:
                continue
            board_type = parts[0]
            num_players = int(parts[1].replace("p", ""))

            logger.info(f"DISTRIBUTED TRAINING: Claiming {config_key} ({game_count} local games, leaderless for {int(leaderless_duration)}s)")
            job_config = {
                "job_type": "nnue",
                "board_type": board_type,
                "num_players": num_players,
                "config_key": config_key,
                "total_games": game_count,
            }
            await self._dispatch_training_job(job_config)
            triggered_count += 1

        if triggered_count > 0:
            self.last_local_training_fallback = current_time
            logger.info(f"LEADERLESS FALLBACK: Triggered {triggered_count} local training job(s)")

    async def _check_improvement_cycles(self):
        """Periodic check for improvement cycle readiness (leader only).

        This integrates with the ImprovementCycleManager to:
        1. Check if any cycles need training based on data thresholds
        2. Trigger export/training jobs for ready cycles
        3. Run evaluations and update Elo ratings
        4. Schedule CMA-ES optimization when needed
        5. Schedule diverse tournaments for AI calibration
        """
        if self.role != NodeRole.LEADER:
            return

        if not self.improvement_cycle_manager:
            return

        current_time = time.time()
        if current_time - self.last_improvement_cycle_check < self.improvement_cycle_check_interval:
            return

        self.last_improvement_cycle_check = current_time

        # Check which cycles are ready for training
        training_ready = self.improvement_cycle_manager.check_training_needed()

        # Convert to job configs
        jobs_to_start = []
        for board_type, num_players in training_ready:
            cycle_key = f"{board_type}_{num_players}p"
            cycle_state = self.improvement_cycle_manager.state.cycles.get(cycle_key)
            if cycle_state and self.improvement_cycle_manager.trigger_training(board_type, num_players):
                jobs_to_start.append({
                    "cycle_id": cycle_key,
                    "board_type": board_type,
                    "num_players": num_players,
                    "total_games": cycle_state.games_since_last_training,
                    "iteration": cycle_state.current_iteration + 1,
                })

        # Also check for CMA-ES optimization opportunities
        cmaes_ready = self.improvement_cycle_manager.check_cmaes_needed()
        for board_type, num_players in cmaes_ready:
            # Trigger distributed CMA-ES
            logger.info(f"CMA-ES optimization ready for {board_type}_{num_players}p")
            asyncio.create_task(self._trigger_auto_cmaes(board_type, num_players))

        # Check for rollback needs (consecutive training failures)
        for key, cycle in self.improvement_cycle_manager.state.cycles.items():
            if not cycle.pending_training and not cycle.pending_evaluation:
                should_rollback, reason = self.improvement_cycle_manager.check_rollback_needed(
                    cycle.board_type, cycle.num_players
                )
                if should_rollback:
                    logger.info(f"ROLLBACK NEEDED for {key}: {reason}")
                    if self.improvement_cycle_manager.execute_rollback(cycle.board_type, cycle.num_players):
                        self.diversity_metrics["rollbacks"] += 1
                        # Increase diversity to escape plateau
                        logger.info(f"Increasing diversity to escape training plateau for {key}")

        for job_config in jobs_to_start:
            cycle_id = job_config["cycle_id"]
            board_type = job_config["board_type"]
            num_players = job_config["num_players"]

            logger.info(f"ImprovementCycle {cycle_id}: Starting training "
                  f"({job_config['total_games']} games)")

            # Find GPU worker for training
            gpu_worker = None
            candidates: list[NodeInfo] = []
            with self.peers_lock:
                candidates.extend([p for p in self.peers.values() if p.is_gpu_node() and p.is_healthy()])
            if self.self_info.is_gpu_node() and self.self_info.is_healthy():
                candidates.append(self.self_info)
            if candidates:
                candidates.sort(
                    key=lambda p: (-p.gpu_power_score(), p.get_load_score(), str(p.node_id))
                )
                gpu_worker = candidates[0]

            if not gpu_worker:
                logger.info(f"ImprovementCycle {cycle_id}: No GPU worker available, deferring")
                self.improvement_cycle_manager.update_cycle_phase(
                    cycle_id, "idle", error_message="No GPU worker available"
                )
                continue

            # Create training job
            job_id = f"cycle_{cycle_id}_{int(time.time())}"
            training_job = TrainingJob(
                job_id=job_id,
                job_type="nnue",
                board_type=board_type,
                num_players=num_players,
                worker_node=gpu_worker.node_id,
                epochs=job_config.get("epochs", 100),
                batch_size=job_config.get("batch_size", 4096),
                learning_rate=job_config.get("learning_rate", 0.001),
                data_games_count=job_config.get("total_games", 0),
            )

            with self.training_lock:
                self.training_jobs[job_id] = training_job

            # Update cycle state
            self.improvement_cycle_manager.update_cycle_phase(
                cycle_id, "training", training_job_id=job_id
            )

            # Dispatch training to worker
            await self._dispatch_improvement_training(training_job, cycle_id)

    async def _dispatch_improvement_training(self, job: TrainingJob, cycle_id: str):
        """Dispatch training job for improvement cycle."""
        try:
            # Find the worker node
            worker_node = None
            if job.worker_node == self.node_id:
                worker_node = self.self_info
            else:
                with self.peers_lock:
                    worker_node = self.peers.get(job.worker_node)

            if not worker_node:
                logger.info(f"ImprovementCycle {cycle_id}: Worker {job.worker_node} not found")
                self.improvement_cycle_manager.update_cycle_phase(
                    cycle_id, "idle", error_message=f"Worker {job.worker_node} not found"
                )
                return

            # Build training payload
            payload = {
                "job_id": job.job_id,
                "cycle_id": cycle_id,
                "board_type": job.board_type,
                "num_players": job.num_players,
                "epochs": job.epochs,
                "batch_size": job.batch_size,
                "learning_rate": job.learning_rate,
            }

            # Send to worker
            timeout = ClientTimeout(total=30)
            async with get_client_session(timeout) as session:
                last_err: str | None = None
                for url in self._urls_for_peer(worker_node, "/training/nnue/start"):
                    try:
                        async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                            if resp.status != 200:
                                last_err = f"http_{resp.status}"
                                continue
                            result = await resp.json()
                        if result.get("success"):
                            job.status = "running"
                            job.started_at = time.time()
                            logger.info(f"ImprovementCycle {cycle_id}: Training started on {worker_node.node_id}")
                            return
                        self.improvement_cycle_manager.update_cycle_phase(
                            cycle_id, "idle", error_message=result.get("error", "Training failed to start")
                        )
                        return
                    except Exception as e:
                        last_err = str(e)
                        continue
                self.improvement_cycle_manager.update_cycle_phase(
                    cycle_id, "idle", error_message=last_err or "dispatch_failed"
                )

        except Exception as e:
            logger.info(f"ImprovementCycle {cycle_id}: Training dispatch failed: {e}")
            self.improvement_cycle_manager.update_cycle_phase(
                cycle_id, "idle", error_message=str(e)
            )

    # Phase 3 HTTP Handlers

    async def handle_training_start(self, request: web.Request) -> web.Response:
        """Handle request to start a training job (from external or leader)."""
        try:
            data = await request.json()
            job_type = data.get("job_type", "nnue")
            board_type = data.get("board_type", "square8")
            num_players = data.get("num_players", 2)

            if self.role != NodeRole.LEADER:
                return web.json_response({
                    "success": False,
                    "error": "Only leader can dispatch training jobs"
                })

            job_config = {
                "job_type": job_type,
                "board_type": board_type,
                "num_players": num_players,
                "config_key": f"{board_type}_{num_players}p",
                "total_games": data.get("total_games", 0),
            }

            job = await self._dispatch_training_job(job_config)
            if job:
                return web.json_response({
                    "success": True,
                    "job_id": job.job_id,
                    "worker": job.worker_node,
                })
            else:
                return web.json_response({
                    "success": False,
                    "error": "No suitable worker available"
                })

        except Exception as e:
            return web.json_response({"success": False, "error": str(e)})

    async def handle_training_status(self, request: web.Request) -> web.Response:
        """Return status of all training jobs."""
        with self.training_lock:
            jobs = [job.to_dict() for job in self.training_jobs.values()]

        return web.json_response({
            "success": True,
            "jobs": jobs,
            "thresholds": self.training_thresholds.to_dict(),
        })

    async def handle_training_update(self, request: web.Request) -> web.Response:
        """Handle training progress/completion update from worker."""
        try:
            data = await request.json()
            job_id = data.get("job_id")

            with self.training_lock:
                job = self.training_jobs.get(job_id)
                if not job:
                    return web.json_response({
                        "success": False,
                        "error": f"Job {job_id} not found"
                    })

                # Update job status
                if data.get("status"):
                    job.status = data["status"]
                if data.get("completed"):
                    job.status = "completed"
                    job.completed_at = time.time()
                if data.get("output_model_path"):
                    job.output_model_path = data["output_model_path"]
                if data.get("final_loss"):
                    job.final_loss = data["final_loss"]
                if data.get("final_accuracy"):
                    job.final_accuracy = data["final_accuracy"]
                if data.get("error"):
                    job.status = "failed"
                    job.error_message = data["error"]
                    # ALERTING: Notify on training failure
                    asyncio.create_task(self.notifier.send(
                        title="Training Job Failed",
                        message=f"Training job {job.job_id} failed: {data['error'][:100]}",
                        level="error",
                        fields={
                            "Job ID": job.job_id,
                            "Type": job.job_type,
                            "Config": f"{job.board_type}_{job.num_players}p",
                            "Worker": job.worker_node or "unknown",
                            "Error": data["error"][:200],
                            "Checkpoint": job.checkpoint_path or "none",
                        },
                        node_id=self.node_id,
                    ))

                # TRAINING CHECKPOINTING: Track checkpoint progress
                if data.get("checkpoint_path"):
                    job.checkpoint_path = data["checkpoint_path"]
                    job.checkpoint_updated_at = time.time()
                if data.get("checkpoint_epoch"):
                    job.checkpoint_epoch = int(data["checkpoint_epoch"])
                if data.get("checkpoint_loss"):
                    job.checkpoint_loss = float(data["checkpoint_loss"])

                # Check if we should trigger evaluation after training
                should_trigger_eval = (
                    data.get("completed") and
                    job.output_model_path and
                    self.improvement_cycle_manager
                )

            self._save_state()

            # Auto-trigger tournament evaluation when training completes
            if should_trigger_eval:
                asyncio.create_task(self._handle_training_job_completion(job))

            return web.json_response({"success": True})

        except Exception as e:
            return web.json_response({"success": False, "error": str(e)})

    async def _handle_training_job_completion(self, job: TrainingJob) -> None:
        """Handle training job completion - run gauntlet, notify cycle manager, trigger evaluation.

        This method bridges the training completion with the improvement cycle:
        1. Runs immediate gauntlet evaluation against median model
        2. Archives model if gauntlet fails (< 50% win rate vs median)
        3. Notifies improvement_cycle_manager of training completion
        4. Schedules a model comparison tournament
        """
        if not self.improvement_cycle_manager:
            return

        try:
            logger.info(f"Training job {job.job_id} completed, triggering evaluation")

            # NEW: Run immediate gauntlet evaluation
            passed = await self._run_post_training_gauntlet(job)

            if not passed:
                # Archive model that failed gauntlet
                await self._archive_failed_model(
                    job.output_model_path,
                    job.board_type,
                    job.num_players,
                    reason="failed_post_training_gauntlet"
                )
                logger.info("Model archived: failed post-training gauntlet (< 50% vs median)")
                return  # Don't proceed with tournament scheduling

            # Notify improvement cycle manager
            self.improvement_cycle_manager.handle_training_complete(
                job.board_type,
                job.num_players,
                job.output_model_path,
                job.data_games_count or 0
            )

            # Schedule model comparison tournament
            await self._schedule_model_comparison_tournament(job)

        except Exception as e:
            logger.error(f"handling training completion for {job.job_id}: {e}")

    async def _schedule_model_comparison_tournament(self, job: TrainingJob) -> None:
        """Schedule a tournament to compare the new model against baseline."""
        if not job.output_model_path:
            return

        try:
            # Get tournament matchups from cycle manager
            matchups = self.improvement_cycle_manager.get_tournament_matchups(
                job.board_type,
                job.num_players,
                new_model_path=job.output_model_path
            )

            if not matchups:
                logger.info(f"No tournament matchups for {job.board_type}_{job.num_players}p")
                return

            logger.info(f"Scheduling {len(matchups)} tournament matchups for new model")

            # Run evaluation games (simplified - in production would dispatch to workers)
            total_games = 0

            for matchup in matchups:
                if matchup.get("purpose") == "primary_evaluation":
                    # Primary evaluation against best model
                    games = matchup.get("games", 20)
                    total_games += games
                    # Placeholder: actual tournament execution would go here
                    # For now, mark as needing external evaluation
                    logger.info(f"Tournament: {matchup['agent_a']} vs {matchup['agent_b']} ({games} games)")

            # Update cycle state - evaluation is now pending
            cycle_key = f"{job.board_type}_{job.num_players}p"
            if cycle_key in self.improvement_cycle_manager.state.cycles:
                self.improvement_cycle_manager.state.cycles[cycle_key].pending_evaluation = True
                self.improvement_cycle_manager._save_state()

        except Exception as e:
            logger.error(f"scheduling tournament: {e}")

    # =========================================================================
    # POST-TRAINING GAUNTLET: Immediate evaluation after training
    # =========================================================================

    def _get_median_model(self, config_key: str) -> str | None:
        """Get the median-rated model for a config from ELO database.

        Returns the model_id at the 50th percentile by rating, or None if
        no models exist for this config.
        """
        elo_db_path = Path(self.ringrift_path) / "ai-service" / "data" / "unified_elo.db"
        if not elo_db_path.exists():
            return None

        # Parse config_key like "square8_2p"
        parts = config_key.rsplit("_", 1)
        if len(parts) != 2:
            return None
        board_type = parts[0]
        num_players = int(parts[1].rstrip("p"))

        try:
            import sqlite3
            conn = sqlite3.connect(str(elo_db_path))
            cursor = conn.cursor()
            cursor.execute("""
                SELECT participant_id FROM elo_ratings
                WHERE board_type = ? AND num_players = ? AND archived_at IS NULL
                ORDER BY rating
            """, (board_type, num_players))
            rows = cursor.fetchall()
            conn.close()

            if not rows:
                return None

            # Return median model (middle of sorted list)
            median_idx = len(rows) // 2
            return rows[median_idx][0]
        except Exception as e:
            logger.error(f"getting median model: {e}")
            return None

    async def _run_post_training_gauntlet(self, job: TrainingJob) -> bool:
        """Run quick gauntlet evaluation for newly trained model.

        Model must beat the median-rated model with 50%+ win rate to pass.
        Runs 8 games total (4 as player 1, 4 as player 2) for fairness.

        OPTIMIZATION: Dispatches gauntlet to CPU-rich nodes (Vast instances)
        to avoid blocking GPU nodes. Falls back to local execution if no
        CPU nodes are available.

        Returns True if model passes, False if it should be archived.
        """
        # Check for skip flag
        if os.environ.get("RINGRIFT_SKIP_POST_TRAINING_GAUNTLET", "0") == "1":
            logger.info("Post-training gauntlet skipped (RINGRIFT_SKIP_POST_TRAINING_GAUNTLET=1)")
            return True

        config_key = f"{job.board_type}_{job.num_players}p"
        model_path = job.output_model_path

        if not model_path or not os.path.exists(model_path):
            logger.info(f"Model path not found: {model_path}, skipping gauntlet")
            return True

        model_id = os.path.splitext(os.path.basename(model_path))[0]

        # Get median model from ELO database
        median_model = self._get_median_model(config_key)
        if not median_model:
            logger.info(f"No median model for {config_key}, skipping gauntlet")
            return True  # Pass if no baseline to compare against

        logger.info(f"Running post-training gauntlet: {model_id} vs {median_model} (median)")

        GAMES_PER_SIDE = 4

        # OPTIMIZATION: Try to dispatch gauntlet to a CPU-rich node first
        # This keeps GPU nodes free for training while Vast nodes handle evaluation
        try:
            remote_result = await self._dispatch_gauntlet_to_cpu_node(
                config_key=config_key,
                model_id=model_id,
                baseline_id=median_model,
                games_per_side=GAMES_PER_SIDE,
            )
            if remote_result and remote_result.get("success"):
                # Remote gauntlet completed successfully
                wins = remote_result.get("wins", 0)
                total_games = remote_result.get("total_games", 0)
                win_rate = remote_result.get("win_rate", 0)
                passed = remote_result.get("passed", False)
                remote_node = remote_result.get("node_id", "unknown")
                logger.info(f"Post-training gauntlet (remote on {remote_node}): "
                      f"{wins}/{total_games} ({win_rate:.1%}) {'PASSED' if passed else 'FAILED'}")
                return passed
        except Exception as e:
            logger.info(f"Remote gauntlet dispatch failed: {e}, falling back to local")

        # Fallback: Run locally if remote dispatch failed or we're the best CPU node
        model_dir = Path(self.ringrift_path) / "ai-service" / "models"

        wins = 0
        total_games = 0

        loop = asyncio.get_event_loop()

        for game_num in range(GAMES_PER_SIDE * 2):
            try:
                # First 4 games: new model as player 1
                # Last 4 games: new model as player 2
                if game_num < GAMES_PER_SIDE:
                    result = await loop.run_in_executor(
                        None,
                        self._run_gauntlet_game_sync,
                        f"gauntlet_{game_num}", model_id, median_model,
                        job.board_type, job.num_players, model_dir
                    )
                    if result.get("model_won"):
                        wins += 1
                else:
                    result = await loop.run_in_executor(
                        None,
                        self._run_gauntlet_game_sync,
                        f"gauntlet_{game_num}", median_model, model_id,
                        job.board_type, job.num_players, model_dir
                    )
                    # When new model is "baseline", baseline_won means we won
                    if result.get("baseline_won"):
                        wins += 1
                total_games += 1
            except Exception as e:
                logger.info(f"Gauntlet game {game_num} error: {e}")
                total_games += 1  # Count as played but not won

        win_rate = wins / total_games if total_games > 0 else 0

        # Pass criteria: beat median with 50%+ win rate
        MIN_WIN_RATE = 0.50
        passed = win_rate >= MIN_WIN_RATE

        logger.info(f"Post-training gauntlet vs median (local): {wins}/{total_games} "
              f"({win_rate:.1%}) {'PASSED' if passed else 'FAILED'}")

        return passed

    async def _archive_failed_model(self, model_path: str, board_type: str,
                                     num_players: int, reason: str) -> None:
        """Archive a model that failed gauntlet evaluation.

        Moves the model file to models/archived/{config_key}/ and updates
        the ELO database to mark it as archived.
        """
        if not model_path or not os.path.exists(model_path):
            return

        config_key = f"{board_type}_{num_players}p"
        archive_dir = os.path.join(self.ringrift_path, "ai-service", "models",
                                   "archived", config_key)
        os.makedirs(archive_dir, exist_ok=True)

        # Move model to archive
        model_name = os.path.basename(model_path)
        archive_path = os.path.join(archive_dir, model_name)

        try:
            shutil.move(model_path, archive_path)
            logger.info(f"Archived {model_name} to {archive_dir} ({reason})")
        except Exception as e:
            logger.error(f"moving model to archive: {e}")
            return

        # Update ELO database to mark as archived
        model_id = os.path.splitext(model_name)[0]
        elo_db_path = Path(self.ringrift_path) / "ai-service" / "data" / "unified_elo.db"

        if elo_db_path.exists():
            try:
                import sqlite3
                conn = sqlite3.connect(str(elo_db_path))
                cursor = conn.cursor()
                cursor.execute("""
                    UPDATE elo_ratings
                    SET archived_at = ?, archive_reason = ?
                    WHERE participant_id = ? AND board_type = ? AND num_players = ?
                """, (time.time(), reason, model_id, board_type, num_players))
                conn.commit()
                conn.close()
            except Exception as e:
                logger.error(f"updating ELO database for archived model: {e}")

    async def handle_nnue_start(self, request: web.Request) -> web.Response:
        """Handle NNUE training start request (worker endpoint)."""
        try:
            data = await request.json()
            job_id = data.get("job_id")
            board_type = data.get("board_type", "square8")
            num_players = data.get("num_players", 2)
            epochs = data.get("epochs", 100)
            batch_size = data.get("batch_size", 4096)
            learning_rate = data.get("learning_rate", None)

            # Start NNUE training subprocess
            output_path = os.path.join(
                self.ringrift_path, "ai-service", "models", "nnue",
                f"{board_type}_{num_players}p_auto.pt"
            )
            os.makedirs(os.path.dirname(output_path), exist_ok=True)

            # Collect local selfplay databases. The NNUE trainer requires at
            # least one DB (it can replay moves when snapshots are absent).
            data_dir = self.get_data_directory()
            board_tokens = [str(board_type).lower()]
            if "hex" in board_tokens[0]:
                board_tokens = ["hexagonal", "hex"]
            players_token = f"_{int(num_players)}p"

            candidate_dbs: list[Path] = []
            for pattern in ("selfplay/**/*.db", "games/**/*.db"):
                for db_path in data_dir.glob(pattern):
                    if not db_path.is_file():
                        continue
                    path_lower = str(db_path).lower()
                    if players_token not in path_lower:
                        continue
                    if not any(tok in path_lower for tok in board_tokens):
                        continue
                    candidate_dbs.append(db_path)

            # Fallback: if naming conventions differ, use any selfplay DBs.
            if not candidate_dbs:
                candidate_dbs = [p for p in data_dir.glob("selfplay/**/*.db") if p.is_file()]

            # De-dupe + prefer newest DBs (avoid overlong argv on large clusters).
            unique_dbs = list({p.resolve() for p in candidate_dbs})
            unique_dbs.sort(key=lambda p: p.stat().st_mtime if p.exists() else 0.0, reverse=True)
            max_dbs = 64
            unique_dbs = unique_dbs[:max_dbs]

            if not unique_dbs:
                return web.json_response(
                    {
                        "success": False,
                        "error": f"No selfplay DBs found under {data_dir} for {board_type} {num_players}p",
                    },
                    status=400,
                )

            cmd = [
                sys.executable, "-m", "scripts.train_nnue",
                "--db", *[str(p) for p in unique_dbs],
                "--board-type", board_type,
                "--num-players", str(num_players),
                "--epochs", str(epochs),
                "--batch-size", str(batch_size),
                "--save-path", output_path,
                # Phase 1: Core Training Optimizations
                "--spectral-norm",  # Gradient stability
                "--cyclic-lr", "--cyclic-lr-period", "5",  # Cyclic LR
                "--mixed-precision", "--amp-dtype", "bfloat16",  # BF16 speed
                "--warmup-epochs", "3",  # LR warmup
                # Phase 2: Advanced Training
                "--value-whitening",  # Value head stability
                "--ema",  # Exponential Moving Average
                "--stochastic-depth", "--stochastic-depth-prob", "0.1",
                "--adaptive-warmup",  # Dataset-aware warmup
                "--hard-example-mining", "--hard-example-top-k", "0.3",
                # Phase 2: Optimizer Enhancements
                "--lookahead", "--lookahead-k", "5", "--lookahead-alpha", "0.5",
                "--adaptive-clip",  # Adaptive gradient clipping
                "--board-nas",  # Board-specific NAS
                "--online-bootstrap", "--bootstrap-temperature", "1.5",
                # Phase 2: Data Pipeline
                "--prefetch-gpu",  # GPU prefetching
                "--difficulty-curriculum",  # Curriculum learning
                "--quantized-eval",  # Fast validation
                # Phase 3: Advanced Learning
                "--grokking-detection",  # Detect delayed generalization
                "--policy-label-smoothing", "0.05",  # Prevent overconfidence
                "--sampling-weights", "victory_type",  # Balanced sampling
                # Phase 4: Training Stability (optional, enabled for production)
                "--adaptive-accumulation",  # Dynamic gradient accumulation
                # Phase 5: Production Optimization (selective)
                "--dynamic-loss-scaling",  # Adaptive FP16 loss scaling
            ]
            # Add hex symmetry augmentation for hex boards (12x effective data)
            if board_type in ('hex8', 'hexagonal', 'hex'):
                cmd.append("--augment-hex-symmetry")
            # Add profiling for debug jobs
            if os.environ.get("RINGRIFT_PROFILE_TRAINING"):
                cmd.extend(["--profile", "--profile-dir", str(Path(output_path).parent / "profile")])
            if learning_rate is not None:
                cmd.extend(["--learning-rate", str(learning_rate)])

            env = os.environ.copy()
            env["PYTHONPATH"] = os.path.join(self.ringrift_path, "ai-service")

            proc = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env,
                cwd=os.path.join(self.ringrift_path, "ai-service"),
            )

            logger.info(f"Started NNUE training subprocess (PID {proc.pid}) for job {job_id}")

            # Don't wait - let it run in background
            asyncio.create_task(self._monitor_training_process(job_id, proc, output_path))

            return web.json_response({
                "success": True,
                "pid": proc.pid,
            })

        except Exception as e:
            return web.json_response({"success": False, "error": str(e)})

    async def _trigger_auto_cmaes(self, board_type: str, num_players: int):
        """Automatically trigger CMA-ES optimization for a configuration.

        Called by improvement cycle manager when optimization is due.
        """
        try:
            job_id = f"auto_cmaes_{board_type}_{num_players}p_{int(time.time())}"
            logger.info(f"Auto-triggering CMA-ES: {job_id}")

            # Check for GPU workers
            gpu_workers = []
            with self.peers_lock:
                for peer in self.peers.values():
                    if peer.is_healthy() and peer.has_gpu and peer.node_id != self.node_id:
                        gpu_workers.append(peer)

            if self.self_info.has_gpu:
                gpu_workers.append(self.self_info)

            if len(gpu_workers) >= 2:
                # DISTRIBUTED MODE
                cmaes_job_id = f"cmaes_auto_{job_id}"
                state = DistributedCMAESState(
                    job_id=cmaes_job_id,
                    board_type=board_type,
                    num_players=num_players,
                    generations=100,
                    population_size=max(32, len(gpu_workers) * 8),
                    games_per_eval=100,
                    status="running",
                    started_at=time.time(),
                    last_update=time.time(),
                    worker_nodes=[w.node_id for w in gpu_workers],
                )
                self.distributed_cmaes_state[cmaes_job_id] = state
                asyncio.create_task(self._run_distributed_cmaes(cmaes_job_id))
                logger.info(f"Started distributed CMA-ES with {len(gpu_workers)} workers")
            else:
                # LOCAL MODE - use GPU CMA-ES script
                output_dir = os.path.join(
                    self.ringrift_path, "ai-service", "data", "cmaes",
                    f"{board_type}_{num_players}p_auto_{int(time.time())}"
                )
                os.makedirs(output_dir, exist_ok=True)

                cmd = [
                    sys.executable,
                    os.path.join(self.ringrift_path, "ai-service", "scripts", "run_gpu_cmaes.py"),
                    "--board", board_type,
                    "--num-players", str(num_players),
                    "--generations", "100",
                    "--population-size", "32",
                    "--games-per-eval", "100",
                    "--max-moves", "10000",
                    "--output-dir", output_dir,
                    "--multi-gpu",
                ]

                env = os.environ.copy()
                env["PYTHONPATH"] = os.path.join(self.ringrift_path, "ai-service")
                env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

                proc = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                    env=env,
                )
                logger.info(f"Started local CMA-ES optimization (PID {proc.pid})")

        except Exception as e:
            logger.info(f"Auto CMA-ES trigger failed: {e}")

    async def handle_cmaes_start_auto(self, request: web.Request) -> web.Response:
        """Handle CMA-ES optimization start request.

        Uses distributed GPU CMA-ES across all cluster GPU nodes for maximum throughput.
        Falls back to local GPU CMA-ES if no remote workers available.
        """
        try:
            data = await request.json()
            job_id = data.get("job_id")
            board_type = data.get("board_type", "square8")
            num_players = data.get("num_players", 2)

            # Check for available GPU workers in the cluster
            gpu_workers = []
            with self.peers_lock:
                for peer in self.peers.values():
                    if peer.is_healthy() and peer.has_gpu and peer.node_id != self.node_id:
                        gpu_workers.append(peer)

            # Include self if we have GPU
            if self.self_info.has_gpu:
                gpu_workers.append(self.self_info)

            if len(gpu_workers) >= 2:
                # DISTRIBUTED MODE: Use P2P distributed CMA-ES across cluster
                logger.info(f"Starting DISTRIBUTED GPU CMA-ES with {len(gpu_workers)} workers")

                # Create distributed CMA-ES state
                cmaes_job_id = f"cmaes_auto_{job_id}_{int(time.time())}"
                state = DistributedCMAESState(
                    job_id=cmaes_job_id,
                    board_type=board_type,
                    num_players=num_players,
                    generations=100,  # More generations for better optimization
                    population_size=max(32, len(gpu_workers) * 8),  # Scale with workers
                    games_per_eval=100,  # More games for accurate fitness
                    status="running",
                    started_at=time.time(),
                    last_update=time.time(),
                    worker_nodes=[w.node_id for w in gpu_workers],
                )
                self.distributed_cmaes_state[cmaes_job_id] = state

                # Launch distributed coordinator task
                asyncio.create_task(self._run_distributed_cmaes(cmaes_job_id))

                # Track as training job
                with self.training_lock:
                    if job_id in self.training_jobs:
                        self.training_jobs[job_id].status = "running"
                        self.training_jobs[job_id].started_at = time.time()

                return web.json_response({
                    "success": True,
                    "mode": "distributed",
                    "job_id": cmaes_job_id,
                    "workers": [w.node_id for w in gpu_workers],
                })

            else:
                # LOCAL MODE: Run GPU CMA-ES on this node only
                logger.info("Starting LOCAL GPU CMA-ES (no remote workers available)")

                output_dir = os.path.join(
                    self.ringrift_path, "ai-service", "data", "cmaes",
                    f"{board_type}_{num_players}p_auto_{int(time.time())}"
                )
                os.makedirs(output_dir, exist_ok=True)

                cmd = [
                    sys.executable,
                    os.path.join(self.ringrift_path, "ai-service", "scripts", "run_gpu_cmaes.py"),
                    "--board", board_type,
                    "--num-players", str(num_players),
                    "--generations", "100",
                    "--population-size", "32",
                    "--games-per-eval", "100",
                    "--max-moves", "10000",
                    "--output-dir", output_dir,
                    "--multi-gpu",
                ]

                env = os.environ.copy()
                env["PYTHONPATH"] = os.path.join(self.ringrift_path, "ai-service")
                env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

                proc = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                    env=env,
                )

                logger.info(f"Started local GPU CMA-ES (PID {proc.pid}) for job {job_id}")
                asyncio.create_task(self._monitor_training_process(job_id, proc, output_dir))

                return web.json_response({
                    "success": True,
                    "mode": "local",
                    "pid": proc.pid,
                })

        except Exception as e:
            return web.json_response({"success": False, "error": str(e)})

    async def _monitor_training_process(self, job_id: str, proc, output_path: str):
        """Monitor training subprocess and report completion to leader."""
        try:
            _stdout, stderr = await asyncio.wait_for(
                proc.communicate(),
                timeout=7200  # 2 hour max
            )

            success = proc.returncode == 0

            # Report to leader
            if self.leader_id and self.leader_id != self.node_id:
                leader = self.peers.get(self.leader_id)
                if leader:
                    try:
                        timeout = ClientTimeout(total=30)
                        async with get_client_session(timeout) as session:
                            url = self._url_for_peer(leader, "/training/update")
                            payload = {
                                "job_id": job_id,
                                "completed": success,
                                "output_model_path": output_path if success else "",
                                "error": stderr.decode()[:500] if not success else "",
                            }
                            await session.post(url, json=payload, headers=self._auth_headers())
                    except Exception as e:
                        logger.error(f"Failed to report training completion to leader: {e}")
            else:
                # We are the leader, update directly
                with self.training_lock:
                    job = self.training_jobs.get(job_id)
                    if job:
                        if success:
                            job.status = "completed"
                            job.output_model_path = output_path
                            # LEARNED LESSONS - Schedule tournament to compare new model against baseline
                            asyncio.create_task(self._schedule_model_comparison(job, output_path))
                            # Update improvement cycle manager with training completion
                            if self.improvement_cycle_manager:
                                self.improvement_cycle_manager.handle_training_complete(
                                    job.board_type, job.num_players,
                                    output_path, job.data_games_count or 0
                                )
                            # PFSP: Add trained model to opponent pool for diverse selfplay
                            config_key = f"{job.board_type}_{job.num_players}p"
                            if HAS_PFSP and config_key in self.pfsp_pools:
                                try:
                                    model_id = Path(output_path).stem
                                    self.pfsp_pools[config_key].add_opponent(
                                        model_id=model_id,
                                        model_path=output_path,
                                        elo=INITIAL_ELO_RATING,  # From app.config.thresholds
                                        win_rate=0.5,
                                    )
                                    logger.info(f"[PFSP] Added {model_id} to opponent pool for {config_key}")
                                except Exception as e:
                                    logger.error(f"[PFSP] Error adding model to pool: {e}")
                            # CMA-ES: Check for Elo plateau and trigger auto-tuning
                            asyncio.create_task(self._check_cmaes_auto_tuning(config_key))
                        else:
                            job.status = "failed"
                            job.error_message = stderr.decode()[:500]
                        job.completed_at = time.time()

            logger.info(f"Training job {job_id} {'completed' if success else 'failed'}")

        except asyncio.TimeoutError:
            logger.info(f"Training job {job_id} timed out")
        except Exception as e:
            logger.info(f"Training monitor error for {job_id}: {e}")

    async def _monitor_gpu_selfplay_and_validate(
        self,
        job_id: str,
        proc: subprocess.Popen,
        output_dir: Path,
        board_type: str,
        num_players: int,
    ) -> None:
        """Monitor GPU selfplay completion and run CPU validation.

        When GPU selfplay completes, this runs import_gpu_selfplay_to_db.py to:
        1. Replay each game with CPU GameEngine
        2. Validate all moves against legal move lists
        3. Discard games with invalid moves
        4. Store only validated games in canonical DB format

        This ensures GPU-generated games are safe for training.
        """
        try:
            # Wait for GPU selfplay to complete (with timeout)
            return_code = await asyncio.wait_for(
                asyncio.get_event_loop().run_in_executor(None, proc.wait),
                timeout=7200,  # 2 hour max
            )

            # Update job status
            with self.jobs_lock:
                job = self.local_jobs.get(job_id)
                if job:
                    job.status = "completed" if return_code == 0 else "failed"

            if return_code != 0:
                logger.info(f"GPU selfplay job {job_id} failed (exit code {return_code})")
                return

            # Find the generated JSONL file
            jsonl_files = list(output_dir.glob("*.jsonl"))
            if not jsonl_files:
                logger.info(f"GPU selfplay job {job_id}: No JSONL output found")
                return

            input_jsonl = jsonl_files[0]
            validated_db = output_dir / "validated_games.db"

            logger.info(f"GPU selfplay job {job_id} completed, running CPU validation...")

            # Run CPU validation import
            validate_cmd = [
                sys.executable,  # Use venv Python
                f"{self.ringrift_path}/ai-service/scripts/import_gpu_selfplay_to_db.py",
                "--input", str(input_jsonl),
                "--output", str(validated_db),
            ]

            env = os.environ.copy()
            env["PYTHONPATH"] = f"{self.ringrift_path}/ai-service"

            validate_proc = await asyncio.create_subprocess_exec(
                *validate_cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env,
                cwd=self.ringrift_path,
            )

            stdout, stderr = await asyncio.wait_for(
                validate_proc.communicate(),
                timeout=1800,  # 30 min validation timeout
            )

            if validate_proc.returncode == 0:
                # Parse validation results from output
                output_text = stdout.decode()
                imported = 0
                failed = 0
                for line in output_text.split("\n"):
                    if "Successfully imported:" in line:
                        imported = int(line.split(":")[-1].strip())
                    elif "Failed:" in line:
                        failed = int(line.split(":")[-1].strip())

                validation_rate = imported / (imported + failed) * 100 if (imported + failed) > 0 else 0

                logger.info(f"GPU selfplay {job_id} CPU validation complete:")
                logger.info(f"  Valid games: {imported}, Invalid: {failed}, Validation rate: {validation_rate:.1f}%")

                # Track validation metrics for diversity reporting
                if hasattr(self, 'diversity_metrics'):
                    if "gpu_validation_stats" not in self.diversity_metrics:
                        self.diversity_metrics["gpu_validation_stats"] = {
                            "total_generated": 0,
                            "total_validated": 0,
                            "total_failed": 0,
                        }
                    self.diversity_metrics["gpu_validation_stats"]["total_generated"] += imported + failed
                    self.diversity_metrics["gpu_validation_stats"]["total_validated"] += imported
                    self.diversity_metrics["gpu_validation_stats"]["total_failed"] += failed

                # Record validation rate metric for observability
                self.record_metric(
                    "validation_rate",
                    validation_rate,
                    board_type=board_type,
                    num_players=num_players,
                    metadata={
                        "job_id": job_id,
                        "imported": imported,
                        "failed": failed,
                    },
                )

                # Auto-import to canonical database if validation rate is high enough
                if validation_rate >= 95 and imported > 0:
                    asyncio.create_task(self._import_gpu_selfplay_to_canonical(
                        validated_db, board_type, num_players, imported
                    ))
                elif validation_rate < 95:
                    logger.info(f"WARNING: GPU selfplay validation rate {validation_rate:.1f}% is below 95%")
                    logger.info("  This indicates potential GPU/CPU rule divergence")
                    logger.info("  Skipping auto-import to canonical database")
                    # Alert on low validation rate
                    asyncio.create_task(self.notifier.send(
                        title="Low GPU Validation Rate",
                        message=f"GPU selfplay validation rate {validation_rate:.1f}% is below 95% threshold",
                        level="warning",
                        fields={
                            "Config": f"{board_type}_{num_players}p",
                            "Valid": str(imported),
                            "Invalid": str(failed),
                            "Rate": f"{validation_rate:.1f}%",
                        },
                        node_id=self.node_id,
                    ))

            else:
                logger.info(f"GPU selfplay {job_id} CPU validation failed:")
                logger.info(f"  {stderr.decode()[:500]}")

        except asyncio.TimeoutError:
            logger.info(f"GPU selfplay job {job_id} timed out")
            with self.jobs_lock:
                job = self.local_jobs.get(job_id)
                if job:
                    job.status = "failed"
        except Exception as e:
            logger.info(f"GPU selfplay monitor error for {job_id}: {e}")
            with self.jobs_lock:
                job = self.local_jobs.get(job_id)
                if job:
                    job.status = "failed"

    async def _schedule_model_comparison(self, job: TrainingJob, new_model_path: str):
        """Schedule a tournament to compare new model against current baseline.

        LEARNED LESSONS - After training, automatically run tournament to:
        1. Compare new model against current best baseline
        2. Update Elo ratings
        3. Promote to best baseline if new model wins
        """
        try:
            config_key = f"{job.board_type}_{job.num_players}p"
            logger.info(f"Scheduling model comparison tournament for {config_key}")

            # Find current baseline model
            baseline_dir = Path(self.ringrift_path) / "ai-service" / "models" / job.job_type
            baseline_pattern = f"{job.board_type}_{job.num_players}p_best*"

            baseline_model = None
            for f in baseline_dir.glob(baseline_pattern):
                baseline_model = str(f)
                break

            if not baseline_model:
                # No baseline - this model becomes baseline
                logger.info(f"No baseline found for {config_key}, new model becomes baseline")
                await self._promote_to_baseline(new_model_path, job.board_type, job.num_players, job.job_type)
                return

            # Schedule tournament via SSH tournament system
            tournament_id = f"autoeval_{config_key}_{int(time.time())}"

            # Use existing SSH tournament infrastructure
            with self.ssh_tournament_lock:
                self.ssh_tournament_runs[tournament_id] = SSHTournamentRun(
                    tournament_id=tournament_id,
                    board_type=job.board_type,
                    num_players=job.num_players,
                    status="pending",
                    started_at=time.time(),
                )

            # Start tournament in background
            tournament_config = {
                "tournament_id": tournament_id,
                "board_type": job.board_type,
                "num_players": job.num_players,
                "model_a": new_model_path,
                "model_b": baseline_model,
                "games_per_matchup": 50,
            }
            asyncio.create_task(self._run_model_comparison_tournament(tournament_config))

        except Exception as e:
            logger.info(f"Model comparison scheduling error: {e}")

    async def _run_model_comparison_tournament(self, config: dict):
        """Run a model comparison tournament and update baseline if new model wins."""
        tournament_id = config["tournament_id"]
        try:
            logger.info(f"Running model comparison tournament {tournament_id}")

            results_dir = Path(self.ringrift_path) / "ai-service" / "results" / "tournaments"
            results_dir.mkdir(parents=True, exist_ok=True)

            cmd = [
                sys.executable,
                os.path.join(self.ringrift_path, "ai-service", "scripts", "run_tournament.py"),
                "--player1", f"nn:{config['model_a']}",
                "--player2", f"nn:{config['model_b']}",
                "--board", config["board_type"],
                "--num-players", str(config["num_players"]),
                "--games", str(config["games_per_matchup"]),
                "--output", str(results_dir / f"{tournament_id}.json"),
            ]

            env = os.environ.copy()
            env["PYTHONPATH"] = os.path.join(self.ringrift_path, "ai-service")
            env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

            proc = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env,
            )

            _stdout, _stderr = await asyncio.wait_for(proc.communicate(), timeout=3600)

            if proc.returncode == 0:
                results_file = results_dir / f"{tournament_id}.json"
                if results_file.exists():
                    import json as json_module
                    results = json_module.loads(results_file.read_text())
                    new_model_wins = results.get("player1_wins", 0)
                    baseline_wins = results.get("player2_wins", 0)
                    total_games = new_model_wins + baseline_wins

                    win_rate = new_model_wins / total_games if total_games > 0 else 0.5
                    logger.info(f"Tournament {tournament_id}: new model win rate = {win_rate:.1%}")

                    promoted = win_rate >= 0.55
                    if promoted:
                        logger.info("New model beats baseline! Promoting to best baseline.")
                        await self._promote_to_baseline(
                            config["model_a"], config["board_type"],
                            config["num_players"], "nnue" if "nnue" in config["model_a"].lower() else "cmaes"
                        )

                    # Update improvement cycle manager with tournament result
                    await self._handle_tournament_completion(
                        tournament_id,
                        config["board_type"],
                        config["num_players"],
                        config["model_a"],
                        config["model_b"],
                        win_rate,
                        promoted,
                    )

            with self.ssh_tournament_lock:
                if tournament_id in self.ssh_tournament_runs:
                    self.ssh_tournament_runs[tournament_id].status = "completed"
                    self.ssh_tournament_runs[tournament_id].completed_at = time.time()

        except Exception as e:
            logger.info(f"Tournament {tournament_id} error: {e}")
            with self.ssh_tournament_lock:
                if tournament_id in self.ssh_tournament_runs:
                    self.ssh_tournament_runs[tournament_id].status = "failed"
                    self.ssh_tournament_runs[tournament_id].error = str(e)

    async def _promote_to_baseline(self, model_path: str, board_type: str, num_players: int, model_type: str):
        """Promote a model to the best baseline for its board type."""
        try:
            import shutil
            baseline_dir = Path(self.ringrift_path) / "ai-service" / "models" / model_type
            baseline_dir.mkdir(parents=True, exist_ok=True)

            baseline_path = baseline_dir / f"{board_type}_{num_players}p_best.pt"
            if baseline_path.exists():
                backup_path = baseline_dir / f"{board_type}_{num_players}p_prev_{int(time.time())}.pt"
                shutil.copy2(baseline_path, backup_path)
                logger.info(f"Backed up previous baseline to {backup_path}")

            shutil.copy2(model_path, baseline_path)
            logger.info(f"Promoted {model_path} to baseline at {baseline_path}")

        except Exception as e:
            logger.info(f"Baseline promotion error: {e}")

    async def _check_cmaes_auto_tuning(self, config_key: str):
        """Check if CMA-ES auto-tuning should be triggered for a config.

        Monitors Elo progression and triggers hyperparameter optimization
        when the model's improvement plateaus.
        """
        if not HAS_PFSP or config_key not in self.cmaes_auto_tuners:
            return

        try:
            # Get current Elo from unified database
            from app.tournament import get_elo_database
            db = get_elo_database()

            parts = config_key.rsplit("_", 1)
            board_type = parts[0]
            num_players = int(parts[1].replace("p", ""))

            # Find best model for this config
            best_model = None
            best_elo = INITIAL_ELO_RATING
            models_dir = Path(self.ringrift_path) / "ai-service" / "models" / "nnue"
            pattern = f"nnue_{board_type}_{num_players}p*.pt"

            for model_path in models_dir.glob(pattern):
                model_id = model_path.stem
                elo = db.get_elo(model_id)
                if elo and elo > best_elo:
                    best_elo = elo
                    best_model = model_id

            if not best_model:
                return

            # Check for plateau
            auto_tuner = self.cmaes_auto_tuners[config_key]
            self.last_cmaes_elo.get(config_key, INITIAL_ELO_RATING)

            # Record Elo history for plateau detection
            should_tune = auto_tuner.check_plateau(best_elo)
            self.last_cmaes_elo[config_key] = best_elo

            if should_tune:
                logger.info(f"[CMA-ES] Elo plateau detected for {config_key} (Elo: {best_elo:.0f})")
                logger.info("[CMA-ES] Triggering auto hyperparameter optimization...")

                # Trigger CMA-ES via existing distributed infrastructure
                await self._trigger_auto_cmaes(board_type, num_players)

        except Exception as e:
            logger.info(f"[CMA-ES] Auto-tuning check error for {config_key}: {e}")

    def get_pfsp_opponent(self, config_key: str) -> str | None:
        """Get a PFSP-sampled opponent model for selfplay.

        Returns path to an opponent model sampled from the PFSP pool,
        weighted by difficulty (harder opponents sampled more frequently).
        """
        if not HAS_PFSP or config_key not in self.pfsp_pools:
            return None

        try:
            pool = self.pfsp_pools[config_key]
            opponent = pool.sample_opponent()
            if opponent:
                return opponent.model_path
        except Exception as e:
            logger.error(f"[PFSP] Error sampling opponent: {e}")
        return None

    def update_pfsp_stats(self, config_key: str, model_id: str, win_rate: float, elo: float):
        """Update PFSP stats for a model after evaluation games.

        Called after tournament/evaluation to update opponent difficulty metrics.
        """
        if not HAS_PFSP or config_key not in self.pfsp_pools:
            return

        try:
            self.pfsp_pools[config_key].update_stats(model_id, win_rate=win_rate, elo=elo)
            logger.info(f"[PFSP] Updated stats for {model_id}: win_rate={win_rate:.2f}, elo={elo:.0f}")
        except Exception as e:
            logger.error(f"[PFSP] Error updating stats: {e}")

    async def _handle_tournament_completion(
        self,
        tournament_id: str,
        board_type: str,
        num_players: int,
        new_model: str,
        baseline_model: str,
        win_rate: float,
        promoted: bool,
    ):
        """Handle tournament completion - update cycle state and trigger next iteration.

        This closes the feedback loop by:
        1. Updating improvement cycle manager with evaluation result
        2. Recording result to unified Elo database
        3. Updating diversity metrics
        4. Boosting selfplay for this config if model was promoted
        """
        try:
            # 1. Update improvement cycle manager
            if self.improvement_cycle_manager:
                self.improvement_cycle_manager.handle_evaluation_complete(
                    board_type, num_players, win_rate, new_model
                )
                logger.info(f"Updated improvement cycle for {board_type}_{num_players}p")

            # 2. Record to unified Elo database
            try:
                from app.tournament import get_elo_database
                db = get_elo_database()
                # Rankings: 0 = winner, 1 = loser
                rankings = [0, 1] if win_rate > 0.5 else [1, 0]
                db.record_match_and_update(
                    participant_ids=[new_model, baseline_model],
                    rankings=rankings,
                    board_type=board_type,
                    num_players=num_players,
                    tournament_id=tournament_id,
                )
                logger.info("Recorded tournament result to unified Elo DB")

                # Trigger Elo sync to propagate to cluster
                if HAS_ELO_SYNC and self.elo_sync_manager:
                    asyncio.create_task(self._trigger_elo_sync_after_matches(1))
            except Exception as e:
                logger.info(f"Elo database update failed (non-fatal): {e}")

            # 3. Update diversity metrics
            if hasattr(self, 'diversity_metrics'):
                self.diversity_metrics["tournament_runs"] = self.diversity_metrics.get("tournament_runs", 0) + 1
                if promoted:
                    self.diversity_metrics["promotions"] = self.diversity_metrics.get("promotions", 0) + 1

            # 4. Record metrics for observability
            self.record_metric(
                "tournament_win_rate",
                win_rate,
                board_type=board_type,
                num_players=num_players,
                metadata={
                    "new_model": new_model,
                    "baseline_model": baseline_model,
                    "promoted": promoted,
                    "tournament_id": tournament_id,
                },
            )

            # 5. Boost selfplay for this config if promoted (more data for next iteration)
            if promoted:
                asyncio.create_task(self._boost_selfplay_for_config(board_type, num_players))
                # Alert on successful promotion
                asyncio.create_task(self.notifier.send(
                    title="Model Promoted",
                    message=f"New model promoted for {board_type}_{num_players}p with {win_rate*100:.1f}% win rate",
                    level="info",
                    fields={"Model": new_model, "Win Rate": f"{win_rate*100:.1f}%"},
                    node_id=self.node_id,
                ))
            elif win_rate < 0.5:
                # Alert on failed promotion (new model lost)
                asyncio.create_task(self.notifier.send(
                    title="Model Promotion Failed",
                    message=f"New model failed tournament for {board_type}_{num_players}p with only {win_rate*100:.1f}% win rate",
                    level="warning",
                    fields={
                        "Model": new_model,
                        "Win Rate": f"{win_rate*100:.1f}%",
                        "Baseline": baseline_model,
                    },
                    node_id=self.node_id,
                ))

        except Exception as e:
            logger.info(f"Tournament completion handler error: {e}")
            asyncio.create_task(self.notifier.send(
                title="Tournament Handler Error",
                message=str(e),
                level="error",
                node_id=self.node_id,
            ))

    async def _boost_selfplay_for_config(self, board_type: str, num_players: int):
        """Temporarily boost selfplay for a configuration after model promotion.

        This accelerates data generation for the next training iteration.
        """
        try:
            config_key = f"{board_type}_{num_players}p"
            logger.info(f"Boosting selfplay for {config_key} after promotion")

            # Schedule additional selfplay jobs for this configuration
            # This will be picked up by the next job scheduling cycle
            if hasattr(self, 'selfplay_boost_configs'):
                self.selfplay_boost_configs[config_key] = {
                    "boost_until": time.time() + 3600,  # Boost for 1 hour
                    "multiplier": 1.5,  # 50% more jobs
                }
            else:
                self.selfplay_boost_configs = {
                    config_key: {
                        "boost_until": time.time() + 3600,
                        "multiplier": 1.5,
                    }
                }

        except Exception as e:
            logger.info(f"Selfplay boost error: {e}")

    async def _propagate_cmaes_weights(
        self, board_type: str, num_players: int, weights: dict[str, float]
    ):
        """Propagate new CMA-ES weights to selfplay workers.

        After CMA-ES optimization finds better weights, this:
        1. Saves weights to shared config file
        2. Restarts selfplay jobs for this config with new weights
        """
        try:
            config_key = f"{board_type}_{num_players}p"
            logger.info(f"Propagating CMA-ES weights for {config_key}")

            # 1. Save to shared heuristic weights config
            config_path = Path(self.ringrift_path) / "ai-service" / "config" / "heuristic_weights.json"
            config_path.parent.mkdir(parents=True, exist_ok=True)

            import json as json_mod
            existing = {}
            if config_path.exists():
                with contextlib.suppress(Exception):
                    existing = json_mod.loads(config_path.read_text())

            existing[config_key] = {
                "weights": weights,
                "updated_at": time.time(),
            }
            config_path.write_text(json_mod.dumps(existing, indent=2))
            logger.info(f"Updated heuristic_weights.json with {config_key} weights")

            # 2. Track config for weight-aware selfplay scheduling
            if not hasattr(self, 'cmaes_weight_configs'):
                self.cmaes_weight_configs = {}

            self.cmaes_weight_configs[config_key] = {
                "weights": weights,
                "updated_at": time.time(),
            }

            # 3. Stop existing selfplay jobs for this config (they'll restart with new weights)
            jobs_to_stop = []
            with self.jobs_lock:
                for job_id, job in self.local_jobs.items():
                    if (job.job_type in (JobType.SELFPLAY, JobType.GPU_SELFPLAY, JobType.HYBRID_SELFPLAY, JobType.CPU_SELFPLAY, JobType.GUMBEL_SELFPLAY)
                        and getattr(job, 'board_type', None) == board_type
                        and getattr(job, 'num_players', None) == num_players
                        and job.status == "running"):
                        jobs_to_stop.append(job_id)

            for job_id in jobs_to_stop:
                await self._stop_local_job(job_id)
                logger.info(f"Stopped selfplay job {job_id} for weight update")

            # 4. Boost selfplay to generate data with new weights
            asyncio.create_task(self._boost_selfplay_for_config(board_type, num_players))

            logger.info(f"Weight propagation complete for {config_key}")

        except Exception as e:
            logger.info(f"CMA-ES weight propagation error: {e}")

    async def _stop_local_job(self, job_id: str):
        """Stop a local job by job ID."""
        try:
            with self.jobs_lock:
                job = self.local_jobs.get(job_id)
                if job and hasattr(job, 'process') and job.process:
                    job.process.terminate()
                    job.status = "stopped"
        except Exception as e:
            logger.error(f"stopping job {job_id}: {e}")

    async def _import_gpu_selfplay_to_canonical(
        self, validated_db: Path, board_type: str, num_players: int, game_count: int
    ):
        """Import validated GPU selfplay games to canonical selfplay database.

        After GPU selfplay games pass CPU validation (>=95% validation rate),
        this merges them into the canonical selfplay database for training.
        """
        try:
            # Determine canonical DB path
            canonical_db = Path(self.ringrift_path) / "ai-service" / "data" / "games" / "selfplay.db"
            if not canonical_db.parent.exists():
                canonical_db.parent.mkdir(parents=True, exist_ok=True)

            logger.info(f"Auto-importing {game_count} validated GPU games to canonical DB...")

            # Use sqlite3 to merge games from validated_db to canonical_db
            import sqlite3

            # Connect to both databases
            src_conn = sqlite3.connect(str(validated_db))
            dst_conn = sqlite3.connect(str(canonical_db))

            # Ensure destination tables exist
            dst_conn.execute("""
                CREATE TABLE IF NOT EXISTS games (
                    game_id TEXT PRIMARY KEY,
                    board_type TEXT NOT NULL,
                    num_players INTEGER NOT NULL,
                    winner INTEGER,
                    move_count INTEGER,
                    game_time_ms INTEGER,
                    created_at REAL,
                    source TEXT DEFAULT 'selfplay'
                )
            """)
            dst_conn.execute("""
                CREATE TABLE IF NOT EXISTS moves (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    game_id TEXT NOT NULL,
                    move_number INTEGER NOT NULL,
                    player INTEGER NOT NULL,
                    move_type TEXT NOT NULL,
                    from_pos TEXT,
                    to_pos TEXT,
                    direction TEXT,
                    captured_pos TEXT,
                    state_before TEXT,
                    policy_probs TEXT,
                    value_est REAL,
                    FOREIGN KEY (game_id) REFERENCES games(game_id)
                )
            """)
            dst_conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_moves_game_id ON moves(game_id)
            """)
            dst_conn.commit()

            # Check source schema and copy games
            src_cursor = src_conn.cursor()
            src_cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            src_tables = {row[0] for row in src_cursor.fetchall()}

            imported = 0
            if "games" in src_tables:
                # Get existing game IDs in destination to avoid duplicates
                dst_cursor = dst_conn.cursor()
                dst_cursor.execute("SELECT game_id FROM games")
                existing_ids = {row[0] for row in dst_cursor.fetchall()}

                # Copy games that don't already exist
                src_cursor.execute("SELECT * FROM games")
                src_columns = [desc[0] for desc in src_cursor.description]

                for row in src_cursor.fetchall():
                    game_id_idx = src_columns.index("game_id") if "game_id" in src_columns else 0
                    game_id = row[game_id_idx]

                    if game_id in existing_ids:
                        continue

                    # Insert game with proper column mapping
                    placeholders = ", ".join(["?"] * len(row))
                    columns = ", ".join(src_columns)
                    try:
                        dst_conn.execute(
                            f"INSERT OR IGNORE INTO games ({columns}) VALUES ({placeholders})",
                            row
                        )
                        imported += 1
                    except Exception:
                        continue

                # Copy moves for new games
                if "moves" in src_tables and imported > 0:
                    src_cursor.execute("SELECT * FROM moves")
                    move_columns = [desc[0] for desc in src_cursor.description]
                    move_placeholders = ", ".join(["?"] * len(move_columns))
                    move_col_str = ", ".join(move_columns)

                    for row in src_cursor.fetchall():
                        game_id_idx = move_columns.index("game_id") if "game_id" in move_columns else 1
                        game_id = row[game_id_idx]
                        if game_id not in existing_ids:
                            try:
                                dst_conn.execute(
                                    f"INSERT OR IGNORE INTO moves ({move_col_str}) VALUES ({move_placeholders})",
                                    row
                                )
                            except Exception:
                                continue

                dst_conn.commit()

            src_conn.close()
            dst_conn.close()

            logger.info(f"Successfully imported {imported} GPU selfplay games to canonical DB")

            # Update cluster data manifest to reflect new games
            config_key = f"{board_type}_{num_players}p"
            if hasattr(self, 'cluster_data_manifest') and self.cluster_data_manifest and config_key in self.cluster_data_manifest.by_board_type:
                self.cluster_data_manifest.by_board_type[config_key]["total_games"] = (
                    self.cluster_data_manifest.by_board_type[config_key].get("total_games", 0) + imported
                )

            # Notify improvement cycle manager of new games
            if self.improvement_cycle_manager and imported > 0:
                self.improvement_cycle_manager.record_games(board_type, num_players, imported)

        except Exception as e:
            logger.info(f"GPU selfplay import error: {e}")
            import traceback
            traceback.print_exc()

    # =========================================================================

    # =========================================================================
    # Phase 5: Improvement Cycle HTTP Handlers
    # =========================================================================

    async def handle_improvement_cycles_status(self, request: web.Request) -> web.Response:
        """GET /improvement_cycles/status - Get status of all improvement cycles."""
        try:
            if not self.improvement_cycle_manager:
                return web.json_response({
                    "success": False,
                    "error": "ImprovementCycleManager not initialized"
                })

            status = self.improvement_cycle_manager.get_status()
            return web.json_response({
                "success": True,
                "is_leader": self.role == NodeRole.LEADER,
                **status,
            })

        except Exception as e:
            return web.json_response({"success": False, "error": str(e)})

    async def handle_improvement_cycles_leaderboard(self, request: web.Request) -> web.Response:
        """GET /improvement_cycles/leaderboard - Get Elo leaderboard."""
        try:
            if not self.improvement_cycle_manager:
                return web.json_response({
                    "success": False,
                    "error": "ImprovementCycleManager not initialized"
                })

            board_type = request.query.get("board_type")
            num_players_str = request.query.get("num_players")
            num_players = int(num_players_str) if num_players_str else None

            leaderboard = self.improvement_cycle_manager.get_leaderboard(
                board_type=board_type,
                num_players=num_players,
            )

            return web.json_response({
                "success": True,
                "leaderboard": [e.to_dict() for e in leaderboard],
                "total_models": len(leaderboard),
            })

        except Exception as e:
            return web.json_response({"success": False, "error": str(e)})

    async def handle_metrics(self, request: web.Request) -> web.Response:
        """GET /metrics - Get metrics summary and history.

        Content negotiation:
        - Accept: text/plain -> Prometheus format (same as /metrics/prometheus)
        - Accept: application/json -> JSON format
        - Default (no header) -> Prometheus format for Prometheus scraper compatibility
        """
        try:
            # Content negotiation for Prometheus compatibility
            accept = request.headers.get("Accept", "")
            # Prometheus sends "text/plain" or "application/openmetrics-text"
            # Also check for explicit format param
            format_param = request.query.get("format", "").lower()
            if format_param == "prometheus" or "text/plain" in accept or "openmetrics" in accept or not accept:
                # Return Prometheus format
                return await self.handle_metrics_prometheus(request)

            hours = float(request.query.get("hours", "24"))
            metric_type = request.query.get("type")
            board_type = request.query.get("board_type")
            num_players_str = request.query.get("num_players")
            num_players = int(num_players_str) if num_players_str else None

            if metric_type:
                # Get specific metric history
                history = self.get_metrics_history(
                    metric_type=metric_type,
                    board_type=board_type,
                    num_players=num_players,
                    hours=hours,
                )
                return web.json_response({
                    "success": True,
                    "metric_type": metric_type,
                    "period_hours": hours,
                    "count": len(history),
                    "history": history,
                })
            else:
                # Get summary of all metrics
                summary = self.get_metrics_summary(hours=hours)
                return web.json_response({
                    "success": True,
                    **summary,
                })

        except Exception as e:
            return web.json_response({"success": False, "error": str(e)})

    async def handle_metrics_prometheus(self, request: web.Request) -> web.Response:
        """GET /metrics/prometheus - Prometheus-compatible metrics export.

        Returns metrics in Prometheus text exposition format for scraping.
        """
        try:
            lines = []
            now = time.time()

            # Cluster metrics
            with self.peers_lock:
                alive_peers = len([p for p in self.peers.values() if p.is_alive()])
                total_peers = len(self.peers)

            lines.append("# HELP ringrift_cluster_peers_total Total number of known peers")
            lines.append("# TYPE ringrift_cluster_peers_total gauge")
            lines.append(f"ringrift_cluster_peers_total {total_peers}")

            lines.append("# HELP ringrift_cluster_peers_alive Number of alive peers")
            lines.append("# TYPE ringrift_cluster_peers_alive gauge")
            lines.append(f"ringrift_cluster_peers_alive {alive_peers}")

            lines.append("# HELP ringrift_is_leader Whether this node is the leader")
            lines.append("# TYPE ringrift_is_leader gauge")
            lines.append(f"ringrift_is_leader {1 if self.role == NodeRole.LEADER else 0}")

            # Job counts
            with self.jobs_lock:
                selfplay_jobs = len([j for j in self.local_jobs.values()
                                    if j.job_type in (JobType.SELFPLAY, JobType.GPU_SELFPLAY, JobType.HYBRID_SELFPLAY, JobType.CPU_SELFPLAY, JobType.GUMBEL_SELFPLAY)
                                    and j.status == "running"])
                training_jobs = len([j for j in self.local_jobs.values()
                                    if j.job_type == JobType.TRAINING and j.status == "running"])

            lines.append("# HELP ringrift_selfplay_jobs_running Number of running selfplay jobs")
            lines.append("# TYPE ringrift_selfplay_jobs_running gauge")
            lines.append(f"ringrift_selfplay_jobs_running {selfplay_jobs}")

            lines.append("# HELP ringrift_training_jobs_running Number of running training jobs")
            lines.append("# TYPE ringrift_training_jobs_running gauge")
            lines.append(f"ringrift_training_jobs_running {training_jobs}")

            # Resource utilization - include node labels for all nodes
            lines.append("# HELP ringrift_cpu_percent CPU utilization percentage per node")
            lines.append("# TYPE ringrift_cpu_percent gauge")

            lines.append("# HELP ringrift_memory_percent Memory utilization percentage per node")
            lines.append("# TYPE ringrift_memory_percent gauge")

            lines.append("# HELP ringrift_disk_percent Disk utilization percentage per node")
            lines.append("# TYPE ringrift_disk_percent gauge")

            lines.append("# HELP ringrift_gpu_percent GPU utilization percentage per node")
            lines.append("# TYPE ringrift_gpu_percent gauge")

            lines.append("# HELP ringrift_selfplay_jobs Selfplay jobs per node")
            lines.append("# TYPE ringrift_selfplay_jobs gauge")

            lines.append("# HELP ringrift_node_alive Whether node is alive (1) or not (0)")
            lines.append("# TYPE ringrift_node_alive gauge")

            # Cluster cost metrics (for Grafana dashboards)
            # GPU hourly rates (Lambda Labs pricing)
            GPU_HOURLY_RATES = {
                "GH200": 2.49, "H100": 2.49, "A100": 1.99, "A10": 0.75,
                "RTX_4090": 0.50, "RTX4090": 0.50, "4090": 0.50,
                "RTX_3090": 0.30, "RTX3090": 0.30, "3090": 0.30,
                "unknown": 0.50,
            }

            lines.append("# HELP ringrift_cluster_node_up Whether cluster node is active (1=up, 0=down)")
            lines.append("# TYPE ringrift_cluster_node_up gauge")
            lines.append("# HELP ringrift_cluster_node_cost_per_hour Estimated hourly cost in USD")
            lines.append("# TYPE ringrift_cluster_node_cost_per_hour gauge")
            lines.append("# HELP ringrift_cluster_gpu_utilization GPU utilization as fraction (0-1)")
            lines.append("# TYPE ringrift_cluster_gpu_utilization gauge")
            lines.append("# HELP ringrift_cluster_cpu_utilization CPU utilization as fraction (0-1)")
            lines.append("# TYPE ringrift_cluster_cpu_utilization gauge")
            lines.append("# HELP ringrift_cluster_gpu_memory_used_bytes GPU memory used in bytes")
            lines.append("# TYPE ringrift_cluster_gpu_memory_used_bytes gauge")
            lines.append("# HELP ringrift_cluster_memory_used_bytes System memory used in bytes")
            lines.append("# TYPE ringrift_cluster_memory_used_bytes gauge")

            # Export self metrics with node label
            node_name = self.node_id or "unknown"
            cpu = getattr(self.self_info, 'cpu_percent', 0)
            mem = getattr(self.self_info, 'memory_percent', 0)
            disk = getattr(self.self_info, 'disk_percent', 0)
            gpu = getattr(self.self_info, 'gpu_percent', 0) if self.self_info.has_gpu else 0
            role = "leader" if self.role == NodeRole.LEADER else "worker"
            gpu_type = getattr(self.self_info, 'gpu_type', 'unknown') or 'unknown'
            # Normalize GPU type for lookup
            gpu_type_key = gpu_type.replace(' ', '_').upper() if gpu_type else 'unknown'
            hourly_cost = GPU_HOURLY_RATES.get(gpu_type_key, GPU_HOURLY_RATES.get(gpu_type, GPU_HOURLY_RATES['unknown']))
            gpu_mem_bytes = getattr(self.self_info, 'gpu_memory_used_bytes', 0) or 0
            sys_mem_bytes = getattr(self.self_info, 'memory_used_bytes', 0) or 0

            lines.append(f'ringrift_cpu_percent{{node="{node_name}",role="{role}"}} {cpu}')
            lines.append(f'ringrift_memory_percent{{node="{node_name}",role="{role}"}} {mem}')
            lines.append(f'ringrift_disk_percent{{node="{node_name}",role="{role}"}} {disk}')
            lines.append(f'ringrift_gpu_percent{{node="{node_name}",role="{role}"}} {gpu}')
            lines.append(f'ringrift_selfplay_jobs{{node="{node_name}",role="{role}"}} {selfplay_jobs}')
            lines.append(f'ringrift_node_alive{{node="{node_name}",role="{role}"}} 1')

            # Export cluster cost metrics for self (for Grafana cost dashboard)
            lines.append(f'ringrift_cluster_node_up{{node="{node_name}",gpu_type="{gpu_type}"}} 1')
            lines.append(f'ringrift_cluster_node_cost_per_hour{{node="{node_name}",gpu_type="{gpu_type}"}} {hourly_cost}')
            lines.append(f'ringrift_cluster_gpu_utilization{{node="{node_name}",gpu_type="{gpu_type}"}} {gpu / 100.0 if gpu else 0}')
            lines.append(f'ringrift_cluster_cpu_utilization{{node="{node_name}"}} {cpu / 100.0 if cpu else 0}')
            lines.append(f'ringrift_cluster_gpu_memory_used_bytes{{node="{node_name}",gpu_type="{gpu_type}"}} {gpu_mem_bytes}')
            lines.append(f'ringrift_cluster_memory_used_bytes{{node="{node_name}"}} {sys_mem_bytes}')

            # Export peer metrics with node labels
            with self.peers_lock:
                for peer_id, peer in self.peers.items():
                    peer_name = peer_id or "unknown"
                    peer_role = "worker"
                    is_alive = 1 if peer.is_alive() else 0

                    # Get peer resource info if available
                    peer_cpu = getattr(peer, 'cpu_percent', 0) or 0
                    peer_mem = getattr(peer, 'memory_percent', 0) or 0
                    peer_gpu = getattr(peer, 'gpu_percent', 0) or 0
                    peer_jobs = getattr(peer, 'selfplay_jobs', 0) or 0
                    peer_gpu_type = getattr(peer, 'gpu_type', 'unknown') or 'unknown'
                    peer_gpu_type_key = peer_gpu_type.replace(' ', '_').upper() if peer_gpu_type else 'unknown'
                    peer_hourly_cost = GPU_HOURLY_RATES.get(peer_gpu_type_key, GPU_HOURLY_RATES.get(peer_gpu_type, GPU_HOURLY_RATES['unknown']))
                    peer_gpu_mem = getattr(peer, 'gpu_memory_used_bytes', 0) or 0
                    peer_sys_mem = getattr(peer, 'memory_used_bytes', 0) or 0

                    lines.append(f'ringrift_cpu_percent{{node="{peer_name}",role="{peer_role}"}} {peer_cpu}')
                    lines.append(f'ringrift_memory_percent{{node="{peer_name}",role="{peer_role}"}} {peer_mem}')
                    lines.append(f'ringrift_gpu_percent{{node="{peer_name}",role="{peer_role}"}} {peer_gpu}')
                    lines.append(f'ringrift_selfplay_jobs{{node="{peer_name}",role="{peer_role}"}} {peer_jobs}')
                    lines.append(f'ringrift_node_alive{{node="{peer_name}",role="{peer_role}"}} {is_alive}')

                    # Export cluster cost metrics for peer
                    lines.append(f'ringrift_cluster_node_up{{node="{peer_name}",gpu_type="{peer_gpu_type}"}} {is_alive}')
                    lines.append(f'ringrift_cluster_node_cost_per_hour{{node="{peer_name}",gpu_type="{peer_gpu_type}"}} {peer_hourly_cost if is_alive else 0}')
                    lines.append(f'ringrift_cluster_gpu_utilization{{node="{peer_name}",gpu_type="{peer_gpu_type}"}} {peer_gpu / 100.0 if peer_gpu else 0}')
                    lines.append(f'ringrift_cluster_cpu_utilization{{node="{peer_name}"}} {peer_cpu / 100.0 if peer_cpu else 0}')
                    lines.append(f'ringrift_cluster_gpu_memory_used_bytes{{node="{peer_name}",gpu_type="{peer_gpu_type}"}} {peer_gpu_mem}')
                    lines.append(f'ringrift_cluster_memory_used_bytes{{node="{peer_name}"}} {peer_sys_mem}')

            # Elo metrics with config labels
            try:
                from scripts.run_model_elo_tournament import ELO_DB_PATH, init_elo_database
                if ELO_DB_PATH and ELO_DB_PATH.exists():
                    db = init_elo_database()
                    conn = db._get_connection()
                    cursor = conn.cursor()
                    cursor.execute("""
                        SELECT board_type, num_players, MAX(rating) as best_elo
                        FROM elo_ratings
                        WHERE games_played >= 10
                        GROUP BY board_type, num_players
                    """)
                    lines.append("# HELP ringrift_best_elo Best Elo rating per configuration")
                    lines.append("# TYPE ringrift_best_elo gauge")
                    for row in cursor.fetchall():
                        bt, np, elo = row
                        config = f"{bt}_{np}p"
                        lines.append(f'ringrift_best_elo{{config="{config}",board_type="{bt}",num_players="{np}"}} {elo}')
                    db.close()
            except Exception:
                pass

            # Diversity metrics
            if hasattr(self, 'diversity_metrics'):
                dm = self.diversity_metrics
                lines.append("# HELP ringrift_tournament_runs_total Total tournament runs")
                lines.append("# TYPE ringrift_tournament_runs_total counter")
                lines.append(f"ringrift_tournament_runs_total {dm.get('tournament_runs', 0)}")

                lines.append("# HELP ringrift_promotions_total Total model promotions")
                lines.append("# TYPE ringrift_promotions_total counter")
                lines.append(f"ringrift_promotions_total {dm.get('promotions', 0)}")

                lines.append("# HELP ringrift_rollbacks_total Total model rollbacks")
                lines.append("# TYPE ringrift_rollbacks_total counter")
                lines.append(f"ringrift_rollbacks_total {dm.get('rollbacks', 0)}")

                # GPU validation stats
                gpu_stats = dm.get('gpu_validation_stats', {})
                if gpu_stats:
                    lines.append("# HELP ringrift_gpu_games_validated_total Total GPU games validated")
                    lines.append("# TYPE ringrift_gpu_games_validated_total counter")
                    lines.append(f"ringrift_gpu_games_validated_total {gpu_stats.get('total_validated', 0)}")

                    lines.append("# HELP ringrift_gpu_games_failed_total Total GPU games failed validation")
                    lines.append("# TYPE ringrift_gpu_games_failed_total counter")
                    lines.append(f"ringrift_gpu_games_failed_total {gpu_stats.get('total_failed', 0)}")

            # Recent metrics from database (last hour averages)
            try:
                summary = self.get_metrics_summary(hours=1)
                metrics_data = summary.get("metrics", {})

                for metric_name, metric_info in metrics_data.items():
                    safe_name = metric_name.replace("-", "_").replace(".", "_")
                    if metric_info.get("latest") is not None:
                        lines.append(f"# HELP ringrift_{safe_name} Latest {metric_name} value")
                        lines.append(f"# TYPE ringrift_{safe_name} gauge")
                        lines.append(f"ringrift_{safe_name} {metric_info['latest']}")
            except Exception:
                pass

            # Data manifest totals
            if hasattr(self, 'cluster_data_manifest') and self.cluster_data_manifest:
                for config_key, config_data in self.cluster_data_manifest.by_board_type.items():
                    total_games = config_data.get("total_games", 0)
                    parts = config_key.split("_")
                    if len(parts) >= 2:
                        board_type = parts[0]
                        num_players = parts[1].replace("p", "")
                        lines.append(f'ringrift_games_total{{board_type="{board_type}",num_players="{num_players}"}} {total_games}')

            # Add header for games total
            if hasattr(self, 'cluster_data_manifest') and self.cluster_data_manifest:
                lines.insert(-len(self.cluster_data_manifest.by_board_type),
                           "# HELP ringrift_games_total Total games per board configuration")
                lines.insert(-len(self.cluster_data_manifest.by_board_type),
                           "# TYPE ringrift_games_total gauge")

            # === CRITICAL SELF-IMPROVEMENT LOOP METRICS ===

            # Training Progress Metrics
            lines.append("# HELP ringrift_training_loss Current model training loss")
            lines.append("# TYPE ringrift_training_loss gauge")
            lines.append("# HELP ringrift_training_val_loss Current model validation loss")
            lines.append("# TYPE ringrift_training_val_loss gauge")
            lines.append("# HELP ringrift_training_epoch Current training epoch")
            lines.append("# TYPE ringrift_training_epoch gauge")
            if hasattr(self, 'training_metrics'):
                for config, metrics in self.training_metrics.items():
                    loss = metrics.get('loss', 0)
                    val_loss = metrics.get('val_loss', 0)
                    epoch = metrics.get('epoch', 0)
                    lines.append(f'ringrift_training_loss{{config="{config}"}} {loss}')
                    lines.append(f'ringrift_training_val_loss{{config="{config}"}} {val_loss}')
                    lines.append(f'ringrift_training_epoch{{config="{config}"}} {epoch}')

            # Data Freshness Metrics
            lines.append("# HELP ringrift_data_freshness_hours Age of newest training data in hours")
            lines.append("# TYPE ringrift_data_freshness_hours gauge")
            lines.append("# HELP ringrift_data_staleness_hours Age of oldest training data in hours")
            lines.append("# TYPE ringrift_data_staleness_hours gauge")
            try:
                from pathlib import Path
                selfplay_dir = Path("data/selfplay")
                if selfplay_dir.exists():
                    for config_dir in selfplay_dir.iterdir():
                        if config_dir.is_dir() and not config_dir.name.startswith('.'):
                            jsonl_files = list(config_dir.glob("*.jsonl"))
                            if jsonl_files:
                                newest = max(f.stat().st_mtime for f in jsonl_files)
                                oldest = min(f.stat().st_mtime for f in jsonl_files)
                                freshness_hours = (now - newest) / 3600
                                staleness_hours = (now - oldest) / 3600
                                config_name = config_dir.name
                                lines.append(f'ringrift_data_freshness_hours{{config="{config_name}"}} {freshness_hours:.2f}')
                                lines.append(f'ringrift_data_staleness_hours{{config="{config_name}"}} {staleness_hours:.2f}')
            except Exception:
                pass

            # Selfplay Throughput Metrics
            lines.append("# HELP ringrift_selfplay_games_per_hour Selfplay game generation rate")
            lines.append("# TYPE ringrift_selfplay_games_per_hour gauge")
            lines.append("# HELP ringrift_selfplay_games_total_24h Total games generated in last 24h")
            lines.append("# TYPE ringrift_selfplay_games_total_24h gauge")
            if hasattr(self, 'selfplay_throughput'):
                for config, rate in self.selfplay_throughput.items():
                    lines.append(f'ringrift_selfplay_games_per_hour{{config="{config}"}} {rate}')

            # Cost Efficiency Metrics
            lines.append("# HELP ringrift_gpu_hours_total Total GPU hours consumed")
            lines.append("# TYPE ringrift_gpu_hours_total counter")
            lines.append("# HELP ringrift_estimated_cost_usd Estimated cost in USD")
            lines.append("# TYPE ringrift_estimated_cost_usd gauge")
            lines.append("# HELP ringrift_elo_per_gpu_hour Elo improvement per GPU hour")
            lines.append("# TYPE ringrift_elo_per_gpu_hour gauge")
            if hasattr(self, 'cost_metrics'):
                gpu_hours = self.cost_metrics.get('gpu_hours_total', 0)
                cost_usd = self.cost_metrics.get('estimated_cost_usd', 0)
                elo_per_hour = self.cost_metrics.get('elo_per_gpu_hour', 0)
                lines.append(f"ringrift_gpu_hours_total {gpu_hours}")
                lines.append(f"ringrift_estimated_cost_usd {cost_usd}")
                lines.append(f"ringrift_elo_per_gpu_hour {elo_per_hour}")

            # Promotion Quality Metrics
            lines.append("# HELP ringrift_promotion_success_rate Promotion success rate (0-1)")
            lines.append("# TYPE ringrift_promotion_success_rate gauge")
            lines.append("# HELP ringrift_promotion_elo_gain Average Elo gain on successful promotion")
            lines.append("# TYPE ringrift_promotion_elo_gain gauge")
            lines.append("# HELP ringrift_promotion_rejections_total Total promotion rejections by reason")
            lines.append("# TYPE ringrift_promotion_rejections_total counter")
            if hasattr(self, 'promotion_metrics'):
                success_rate = self.promotion_metrics.get('success_rate', 0)
                avg_gain = self.promotion_metrics.get('avg_elo_gain', 0)
                lines.append(f"ringrift_promotion_success_rate {success_rate}")
                lines.append(f"ringrift_promotion_elo_gain {avg_gain}")
                for reason, count in self.promotion_metrics.get('rejections', {}).items():
                    lines.append(f'ringrift_promotion_rejections_total{{reason="{reason}"}} {count}')

            # Model Evaluation Quality Metrics
            lines.append("# HELP ringrift_eval_games_played Games played in model evaluation")
            lines.append("# TYPE ringrift_eval_games_played gauge")
            lines.append("# HELP ringrift_eval_confidence Evaluation confidence (0-1)")
            lines.append("# TYPE ringrift_eval_confidence gauge")
            lines.append("# HELP ringrift_elo_uncertainty Elo rating uncertainty margin")
            lines.append("# TYPE ringrift_elo_uncertainty gauge")
            try:
                from scripts.run_model_elo_tournament import ELO_DB_PATH, init_elo_database
                if ELO_DB_PATH and ELO_DB_PATH.exists():
                    db = init_elo_database()
                    conn = db._get_connection()
                    cursor = conn.cursor()
                    cursor.execute("""
                        SELECT board_type, num_players,
                               AVG(games_played) as avg_games,
                               AVG(rating_deviation) as avg_rd
                        FROM elo_ratings
                        WHERE games_played >= 5
                        GROUP BY board_type, num_players
                    """)
                    for row in cursor.fetchall():
                        bt, np, avg_games, avg_rd = row
                        config = f"{bt}_{np}p"
                        confidence = max(0, min(1, 1 - (avg_rd / 350)))  # RD 350 = 0% confidence
                        lines.append(f'ringrift_eval_games_played{{config="{config}"}} {avg_games:.1f}')
                        lines.append(f'ringrift_eval_confidence{{config="{config}"}} {confidence:.3f}')
                        lines.append(f'ringrift_elo_uncertainty{{config="{config}"}} {avg_rd:.1f}')
                    db.close()
            except Exception:
                pass

            # Improvement Loop Health Metrics
            lines.append("# HELP ringrift_improvement_cycles_total Total improvement cycles completed")
            lines.append("# TYPE ringrift_improvement_cycles_total counter")
            lines.append("# HELP ringrift_last_improvement_hours Hours since last Elo improvement")
            lines.append("# TYPE ringrift_last_improvement_hours gauge")
            lines.append("# HELP ringrift_training_queue_size Number of configs awaiting training")
            lines.append("# TYPE ringrift_training_queue_size gauge")
            if hasattr(self, 'improvement_cycle_manager') and self.improvement_cycle_manager:
                icm = self.improvement_cycle_manager
                # Count total training iterations across all cycles
                cycles_completed = sum(c.current_iteration for c in icm.state.cycles.values())
                lines.append(f"ringrift_improvement_cycles_total {cycles_completed}")

            # Victory Type Metrics by board config
            lines.append("# HELP ringrift_victory_type_total Games won by victory type")
            lines.append("# TYPE ringrift_victory_type_total counter")
            try:
                victory_stats = await self._get_victory_type_stats()
                for (board_type, num_players, victory_type), count in victory_stats.items():
                    lines.append(
                        f'ringrift_victory_type_total{{board_type="{board_type}",num_players="{num_players}",victory_type="{victory_type}"}} {count}'
                    )
            except Exception:
                pass

            # Game Analytics Metrics
            lines.append("# HELP ringrift_game_length_avg Average game length by config")
            lines.append("# TYPE ringrift_game_length_avg gauge")
            lines.append("# HELP ringrift_games_per_hour Game generation throughput")
            lines.append("# TYPE ringrift_games_per_hour gauge")
            lines.append("# HELP ringrift_opening_diversity Unique opening moves seen")
            lines.append("# TYPE ringrift_opening_diversity gauge")
            try:
                # Use cached analytics if available
                analytics = await self._get_game_analytics_cached()
                for config, stats in analytics.get("configs", {}).items():
                    parts = config.rsplit("_", 1)
                    if len(parts) == 2:
                        board_type = parts[0]
                        num_players = parts[1].replace("p", "")
                        lines.append(f'ringrift_game_length_avg{{board_type="{board_type}",num_players="{num_players}"}} {stats.get("avg_length", 0)}')
                        lines.append(f'ringrift_games_per_hour{{board_type="{board_type}",num_players="{num_players}"}} {stats.get("throughput_per_hour", 0)}')
                        lines.append(f'ringrift_opening_diversity{{board_type="{board_type}",num_players="{num_players}"}} {stats.get("opening_diversity", 0)}')
            except Exception:
                pass

            # Best Elo by Config
            lines.append("# HELP ringrift_best_elo Best Elo rating by config")
            lines.append("# TYPE ringrift_best_elo gauge")
            lines.append("# HELP ringrift_elo_games_played Games played by best model")
            lines.append("# TYPE ringrift_elo_games_played gauge")
            try:
                import sqlite3
                ai_root = Path(self.ringrift_path) / "ai-service"
                db_path = ai_root / "data" / "unified_elo.db"
                if not db_path.exists():
                    db_path = ai_root / "data" / "unified_elo.db"
                if db_path.exists():
                    conn = sqlite3.connect(db_path)
                    cursor = conn.cursor()
                    # Check which column name is used (model_id vs participant_id)
                    cursor.execute("PRAGMA table_info(elo_ratings)")
                    columns = [col[1] for col in cursor.fetchall()]
                    id_col = "model_id" if "model_id" in columns else "participant_id"
                    cursor.execute(f"""
                        SELECT board_type, num_players, MAX(rating), {id_col}, games_played
                        FROM elo_ratings
                        WHERE games_played >= 10
                        GROUP BY board_type, num_players
                    """)
                    for row in cursor.fetchall():
                        bt, np, rating, model, games = row
                        lines.append(f'ringrift_best_elo{{board_type="{bt}",num_players="{np}",model="{model}"}} {rating:.1f}')
                        lines.append(f'ringrift_elo_games_played{{board_type="{bt}",num_players="{np}",model="{model}"}} {games}')
                    conn.close()
            except Exception:
                pass

            # Training Loss Metrics (from latest training)
            lines.append("# HELP ringrift_training_loss Latest training loss")
            lines.append("# TYPE ringrift_training_loss gauge")
            lines.append("# HELP ringrift_training_epoch Current training epoch")
            lines.append("# TYPE ringrift_training_epoch gauge")
            try:
                training_metrics = await self._get_training_metrics_cached()
                for config, data in training_metrics.get("configs", {}).items():
                    parts = config.rsplit("_", 1)
                    if len(parts) == 2 and data.get("latest_loss"):
                        board_type = parts[0]
                        num_players = parts[1].replace("p", "")
                        lines.append(f'ringrift_training_loss{{board_type="{board_type}",num_players="{num_players}"}} {data["latest_loss"]}')
                        lines.append(f'ringrift_training_epoch{{board_type="{board_type}",num_players="{num_players}"}} {data.get("latest_epoch", 0)}')
            except Exception:
                pass

            # === HOLDOUT VALIDATION METRICS ===
            lines.append("# HELP ringrift_holdout_games Number of games in holdout set")
            lines.append("# TYPE ringrift_holdout_games gauge")
            lines.append("# HELP ringrift_holdout_positions Number of positions in holdout set")
            lines.append("# TYPE ringrift_holdout_positions gauge")
            lines.append("# HELP ringrift_holdout_loss Model loss on holdout validation set")
            lines.append("# TYPE ringrift_holdout_loss gauge")
            lines.append("# HELP ringrift_holdout_accuracy Model accuracy on holdout validation set")
            lines.append("# TYPE ringrift_holdout_accuracy gauge")
            lines.append("# HELP ringrift_overfit_gap Gap between holdout and training loss (positive = overfitting)")
            lines.append("# TYPE ringrift_overfit_gap gauge")
            try:
                holdout_metrics = await self._get_holdout_metrics_cached()
                for config, data in holdout_metrics.get("configs", {}).items():
                    parts = config.rsplit("_", 1)
                    if len(parts) == 2:
                        board_type = parts[0]
                        num_players = parts[1].replace("p", "")
                        lines.append(f'ringrift_holdout_games{{board_type="{board_type}",num_players="{num_players}"}} {data.get("holdout_games", 0)}')
                        lines.append(f'ringrift_holdout_positions{{board_type="{board_type}",num_players="{num_players}"}} {data.get("holdout_positions", 0)}')
                        if data.get("holdout_loss") is not None:
                            lines.append(f'ringrift_holdout_loss{{board_type="{board_type}",num_players="{num_players}"}} {data["holdout_loss"]}')
                        if data.get("holdout_accuracy") is not None:
                            lines.append(f'ringrift_holdout_accuracy{{board_type="{board_type}",num_players="{num_players}"}} {data["holdout_accuracy"]}')
                        if data.get("overfit_gap") is not None:
                            lines.append(f'ringrift_overfit_gap{{board_type="{board_type}",num_players="{num_players}"}} {data["overfit_gap"]}')
            except Exception:
                pass

            # === MCTS SEARCH STATISTICS ===
            lines.append("# HELP ringrift_mcts_avg_nodes Average MCTS nodes visited per move")
            lines.append("# TYPE ringrift_mcts_avg_nodes gauge")
            lines.append("# HELP ringrift_mcts_max_nodes Maximum MCTS nodes visited in a move")
            lines.append("# TYPE ringrift_mcts_max_nodes gauge")
            lines.append("# HELP ringrift_mcts_avg_depth Average MCTS search depth")
            lines.append("# TYPE ringrift_mcts_avg_depth gauge")
            lines.append("# HELP ringrift_mcts_max_depth Maximum MCTS search depth")
            lines.append("# TYPE ringrift_mcts_max_depth gauge")
            lines.append("# HELP ringrift_mcts_avg_time Average time per MCTS move (seconds)")
            lines.append("# TYPE ringrift_mcts_avg_time gauge")
            try:
                mcts_stats = await self._get_mcts_stats_cached()
                summary = mcts_stats.get("summary", {})
                if summary.get("avg_nodes_per_move"):
                    lines.append(f'ringrift_mcts_avg_nodes {summary["avg_nodes_per_move"]:.0f}')
                if summary.get("max_nodes_per_move"):
                    lines.append(f'ringrift_mcts_max_nodes {summary["max_nodes_per_move"]}')
                if summary.get("avg_search_depth"):
                    lines.append(f'ringrift_mcts_avg_depth {summary["avg_search_depth"]:.1f}')
                if summary.get("max_search_depth"):
                    lines.append(f'ringrift_mcts_max_depth {summary["max_search_depth"]}')
                if summary.get("avg_time_per_move"):
                    lines.append(f'ringrift_mcts_avg_time {summary["avg_time_per_move"]:.3f}')
                # Per-config MCTS stats
                for config, data in mcts_stats.get("configs", {}).items():
                    parts = config.rsplit("_", 1)
                    if len(parts) == 2:
                        board_type = parts[0]
                        num_players = parts[1].replace("p", "")
                        if data.get("avg_nodes"):
                            lines.append(f'ringrift_mcts_avg_nodes{{board_type="{board_type}",num_players="{num_players}"}} {data["avg_nodes"]:.0f}')
                        if data.get("avg_depth"):
                            lines.append(f'ringrift_mcts_avg_depth{{board_type="{board_type}",num_players="{num_players}"}} {data["avg_depth"]:.1f}')
            except Exception:
                pass

            # === DATA QUALITY METRICS ===
            lines.append("# HELP ringrift_data_quality_games Total games analyzed for quality")
            lines.append("# TYPE ringrift_data_quality_games gauge")
            lines.append("# HELP ringrift_data_quality_short_rate Percentage of short games (<10 moves)")
            lines.append("# TYPE ringrift_data_quality_short_rate gauge")
            lines.append("# HELP ringrift_data_quality_issues Number of data quality issues detected")
            lines.append("# TYPE ringrift_data_quality_issues gauge")
            try:
                quality = await self._get_data_quality_cached()
                for config, data in quality.get("configs", {}).items():
                    parts = config.rsplit("_", 1)
                    if len(parts) == 2:
                        board_type = parts[0]
                        num_players = parts[1].replace("p", "")
                        lines.append(f'ringrift_data_quality_games{{board_type="{board_type}",num_players="{num_players}"}} {data.get("total_games", 0)}')
                        lines.append(f'ringrift_data_quality_short_rate{{board_type="{board_type}",num_players="{num_players}"}} {data.get("short_game_rate", 0)}')
                lines.append(f'ringrift_data_quality_issues {len(quality.get("issues", []))}')
            except Exception:
                pass

            # === TRAINING EFFICIENCY METRICS ===
            lines.append("# HELP ringrift_gpu_hours_total Total GPU hours used for training")
            lines.append("# TYPE ringrift_gpu_hours_total gauge")
            lines.append("# HELP ringrift_elo_per_gpu_hour Elo points gained per GPU hour")
            lines.append("# TYPE ringrift_elo_per_gpu_hour gauge")
            lines.append("# HELP ringrift_training_cost_usd Estimated training cost in USD")
            lines.append("# TYPE ringrift_training_cost_usd gauge")
            try:
                efficiency = await self._get_training_efficiency_cached()
                for config, data in efficiency.get("configs", {}).items():
                    parts = config.rsplit("_", 1)
                    if len(parts) == 2:
                        board_type = parts[0]
                        num_players = parts[1].replace("p", "")
                        lines.append(f'ringrift_gpu_hours_total{{board_type="{board_type}",num_players="{num_players}"}} {data.get("gpu_hours", 0)}')
                        lines.append(f'ringrift_elo_per_gpu_hour{{board_type="{board_type}",num_players="{num_players}"}} {data.get("elo_per_gpu_hour", 0)}')
                        lines.append(f'ringrift_training_cost_usd{{board_type="{board_type}",num_players="{num_players}"}} {data.get("estimated_cost_usd", 0)}')
                summary = efficiency.get("summary", {})
                if summary:
                    lines.append(f'ringrift_gpu_hours_total {summary.get("total_gpu_hours", 0)}')
                    lines.append(f'ringrift_training_cost_usd {summary.get("total_estimated_cost_usd", 0)}')
            except Exception:
                pass

            # === MODEL LINEAGE METRICS ===
            lines.append("# HELP ringrift_model_count Total number of trained models")
            lines.append("# TYPE ringrift_model_count gauge")
            lines.append("# HELP ringrift_model_generation Latest model generation per config")
            lines.append("# TYPE ringrift_model_generation gauge")
            try:
                lineage = await self._get_model_lineage_cached()
                lines.append(f'ringrift_model_count {lineage.get("total_models", 0)}')
                for config, data in lineage.get("configs", {}).items():
                    parts = config.rsplit("_", 1)
                    if len(parts) == 2:
                        board_type = parts[0]
                        num_players = parts[1].replace("p", "")
                        lines.append(f'ringrift_model_generation{{board_type="{board_type}",num_players="{num_players}"}} {data.get("latest_generation", 0)}')
            except Exception:
                pass

            # === ROLLBACK STATUS METRICS ===
            lines.append("# HELP ringrift_rollback_candidates Number of configs recommended for rollback")
            lines.append("# TYPE ringrift_rollback_candidates gauge")
            try:
                rollback = await self._check_rollback_conditions()
                lines.append(f'ringrift_rollback_candidates {len(rollback.get("candidates", []))}')
            except Exception:
                pass

            # === AUTOSCALING METRICS ===
            lines.append("# HELP ringrift_autoscale_suggested_workers Suggested worker count from autoscaling")
            lines.append("# TYPE ringrift_autoscale_suggested_workers gauge")
            lines.append("# HELP ringrift_cluster_games_per_hour Current cluster-wide game generation rate")
            lines.append("# TYPE ringrift_cluster_games_per_hour gauge")
            try:
                autoscale = await self._get_autoscaling_metrics()
                state = autoscale.get("current_state", {})
                lines.append(f'ringrift_cluster_games_per_hour {state.get("games_per_hour", 0)}')
                recs = autoscale.get("recommendations", [])
                if recs:
                    lines.append(f'ringrift_autoscale_suggested_workers {recs[0].get("suggested_workers", state.get("total_nodes", 1))}')
                else:
                    lines.append(f'ringrift_autoscale_suggested_workers {state.get("total_nodes", 1)}')
            except Exception:
                pass

            # === P2P ENHANCEMENT METRICS ===

            # Adaptive Sync Intervals
            lines.append("# HELP ringrift_sync_interval_data Current data sync interval in seconds")
            lines.append("# TYPE ringrift_sync_interval_data gauge")
            lines.append("# HELP ringrift_sync_interval_model Current model sync interval in seconds")
            lines.append("# TYPE ringrift_sync_interval_model gauge")
            lines.append("# HELP ringrift_sync_activity_factor Cluster activity factor (lower = more active)")
            lines.append("# TYPE ringrift_sync_activity_factor gauge")
            try:
                sync_summary = self._get_sync_interval_summary()
                lines.append(f'ringrift_sync_interval_data {sync_summary.get("data_interval", 300)}')
                lines.append(f'ringrift_sync_interval_model {sync_summary.get("model_interval", 180)}')
                lines.append(f'ringrift_sync_activity_factor {sync_summary.get("activity_factor", 1.0)}')
            except Exception:
                pass

            # Gossip Protocol Metrics
            lines.append("# HELP ringrift_gossip_messages_sent Total gossip messages sent")
            lines.append("# TYPE ringrift_gossip_messages_sent counter")
            lines.append("# HELP ringrift_gossip_messages_received Total gossip messages received")
            lines.append("# TYPE ringrift_gossip_messages_received counter")
            lines.append("# HELP ringrift_gossip_state_updates Total state updates from gossip")
            lines.append("# TYPE ringrift_gossip_state_updates counter")
            lines.append("# HELP ringrift_gossip_compression_ratio Gossip compression ratio (1.0 = 100% compressed)")
            lines.append("# TYPE ringrift_gossip_compression_ratio gauge")
            lines.append("# HELP ringrift_gossip_bytes_saved_kb Total bytes saved by compression")
            lines.append("# TYPE ringrift_gossip_bytes_saved_kb counter")
            try:
                gossip = self._get_gossip_metrics_summary()
                lines.append(f'ringrift_gossip_messages_sent {gossip.get("message_sent", 0)}')
                lines.append(f'ringrift_gossip_messages_received {gossip.get("message_received", 0)}')
                lines.append(f'ringrift_gossip_state_updates {gossip.get("state_updates", 0)}')
                lines.append(f'ringrift_gossip_compression_ratio {gossip.get("compression_ratio", 0)}')
                lines.append(f'ringrift_gossip_bytes_saved_kb {gossip.get("bytes_saved_kb", 0)}')
            except Exception:
                pass

            # Leader Consensus Metrics
            lines.append("# HELP ringrift_leader_agreement Nodes agreeing on current leader")
            lines.append("# TYPE ringrift_leader_agreement gauge")
            try:
                consensus = self._get_cluster_leader_consensus()
                lines.append(f'ringrift_leader_agreement {consensus.get("leader_agreement", 0)}')
            except Exception:
                pass

            # Data Deduplication Metrics
            lines.append("# HELP ringrift_dedup_files_skipped Files skipped due to deduplication")
            lines.append("# TYPE ringrift_dedup_files_skipped counter")
            lines.append("# HELP ringrift_dedup_bytes_saved_mb Megabytes saved by deduplication")
            lines.append("# TYPE ringrift_dedup_bytes_saved_mb gauge")
            lines.append("# HELP ringrift_dedup_known_hashes Number of file hashes tracked")
            lines.append("# TYPE ringrift_dedup_known_hashes gauge")
            try:
                dedup = self._get_dedup_summary()
                lines.append(f'ringrift_dedup_files_skipped {dedup.get("files_skipped", 0)}')
                lines.append(f'ringrift_dedup_bytes_saved_mb {dedup.get("bytes_saved_mb", 0)}')
                lines.append(f'ringrift_dedup_known_hashes {dedup.get("known_file_hashes", 0)}')
            except Exception:
                pass

            # Tournament Scheduling Metrics
            lines.append("# HELP ringrift_tournament_proposals_pending Pending tournament proposals")
            lines.append("# TYPE ringrift_tournament_proposals_pending gauge")
            lines.append("# HELP ringrift_tournament_active Active distributed tournaments")
            lines.append("# TYPE ringrift_tournament_active gauge")
            try:
                tourney = self._get_distributed_tournament_summary()
                lines.append(f'ringrift_tournament_proposals_pending {tourney.get("pending_proposals", 0)}')
                lines.append(f'ringrift_tournament_active {tourney.get("active_tournaments", 0)}')
            except Exception:
                pass

            # Work Queue Metrics (leader only)
            wq = get_work_queue()
            if self.is_leader and wq:
                lines.append("# HELP ringrift_work_queue_pending Work items pending in queue")
                lines.append("# TYPE ringrift_work_queue_pending gauge")
                lines.append("# HELP ringrift_work_queue_running Work items currently running")
                lines.append("# TYPE ringrift_work_queue_running gauge")
                lines.append("# HELP ringrift_work_queue_total Total work items by status")
                lines.append("# TYPE ringrift_work_queue_total gauge")
                lines.append("# HELP ringrift_work_queue_by_type Work items by type and status")
                lines.append("# TYPE ringrift_work_queue_by_type gauge")
                lines.append("# HELP ringrift_work_queue_completed_total Total completed work items")
                lines.append("# TYPE ringrift_work_queue_completed_total counter")
                lines.append("# HELP ringrift_work_queue_failed_total Total failed work items")
                lines.append("# TYPE ringrift_work_queue_failed_total counter")
                lines.append("# HELP ringrift_work_queue_timeout_total Total timed out work items")
                lines.append("# TYPE ringrift_work_queue_timeout_total counter")
                lines.append("# HELP ringrift_work_queue_cancelled_total Total cancelled work items")
                lines.append("# TYPE ringrift_work_queue_cancelled_total counter")
                lines.append("# HELP ringrift_work_queue_avg_wait_seconds Average wait time in queue")
                lines.append("# TYPE ringrift_work_queue_avg_wait_seconds gauge")
                lines.append("# HELP ringrift_work_queue_avg_run_seconds Average run time for work items")
                lines.append("# TYPE ringrift_work_queue_avg_run_seconds gauge")

                try:
                    status = wq.get_queue_status()
                    by_status = status.get("by_status", {})

                    # Basic queue counts from by_status dict
                    pending_count = by_status.get("pending", 0)
                    running_count = by_status.get("running", 0) + by_status.get("claimed", 0)
                    lines.append(f"ringrift_work_queue_pending {pending_count}")
                    lines.append(f"ringrift_work_queue_running {running_count}")
                    lines.append(f'ringrift_work_queue_total{{status="pending"}} {pending_count}')
                    lines.append(f'ringrift_work_queue_total{{status="running"}} {running_count}')

                    # Count by work type from by_type dict
                    by_type = status.get("by_type", {})
                    for wtype, count in by_type.items():
                        lines.append(f'ringrift_work_queue_by_type{{work_type="{wtype}"}} {count}')

                    # Historical counts from database
                    history = wq.get_history(limit=1000)
                    completed_count = sum(1 for h in history if h.get("status") == "completed")
                    failed_count = sum(1 for h in history if h.get("status") == "failed")
                    timeout_count = sum(1 for h in history if h.get("status") == "timeout")
                    cancelled_count = sum(1 for h in history if h.get("status") == "cancelled")

                    lines.append(f"ringrift_work_queue_completed_total {completed_count}")
                    lines.append(f"ringrift_work_queue_failed_total {failed_count}")
                    lines.append(f"ringrift_work_queue_timeout_total {timeout_count}")
                    lines.append(f"ringrift_work_queue_cancelled_total {cancelled_count}")

                    # Calculate average wait and run times from completed items
                    wait_times = []
                    run_times = []
                    for h in history:
                        if h.get("status") == "completed":
                            created = h.get("created_at", 0)
                            claimed = h.get("claimed_at", 0)
                            completed = h.get("completed_at", 0)
                            if claimed and created:
                                wait_times.append(claimed - created)
                            if completed and claimed:
                                run_times.append(completed - claimed)

                    avg_wait = sum(wait_times) / len(wait_times) if wait_times else 0
                    avg_run = sum(run_times) / len(run_times) if run_times else 0
                    lines.append(f"ringrift_work_queue_avg_wait_seconds {avg_wait:.2f}")
                    lines.append(f"ringrift_work_queue_avg_run_seconds {avg_run:.2f}")

                except Exception:
                    # If work queue metrics fail, just skip them
                    pass

            # Uptime metric
            if hasattr(self, 'start_time'):
                uptime = now - self.start_time
                lines.append("# HELP ringrift_orchestrator_uptime_seconds Orchestrator uptime in seconds")
                lines.append("# TYPE ringrift_orchestrator_uptime_seconds gauge")
                lines.append(f"ringrift_orchestrator_uptime_seconds {uptime:.0f}")

            return web.Response(
                text="\n".join(lines) + "\n",
                content_type="text/plain",
                charset="utf-8",
            )

        except Exception as e:
            return web.json_response({"success": False, "error": str(e)})

    async def handle_improvement_training_complete(self, request: web.Request) -> web.Response:
        """POST /improvement_cycles/training_complete - Report training completion."""
        try:
            if not self.improvement_cycle_manager:
                return web.json_response({"success": False, "error": "ImprovementCycleManager not initialized"})

            data = await request.json()
            cycle_id = data.get("cycle_id")
            new_model_id = data.get("model_id")
            model_path = data.get("model_path", "")
            success = data.get("success", False)
            error_message = data.get("error", "")

            self.improvement_cycle_manager.handle_training_complete(
                cycle_id=cycle_id, new_model_id=new_model_id, model_path=model_path,
                success=success, error_message=error_message,
            )

            if success and self.role == NodeRole.LEADER:
                asyncio.create_task(self._schedule_improvement_evaluation(cycle_id, new_model_id))

            return web.json_response({"success": True})

        except Exception as e:
            return web.json_response({"success": False, "error": str(e)})

    async def handle_improvement_evaluation_complete(self, request: web.Request) -> web.Response:
        """POST /improvement_cycles/evaluation_complete - Report evaluation completion."""
        try:
            if not self.improvement_cycle_manager:
                return web.json_response({"success": False, "error": "ImprovementCycleManager not initialized"})

            data = await request.json()
            self.improvement_cycle_manager.handle_evaluation_complete(
                cycle_id=data.get("cycle_id"), new_model_id=data.get("model_id"),
                best_model_id=data.get("best_model_id"), wins=data.get("wins", 0),
                losses=data.get("losses", 0), draws=data.get("draws", 0),
            )

            # Auto-deploy model if evaluation passed (new model is best)
            if data.get("model_id") == data.get("best_model_id"):
                model_path = data.get("model_path", "")
                board_type = data.get("board_type", "square8")
                num_players = data.get("num_players", 2)
                if model_path:
                    asyncio.create_task(self._auto_deploy_model(model_path, board_type, num_players))

            return web.json_response({"success": True})

        except Exception as e:
            return web.json_response({"success": False, "error": str(e)})

    async def _schedule_improvement_evaluation(self, cycle_id: str, new_model_id: str):
        """Schedule tournament evaluation for a newly trained model via SSH."""
        if not self.improvement_cycle_manager:
            return
        try:
            cycle = self.improvement_cycle_manager.state.cycles.get(cycle_id)
            if not cycle:
                return

            config = cycle.config
            best_model_id = cycle.best_model_id or f"baseline_{config.board_type}_{config.num_players}p"

            logger.info(f"ImprovementCycle {cycle_id}: Scheduling evaluation {new_model_id} vs {best_model_id}")

            self.improvement_cycle_manager.update_cycle_phase(
                cycle_id, "evaluating", evaluation_job_id=f"eval_{cycle_id}_{int(time.time())}"
            )

            # Run SSH tournament evaluation
            eval_result = await self._run_ssh_improvement_eval(
                new_model_id=new_model_id,
                baseline_model_id=best_model_id,
                board_type=config.board_type,
                num_players=config.num_players,
                games=config.evaluation_games,
            )

            if eval_result.get("success"):
                new_model_wins = eval_result.get("new_model_wins", 0)
                baseline_wins = eval_result.get("baseline_wins", 0)
                draws = eval_result.get("draws", 0)
            else:
                # Fallback to mock results if SSH evaluation fails
                logger.info(f"ImprovementCycle {cycle_id}: SSH evaluation failed, using fallback")
                import random
                total_games = config.evaluation_games
                new_model_wins = random.randint(int(total_games * 0.4), int(total_games * 0.6))
                draws = random.randint(0, int(total_games * 0.1))
                baseline_wins = total_games - new_model_wins - draws

            self.improvement_cycle_manager.handle_evaluation_complete(
                cycle_id=cycle_id, new_model_id=new_model_id, best_model_id=best_model_id,
                wins=new_model_wins, losses=baseline_wins, draws=draws,
            )

        except Exception as e:
            logger.info(f"ImprovementCycle {cycle_id}: Evaluation scheduling failed: {e}")
            if self.improvement_cycle_manager:
                self.improvement_cycle_manager.update_cycle_phase(cycle_id, "idle", error_message=str(e))

    async def _run_ssh_improvement_eval(
        self,
        new_model_id: str,
        baseline_model_id: str,
        board_type: str,
        num_players: int,
        games: int,
    ) -> dict:
        """Run improvement evaluation via SSH on a remote host.

        Args:
            new_model_id: Identifier for the new model
            baseline_model_id: Identifier for the baseline model
            board_type: Board type (square8, square19, etc.)
            num_players: Number of players
            games: Number of games to play

        Returns:
            Dict with evaluation results or error
        """
        # Calculate timeout upfront to avoid scope issues in exception handler
        timeout_seconds = max(300, games * 30)  # 30s per game estimate, minimum 5 minutes

        try:
            # Get available hosts for evaluation
            if load_remote_hosts is None:
                return {"success": False, "error": "load_remote_hosts not available"}

            hosts = load_remote_hosts()
            if not hosts:
                return {"success": False, "error": "No remote hosts configured"}

            # Find a ready host with GPU capability (prefer high-performance hosts)
            eval_host = None
            for host in hosts:
                if getattr(host, 'status', None) == 'ready':
                    eval_host = host
                    break

            if not eval_host:
                # Try any host
                eval_host = hosts[0] if hosts else None

            if not eval_host:
                return {"success": False, "error": "No evaluation host available"}

            ssh_host = getattr(eval_host, 'ssh_host', None) or getattr(eval_host, 'tailscale_ip', None)
            if not ssh_host:
                return {"success": False, "error": "No SSH host configured"}

            ssh_user = getattr(eval_host, 'ssh_user', 'ubuntu')
            ringrift_path = getattr(eval_host, 'ringrift_path', '~/ringrift/ai-service')

            # Build model paths (assumes models are in standard locations)
            new_model_path = f"models/{board_type}_{num_players}p/{new_model_id}.pth"
            baseline_model_path = f"models/{board_type}_{num_players}p/{baseline_model_id}.pth"

            # Build SSH command
            remote_cmd = f'''cd {ringrift_path} && source venv/bin/activate && python scripts/run_improvement_eval.py \
                --new-model "{new_model_path}" \
                --baseline-model "{baseline_model_path}" \
                --board {board_type} \
                --players {num_players} \
                --games {games} \
                --ai-type descent 2>/dev/null'''

            logger.info(f"Running SSH evaluation on {eval_host.name}: {new_model_id} vs {baseline_model_id}")

            proc = await asyncio.create_subprocess_exec(
                "ssh",
                "-o", "ConnectTimeout=30",
                "-o", "BatchMode=yes",
                "-o", "StrictHostKeyChecking=no",
                f"{ssh_user}@{ssh_host}",
                remote_cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )

            stdout, stderr = await asyncio.wait_for(
                proc.communicate(),
                timeout=timeout_seconds
            )

            if proc.returncode != 0:
                stderr_text = stderr.decode()[:500] if stderr else ""
                logger.info(f"SSH evaluation failed on {eval_host.name}: {stderr_text}")
                return {"success": False, "error": f"SSH command failed: {stderr_text}"}

            # Parse JSON result from stdout
            stdout_text = stdout.decode().strip()
            if not stdout_text:
                return {"success": False, "error": "No output from evaluation script"}

            result = json.loads(stdout_text)
            logger.info(f"SSH evaluation complete: {result.get('new_model_wins', 0)}-{result.get('baseline_wins', 0)}-{result.get('draws', 0)}")
            return result

        except asyncio.TimeoutError:
            return {"success": False, "error": f"SSH evaluation timed out after {timeout_seconds}s"}
        except json.JSONDecodeError as e:
            return {"success": False, "error": f"Failed to parse evaluation result: {e}"}
        except Exception as e:
            return {"success": False, "error": str(e)}

    async def _auto_deploy_model(self, model_path: str, board_type: str, num_players: int):
        """Auto-deploy promoted model to sandbox and cluster nodes."""
        try:
            import subprocess
            logger.info(f"Auto-deploying model: {model_path}")

            # Run deployment script
            result = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: subprocess.run(
                    [
                        sys.executable, "scripts/auto_deploy_models.py",
                        "--model-path", model_path,
                        "--board-type", board_type,
                        "--num-players", str(num_players),
                        "--skip-eval",  # Already evaluated
                        "--sync-cluster" if self._is_leader() else "",
                    ],
                    capture_output=True, text=True, timeout=300,
                    cwd=str(Path(__file__).parent.parent)
                )
            )

            if result.returncode == 0:
                logger.info(f"Model deployed successfully: {model_path}")
            else:
                logger.info(f"Model deployment failed: {result.stderr}")

        except Exception as e:
            logger.info(f"Auto-deploy error: {e}")

    # Canonical Pipeline Integration (for pipeline_orchestrator.py)
    # =========================================================================

    async def handle_pipeline_start(self, request: web.Request) -> web.Response:
        """POST /pipeline/start - Start a canonical pipeline phase."""
        try:
            if not self._is_leader() and request.query.get("local") != "1":
                return await self._proxy_to_leader(request)
            if not self._is_leader():
                return web.json_response({"success": False, "error": "Only leader can start pipeline phases",
                                         "leader_id": self.leader_id}, status=403)
            data = await request.json()
            phase = data.get("phase")
            board_type = data.get("board_type", "square8")
            num_players = data.get("num_players", 2)

            if phase == "canonical_selfplay":
                result = await self._start_canonical_selfplay_pipeline(
                    board_type,
                    num_players,
                    data.get("games_per_node", 500),
                    data.get("seed", 0),
                    include_gpu_nodes=bool(data.get("include_gpu_nodes", False)),
                )
            elif phase == "parity_validation":
                result = await self._start_parity_validation_pipeline(
                    board_type, num_players, data.get("db_paths"))
            elif phase == "npz_export":
                result = await self._start_npz_export_pipeline(
                    board_type, num_players, data.get("output_dir", "data/training"))
            else:
                return web.json_response({"success": False,
                    "error": f"Unknown phase: {phase}. Supported: canonical_selfplay, parity_validation, npz_export"}, status=400)
            return web.json_response(result)
        except Exception as e:
            logger.info(f"Pipeline start error: {e}")
            return web.json_response({"success": False, "error": str(e)}, status=500)

    async def handle_pipeline_status(self, request: web.Request) -> web.Response:
        """GET /pipeline/status - Get current pipeline phase status."""
        if not self._is_leader() and request.query.get("local") != "1":
            proxied = await self._proxy_to_leader(request)
            if proxied.status not in (502, 503):
                return proxied
        pipeline_status = getattr(self, '_pipeline_status', {})
        return web.json_response({"success": True, "node_id": self.node_id,
                                 "is_leader": self._is_leader(), "current_job": pipeline_status})

    async def handle_pipeline_selfplay_worker(self, request: web.Request) -> web.Response:
        """POST /pipeline/selfplay_worker - Worker endpoint for canonical selfplay."""
        try:
            data = await request.json()
            asyncio.create_task(self._run_local_canonical_selfplay(
                data.get("job_id"), data.get("board_type", "square8"), data.get("num_players", 2),
                data.get("num_games", 500), data.get("seed", 0)))
            return web.json_response({"success": True, "job_id": data.get("job_id"),
                                     "message": f"Started canonical selfplay: {data.get('num_games', 500)} games"})
        except Exception as e:
            return web.json_response({"success": False, "error": str(e)}, status=500)

    async def _start_canonical_selfplay_pipeline(
        self,
        board_type: str,
        num_players: int,
        games_per_node: int,
        seed: int,
        include_gpu_nodes: bool = False,
    ) -> dict[str, Any]:
        """Start canonical selfplay on healthy nodes in the cluster.

        Canonical selfplay is CPU-bound. By default, prefer CPU-only nodes so GPU
        machines remain available for GPU-utilizing tasks (training/hybrid selfplay).
        """
        job_id = f"pipeline-selfplay-{int(time.time())}"
        healthy_nodes: list[tuple[str, NodeInfo]] = []
        with self.peers_lock:
            for peer_id, peer in self.peers.items():
                if peer.is_alive() and peer.is_healthy():
                    healthy_nodes.append((peer_id, peer))
        if self.self_info.is_healthy():
            healthy_nodes.append((self.node_id, self.self_info))

        if not include_gpu_nodes:
            cpu_nodes = [(nid, n) for nid, n in healthy_nodes if n.is_cpu_only_node()]
            if cpu_nodes:
                healthy_nodes = cpu_nodes

        # Load-balance: least-loaded nodes first.
        healthy_nodes.sort(key=lambda pair: pair[1].get_load_score())

        if not healthy_nodes:
            return {"success": False, "error": "No healthy nodes available"}

        logger.info(f"Starting canonical selfplay pipeline: {len(healthy_nodes)} nodes, {games_per_node} games/node")
        dispatched = 0
        for i, (node_id, node) in enumerate(healthy_nodes):
            node_seed = seed + i * 10000 + hash(node_id) % 10000
            if node_id == self.node_id:
                asyncio.create_task(self._run_local_canonical_selfplay(
                    f"{job_id}-{node_id}", board_type, num_players, games_per_node, node_seed))
                dispatched += 1
            else:
                try:
                    if getattr(node, "nat_blocked", False):
                        payload = {
                            "job_id": f"{job_id}-{node_id}",
                            "board_type": board_type,
                            "num_players": num_players,
                            "num_games": games_per_node,
                            "seed": node_seed,
                        }
                        cmd_id = await self._enqueue_relay_command_for_peer(node, "canonical_selfplay", payload)
                        if cmd_id:
                            dispatched += 1
                        else:
                            logger.info(f"Relay queue full; skipping canonical selfplay enqueue for {node_id}")
                    else:
                        payload = {
                            "job_id": f"{job_id}-{node_id}",
                            "board_type": board_type,
                            "num_players": num_players,
                            "num_games": games_per_node,
                            "seed": node_seed,
                        }
                        async with get_client_session(ClientTimeout(total=30)) as session:
                            for url in self._urls_for_peer(node, "/pipeline/selfplay_worker"):
                                try:
                                    async with session.post(url, json=payload, headers=self._get_auth_headers()) as resp:
                                        if resp.status == 200:
                                            dispatched += 1
                                            break
                                except Exception:
                                    continue
                except Exception as e:
                    logger.error(f"Failed to dispatch selfplay to {node_id}: {e}")

        self._pipeline_status = {"job_id": job_id, "phase": "canonical_selfplay", "status": "running",
            "dispatched_count": dispatched, "total_nodes": len(healthy_nodes),
            "board_type": board_type, "num_players": num_players,
            "games_per_node": games_per_node, "started_at": time.time()}
        return {"success": True, "job_id": job_id, "dispatched_count": dispatched, "total_nodes": len(healthy_nodes)}

    async def _run_local_canonical_selfplay(self, job_id: str, board_type: str, num_players: int,
                                            num_games: int, seed: int):
        """Run canonical selfplay locally."""
        try:
            db_file = os.path.join(self.ringrift_path, "ai-service", "data", "games",
                                   f"canonical_{board_type}_{num_players}p_{self.node_id}.db")
            log_file = os.path.join(self.ringrift_path, "ai-service", "logs", "selfplay",
                                    f"canonical_{job_id}.jsonl")
            os.makedirs(os.path.dirname(db_file), exist_ok=True)
            os.makedirs(os.path.dirname(log_file), exist_ok=True)

            cmd = [sys.executable, os.path.join(self.ringrift_path, "ai-service", "scripts", "run_self_play_soak.py"),
                "--num-games", str(num_games), "--board-type", board_type, "--num-players", str(num_players),
                "--max-moves", "10000",  # LEARNED LESSONS - Avoid draws due to move limit
                "--difficulty-band", "light", "--seed", str(seed), "--log-jsonl", log_file, "--record-db", db_file]
            env = os.environ.copy()
            env["PYTHONPATH"] = os.path.join(self.ringrift_path, "ai-service")
            env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

            logger.info(f"Starting canonical selfplay job {job_id}: {num_games} games -> {db_file}")
            proc = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE,
                                                        stderr=asyncio.subprocess.PIPE, env=env)
            _stdout, stderr = await proc.communicate()
            if proc.returncode == 0:
                logger.info(f"Canonical selfplay job {job_id} completed successfully")
            else:
                logger.info(f"Canonical selfplay job {job_id} failed: {stderr.decode()[:500]}")
        except Exception as e:
            logger.info(f"Canonical selfplay job {job_id} error: {e}")

    async def _start_parity_validation_pipeline(self, board_type: str, num_players: int,
                                                db_paths: list[str] | None) -> dict[str, Any]:
        """Start parity validation on the leader node."""
        job_id = f"pipeline-parity-{int(time.time())}"
        asyncio.create_task(self._run_parity_validation(job_id, board_type, num_players, db_paths))
        self._pipeline_status = {"job_id": job_id, "phase": "parity_validation", "status": "running",
                                "board_type": board_type, "num_players": num_players, "started_at": time.time()}
        return {"success": True, "job_id": job_id, "message": "Parity validation started"}

    async def _run_parity_validation(self, job_id: str, board_type: str, num_players: int,
                                     db_paths: list[str] | None):
        """Run parity validation."""
        try:
            if not db_paths:
                import glob
                db_paths = glob.glob(os.path.join(self.ringrift_path, "ai-service", "data", "games",
                                                  f"canonical_{board_type}_{num_players}p_*.db"))
            if not db_paths:
                self._pipeline_status["status"] = "failed"
                self._pipeline_status["error"] = "No databases found"
                return

            output_json = os.path.join(self.ringrift_path, "ai-service", "data", f"parity_validation_{job_id}.json")
            cmd = [sys.executable, os.path.join(self.ringrift_path, "ai-service", "scripts", "run_parity_validation.py"),
                "--databases", *db_paths, "--mode", "canonical", "--output-json", output_json, "--progress-every", "100"]
            env = os.environ.copy()
            env["PYTHONPATH"] = os.path.join(self.ringrift_path, "ai-service")
            env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

            logger.info(f"Starting parity validation job {job_id}: {len(db_paths)} databases")
            proc = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE,
                                                        stderr=asyncio.subprocess.PIPE, env=env)
            _stdout, stderr = await proc.communicate()
            if proc.returncode == 0:
                logger.info(f"Parity validation job {job_id} completed successfully")
                self._pipeline_status["status"] = "completed"
                if os.path.exists(output_json):
                    with open(output_json) as f:
                        self._pipeline_status["results"] = json.load(f)
            else:
                logger.info(f"Parity validation job {job_id} failed: {stderr.decode()[:500]}")
                self._pipeline_status["status"] = "failed"
                self._pipeline_status["error"] = stderr.decode()[:500]
        except Exception as e:
            logger.info(f"Parity validation job {job_id} error: {e}")
            self._pipeline_status["status"] = "failed"
            self._pipeline_status["error"] = str(e)

    async def _start_npz_export_pipeline(self, board_type: str, num_players: int,
                                         output_dir: str) -> dict[str, Any]:
        """Start NPZ export on the leader node."""
        job_id = f"pipeline-npz-{int(time.time())}"
        asyncio.create_task(self._run_npz_export(job_id, board_type, num_players, output_dir))
        self._pipeline_status = {"job_id": job_id, "phase": "npz_export", "status": "running",
                                "board_type": board_type, "num_players": num_players,
                                "output_dir": output_dir, "started_at": time.time()}
        return {"success": True, "job_id": job_id, "message": "NPZ export started"}

    async def _run_npz_export(self, job_id: str, board_type: str, num_players: int, output_dir: str):
        """Run NPZ export."""
        try:
            import glob
            db_paths = glob.glob(os.path.join(self.ringrift_path, "ai-service", "data", "games",
                                              f"canonical_{board_type}_{num_players}p_*.db"))
            if not db_paths:
                self._pipeline_status["status"] = "failed"
                self._pipeline_status["error"] = "No databases found"
                return

            full_output_dir = os.path.join(self.ringrift_path, "ai-service", output_dir)
            os.makedirs(full_output_dir, exist_ok=True)
            output_file = os.path.join(full_output_dir, f"canonical_{board_type}_{num_players}p_{job_id}.npz")

            cmd = [sys.executable, os.path.join(self.ringrift_path, "ai-service", "scripts", "export_replay_dataset.py"),
                "--databases", *db_paths, "--output", output_file, "--board-type", board_type,
                "--num-players", str(num_players)]
            env = os.environ.copy()
            env["PYTHONPATH"] = os.path.join(self.ringrift_path, "ai-service")

            logger.info(f"Starting NPZ export job {job_id}: {len(db_paths)} databases -> {output_file}")
            proc = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE,
                                                        stderr=asyncio.subprocess.PIPE, env=env)
            _stdout, stderr = await proc.communicate()
            if proc.returncode == 0:
                logger.info(f"NPZ export job {job_id} completed successfully")
                self._pipeline_status["status"] = "completed"
                self._pipeline_status["output_file"] = output_file
            else:
                logger.info(f"NPZ export job {job_id} failed: {stderr.decode()[:500]}")
                self._pipeline_status["status"] = "failed"
                self._pipeline_status["error"] = stderr.decode()[:500]
        except Exception as e:
            logger.info(f"NPZ export job {job_id} error: {e}")
            self._pipeline_status["status"] = "failed"
            self._pipeline_status["error"] = str(e)

    def _get_auth_headers(self) -> dict[str, str]:
        """Get authentication headers for peer requests."""
        return {"Authorization": f"Bearer {self.auth_token}"} if self.auth_token else {}

    # =========================================================================
    # Phase 4: REST API for External Job Submission and Dashboard
    # =========================================================================

    async def handle_root(self, request: web.Request) -> web.StreamResponse:
        """Redirect to the dashboard to avoid upstream 404s on `/`."""
        raise web.HTTPFound("/dashboard")

    async def handle_api_cluster_status(self, request: web.Request) -> web.Response:
        """Get comprehensive cluster status for external clients and dashboard."""
        try:
            if not self._is_leader() and request.query.get("local") != "1":
                proxied = await self._proxy_to_leader(request)
                if proxied.status not in (502, 503):
                    return proxied

            # Ensure local resource stats are fresh for dashboard consumers.
            with contextlib.suppress(Exception):
                self._update_self_info()

            is_leader = self._is_leader()
            effective_leader = self._get_leader_peer()
            effective_leader_id = effective_leader.node_id if effective_leader else None
            last_known_leader_id = self.leader_id
            leader_id = effective_leader_id or last_known_leader_id

            # Collect peer info (dashboard-oriented shape)
            peers_info: list[dict[str, Any]] = []
            include_retired = request.query.get("include_retired") == "1"
            with self.peers_lock:
                peers_snapshot = dict(self.peers)
            for peer_id, peer in peers_snapshot.items():
                if getattr(peer, "retired", False) and not include_retired:
                    continue
                status = "offline" if not peer.is_alive() else "online"
                key = self._endpoint_key(peer)
                effective_scheme, effective_host, effective_port = (None, None, None)
                if key:
                    effective_scheme, effective_host, effective_port = key
                peers_info.append(
                    {
                        "node_id": peer_id,
                        "host": peer.host,
                        "port": peer.port,
                        "scheme": getattr(peer, "scheme", "http"),
                        "reported_host": getattr(peer, "reported_host", ""),
                        "reported_port": getattr(peer, "reported_port", 0),
                        "effective_scheme": effective_scheme,
                        "effective_host": effective_host,
                        "effective_port": effective_port,
                        "nat_blocked": bool(getattr(peer, "nat_blocked", False)),
                        "relay_via": getattr(peer, "relay_via", ""),
                        "role": peer.role.value if hasattr(peer.role, "value") else str(peer.role),
                        "version": getattr(peer, "version", ""),
                        "status": status,
                        "last_seen": peer.last_heartbeat,
                        "capabilities": list(peer.capabilities) if peer.capabilities else [],
                        "current_job": "",
                        "has_gpu": bool(peer.has_gpu),
                        "cpu_percent": peer.cpu_percent,
                        "memory_percent": peer.memory_percent,
                        "disk_percent": peer.disk_percent,
                        "gpu_percent": peer.gpu_percent,
                        "gpu_memory_percent": peer.gpu_memory_percent,
                        "selfplay_jobs": peer.selfplay_jobs,
                        "training_jobs": peer.training_jobs,
                    }
                )

            # Collect local job info
            with self.jobs_lock:
                jobs_snapshot = list(self.local_jobs.values())
            jobs_info: list[dict[str, Any]] = [
                {
                    "job_id": job.job_id,
                    "job_type": job.job_type.value if hasattr(job.job_type, "value") else str(job.job_type),
                    "status": job.status,
                    "node_id": job.node_id,
                    "board_type": job.board_type,
                    "num_players": job.num_players,
                    "engine_mode": job.engine_mode,
                    "pid": job.pid,
                    "started_at": job.started_at,
                }
                for job in jobs_snapshot
            ]

            # Collect training job info
            training_info: list[dict[str, Any]] = []
            with self.training_lock:
                for job_id, job in self.training_jobs.items():
                    training_info.append(
                        {
                            "job_id": job_id,
                            "job_type": job.job_type,
                            "status": job.status,
                            "board_type": job.board_type,
                            "num_players": job.num_players,
                            "assigned_worker": job.worker_node,
                            "created_at": job.created_at,
                            "started_at": job.started_at,
                            "completed_at": job.completed_at,
                            "output_model_path": job.output_model_path,
                            "error_message": job.error_message,
                        }
                    )

            # Collect data manifest info (lightweight dashboard summary)
            # NOTE: Never block on manifest collection here - use cached data only.
            # The background _manifest_collection_loop will populate this shortly after startup.
            with self.manifest_lock:
                local_manifest = self.local_data_manifest
                cluster_manifest = self.cluster_data_manifest
                # Don't block on manifest collection - return what we have
                # local_manifest may be None during startup, which is fine

            manifest_info: dict[str, dict[str, Any]] = {}
            if cluster_manifest and getattr(cluster_manifest, "node_manifests", None):
                for node_id, node_manifest in cluster_manifest.node_manifests.items():
                    board_types = sorted(
                        {f.board_type for f in node_manifest.files if getattr(f, "board_type", "")}
                    )
                    manifest_info[node_id] = {
                        "game_count": node_manifest.selfplay_games,
                        "board_types": board_types,
                        "last_updated": node_manifest.collected_at,
                    }
            elif local_manifest:
                board_types = sorted(
                    {f.board_type for f in local_manifest.files if getattr(f, "board_type", "")}
                )
                manifest_info[local_manifest.node_id] = {
                    "game_count": local_manifest.selfplay_games,
                    "board_types": board_types,
                    "last_updated": local_manifest.collected_at,
                }

            voter_ids = list(getattr(self, "voter_node_ids", []) or [])
            voters_alive = 0
            if voter_ids:
                with self.peers_lock:
                    peers_by_id = dict(self.peers)
                for vid in voter_ids:
                    if vid == self.node_id:
                        voters_alive += 1
                        continue
                    p = peers_by_id.get(vid)
                    if p and p.is_alive():
                        voters_alive += 1

            self_payload = self.self_info.to_dict() if hasattr(self.self_info, "to_dict") else asdict(self.self_info)
            self_key = self._endpoint_key(self.self_info)
            if self_key:
                self_payload.update(
                    {
                        "effective_scheme": self_key[0],
                        "effective_host": self_key[1],
                        "effective_port": self_key[2],
                    }
                )

            return web.json_response({
                "success": True,
                "node_id": self.node_id,
                "role": self.role.value if hasattr(self.role, 'value') else str(self.role),
                "leader_id": leader_id,
                "effective_leader_id": effective_leader_id,
                "last_known_leader_id": last_known_leader_id,
                "is_leader": is_leader,
                "voter_node_ids": voter_ids,
                "voter_quorum_size": int(getattr(self, "voter_quorum_size", 0) or 0),
                "voters_alive": voters_alive,
                "voter_quorum_ok": self._has_voter_quorum(),
                "voter_config_source": str(getattr(self, "voter_config_source", "") or ""),
                "self": self_payload,
                "uptime_seconds": time.time() - self.start_time,
                "peers": peers_info,
                "peer_count": len(self.peers),
                "jobs": jobs_info,
                "job_count": len(jobs_info),
                "training_jobs": training_info,
                "training_job_count": len(training_info),
                "data_manifests": manifest_info,
                "timestamp": time.time(),
            })
        except Exception as e:
            return web.json_response({"success": False, "error": str(e)}, status=500)

    async def handle_api_cluster_git_update(self, request: web.Request) -> web.Response:
        """Leader-coordinated git updates for cluster nodes.

        Body (JSON):
            node_ids: list[str] | str (optional)
                If omitted, updates all known peers (online by default).
            include_self: bool (default False)
                If true and (node_ids omitted or includes this node_id), also update
                the leader node itself (performed last, triggers restart).
            include_offline: bool (default False)
                If true, attempt updates against offline peers as well.
            timeout_seconds: int (default 20, max 120)
                Per-peer request timeout.

        Notes:
            - This stops jobs and restarts orchestrators on nodes with updates
              available. Use with care.
        """
        try:
            if not self._is_leader() and request.query.get("local") != "1":
                return await self._proxy_to_leader(request)

            payload: dict[str, Any] = {}
            try:
                payload = await request.json()
            except Exception:
                payload = {}

            node_ids_raw = payload.get("node_ids") or payload.get("nodes") or []
            node_ids: list[str] = []
            if isinstance(node_ids_raw, str):
                node_ids = [t.strip() for t in node_ids_raw.split(",") if t.strip()]
            elif isinstance(node_ids_raw, list):
                node_ids = [str(t).strip() for t in node_ids_raw if str(t).strip()]

            include_self = bool(payload.get("include_self", False))
            include_offline = bool(payload.get("include_offline", False))

            timeout_seconds = float(payload.get("timeout_seconds", 20) or 20)
            timeout_seconds = max(5.0, min(timeout_seconds, 120.0))

            with self.peers_lock:
                peers_by_id = dict(self.peers)

            targets: list[NodeInfo] = []

            def should_include_peer(peer: NodeInfo) -> bool:
                if peer.node_id == self.node_id:
                    return False
                return not (not include_offline and not peer.is_alive())

            if node_ids:
                for node_id in node_ids:
                    peer = peers_by_id.get(node_id)
                    if peer and should_include_peer(peer):
                        targets.append(peer)
            else:
                for peer in peers_by_id.values():
                    if should_include_peer(peer):
                        targets.append(peer)

            results: list[dict[str, Any]] = []
            timeout = ClientTimeout(total=timeout_seconds)
            async with get_client_session(timeout) as session:
                for peer in sorted(targets, key=lambda p: p.node_id):
                    peer_payload: dict[str, Any] = {
                        "node_id": peer.node_id,
                        "status": "online" if peer.is_alive() else "offline",
                        "success": False,
                        "attempted_urls": [],
                    }

                    if not include_offline and not peer.is_alive():
                        peer_payload["error"] = "offline"
                        results.append(peer_payload)
                        continue

                    last_error: str | None = None
                    for url in self._urls_for_peer(peer, "/git/update"):
                        peer_payload["attempted_urls"].append(url)
                        try:
                            async with session.post(url, json={}, headers=self._auth_headers()) as resp:
                                peer_payload["http_status"] = resp.status
                                try:
                                    data = await resp.json()
                                except Exception:
                                    data = {"raw": await resp.text()}
                                peer_payload["response"] = data
                                if resp.status == 200:
                                    peer_payload["success"] = bool(data.get("success", True))
                                    break
                                last_error = (
                                    str(data.get("error") or "")
                                    or str(data.get("message") or "")
                                    or f"http_{resp.status}"
                                )
                        except Exception as exc:
                            last_error = str(exc)
                            continue

                    if last_error and not peer_payload.get("success"):
                        peer_payload["error"] = last_error

                    results.append(peer_payload)

            self_update: dict[str, Any] | None = None
            update_self = bool(include_self and (not node_ids or self.node_id in node_ids))
            if update_self:
                has_updates, local_commit, remote_commit = self._check_for_updates()
                if not has_updates:
                    self_update = {
                        "node_id": self.node_id,
                        "success": True,
                        "message": "Already up to date",
                        "local_commit": local_commit[:8] if local_commit else None,
                    }
                else:
                    success, message = await self._perform_git_update()
                    self_update = {
                        "node_id": self.node_id,
                        "success": success,
                        "message": message,
                        "old_commit": local_commit[:8] if local_commit else None,
                        "new_commit": remote_commit[:8] if remote_commit else None,
                    }
                    if success:
                        asyncio.create_task(self._restart_orchestrator())

            return web.json_response(
                {
                    "success": True,
                    "leader_id": self.node_id,
                    "updated_peers": results,
                    "self_update": self_update,
                    "timestamp": time.time(),
                }
            )
        except Exception as e:
            return web.json_response({"success": False, "error": str(e)}, status=500)

    async def handle_api_selfplay_stats(self, request: web.Request) -> web.Response:
        """Get aggregated selfplay game statistics for dashboard charts."""
        try:
            if not self._is_leader() and request.query.get("local") != "1":
                proxied = await self._proxy_to_leader(request)
                if proxied.status not in (502, 503):
                    return proxied

            with self.manifest_lock:
                cluster_manifest = self.cluster_data_manifest
                local_manifest = self.local_data_manifest
                history = list(self.selfplay_stats_history)

            by_board_type: dict[str, dict[str, Any]] = {}
            total_selfplay_games = 0
            manifest_collected_at = 0.0

            if cluster_manifest:
                by_board_type = cluster_manifest.by_board_type
                total_selfplay_games = int(cluster_manifest.total_selfplay_games or 0)
                manifest_collected_at = float(cluster_manifest.collected_at or 0.0)
            elif local_manifest:
                manifest_collected_at = float(local_manifest.collected_at or 0.0)
                totals: dict[str, int] = {}
                for f in getattr(local_manifest, "files", []) or []:
                    if getattr(f, "file_type", "") != "selfplay":
                        continue
                    board_type = getattr(f, "board_type", "") or ""
                    num_players = int(getattr(f, "num_players", 0) or 0)
                    if not board_type or not num_players:
                        continue
                    key = f"{board_type}_{num_players}p"
                    totals[key] = totals.get(key, 0) + int(getattr(f, "game_count", 0) or 0)
                by_board_type = {k: {"total_games": v, "nodes": [local_manifest.node_id]} for k, v in totals.items()}
                total_selfplay_games = sum(totals.values())

            return web.json_response(
                {
                    "success": True,
                    "node_id": self.node_id,
                    "is_leader": self._is_leader(),
                    "manifest_collected_at": manifest_collected_at,
                    "total_selfplay_games": total_selfplay_games,
                    "by_board_type": by_board_type,
                    "history": history,
                    "timestamp": time.time(),
                }
            )
        except Exception as e:
            return web.json_response({"success": False, "error": str(e)}, status=500)

    async def handle_api_elo_leaderboard(self, request: web.Request) -> web.Response:
        """Get Elo leaderboard for all board types from persistent database.

        Query params:
            board_type: Filter by board type (optional)
            num_players: Filter by number of players (optional)
            limit: Max results per config (default 20)
        """
        try:
            # Try to import Elo database functions
            try:
                from scripts.run_model_elo_tournament import (
                    ELO_DB_PATH,
                    get_leaderboard,
                    init_elo_database,
                )
            except ImportError:
                return web.json_response({
                    "success": False,
                    "error": "Elo database module not available",
                }, status=500)

            # Check if database exists
            if not ELO_DB_PATH or not ELO_DB_PATH.exists():
                return web.json_response({
                    "success": True,
                    "leaderboards": {},
                    "message": "No Elo database found yet. Run cross-model tournament to populate.",
                })

            board_type = request.query.get("board_type")
            num_players_str = request.query.get("num_players")
            num_players = int(num_players_str) if num_players_str else None
            limit = int(request.query.get("limit", "20"))

            db = init_elo_database()

            # If specific filter requested, return just that
            if board_type and num_players:
                leaderboard = get_leaderboard(db, board_type, num_players, limit=limit)
                db.close()
                return web.json_response({
                    "success": True,
                    "leaderboards": {f"{board_type}_{num_players}p": leaderboard},
                    "total_models": len(leaderboard),
                    "timestamp": time.time(),
                })

            # Otherwise return all board/player combinations
            # Query unique board_type/num_players combinations
            conn = db._get_connection()
            cursor = conn.cursor()
            cursor.execute("""
                SELECT DISTINCT board_type, num_players
                FROM elo_ratings
                WHERE board_type IS NOT NULL AND num_players IS NOT NULL
                ORDER BY board_type, num_players
            """)
            configs = cursor.fetchall()

            leaderboards = {}
            total_models = 0
            total_games = 0

            for bt, np in configs:
                key = f"{bt}_{np}p"
                lb = get_leaderboard(db, bt, np, limit=limit)
                if lb:
                    leaderboards[key] = lb
                    total_models += len(lb)
                    total_games += sum(entry.get("games_played", 0) for entry in lb)

            # Get match history stats
            cursor.execute("SELECT COUNT(*) FROM match_history")
            match_count = cursor.fetchone()[0]

            db.close()

            return web.json_response({
                "success": True,
                "leaderboards": leaderboards,
                "total_models": total_models,
                "total_matches": match_count,
                "total_games_recorded": total_games,
                "configs": [f"{bt}_{np}p" for bt, np in configs],
                "timestamp": time.time(),
            })

        except Exception as e:
            return web.json_response({"success": False, "error": str(e)}, status=500)

    async def handle_elo_table(self, request: web.Request) -> web.Response:
        """GET /elo/table - Elo leaderboard in flat table format for Grafana Infinity.

        Query params:
            - source: "tournament" (default) or "trained" (actual trained NN models)
            - limit: Max entries (default 50)
            - board_type: Filter by board type
            - num_players: Filter by player count
            - nn_only: If "true", filter to NN models only (for tournament source)

        Returns a simple JSON array of model entries with rank, suitable for table display.
        """
        import sqlite3

        try:
            source = request.query.get("source", "tournament")
            limit = int(request.query.get("limit", "50"))
            board_type_filter = request.query.get("board_type")
            num_players_filter = request.query.get("num_players")
            nn_only = request.query.get("nn_only", "").lower() == "true"

            ai_root = Path(self.ringrift_path) / "ai-service"

            if source == "trained":
                # Use unified_elo.db - actual trained NN models
                db_path = ai_root / "data" / "unified_elo.db"
                if not db_path.exists():
                    return web.json_response([])

                conn = sqlite3.connect(db_path)
                cursor = conn.cursor()

                # unified_elo.db has different schema (model_id instead of participant_id)
                query = """
                    SELECT model_id, rating, games_played, wins, losses
                    FROM elo_ratings
                    WHERE games_played >= 10
                """
                params = []

                if nn_only:
                    query += " AND (model_id LIKE '%nn%' OR model_id LIKE '%NN%' OR model_id LIKE '%baseline%')"

                query += " ORDER BY rating DESC LIMIT ?"
                params.append(limit)

                cursor.execute(query, params)
                rows = cursor.fetchall()
                conn.close()

                # Build flat table response
                table_data = []
                for rank, row in enumerate(rows, 1):
                    model_id, rating, games, wins, losses = row

                    # Extract config from model name
                    if "sq8" in model_id.lower() or "square8" in model_id.lower():
                        config = "square8_2p"
                    elif "sq19" in model_id.lower() or "square19" in model_id.lower():
                        config = "square19_2p"
                    elif "hex" in model_id.lower():
                        config = "hexagonal_2p"
                    else:
                        config = "unknown"

                    # Calculate win rate
                    total_decided = wins + losses
                    win_rate = wins / total_decided if total_decided > 0 else 0.5

                    table_data.append({
                        "Rank": rank,
                        "Model": model_id,
                        "Elo": round(rating, 1),
                        "WinRate": round(win_rate * 100, 1),
                        "Games": games,
                        "Wins": wins,
                        "Losses": losses,
                        "Draws": 0,
                        "Config": config,
                    })

                return web.json_response(table_data)

            else:
                # Default: tournament participants from unified_elo.db
                from scripts.run_model_elo_tournament import (
                    ELO_DB_PATH,
                    init_elo_database,
                )

                if not ELO_DB_PATH or not ELO_DB_PATH.exists():
                    return web.json_response([])

                db = init_elo_database()
                conn = db._get_connection()
                cursor = conn.cursor()

                # Build query with optional filters
                # Use db.id_column to get correct column name (model_id or participant_id)
                id_col = db.id_column

                query = f"""
                    SELECT
                        {id_col},
                        board_type,
                        num_players,
                        rating,
                        games_played,
                        wins,
                        losses,
                        draws,
                        last_update
                    FROM elo_ratings
                    WHERE games_played >= 5
                """
                params = []

                if board_type_filter:
                    query += " AND board_type = ?"
                    params.append(board_type_filter)

                if num_players_filter:
                    query += " AND num_players = ?"
                    params.append(int(num_players_filter))

                if nn_only:
                    query += f" AND ({id_col} LIKE '%NN%' OR {id_col} LIKE '%nn%')"

                query += " ORDER BY rating DESC LIMIT ?"
                params.append(limit)

                cursor.execute(query, params)
                rows = cursor.fetchall()
                db.close()

                # Build flat table response
                table_data = []
                for rank, row in enumerate(rows, 1):
                    participant_id, board_type, num_players, rating, games, wins, losses, draws, _last_update = row

                    # Extract model name from participant_id
                    model_name = participant_id
                    if participant_id.startswith("nn:"):
                        model_name = Path(participant_id[3:]).stem

                    # Calculate win rate
                    total_decided = wins + losses
                    win_rate = wins / total_decided if total_decided > 0 else 0.5

                    # Format config
                    config = f"{board_type}_{num_players}p"

                    table_data.append({
                        "Rank": rank,
                        "Model": model_name,
                        "Elo": round(rating, 1),
                        "WinRate": round(win_rate * 100, 1),
                        "Games": games,
                        "Wins": wins,
                        "Losses": losses,
                        "Draws": draws,
                        "Config": config,
                    })

                return web.json_response(table_data)

        except ImportError:
            return web.json_response([{"error": "Elo database module not available"}])
        except Exception as e:
            return web.json_response([{"error": str(e)}])

    async def handle_nodes_table(self, request: web.Request) -> web.Response:
        """GET /nodes/table - Node status in flat table format for Grafana Infinity.

        Returns current status of all cluster nodes in table format.
        """
        try:
            nodes = []

            # Add self
            node_name = self.node_id or "unknown"
            role = "Leader" if self.role == NodeRole.LEADER else "Worker"
            cpu = getattr(self.self_info, 'cpu_percent', 0)
            mem = getattr(self.self_info, 'memory_percent', 0)
            gpu = getattr(self.self_info, 'gpu_percent', 0) if self.self_info.has_gpu else 0
            gpu_mem = getattr(self.self_info, 'gpu_memory_percent', 0) if self.self_info.has_gpu else 0

            with self.jobs_lock:
                selfplay_jobs = len([j for j in self.local_jobs.values()
                                    if j.job_type in (JobType.SELFPLAY, JobType.GPU_SELFPLAY, JobType.HYBRID_SELFPLAY, JobType.CPU_SELFPLAY, JobType.GUMBEL_SELFPLAY)
                                    and j.status == "running"])

            nodes.append({
                "Node": node_name,
                "Role": role,
                "Status": "Online",
                "CPU": round(cpu, 1),
                "Memory": round(mem, 1),
                "GPU": round(gpu, 1),
                "GPUMem": round(gpu_mem, 1),
                "Jobs": selfplay_jobs,
                "HasGPU": "Yes" if self.self_info.has_gpu else "No",
            })

            # Add peers
            with self.peers_lock:
                for peer_id, peer in self.peers.items():
                    peer_name = peer_id or "unknown"
                    is_alive = peer.is_alive()
                    status = "Online" if is_alive else "Offline"

                    peer_cpu = getattr(peer, 'cpu_percent', 0) or 0
                    peer_mem = getattr(peer, 'memory_percent', 0) or 0
                    peer_gpu = getattr(peer, 'gpu_percent', 0) or 0
                    peer_gpu_mem = getattr(peer, 'gpu_memory_percent', 0) or 0
                    peer_jobs = getattr(peer, 'selfplay_jobs', 0) or 0
                    has_gpu = getattr(peer, 'has_gpu', False)

                    nodes.append({
                        "Node": peer_name,
                        "Role": "Worker",
                        "Status": status,
                        "CPU": round(peer_cpu, 1),
                        "Memory": round(peer_mem, 1),
                        "GPU": round(peer_gpu, 1),
                        "GPUMem": round(peer_gpu_mem, 1),
                        "Jobs": peer_jobs,
                        "HasGPU": "Yes" if has_gpu else "No",
                    })

            # Sort by role (leader first) then by name
            nodes.sort(key=lambda n: (0 if n["Role"] == "Leader" else 1, n["Node"]))

            return web.json_response(nodes)

        except Exception as e:
            return web.json_response([{"error": str(e)}])

    async def _get_victory_type_stats(self) -> dict[tuple[str, int, str], int]:
        """Aggregate victory types from recent game data.

        Returns dict mapping (board_type, num_players, victory_type) -> count.
        Caches results for 5 minutes to avoid excessive I/O.
        """
        import json
        from collections import defaultdict

        cache_key = "_victory_stats_cache"
        cache_time_key = "_victory_stats_cache_time"
        cache_ttl = 300  # 5 minutes

        # Check cache
        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        # Skip JSONL scanning during startup grace period
        if self._is_in_startup_grace_period():
            return {}

        stats: dict[tuple[str, int, str], int] = defaultdict(int)

        # Scan recent game files (last 24 hours)
        ai_root = Path(self.ringrift_path) / "ai-service"
        data_dirs = [
            ai_root / "data" / "games" / "daemon_sync",
            ai_root / "data" / "selfplay",
        ]

        cutoff_time = now - 86400  # 24 hours ago

        for data_dir in data_dirs:
            if not data_dir.exists():
                continue
            for jsonl_path in data_dir.rglob("*.jsonl"):
                try:
                    # Skip files older than 24h
                    if jsonl_path.stat().st_mtime < cutoff_time:
                        continue
                    with open_jsonl_file(jsonl_path) as f:
                        for line in f:
                            try:
                                game = json.loads(line)
                                board_type = game.get("board_type", "unknown")
                                num_players = game.get("num_players", 0)
                                victory_type = game.get("victory_type", "unknown")
                                if victory_type and victory_type != "unknown":
                                    stats[(board_type, num_players, victory_type)] += 1
                            except json.JSONDecodeError:
                                continue
                except Exception:
                    continue

        # Update cache
        setattr(self, cache_key, dict(stats))
        setattr(self, cache_time_key, now)

        return dict(stats)

    async def _get_game_analytics_cached(self) -> dict[str, Any]:
        """Get game analytics with caching (5 min TTL)."""
        import json
        from collections import defaultdict

        cache_key = "_game_analytics_cache"
        cache_time_key = "_game_analytics_cache_time"
        cache_ttl = 300

        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        # Skip JSONL scanning during startup grace period
        if self._is_in_startup_grace_period():
            return {"configs": {}}

        hours = 24
        cutoff = now - (hours * 3600)

        ai_root = Path(self.ringrift_path) / "ai-service"
        data_dirs = [
            ai_root / "data" / "games" / "daemon_sync",
            ai_root / "data" / "selfplay",
        ]

        game_lengths: dict[str, list[int]] = defaultdict(list)
        games_by_hour: dict[str, dict[int, int]] = defaultdict(lambda: defaultdict(int))
        opening_moves: dict[str, dict[str, int]] = defaultdict(lambda: defaultdict(int))

        for data_dir in data_dirs:
            if not data_dir.exists():
                continue
            for jsonl_path in data_dir.rglob("*.jsonl"):
                try:
                    if jsonl_path.stat().st_mtime < cutoff:
                        continue
                    with open_jsonl_file(jsonl_path) as f:
                        for line in f:
                            try:
                                game = json.loads(line)
                                board_type = game.get("board_type", "unknown")
                                num_players = game.get("num_players", 0)
                                config = f"{board_type}_{num_players}p"

                                length = game.get("length", 0)
                                if length > 0:
                                    game_lengths[config].append(length)

                                hour_bucket = int(jsonl_path.stat().st_mtime // 3600)
                                games_by_hour[config][hour_bucket] += 1

                                moves = game.get("moves", [])
                                if moves and len(moves) >= 1:
                                    first_move = str(moves[0].get("action", ""))[:20]
                                    if first_move:
                                        opening_moves[config][first_move] += 1
                            except json.JSONDecodeError:
                                continue
                except Exception:
                    continue

        analytics = {"configs": {}}
        for config in set(list(game_lengths.keys()) + list(games_by_hour.keys())):
            lengths = game_lengths.get(config, [])
            hourly = games_by_hour.get(config, {})
            openings = opening_moves.get(config, {})
            throughput = sum(hourly.values()) / max(len(hourly), 1) if hourly else 0

            analytics["configs"][config] = {
                "avg_length": round(sum(lengths) / len(lengths), 1) if lengths else 0,
                "throughput_per_hour": round(throughput, 1),
                "opening_diversity": len(openings),
            }

        setattr(self, cache_key, analytics)
        setattr(self, cache_time_key, now)
        return analytics

    async def _get_training_metrics_cached(self) -> dict[str, Any]:
        """Get training metrics with caching (2 min TTL)."""
        import re

        cache_key = "_training_metrics_cache"
        cache_time_key = "_training_metrics_cache_time"
        cache_ttl = 120

        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        ai_root = Path(self.ringrift_path) / "ai-service"
        logs_dir = ai_root / "logs" / "training"

        metrics = {"configs": {}}

        if logs_dir.exists():
            log_files = sorted(logs_dir.glob("*.log"), key=lambda f: f.stat().st_mtime, reverse=True)[:10]

            for log_file in log_files:
                try:
                    content = log_file.read_text()
                    config_match = re.search(r"(square\d+|hexagonal|hex)_(\d+)p", log_file.name)
                    if not config_match:
                        continue
                    config = f"{config_match.group(1)}_{config_match.group(2)}p"

                    loss_pattern = re.compile(r"[Ee]poch\s+(\d+).*?loss[=:]\s*([\d.]+)")
                    epochs = []
                    for match in loss_pattern.finditer(content):
                        epochs.append({
                            "epoch": int(match.group(1)),
                            "loss": float(match.group(2)),
                        })

                    if epochs:
                        metrics["configs"][config] = {
                            "latest_loss": epochs[-1]["loss"],
                            "latest_epoch": epochs[-1]["epoch"],
                        }
                except Exception:
                    continue

        setattr(self, cache_key, metrics)
        setattr(self, cache_time_key, now)
        return metrics

    async def _get_holdout_metrics_cached(self) -> dict[str, Any]:
        """Get holdout validation metrics with caching (5 min TTL)."""
        import sqlite3

        cache_key = "_holdout_metrics_cache"
        cache_time_key = "_holdout_metrics_cache_time"
        cache_ttl = 300

        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        ai_root = Path(self.ringrift_path) / "ai-service"
        db_path = ai_root / "data" / "holdouts" / "holdout_validation.db"

        metrics = {"configs": {}, "evaluations": [], "summary": {}}

        if not db_path.exists():
            setattr(self, cache_key, metrics)
            setattr(self, cache_time_key, now)
            return metrics

        try:
            conn = sqlite3.connect(db_path)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            # Get holdout game counts by config
            cursor.execute("""
                SELECT board_type, num_players, COUNT(*) as game_count, SUM(num_positions) as total_positions
                FROM holdout_games
                GROUP BY board_type, num_players
            """)
            for row in cursor.fetchall():
                config = f"{row['board_type']}_{row['num_players']}p"
                metrics["configs"][config] = {
                    "holdout_games": row["game_count"],
                    "holdout_positions": row["total_positions"] or 0,
                }

            # Get latest evaluations per config
            cursor.execute("""
                SELECT model_path, board_type, num_players, holdout_loss, holdout_accuracy,
                       train_loss, num_samples, evaluated_at, overfit_gap
                FROM evaluations
                WHERE id IN (
                    SELECT MAX(id) FROM evaluations
                    GROUP BY board_type, num_players
                )
                ORDER BY evaluated_at DESC
            """)
            for row in cursor.fetchall():
                config = f"{row['board_type']}_{row['num_players']}p"
                eval_data = {
                    "config": config,
                    "model": row["model_path"],
                    "holdout_loss": row["holdout_loss"],
                    "holdout_accuracy": row["holdout_accuracy"],
                    "train_loss": row["train_loss"],
                    "overfit_gap": row["overfit_gap"],
                    "num_samples": row["num_samples"],
                    "evaluated_at": row["evaluated_at"],
                }
                metrics["evaluations"].append(eval_data)
                # Update config metrics
                if config in metrics["configs"]:
                    metrics["configs"][config].update({
                        "holdout_loss": row["holdout_loss"],
                        "holdout_accuracy": row["holdout_accuracy"],
                        "overfit_gap": row["overfit_gap"],
                    })

            # Get summary stats
            cursor.execute("SELECT COUNT(*) FROM holdout_games")
            metrics["summary"]["total_holdout_games"] = cursor.fetchone()[0]
            cursor.execute("SELECT COUNT(*) FROM evaluations")
            metrics["summary"]["total_evaluations"] = cursor.fetchone()[0]

            conn.close()
        except Exception:
            pass

        setattr(self, cache_key, metrics)
        setattr(self, cache_time_key, now)
        return metrics

    async def _get_mcts_stats_cached(self) -> dict[str, Any]:
        """Get MCTS search statistics with caching (2 min TTL)."""
        import json
        import re

        cache_key = "_mcts_stats_cache"
        cache_time_key = "_mcts_stats_cache_time"
        cache_ttl = 120

        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        # Skip JSONL scanning during startup grace period
        if self._is_in_startup_grace_period():
            return {"configs": {}, "summary": {}}

        ai_root = Path(self.ringrift_path) / "ai-service"
        stats = {"configs": {}, "summary": {}}

        # Parse selfplay logs for MCTS stats
        logs_dir = ai_root / "logs" / "selfplay"
        if logs_dir.exists():
            log_files = sorted(logs_dir.glob("*.log"), key=lambda f: f.stat().st_mtime, reverse=True)[:20]

            nodes_per_move = []
            depth_stats = []
            time_per_move = []

            for log_file in log_files:
                try:
                    content = log_file.read_text(errors='ignore')
                    # Parse MCTS stats patterns (nodes visited, search depth, time)
                    # Pattern: "nodes: 1234" or "nodes_visited: 1234"
                    for match in re.finditer(r'nodes[_\s]*(?:visited)?[:\s]*(\d+)', content, re.I):
                        nodes_per_move.append(int(match.group(1)))
                    # Pattern: "depth: 12" or "search_depth: 12"
                    for match in re.finditer(r'(?:search_)?depth[:\s]*(\d+)', content, re.I):
                        depth_stats.append(int(match.group(1)))
                    # Pattern: "time: 0.123s" or "move_time: 123ms"
                    for match in re.finditer(r'(?:move_)?time[:\s]*([\d.]+)\s*(?:s|ms)?', content, re.I):
                        time_per_move.append(float(match.group(1)))
                except Exception:
                    continue

            if nodes_per_move:
                stats["summary"]["avg_nodes_per_move"] = sum(nodes_per_move) / len(nodes_per_move)
                stats["summary"]["max_nodes_per_move"] = max(nodes_per_move)
            if depth_stats:
                stats["summary"]["avg_search_depth"] = sum(depth_stats) / len(depth_stats)
                stats["summary"]["max_search_depth"] = max(depth_stats)
            if time_per_move:
                stats["summary"]["avg_time_per_move"] = sum(time_per_move) / len(time_per_move)

        # Also check game JSONL files for MCTS metadata
        data_dirs = [
            ai_root / "data" / "games" / "daemon_sync",
            ai_root / "data" / "selfplay",
        ]
        cutoff = now - 3600  # Last hour

        for data_dir in data_dirs:
            if not data_dir.exists():
                continue
            for jsonl_path in data_dir.rglob("*.jsonl"):
                try:
                    if jsonl_path.stat().st_mtime < cutoff:
                        continue
                    with open_jsonl_file(jsonl_path) as f:
                        for line in f:
                            try:
                                game = json.loads(line)
                                board_type = game.get("board_type", "unknown")
                                num_players = game.get("num_players", 0)
                                config = f"{board_type}_{num_players}p"

                                # Check for MCTS metadata in game
                                mcts_data = game.get("mcts_stats", {})
                                if mcts_data:
                                    if config not in stats["configs"]:
                                        stats["configs"][config] = {
                                            "nodes_samples": [],
                                            "depth_samples": [],
                                        }
                                    if "avg_nodes" in mcts_data:
                                        stats["configs"][config]["nodes_samples"].append(mcts_data["avg_nodes"])
                                    if "avg_depth" in mcts_data:
                                        stats["configs"][config]["depth_samples"].append(mcts_data["avg_depth"])
                            except json.JSONDecodeError:
                                continue
                except Exception:
                    continue

        # Compute per-config averages
        for _config, data in stats["configs"].items():
            if data.get("nodes_samples"):
                data["avg_nodes"] = sum(data["nodes_samples"]) / len(data["nodes_samples"])
            if data.get("depth_samples"):
                data["avg_depth"] = sum(data["depth_samples"]) / len(data["depth_samples"])
            # Clean up sample lists
            data.pop("nodes_samples", None)
            data.pop("depth_samples", None)

        setattr(self, cache_key, stats)
        setattr(self, cache_time_key, now)
        return stats

    # =========================================================================
    # Feature 1: Tournament Matchup Analysis
    # =========================================================================

    async def _get_matchup_matrix_cached(self) -> dict[str, Any]:
        """Get head-to-head matchup statistics with caching (5 min TTL)."""
        import sqlite3
        from collections import defaultdict

        cache_key = "_matchup_matrix_cache"
        cache_time_key = "_matchup_matrix_cache_time"
        cache_ttl = 300

        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        ai_root = Path(self.ringrift_path) / "ai-service"
        db_path = ai_root / "data" / "unified_elo.db"

        matrix = {"matchups": [], "models": [], "configs": {}}

        if not db_path.exists():
            setattr(self, cache_key, matrix)
            setattr(self, cache_time_key, now)
            return matrix

        try:
            conn = sqlite3.connect(db_path)
            conn.row_factory = sqlite3.Row

            # Get all match history
            rows = conn.execute("""
                SELECT participant_a, participant_b, winner, board_type, num_players,
                       game_length, duration_sec, timestamp
                FROM match_history
                WHERE timestamp > ?
                ORDER BY timestamp DESC
                LIMIT 10000
            """, (now - 86400 * 7,)).fetchall()  # Last 7 days

            # Build matchup stats
            h2h: dict[str, dict[str, dict[str, int]]] = defaultdict(lambda: defaultdict(lambda: {"wins": 0, "losses": 0, "draws": 0}))
            models = set()
            config_stats = defaultdict(lambda: {"total_matches": 0, "avg_game_length": [], "avg_duration": []})

            for row in rows:
                a = row["participant_a"]
                b = row["participant_b"]
                winner = row["winner"]
                config = f"{row['board_type']}_{row['num_players']}p"

                if a and b:
                    models.add(a)
                    models.add(b)

                    if winner == a:
                        h2h[a][b]["wins"] += 1
                        h2h[b][a]["losses"] += 1
                    elif winner == b:
                        h2h[b][a]["wins"] += 1
                        h2h[a][b]["losses"] += 1
                    else:
                        h2h[a][b]["draws"] += 1
                        h2h[b][a]["draws"] += 1

                    config_stats[config]["total_matches"] += 1
                    if row["game_length"]:
                        config_stats[config]["avg_game_length"].append(row["game_length"])
                    if row["duration_sec"]:
                        config_stats[config]["avg_duration"].append(row["duration_sec"])

            # Convert to matchup list
            matchups = []
            for model_a in sorted(models):
                for model_b in sorted(models):
                    if model_a < model_b:  # Avoid duplicates
                        stats = h2h[model_a][model_b]
                        total = stats["wins"] + stats["losses"] + stats["draws"]
                        if total > 0:
                            matchups.append({
                                "model_a": model_a,
                                "model_b": model_b,
                                "a_wins": stats["wins"],
                                "b_wins": stats["losses"],
                                "draws": stats["draws"],
                                "total": total,
                                "a_win_rate": round(stats["wins"] / total, 3) if total > 0 else 0,
                            })

            # Compute config averages
            for _config, data in config_stats.items():
                if data["avg_game_length"]:
                    data["avg_game_length"] = round(sum(data["avg_game_length"]) / len(data["avg_game_length"]), 1)
                else:
                    data["avg_game_length"] = 0
                if data["avg_duration"]:
                    data["avg_duration"] = round(sum(data["avg_duration"]) / len(data["avg_duration"]), 2)
                else:
                    data["avg_duration"] = 0

            matrix["matchups"] = matchups
            matrix["models"] = sorted(models)
            matrix["configs"] = dict(config_stats)
            matrix["total_matches"] = sum(c["total_matches"] for c in config_stats.values())

            conn.close()
        except Exception:
            pass

        setattr(self, cache_key, matrix)
        setattr(self, cache_time_key, now)
        return matrix

    # =========================================================================
    # Feature 2: Model Lineage Tracking
    # =========================================================================

    async def _get_model_lineage_cached(self) -> dict[str, Any]:
        """Get model lineage and ancestry with caching (10 min TTL)."""
        import re

        cache_key = "_model_lineage_cache"
        cache_time_key = "_model_lineage_cache_time"
        cache_ttl = 600

        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        ai_root = Path(self.ringrift_path) / "ai-service"
        models_dir = ai_root / "models"

        lineage = {"models": [], "generations": {}, "configs": {}}

        if not models_dir.exists():
            setattr(self, cache_key, lineage)
            setattr(self, cache_time_key, now)
            return lineage

        try:
            # Discover all models
            model_files = list(models_dir.glob("**/*.pt")) + list(models_dir.glob("**/*.pth"))

            for model_path in model_files:
                model_name = model_path.stem
                model_stat = model_path.stat()

                # Parse model name for lineage info
                # Common patterns:
                #   - square8_2p_v5_gen12, nnue_square8_2p_epoch50
                #   - ringrift_best_sq8_2p, ringrift_best_sq19_2p
                #   - hex_3p_nn_baseline, ringrift_best_hex_2p
                # Handle both full names (square8, hexagonal) and abbreviations (sq8, hex)
                config_match = re.search(
                    r"(square\d+|sq\d+|hexagonal|hex)[\W_]*(\d+)p",
                    model_name,
                    re.I
                )
                gen_match = re.search(r"gen(\d+)|v(\d+)|epoch(\d+)", model_name, re.I)

                if config_match:
                    board = config_match.group(1).lower()
                    players = config_match.group(2)
                    # Normalize board names (only transform abbreviations, not full names)
                    if board.startswith("sq") and not board.startswith("square"):
                        # sq8 -> square8, sq19 -> square19
                        board = f"square{board[2:]}"
                    elif board == "hex":
                        board = "hexagonal"
                    config = f"{board}_{players}p"
                else:
                    config = "unknown"
                generation = int(gen_match.group(1) or gen_match.group(2) or gen_match.group(3) or 0) if gen_match else 0

                model_info = {
                    "name": model_name,
                    "path": str(model_path.relative_to(ai_root)),
                    "config": config,
                    "generation": generation,
                    "size_mb": round(model_stat.st_size / 1024 / 1024, 2),
                    "created_at": model_stat.st_mtime,
                    "age_hours": round((now - model_stat.st_mtime) / 3600, 1),
                }
                lineage["models"].append(model_info)

                # Track generations per config
                if config not in lineage["generations"]:
                    lineage["generations"][config] = []
                lineage["generations"][config].append(model_info)

            # Sort models by generation within each config
            for config in lineage["generations"]:
                lineage["generations"][config].sort(key=lambda m: m["generation"])

            # Summary per config
            for config, models in lineage["generations"].items():
                lineage["configs"][config] = {
                    "total_models": len(models),
                    "latest_generation": max(m["generation"] for m in models) if models else 0,
                    "latest_model": models[-1]["name"] if models else None,
                    "total_size_mb": round(sum(m["size_mb"] for m in models), 1),
                }

            lineage["total_models"] = len(lineage["models"])

        except Exception:
            pass

        setattr(self, cache_key, lineage)
        setattr(self, cache_time_key, now)
        return lineage

    # =========================================================================
    # Feature 3: Data Quality Metrics
    # =========================================================================

    async def _get_data_quality_cached(self) -> dict[str, Any]:
        """Get data quality metrics with caching (5 min TTL)."""
        import json
        from collections import defaultdict

        cache_key = "_data_quality_cache"
        cache_time_key = "_data_quality_cache_time"
        cache_ttl = 300

        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        # Skip JSONL scanning during startup grace period
        if self._is_in_startup_grace_period():
            return {"configs": {}, "issues": [], "summary": {}}

        ai_root = Path(self.ringrift_path) / "ai-service"
        quality = {"configs": {}, "issues": [], "summary": {}}

        data_dirs = [
            ai_root / "data" / "games" / "daemon_sync",
            ai_root / "data" / "selfplay",
        ]
        cutoff = now - 86400  # Last 24 hours

        try:
            config_stats = defaultdict(lambda: {
                "total_games": 0,
                "game_lengths": [],
                "short_games": 0,  # < 10 moves
                "long_games": 0,   # > 500 moves
                "stalemates": 0,
                "unique_openings": set(),
                "player_wins": defaultdict(int),
                "parse_errors": 0,
            })

            for data_dir in data_dirs:
                if not data_dir.exists():
                    continue
                for jsonl_path in data_dir.rglob("*.jsonl"):
                    try:
                        if jsonl_path.stat().st_mtime < cutoff:
                            continue
                        with open_jsonl_file(jsonl_path) as f:
                            for line in f:
                                try:
                                    game = json.loads(line)
                                    board_type = game.get("board_type", "unknown")
                                    num_players = game.get("num_players", 0)
                                    config = f"{board_type}_{num_players}p"

                                    stats = config_stats[config]
                                    stats["total_games"] += 1

                                    length = game.get("length", 0)
                                    if length > 0:
                                        stats["game_lengths"].append(length)
                                        if length < 10:
                                            stats["short_games"] += 1
                                        elif length > 500:
                                            stats["long_games"] += 1

                                    victory_type = game.get("victory_type", "")
                                    if victory_type == "stalemate":
                                        stats["stalemates"] += 1

                                    # Track opening diversity
                                    moves = game.get("moves", [])
                                    if moves and len(moves) >= 2:
                                        opening = str(moves[0].get("action", ""))[:15] + "-" + str(moves[1].get("action", ""))[:15]
                                        stats["unique_openings"].add(opening)

                                    # Track winner distribution
                                    winner = game.get("winner")
                                    if winner is not None:
                                        stats["player_wins"][winner] += 1

                                except json.JSONDecodeError:
                                    config_stats["unknown"]["parse_errors"] += 1
                    except Exception:
                        continue

            # Convert to quality metrics
            issues = []
            for config, stats in config_stats.items():
                total = stats["total_games"]
                if total == 0:
                    continue

                lengths = stats["game_lengths"]
                avg_length = sum(lengths) / len(lengths) if lengths else 0
                length_std = (sum((length - avg_length) ** 2 for length in lengths) / len(lengths)) ** 0.5 if len(lengths) > 1 else 0

                short_rate = stats["short_games"] / total
                long_rate = stats["long_games"] / total
                stalemate_rate = stats["stalemates"] / total
                opening_diversity = len(stats["unique_openings"])

                # Detect issues
                if short_rate > 0.1:
                    issues.append({"config": config, "issue": "high_short_game_rate", "value": round(short_rate * 100, 1), "severity": "warning"})
                if stalemate_rate > 0.3:
                    issues.append({"config": config, "issue": "high_stalemate_rate", "value": round(stalemate_rate * 100, 1), "severity": "warning"})
                if opening_diversity < 5 and total > 50:
                    issues.append({"config": config, "issue": "low_opening_diversity", "value": opening_diversity, "severity": "warning"})

                # Check for player bias
                wins = stats["player_wins"]
                if len(wins) >= 2 and total > 20:
                    max_win_rate = max(wins.values()) / total
                    if max_win_rate > 0.7:
                        issues.append({"config": config, "issue": "player_bias", "value": round(max_win_rate * 100, 1), "severity": "info"})

                quality["configs"][config] = {
                    "total_games": total,
                    "avg_length": round(avg_length, 1),
                    "length_std": round(length_std, 1),
                    "short_game_rate": round(short_rate * 100, 1),
                    "long_game_rate": round(long_rate * 100, 1),
                    "stalemate_rate": round(stalemate_rate * 100, 1),
                    "opening_diversity": opening_diversity,
                    "parse_errors": stats["parse_errors"],
                }

            quality["issues"] = issues
            quality["summary"] = {
                "total_configs": len(quality["configs"]),
                "total_issues": len(issues),
                "critical_issues": len([i for i in issues if i["severity"] == "critical"]),
                "warning_issues": len([i for i in issues if i["severity"] == "warning"]),
            }

        except Exception:
            pass

        setattr(self, cache_key, quality)
        setattr(self, cache_time_key, now)
        return quality

    # =========================================================================
    # Feature 4: Training Efficiency Dashboard
    # =========================================================================

    async def _get_training_efficiency_cached(self) -> dict[str, Any]:
        """Get training efficiency metrics with caching (5 min TTL)."""
        import re
        import sqlite3

        cache_key = "_training_efficiency_cache"
        cache_time_key = "_training_efficiency_cache_time"
        cache_ttl = 300

        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        ai_root = Path(self.ringrift_path) / "ai-service"
        efficiency = {"configs": {}, "summary": {}, "cost_tracking": {}}

        try:
            # Get Elo history to track improvements
            db_path = ai_root / "data" / "unified_elo.db"
            elo_history = {}

            if db_path.exists():
                conn = sqlite3.connect(db_path)
                rows = conn.execute("""
                    SELECT board_type, num_players, participant_id, rating, timestamp
                    FROM rating_history
                    WHERE timestamp > ?
                    ORDER BY timestamp ASC
                """, (now - 86400 * 7,)).fetchall()  # Last 7 days

                for row in rows:
                    config = f"{row[0]}_{row[1]}p"
                    if config not in elo_history:
                        elo_history[config] = {"ratings": [], "timestamps": []}
                    elo_history[config]["ratings"].append(row[3])
                    elo_history[config]["timestamps"].append(row[4])
                conn.close()

            # Parse training logs for GPU hours
            logs_dir = ai_root / "logs" / "training"
            gpu_hours_per_config = {}

            if logs_dir.exists():
                for log_file in logs_dir.glob("*.log"):
                    try:
                        content = log_file.read_text(errors='ignore')
                        config_match = re.search(r"(square\d+|hex\w*)_(\d+)p", log_file.name)
                        if not config_match:
                            continue
                        config = f"{config_match.group(1)}_{config_match.group(2)}p"

                        # Extract training duration
                        duration_match = re.search(r"(?:total[_\s]?time|duration)[:\s]*([\d.]+)\s*(?:s|sec|min|h)", content, re.I)
                        if duration_match:
                            duration = float(duration_match.group(1))
                            # Assume hours if > 100, else assume minutes
                            if duration > 100:
                                duration = duration / 3600  # seconds to hours
                            elif duration < 24:
                                duration = duration / 60  # minutes to hours

                            if config not in gpu_hours_per_config:
                                gpu_hours_per_config[config] = 0
                            gpu_hours_per_config[config] += duration
                    except Exception:
                        continue

            # Calculate efficiency metrics per config
            for config in set(list(elo_history.keys()) + list(gpu_hours_per_config.keys())):
                elo_data = elo_history.get(config, {"ratings": [], "timestamps": []})
                gpu_hours = gpu_hours_per_config.get(config, 0)

                if elo_data["ratings"]:
                    initial_elo = elo_data["ratings"][0] if elo_data["ratings"] else INITIAL_ELO_RATING
                    current_elo = elo_data["ratings"][-1] if elo_data["ratings"] else INITIAL_ELO_RATING
                    elo_gain = current_elo - initial_elo
                else:
                    initial_elo = current_elo = INITIAL_ELO_RATING
                    elo_gain = 0

                # Elo per GPU hour
                elo_per_hour = elo_gain / gpu_hours if gpu_hours > 0 else 0

                # Estimated cost (assuming $2/GPU-hour average)
                estimated_cost = gpu_hours * 2.0

                efficiency["configs"][config] = {
                    "gpu_hours": round(gpu_hours, 2),
                    "initial_elo": round(initial_elo, 1),
                    "current_elo": round(current_elo, 1),
                    "elo_gain": round(elo_gain, 1),
                    "elo_per_gpu_hour": round(elo_per_hour, 2),
                    "estimated_cost_usd": round(estimated_cost, 2),
                    "cost_per_elo_point": round(estimated_cost / max(elo_gain, 1), 2) if elo_gain > 0 else None,
                }

            # Summary
            total_gpu_hours = sum(c.get("gpu_hours", 0) for c in efficiency["configs"].values())
            total_elo_gain = sum(c.get("elo_gain", 0) for c in efficiency["configs"].values())
            total_cost = sum(c.get("estimated_cost_usd", 0) for c in efficiency["configs"].values())

            efficiency["summary"] = {
                "total_gpu_hours": round(total_gpu_hours, 2),
                "total_elo_gain": round(total_elo_gain, 1),
                "total_estimated_cost_usd": round(total_cost, 2),
                "overall_elo_per_gpu_hour": round(total_elo_gain / max(total_gpu_hours, 1), 2),
            }

        except Exception:
            pass

        setattr(self, cache_key, efficiency)
        setattr(self, cache_time_key, now)
        return efficiency

    # =========================================================================
    # Feature 5: Automated Model Rollback
    # =========================================================================

    async def _check_rollback_conditions(self) -> dict[str, Any]:
        """Check if any models should be rolled back based on metrics."""
        rollback_status = {"candidates": [], "recent_rollbacks": [], "config_status": {}}

        try:
            # Get holdout metrics for overfitting detection
            holdout = await self._get_holdout_metrics_cached()

            # Get Elo data for regression detection
            ai_root = Path(self.ringrift_path) / "ai-service"
            db_path = ai_root / "data" / "unified_elo.db"

            elo_data = {}
            if db_path.exists():
                import sqlite3
                conn = sqlite3.connect(db_path)
                rows = conn.execute("""
                    SELECT board_type, num_players, participant_id, rating, timestamp
                    FROM rating_history
                    ORDER BY timestamp DESC
                    LIMIT 1000
                """).fetchall()

                for row in rows:
                    config = f"{row[0]}_{row[1]}p"
                    if config not in elo_data:
                        elo_data[config] = []
                    elo_data[config].append({"model": row[2], "rating": row[3], "timestamp": row[4]})
                conn.close()

            # Check each config for rollback conditions
            for config, holdout_data in holdout.get("configs", {}).items():
                status = {"config": config, "rollback_recommended": False, "reasons": []}

                # Check 1: Overfitting (overfit_gap > 0.15)
                overfit_gap = holdout_data.get("overfit_gap", 0)
                if overfit_gap and overfit_gap > 0.15:
                    status["rollback_recommended"] = True
                    status["reasons"].append(f"Overfitting detected: gap={overfit_gap:.3f}")

                # Check 2: Low holdout accuracy (< 60%)
                holdout_acc = holdout_data.get("holdout_accuracy", 1.0)
                if holdout_acc and holdout_acc < 0.6:
                    status["rollback_recommended"] = True
                    status["reasons"].append(f"Low holdout accuracy: {holdout_acc*100:.1f}%")

                # Check 3: Elo regression (dropped > 50 points recently)
                if config in elo_data and len(elo_data[config]) >= 2:
                    recent = elo_data[config][0]["rating"]
                    previous = max(e["rating"] for e in elo_data[config][:10])
                    if previous - recent > 50:
                        status["rollback_recommended"] = True
                        status["reasons"].append(f"Elo regression: {previous:.0f} -> {recent:.0f}")

                rollback_status["config_status"][config] = status
                if status["rollback_recommended"]:
                    rollback_status["candidates"].append(status)

            # Load recent rollback history if exists
            rollback_log = ai_root / "logs" / "rollbacks.json"
            if rollback_log.exists():
                import json
                with contextlib.suppress(Exception):
                    rollback_status["recent_rollbacks"] = json.loads(rollback_log.read_text())[-10:]

        except Exception:
            pass

        return rollback_status

    async def _execute_rollback(self, config: str, dry_run: bool = False) -> dict[str, Any]:
        """Execute a rollback for the given config by restoring previous model.

        Args:
            config: Config string like "square8_2p"
            dry_run: If True, only simulate the rollback without making changes

        Returns:
            Dict with rollback results (success, message, details)
        """
        import json
        import shutil

        result = {
            "success": False,
            "config": config,
            "dry_run": dry_run,
            "message": "",
            "details": {},
        }

        try:
            ai_root = Path(self.ringrift_path) / "ai-service"
            models_dir = ai_root / "models"
            archive_dir = models_dir / "archive"
            archive_dir.mkdir(parents=True, exist_ok=True)

            # Parse config to get board type and player count
            parts = config.rsplit("_", 1)
            if len(parts) != 2 or not parts[1].endswith("p"):
                result["message"] = f"Invalid config format: {config}"
                return result

            board = parts[0]
            players = parts[1][:-1]

            # Find the current best model alias
            # Common patterns: ringrift_best_sq8_2p, ringrift_best_square8_2p
            board_abbrev = board.replace("square", "sq").replace("hexagonal", "hex")
            best_patterns = [
                f"ringrift_best_{board_abbrev}_{players}p.pth",
                f"ringrift_best_{board}_{players}p.pth",
            ]

            current_best = None
            for pattern in best_patterns:
                candidate = models_dir / pattern
                if candidate.exists():
                    current_best = candidate
                    break

            if not current_best:
                result["message"] = f"No best model found for {config}"
                return result

            # Find previous checkpoints for this config
            checkpoint_dir = models_dir / "checkpoints"
            checkpoints = []
            if checkpoint_dir.exists():
                for ckpt in checkpoint_dir.glob(f"*{board_abbrev}*{players}p*.pth"):
                    try:
                        stat = ckpt.stat()
                        checkpoints.append({
                            "path": ckpt,
                            "mtime": stat.st_mtime,
                            "name": ckpt.name,
                        })
                    except Exception:
                        continue

            # Also check archive for previous best models
            for archived in archive_dir.glob(f"*{board_abbrev}*{players}p*.pth"):
                try:
                    stat = archived.stat()
                    checkpoints.append({
                        "path": archived,
                        "mtime": stat.st_mtime,
                        "name": archived.name,
                    })
                except Exception:
                    continue

            # Sort by modification time descending
            checkpoints.sort(key=lambda x: x["mtime"], reverse=True)

            # Filter out the current best model
            current_mtime = current_best.stat().st_mtime
            previous_checkpoints = [c for c in checkpoints if abs(c["mtime"] - current_mtime) > 60]

            if not previous_checkpoints:
                result["message"] = f"No previous checkpoints found for rollback of {config}"
                return result

            # Select the most recent previous checkpoint
            rollback_source = previous_checkpoints[0]

            result["details"] = {
                "current_model": current_best.name,
                "rollback_to": rollback_source["name"],
                "rollback_age_hours": round((time.time() - rollback_source["mtime"]) / 3600, 1),
                "available_checkpoints": len(previous_checkpoints),
            }

            if dry_run:
                result["success"] = True
                result["message"] = f"Dry run: Would rollback {current_best.name} to {rollback_source['name']}"
                return result

            # Archive the current model
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            archived_name = f"{current_best.stem}_archived_{timestamp}.pth"
            shutil.copy2(current_best, archive_dir / archived_name)

            # Restore the previous checkpoint
            shutil.copy2(rollback_source["path"], current_best)

            # Log the rollback
            rollback_log = ai_root / "logs" / "rollbacks.json"
            rollback_log.parent.mkdir(parents=True, exist_ok=True)

            rollback_entry = {
                "timestamp": time.strftime("%Y-%m-%dT%H:%M:%S"),
                "config": config,
                "previous_model": current_best.name,
                "rolled_back_to": rollback_source["name"],
                "archived_as": archived_name,
            }

            try:
                existing = json.loads(rollback_log.read_text()) if rollback_log.exists() else []
            except Exception:
                existing = []

            existing.append(rollback_entry)
            rollback_log.write_text(json.dumps(existing[-100:], indent=2))  # Keep last 100 rollbacks

            result["success"] = True
            result["message"] = f"Successfully rolled back {config} from {current_best.name} to {rollback_source['name']}"

            # Increment rollback counter
            self.diversity_metrics["rollbacks"] += 1

            # Send alert notification
            asyncio.create_task(self.notifier.send(
                title="Model Rollback Executed",
                message=f"Rolled back {config} from {current_best.name} to {rollback_source['name']}",
                level="warning",
                fields={
                    "Config": config,
                    "Previous": current_best.name,
                    "Restored": rollback_source["name"],
                    "Age": f"{result['details']['rollback_age_hours']:.1f}h",
                },
                node_id=self.node_id,
            ))

        except Exception as e:
            result["message"] = f"Rollback failed: {e!s}"

        return result

    async def _auto_rollback_check(self) -> list[dict[str, Any]]:
        """Automatically check and execute rollbacks for critical candidates.

        Returns list of executed rollbacks.
        """
        # Check if auto-rollback is enabled
        if os.environ.get("RINGRIFT_AUTO_ROLLBACK", "").lower() not in ("1", "true", "yes"):
            return []

        executed = []
        try:
            status = await self._check_rollback_conditions()
            for candidate in status.get("candidates", []):
                # Only auto-rollback if multiple serious conditions are met
                reasons = candidate.get("reasons", [])
                if len(reasons) >= 2 or any("Overfitting" in r for r in reasons):
                    config = candidate["config"]
                    result = await self._execute_rollback(config, dry_run=False)
                    executed.append(result)
                    if result["success"]:
                        logger.warning(f"[AUTO-ROLLBACK] Executed for {config}: {reasons}")
        except Exception as e:
            logger.error(f"[AUTO-ROLLBACK] Error: {e}")

        return executed

    # =========================================================================
    # Feature 6: Distributed Selfplay Autoscaling
    # =========================================================================

    async def _get_autoscaling_metrics(self) -> dict[str, Any]:
        """Get metrics for autoscaling decisions."""
        # Autoscaling thresholds tuned for 46-node cluster
        # These can be overridden via environment variables
        max_workers = int(os.environ.get("RINGRIFT_AUTOSCALE_MAX_WORKERS", "46"))
        min_workers = int(os.environ.get("RINGRIFT_AUTOSCALE_MIN_WORKERS", "2"))
        scale_up_threshold = int(os.environ.get("RINGRIFT_AUTOSCALE_SCALE_UP_GPH", "100"))
        scale_down_threshold = int(os.environ.get("RINGRIFT_AUTOSCALE_SCALE_DOWN_GPH", "500"))
        target_freshness = float(os.environ.get("RINGRIFT_AUTOSCALE_TARGET_FRESHNESS_HOURS", "2"))

        autoscale = {
            "current_state": {},
            "recommendations": [],
            "thresholds": {
                "scale_up_games_per_hour": scale_up_threshold,  # Scale up if below this
                "scale_down_games_per_hour": scale_down_threshold,  # Scale down if above this
                "max_workers": max_workers,
                "min_workers": min_workers,
                "target_data_freshness_hours": target_freshness,
            },
        }

        try:
            # Get current worker count
            with self.peers_lock:
                total_nodes = len(self.peers) + 1
                gpu_nodes = len([p for p in self.peers.values() if getattr(p, "has_gpu", False)])
                if self.self_info.has_gpu:
                    gpu_nodes += 1

            with self.jobs_lock:
                active_selfplay = len([j for j in self.local_jobs.values()
                                      if j.job_type in (JobType.SELFPLAY, JobType.GPU_SELFPLAY, JobType.HYBRID_SELFPLAY, JobType.CPU_SELFPLAY, JobType.GUMBEL_SELFPLAY)
                                      and j.status == "running"])

            autoscale["current_state"] = {
                "total_nodes": total_nodes,
                "gpu_nodes": gpu_nodes,
                "active_selfplay_jobs": active_selfplay,
            }

            # Get game generation throughput
            analytics = await self._get_game_analytics_cached()
            total_throughput = sum(c.get("throughput_per_hour", 0) for c in analytics.get("configs", {}).values())

            autoscale["current_state"]["games_per_hour"] = round(total_throughput, 1)

            # Get data freshness
            now = time.time()
            ai_root = Path(self.ringrift_path) / "ai-service"
            selfplay_dir = ai_root / "data" / "selfplay"

            freshest_data = 0
            if selfplay_dir.exists():
                for jsonl in selfplay_dir.rglob("*.jsonl"):
                    try:
                        mtime = jsonl.stat().st_mtime
                        if mtime > freshest_data:
                            freshest_data = mtime
                    except Exception:
                        continue

            data_age_hours = (now - freshest_data) / 3600 if freshest_data > 0 else 999
            autoscale["current_state"]["data_freshness_hours"] = round(data_age_hours, 2)

            # Generate recommendations
            thresholds = autoscale["thresholds"]

            if total_throughput < thresholds["scale_up_games_per_hour"] and total_nodes < thresholds["max_workers"]:
                autoscale["recommendations"].append({
                    "action": "scale_up",
                    "reason": f"Low throughput ({total_throughput:.0f} games/h < {thresholds['scale_up_games_per_hour']})",
                    "suggested_workers": min(total_nodes + 2, thresholds["max_workers"]),
                })

            if total_throughput > thresholds["scale_down_games_per_hour"] and total_nodes > thresholds["min_workers"]:
                autoscale["recommendations"].append({
                    "action": "scale_down",
                    "reason": f"High throughput ({total_throughput:.0f} games/h > {thresholds['scale_down_games_per_hour']})",
                    "suggested_workers": max(total_nodes - 1, thresholds["min_workers"]),
                })

            if data_age_hours > thresholds["target_data_freshness_hours"]:
                autoscale["recommendations"].append({
                    "action": "scale_up",
                    "reason": f"Stale data ({data_age_hours:.1f}h > {thresholds['target_data_freshness_hours']}h)",
                    "suggested_workers": min(total_nodes + 1, thresholds["max_workers"]),
                })

            # Cost optimization recommendation
            efficiency = await self._get_training_efficiency_cached()
            elo_per_hour = efficiency.get("summary", {}).get("overall_elo_per_gpu_hour", 0)
            if elo_per_hour < 1 and total_nodes > 2:
                autoscale["recommendations"].append({
                    "action": "optimize",
                    "reason": f"Low efficiency ({elo_per_hour:.2f} Elo/GPU-h) - consider reducing workers",
                    "suggested_workers": max(total_nodes - 1, thresholds["min_workers"]),
                })

        except Exception:
            pass

        return autoscale

    async def handle_victory_table(self, request: web.Request) -> web.Response:
        """GET /victory/table - Victory type breakdown for Grafana Infinity.

        Returns victory type counts by board config in table format.
        Supports optional query params:
            - board_type: filter by board type
            - num_players: filter by player count
        """
        from collections import defaultdict

        try:
            board_type_filter = request.query.get("board_type")
            num_players_filter = request.query.get("num_players")
            if num_players_filter:
                try:
                    num_players_filter = int(num_players_filter)
                except ValueError:
                    num_players_filter = None

            stats = await self._get_victory_type_stats()

            # Group by config for table display
            config_stats: dict[str, dict[str, int]] = defaultdict(lambda: defaultdict(int))
            for (board_type, num_players, victory_type), count in stats.items():
                # Apply filters
                if board_type_filter and board_type != board_type_filter:
                    continue
                if num_players_filter and num_players != num_players_filter:
                    continue
                config = f"{board_type}_{num_players}p"
                config_stats[config][victory_type] = count

            # Build table rows
            table_data = []
            for config in sorted(config_stats.keys()):
                vt_counts = config_stats[config]
                total = sum(vt_counts.values())
                row = {
                    "Config": config,
                    "Total": total,
                    "Territory": vt_counts.get("territory", 0),
                    "LPS": vt_counts.get("lps", 0),
                    "Elimination": vt_counts.get("elimination", 0),
                    "RingElim": vt_counts.get("ring_elimination", 0),
                    "Stalemate": vt_counts.get("stalemate", 0),
                }
                # Add percentages
                if total > 0:
                    row["Territory%"] = round(100 * vt_counts.get("territory", 0) / total, 1)
                    row["LPS%"] = round(100 * vt_counts.get("lps", 0) / total, 1)
                    row["Elimination%"] = round(100 * vt_counts.get("elimination", 0) / total, 1)
                    row["RingElim%"] = round(100 * vt_counts.get("ring_elimination", 0) / total, 1)
                    row["Stalemate%"] = round(100 * vt_counts.get("stalemate", 0) / total, 1)
                else:
                    row["Territory%"] = row["LPS%"] = row["Elimination%"] = row["RingElim%"] = row["Stalemate%"] = 0
                table_data.append(row)

            return web.json_response(table_data)

        except Exception as e:
            return web.json_response([{"error": str(e)}])

    async def handle_elo_history(self, request: web.Request) -> web.Response:
        """GET /elo/history - Historical Elo ratings for time series visualization.

        Query params:
            - config: Filter by config (e.g., square8_2p)
            - model: Filter by model/participant_id (supports partial match)
            - nn_only: If "true", filter to NN models only
            - hours: Hours of history (default 168 = 1 week)
            - limit: Max entries to return (default 5000)
        """
        import sqlite3

        try:
            config_filter = request.query.get("config")
            model_filter = request.query.get("model")
            nn_only = request.query.get("nn_only", "").lower() == "true"
            hours = int(request.query.get("hours", "168"))
            limit = int(request.query.get("limit", "5000"))

            ai_root = Path(self.ringrift_path) / "ai-service"

            # Canonical Elo database for trained models
            db_paths = [
                ai_root / "data" / "unified_elo.db",
            ]

            data = []
            cutoff = time.time() - (hours * 3600)

            for db_path in db_paths:
                if not db_path.exists():
                    continue

                try:
                    conn = sqlite3.connect(db_path)
                    cursor = conn.cursor()

                    # Check if this DB has data
                    cursor.execute("SELECT COUNT(*) FROM rating_history WHERE timestamp > ?", (cutoff,))
                    count = cursor.fetchone()[0]
                    if count == 0:
                        conn.close()
                        continue

                    # Build query - unified_elo.db has different schema (no board_type/num_players)
                    cursor.execute("PRAGMA table_info(rating_history)")
                    columns = {col[1] for col in cursor.fetchall()}

                    if "board_type" in columns:
                        # unified_elo.db schema
                        query = """
                            SELECT participant_id, board_type, num_players, rating, games_played, timestamp
                            FROM rating_history
                            WHERE timestamp > ?
                        """
                        params = [cutoff]

                        if config_filter:
                            parts = config_filter.replace("_", " ").split()
                            if len(parts) >= 2:
                                board_type = parts[0]
                                num_players = int(parts[1].replace("p", ""))
                                query += " AND board_type = ? AND num_players = ?"
                                params.extend([board_type, num_players])
                    else:
                        # unified_elo.db schema (model_id instead of participant_id)
                        query = """
                            SELECT model_id, rating, games_played, timestamp
                            FROM rating_history
                            WHERE timestamp > ?
                        """
                        params = [cutoff]

                    if model_filter:
                        col = "participant_id" if "participant_id" in columns else "model_id"
                        query += f" AND {col} LIKE ?"
                        params.append(f"%{model_filter}%")

                    if nn_only:
                        col = "participant_id" if "participant_id" in columns else "model_id"
                        query += f" AND ({col} LIKE '%nn%' OR {col} LIKE '%NN%')"

                    query += f" ORDER BY timestamp DESC LIMIT {limit}"

                    cursor.execute(query, params)
                    rows = cursor.fetchall()
                    conn.close()

                    # Format for Grafana time series
                    for row in rows:
                        if "board_type" in columns:
                            participant_id, board_type, num_players, rating, games_played, ts = row
                            config = f"{board_type}_{num_players}p"
                        else:
                            model_id, rating, games_played, ts = row
                            participant_id = model_id
                            # Extract config from model name (e.g., sq8_2p_nn_baseline -> square8_2p)
                            if "sq8" in model_id.lower() or "square8" in model_id.lower():
                                config = "square8_2p"
                            elif "sq19" in model_id.lower() or "square19" in model_id.lower():
                                config = "square19_2p"
                            else:
                                config = "unknown"

                        data.append({
                            "time": int(ts * 1000),  # Grafana expects ms
                            "model": participant_id,
                            "config": config,
                            "elo": round(rating, 1),
                            "games": games_played,
                        })

                    # If we got data from this DB, don't check others
                    if data:
                        break

                except sqlite3.Error:
                    continue

            # Sort by time ascending for time series
            data.sort(key=lambda x: x["time"])

            return web.json_response(data)

        except Exception as e:
            return web.json_response([{"error": str(e)}])

    # === Elo Sync Endpoints ===

    async def handle_elo_sync_status(self, request: web.Request) -> web.Response:
        """GET /elo/sync/status - Get Elo database sync status."""
        try:
            if not self.elo_sync_manager:
                return web.json_response({
                    "enabled": False,
                    "error": "EloSyncManager not initialized"
                })

            status = self.elo_sync_manager.get_status()
            status["enabled"] = True
            status["node_id"] = self.node_id

            return web.json_response(status)
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_elo_sync_trigger(self, request: web.Request) -> web.Response:
        """POST /elo/sync/trigger - Manually trigger Elo database sync."""
        try:
            if not self.elo_sync_manager:
                return web.json_response({
                    "success": False,
                    "error": "EloSyncManager not initialized"
                }, status=503)

            # Trigger sync
            success = await self.elo_sync_manager.sync_with_cluster()

            return web.json_response({
                "success": success,
                "status": self.elo_sync_manager.get_status()
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_elo_sync_download(self, request: web.Request) -> web.Response:
        """GET /elo/sync/db - Download unified_elo.db for cluster sync."""
        try:
            ai_root = Path(self.ringrift_path) / "ai-service"
            db_path = ai_root / "data" / "unified_elo.db"

            if not db_path.exists():
                return web.json_response({"error": "Database not found"}, status=404)

            # Read and return the database file
            with open(db_path, 'rb') as f:
                data = f.read()

            return web.Response(
                body=data,
                content_type='application/octet-stream',
                headers={
                    'Content-Disposition': 'attachment; filename="unified_elo.db"',
                    'Content-Length': str(len(data))
                }
            )
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_elo_sync_upload(self, request: web.Request) -> web.Response:
        """POST /elo/sync/upload - Upload/merge unified_elo.db from another node."""
        try:
            if not self.elo_sync_manager:
                return web.json_response({
                    "success": False,
                    "error": "EloSyncManager not initialized"
                }, status=503)

            # Read uploaded database
            data = await request.read()
            if not data:
                return web.json_response({"error": "No data received"}, status=400)

            # Save to temp file and merge
            import tempfile
            with tempfile.NamedTemporaryFile(delete=False, suffix='.db') as f:
                f.write(data)
                temp_path = Path(f.name)

            try:
                # Use merge if enabled
                if self.elo_sync_manager.enable_merge:
                    success = await self.elo_sync_manager._merge_databases(temp_path)
                else:
                    # Simple replace
                    shutil.copy(temp_path, self.elo_sync_manager.db_path)
                    success = True

                self.elo_sync_manager._update_local_stats()

                return web.json_response({
                    "success": success,
                    "match_count": self.elo_sync_manager.state.local_match_count
                })
            finally:
                temp_path.unlink(missing_ok=True)

        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def _trigger_elo_sync_after_matches(self, num_matches: int = 1):
        """Trigger Elo sync after recording new matches.

        This is called after recording match results to ensure cluster-wide
        consistency. It debounces sync requests to avoid overwhelming the
        network with syncs after every individual match.
        """
        if not self.elo_sync_manager:
            return

        # Debounce: only sync if enough new matches or enough time has passed
        # This prevents sync storms when processing many matches in quick succession
        MIN_MATCHES_FOR_IMMEDIATE_SYNC = 10
        MIN_INTERVAL_BETWEEN_SYNCS = 30  # seconds

        try:
            last_sync = getattr(self, '_last_elo_sync_trigger', 0)
            now = time.time()

            # Accumulate pending matches
            pending = getattr(self, '_pending_sync_matches', 0) + num_matches
            self._pending_sync_matches = pending

            # Check if we should sync now
            should_sync = (
                pending >= MIN_MATCHES_FOR_IMMEDIATE_SYNC or
                (now - last_sync) >= MIN_INTERVAL_BETWEEN_SYNCS
            )

            if should_sync and not self.sync_in_progress:
                self._last_elo_sync_trigger = now
                self._pending_sync_matches = 0
                success = await self.elo_sync_manager.sync_with_cluster()
                if success:
                    logger.info(f"Elo sync triggered after {pending} matches: "
                          f"{self.elo_sync_manager.state.local_match_count} total")
        except Exception as e:
            logger.info(f"Elo sync trigger error: {e}")

    async def _elo_sync_loop(self):
        """Background loop for periodic Elo database synchronization."""
        if not self.elo_sync_manager:
            return

        # Initialize the sync manager
        try:
            await self.elo_sync_manager.initialize()
        except Exception as e:
            logger.info(f"EloSyncManager initialization failed: {e}")
            return

        logger.info(f"Elo sync loop started (interval: {self.elo_sync_manager.sync_interval}s)")

        while self.running:
            try:
                # Only sync if we're not currently busy with training
                if not self.sync_in_progress:
                    success = await self.elo_sync_manager.sync_with_cluster()
                    if success:
                        logger.info(f"Elo sync completed: {self.elo_sync_manager.state.local_match_count} matches")
            except Exception as e:
                logger.info(f"Elo sync error: {e}")

            await asyncio.sleep(self.elo_sync_manager.sync_interval)

    async def _worker_pull_loop(self):
        """Background loop for workers to poll leader for work (pull model).

        This implements a worker pull model where nodes periodically check
        if they are idle and pull work from the leader's work queue.

        Benefits:
        - Workers claim work at their own pace
        - Naturally load balances across the cluster
        - Works with NAT-blocked nodes (they initiate connections)
        - No need to track worker connectivity for pushing
        """
        PULL_INTERVAL = 30  # Check every 30 seconds
        GPU_IDLE_THRESHOLD = 15.0  # Consider idle if GPU < 15%
        CPU_IDLE_THRESHOLD = 30.0  # Consider idle if CPU < 30%

        await asyncio.sleep(30)  # Initial delay for cluster stabilization

        logger.info("Worker pull loop started")

        while self.running:
            try:
                # Skip if we are the leader (leader pushes, doesn't pull)
                if self.role == NodeRole.LEADER:
                    await asyncio.sleep(PULL_INTERVAL)
                    continue

                # Skip if no leader known
                if not self.leader_id:
                    await asyncio.sleep(PULL_INTERVAL)
                    continue

                # Check if we're idle enough to take on work
                self._update_self_info()
                gpu_percent = float(getattr(self.self_info, "gpu_percent", 0) or 0)
                cpu_percent = float(getattr(self.self_info, "cpu_percent", 0) or 0)
                training_jobs = int(getattr(self.self_info, "training_jobs", 0) or 0)
                has_gpu = bool(getattr(self.self_info, "has_gpu", False))

                # Don't pull work if already running training
                if training_jobs > 0:
                    await asyncio.sleep(PULL_INTERVAL)
                    continue

                # Check if we're actually idle
                is_idle = False
                if has_gpu:
                    is_idle = gpu_percent < GPU_IDLE_THRESHOLD
                else:
                    is_idle = cpu_percent < CPU_IDLE_THRESHOLD

                if not is_idle:
                    await asyncio.sleep(PULL_INTERVAL)
                    continue

                # Get allowed work types from policy
                capabilities = ["selfplay", "training", "gpu_cmaes", "tournament"]
                try:
                    from app.coordination.node_policies import get_policy_manager
                    pm = get_policy_manager()
                    capabilities = list(pm.get_allowed_work_types(self.node_id))
                except ImportError:
                    pass

                # Try to claim work from the leader
                work_item = await self._claim_work_from_leader(capabilities)
                if work_item:
                    logger.info(f"Claimed work {work_item.get('work_id')}: {work_item.get('work_type')}")

                    # Execute the work
                    success = await self._execute_claimed_work(work_item)

                    # Report completion/failure
                    await self._report_work_result(work_item, success)

            except Exception as e:
                logger.debug(f"Worker pull loop error: {e}")

            await asyncio.sleep(PULL_INTERVAL)

    async def _claim_work_from_leader(self, capabilities: list[str]) -> dict[str, Any] | None:
        """Claim work from the leader's work queue."""
        if not self.leader_id or self.leader_id == self.node_id:
            return None

        # Find leader peer
        with self.peers_lock:
            leader_peer = self.peers.get(self.leader_id)

        if not leader_peer:
            return None

        try:
            timeout = ClientTimeout(total=15)
            async with get_client_session(timeout) as session:
                caps_str = ",".join(capabilities)
                url = self._url_for_peer(leader_peer, f"/work/claim?node_id={self.node_id}&capabilities={caps_str}")
                async with session.get(url, headers=self._auth_headers()) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        if data.get("status") == "claimed":
                            return data.get("work")
        except Exception as e:
            logger.debug(f"Failed to claim work from leader: {e}")

        return None

    async def _execute_claimed_work(self, work_item: dict[str, Any]) -> bool:
        """Execute a claimed work item locally."""
        work_type = work_item.get("work_type", "")
        config = work_item.get("config", {})
        work_id = work_item.get("work_id", "")

        try:
            if work_type == "training":
                # Start training job
                board_type = config.get("board_type", "square8")
                num_players = config.get("num_players", 2)
                # Queue it for local training
                f"pull-{work_id}-{int(time.time())}"
                logger.info(f"Executing training work: {board_type}/{num_players}p")
                # Simplified: trigger training via existing mechanisms
                return True

            elif work_type == "selfplay":
                # Start selfplay job
                board_type = config.get("board_type", "square8")
                num_players = config.get("num_players", 2)
                num_games = config.get("num_games", 500)

                asyncio.create_task(self._run_gpu_selfplay_job(
                    job_id=f"pull-{work_id}",
                    board_type=board_type,
                    num_players=num_players,
                    num_games=num_games,
                    engine_mode="mixed",
                ))
                return True

            elif work_type == "gpu_cmaes":
                # Start CMA-ES optimization
                logger.info(f"Executing GPU CMA-ES work: {config}")
                return True

            elif work_type == "tournament":
                # Start tournament
                logger.info(f"Executing tournament work: {config}")
                return True

            else:
                logger.warning(f"Unknown work type: {work_type}")
                return False

        except Exception as e:
            logger.error(f"Error executing work {work_id}: {e}")
            return False

    async def _report_work_result(self, work_item: dict[str, Any], success: bool) -> None:
        """Report work completion/failure to the leader."""
        if not self.leader_id or self.leader_id == self.node_id:
            return

        work_id = work_item.get("work_id", "")
        if not work_id:
            return

        with self.peers_lock:
            leader_peer = self.peers.get(self.leader_id)

        if not leader_peer:
            return

        try:
            timeout = ClientTimeout(total=15)
            async with get_client_session(timeout) as session:
                if success:
                    url = self._url_for_peer(leader_peer, "/work/complete")
                    payload = {"work_id": work_id, "result": {"node_id": self.node_id}}
                else:
                    url = self._url_for_peer(leader_peer, "/work/fail")
                    payload = {"work_id": work_id, "error": "execution_failed"}

                async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                    if resp.status == 200:
                        logger.debug(f"Reported work {work_id} result: {'success' if success else 'failed'}")
        except Exception as e:
            logger.debug(f"Failed to report work result: {e}")

    async def _work_queue_maintenance_loop(self):
        """Background loop for leader to maintain the work queue.

        Runs periodically to:
        - Check for timed out work items
        - Clean up old completed items from the database
        """
        MAINTENANCE_INTERVAL = 300  # Every 5 minutes
        CLEANUP_AGE = 86400.0  # Clean up items older than 24 hours

        await asyncio.sleep(60)  # Initial delay

        logger.info("Work queue maintenance loop started")

        while self.running:
            try:
                # Only leader performs maintenance
                if self.role != NodeRole.LEADER:
                    await asyncio.sleep(MAINTENANCE_INTERVAL)
                    continue

                wq = get_work_queue()
                if wq is None:
                    await asyncio.sleep(MAINTENANCE_INTERVAL)
                    continue

                # Check for timeouts
                timed_out = wq.check_timeouts()
                if timed_out:
                    logger.info(f"Work queue maintenance: {len(timed_out)} items timed out")

                # Cleanup old items
                removed = wq.cleanup_old_items(max_age_seconds=CLEANUP_AGE)
                if removed:
                    logger.info(f"Work queue maintenance: cleaned up {removed} old items")

                await asyncio.sleep(MAINTENANCE_INTERVAL)

            except Exception as e:
                logger.error(f"Work queue maintenance error: {e}")
                await asyncio.sleep(MAINTENANCE_INTERVAL)

    async def _idle_detection_loop(self):
        """Background loop for leader to detect idle nodes and auto-assign work.

        Uses the unified inventory to discover ALL nodes (Vast, Tailscale, Lambda, Hetzner)
        and automatically assigns work to nodes that have been idle for too long.

        This ensures GPUs never sit idle waiting for manual intervention.
        """
        await asyncio.sleep(30)  # Initial delay to let cluster stabilize

        logger.info(f"Idle detection loop started (interval={IDLE_CHECK_INTERVAL}s, threshold={IDLE_GPU_THRESHOLD}%)")

        # Track how long each node has been idle (for grace period)
        idle_since: dict[str, float] = {}

        while self.running:
            try:
                # Only leader performs idle detection
                if self.role != NodeRole.LEADER or not AUTO_ASSIGN_ENABLED:
                    await asyncio.sleep(IDLE_CHECK_INTERVAL)
                    continue

                # Run unified discovery and use it as primary source for idle nodes
                inventory_idle_nodes = []
                if HAS_UNIFIED_INVENTORY and get_inventory:
                    inventory = get_inventory()
                    try:
                        await inventory.discover_all()
                        # Get idle nodes from unified inventory (includes Vast, Tailscale, Lambda, Hetzner)
                        inventory_idle_nodes = inventory.get_idle_nodes(IDLE_GPU_THRESHOLD)
                        if inventory_idle_nodes:
                            logger.debug(f"Unified inventory found {len(inventory_idle_nodes)} idle nodes")
                    except Exception as e:
                        logger.debug(f"Unified discovery error: {e}")

                # Get idle nodes from P2P peers (as additional source)
                idle_nodes = []
                now = time.time()
                seen_node_ids = set()

                # First, process inventory idle nodes (fresher data from CLI discovery)
                for node in inventory_idle_nodes:
                    node_id = getattr(node, "node_id", "") or ""
                    if not node_id:
                        continue
                    seen_node_ids.add(node_id)

                    # Track when this node became idle
                    if node_id not in idle_since:
                        idle_since[node_id] = now
                        gpu_pct = float(getattr(node, "gpu_percent", 0) or 0)
                        logger.debug(f"Node {node_id} became idle (GPU={gpu_pct:.0f}%, source=inventory)")

                    # Check if idle long enough (grace period)
                    idle_duration = now - idle_since[node_id]
                    if idle_duration >= IDLE_GRACE_PERIOD:
                        idle_nodes.append((node, idle_duration))

                # Then, process P2P peers (for nodes not in inventory)
                with self.peers_lock:
                    for peer in self.peers.values():
                        # Skip nodes already processed from inventory
                        if peer.node_id in seen_node_ids:
                            continue

                        if not peer.is_alive() or peer.retired:
                            # Remove from idle tracking if no longer active
                            idle_since.pop(peer.node_id, None)
                            continue

                        # Check if node is GPU-idle: low GPU utilization, regardless of CPU job count
                        # A node with CPU-bound selfplay jobs but idle GPU should still get GPU work
                        gpu_pct = float(getattr(peer, "gpu_percent", 0) or 0)
                        int(getattr(peer, "selfplay_jobs", 0) or 0)
                        training_jobs = int(getattr(peer, "training_jobs", 0) or 0)
                        has_external = getattr(peer, "has_external_work", lambda: False)()
                        has_gpu = bool(getattr(peer, "gpu_name", "") or getattr(peer, "has_gpu", False))

                        # GPU-idle: GPU is underutilized. Training jobs use GPU, so exclude those.
                        # CPU selfplay jobs don't prevent GPU idleness - in fact, that's exactly
                        # when we SHOULD start GPU work to utilize the idle GPU.
                        is_idle = (
                            has_gpu
                            and gpu_pct < IDLE_GPU_THRESHOLD
                            and training_jobs == 0  # Training uses GPU, so node isn't GPU-idle
                            and not has_external
                        )

                        if is_idle:
                            # Track when this node became idle
                            if peer.node_id not in idle_since:
                                idle_since[peer.node_id] = now
                                logger.debug(f"Node {peer.node_id} became idle (GPU={gpu_pct:.0f}%, source=p2p)")

                            # Check if idle long enough (grace period)
                            idle_duration = now - idle_since[peer.node_id]
                            if idle_duration >= IDLE_GRACE_PERIOD:
                                idle_nodes.append((peer, idle_duration))
                        else:
                            # Node is no longer idle
                            if peer.node_id in idle_since:
                                logger.debug(f"Node {peer.node_id} no longer idle (GPU={gpu_pct:.0f}%)")
                            idle_since.pop(peer.node_id, None)

                # Auto-assign work to idle nodes
                if idle_nodes:
                    logger.info(f"Found {len(idle_nodes)} idle node(s), auto-assigning work")

                    wq = get_work_queue()
                    if wq is not None:
                        # Ensure queue has enough work
                        wq.ensure_work_available(len(idle_nodes), max_batch=AUTO_WORK_BATCH_SIZE)

                        # Try to start selfplay on each idle node
                        for peer, idle_duration in idle_nodes:
                            try:
                                await self._auto_start_selfplay(peer, idle_duration)
                            except Exception as e:
                                logger.warning(f"Failed to auto-start work on {peer.node_id}: {e}")

                await asyncio.sleep(IDLE_CHECK_INTERVAL)

            except Exception as e:
                logger.error(f"Idle detection error: {e}")
                await asyncio.sleep(IDLE_CHECK_INTERVAL)

    async def _auto_start_selfplay(self, peer, idle_duration: float):
        """Auto-start diverse hybrid selfplay on an idle node.

        Works with both NodeInfo (P2P peers) and DiscoveredNode (unified inventory).
        Uses diverse profiles for high-quality training data:
        - Multiple engine modes (gumbel-mcts, nnue-guided, policy-only, mcts)
        - Multiple board types (hex8, square8, square19)
        - Multiple player counts (2, 3, 4)
        - Multiple heuristic profiles (balanced, aggressive, territorial, defensive)
        """
        # Check for GPU - works with both NodeInfo and DiscoveredNode
        gpu_name = getattr(peer, "gpu_name", "") or ""

        # Don't auto-start on nodes that aren't GPU nodes
        has_gpu = bool(gpu_name) or getattr(peer, "has_gpu", False)
        is_gpu_node = getattr(peer, "is_gpu_node", lambda: has_gpu)()
        if not has_gpu and not is_gpu_node:
            return

        # GPU selfplay uses batch processing - scale based on GPU power
        if "GH200" in gpu_name.upper() or "H100" in gpu_name.upper() or "H200" in gpu_name.upper():
            num_processes = 4
            games_per_process = 10000
            gpu_tier = "high"
        elif "A100" in gpu_name.upper() or "A40" in gpu_name.upper():
            num_processes = 3
            games_per_process = 5000
            gpu_tier = "high"
        elif "4090" in gpu_name.upper() or "5090" in gpu_name.upper():
            num_processes = 3
            games_per_process = 5000
            gpu_tier = "mid"
        elif "4080" in gpu_name.upper() or "5080" in gpu_name.upper() or "5070" in gpu_name.upper():
            num_processes = 2
            games_per_process = 3000
            gpu_tier = "mid"
        elif "3090" in gpu_name.upper() or "4070" in gpu_name.upper():
            num_processes = 2
            games_per_process = 2500
            gpu_tier = "mid"
        else:
            num_processes = 2
            games_per_process = 2000
            gpu_tier = "low"

        logger.info(f"Auto-starting {num_processes} diverse selfplay processes on idle node {peer.node_id} "
                   f"(GPU={gpu_name}, tier={gpu_tier}, {games_per_process} games each, idle for {idle_duration:.0f}s)")

        # Send parallel requests to /selfplay/start endpoint
        try:
            url = self._url_for_peer(peer, "/selfplay/start")
            timeout = ClientTimeout(total=30)

            # Diverse profile configurations for high-quality training data
            # Each profile targets different aspects of game understanding
            DIVERSE_PROFILES = [
                # High-quality neural-guided profiles (50% of games)
                {
                    "engine_mode": "gumbel-mcts",
                    "board_type": "hex",
                    "num_players": 2,
                    "profile": "balanced",
                    "weight": 0.18,
                    "description": "Gumbel MCTS 2P hex - highest quality",
                },
                {
                    "engine_mode": "policy-only",
                    "board_type": "hex",
                    "num_players": 2,
                    "profile": "balanced",
                    "weight": 0.12,
                    "description": "Policy-only 2P hex - fast NN inference",
                },
                {
                    "engine_mode": "nnue-guided",
                    "board_type": "square8",
                    "num_players": 2,
                    "profile": "aggressive",
                    "weight": 0.08,
                    "description": "NNUE-guided 2P square - aggressive style",
                },
                {
                    "engine_mode": "gumbel-mcts",
                    "board_type": "square8",
                    "num_players": 3,
                    "profile": "balanced",
                    "weight": 0.06,
                    "description": "Gumbel MCTS 3P square - multiplayer strategy",
                },
                {
                    "engine_mode": "mcts",
                    "board_type": "hex",
                    "num_players": 2,
                    "profile": "territorial",
                    "weight": 0.06,
                    "description": "MCTS 2P hex - territorial focus",
                },
                # MaxN/BRS multiplayer profiles (15% of games)
                # Benchmarks show: MaxN >> Descent in 3P/4P, MaxN  BRS
                {
                    "engine_mode": "maxn",
                    "board_type": "hex",
                    "num_players": 3,
                    "profile": "balanced",
                    "weight": 0.05,
                    "description": "MaxN 3P hex - optimal multiplayer search",
                },
                {
                    "engine_mode": "maxn",
                    "board_type": "square8",
                    "num_players": 4,
                    "profile": "balanced",
                    "weight": 0.04,
                    "description": "MaxN 4P square - best for 4-player",
                },
                {
                    "engine_mode": "brs",
                    "board_type": "hex",
                    "num_players": 3,
                    "profile": "aggressive",
                    "weight": 0.03,
                    "description": "BRS 3P hex - fast multiplayer search",
                },
                {
                    "engine_mode": "brs",
                    "board_type": "square8",
                    "num_players": 4,
                    "profile": "territorial",
                    "weight": 0.03,
                    "description": "BRS 4P square - territorial multiplayer",
                },
                # GPU-accelerated throughput profiles (25% of games)
                {
                    "engine_mode": "heuristic-only",
                    "board_type": "hex",
                    "num_players": 2,
                    "profile": "balanced",
                    "weight": 0.10,
                    "description": "GPU heuristic 2P hex - fast throughput",
                },
                {
                    "engine_mode": "heuristic-only",
                    "board_type": "square8",
                    "num_players": 2,
                    "profile": "defensive",
                    "weight": 0.07,
                    "description": "GPU heuristic 2P square - defensive style",
                },
                {
                    "engine_mode": "heuristic-only",
                    "board_type": "hex",
                    "num_players": 4,
                    "profile": "balanced",
                    "weight": 0.05,
                    "description": "GPU heuristic 4P hex - large multiplayer",
                },
                # Exploration profiles (10% of games)
                {
                    "engine_mode": "mixed",
                    "board_type": "square19",
                    "num_players": 2,
                    "profile": "balanced",
                    "weight": 0.04,
                    "description": "Mixed 2P large board - strategic depth",
                },
                {
                    "engine_mode": "nnue-guided",
                    "board_type": "hex",
                    "num_players": 3,
                    "profile": "aggressive",
                    "weight": 0.04,
                    "description": "NNUE 3P hex - aggressive multiplayer",
                },
                {
                    "engine_mode": "policy-only",
                    "board_type": "square8",
                    "num_players": 4,
                    "profile": "territorial",
                    "weight": 0.05,
                    "description": "Policy 4P square - territory control",
                },
            ]

            # Select profiles based on weighted random sampling
            import random
            weights = [p["weight"] for p in DIVERSE_PROFILES]
            selected_profiles = random.choices(DIVERSE_PROFILES, weights=weights, k=num_processes)

            # Build job configs from selected profiles
            job_configs = []
            for i, profile in enumerate(selected_profiles):
                job_configs.append({
                    "board_type": profile["board_type"],
                    "num_players": profile["num_players"],
                    "num_games": games_per_process,
                    "engine_mode": profile["engine_mode"],
                    "heuristic_profile": profile["profile"],
                    "auto_assigned": True,
                    "reason": f"auto_idle_{profile['engine_mode']}_{profile['board_type']}_{profile['num_players']}p_{int(idle_duration)}s",
                })
                logger.debug(f"  Process {i}: {profile['description']}")

            async def send_selfplay_request(session, payload):
                """Send a single selfplay start request."""
                async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                    if resp.status == 200:
                        return True
                    else:
                        body = await resp.text()
                        logger.warning(f"Failed selfplay request on {peer.node_id}: {resp.status} {body[:100]}")
                        return False

            async with get_client_session(timeout) as session:
                tasks = [send_selfplay_request(session, cfg) for cfg in job_configs]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                started = sum(1 for r in results if r is True)

                # Log profile distribution
                from collections import Counter
                engine_counts = Counter(cfg["engine_mode"] for cfg in job_configs)
                board_counts = Counter(cfg["board_type"] for cfg in job_configs)
                profile_summary = ", ".join(f"{k}:{v}" for k, v in engine_counts.items())
                board_summary = ", ".join(f"{k}:{v}" for k, v in board_counts.items())

                logger.info(f"Auto-started {started}/{num_processes} diverse selfplay on {peer.node_id} "
                           f"[engines: {profile_summary}] [boards: {board_summary}]")

        except Exception as e:
            logger.warning(f"Auto-start request failed for {peer.node_id}: {e}")

    # =========================================================================
    # AUTOMATION LOOPS (2024-12)
    # These loops enable hands-free cluster operation
    # =========================================================================

    async def _auto_scaling_loop(self):
        """Background loop for auto-scaling Vast.ai instances based on queue depth.

        Only runs on the leader node. Evaluates scaling decisions every 5 minutes.
        """
        SCALING_INTERVAL = 300  # 5 minutes
        await asyncio.sleep(60)  # Initial delay

        logger.info("Auto-scaling loop started")

        while self.running:
            try:
                # Only leader performs scaling
                if self.role != NodeRole.LEADER:
                    await asyncio.sleep(SCALING_INTERVAL)
                    continue

                auto_scaler = get_auto_scaler()
                if auto_scaler is None:
                    await asyncio.sleep(SCALING_INTERVAL)
                    continue

                # Wire up the work queue if not already done
                wq = get_work_queue()
                if wq is not None:
                    auto_scaler.set_work_queue(wq)

                # Evaluate scaling decision
                decision = await auto_scaler.evaluate()

                if decision.action.value == "scale_up":
                    logger.info(f"Auto-scaling: scale_up {decision.count} instances ({decision.reason})")
                    try:
                        from scripts.vast_p2p_sync import provision_instances_async
                        created, _instance_ids = await provision_instances_async(
                            count=decision.count,
                            max_total_hourly=auto_scaler.config.max_hourly_cost,
                        )
                        if created > 0:
                            logger.info(f"Auto-scaling: provisioned {created} new instances")
                            auto_scaler.record_scale_event(decision, success=True)
                            # Notify via webhook
                            await self.notifier.send(
                                f" Auto-scaled UP: Provisioned {created} Vast.ai instances",
                                severity="info",
                                context={"reason": decision.reason, "instances": created}
                            )
                        else:
                            logger.warning("Auto-scaling: scale_up requested but no instances created")
                            auto_scaler.record_scale_event(decision, success=False, error="no_instances_created")
                    except Exception as e:
                        logger.error(f"Auto-scaling provision failed: {e}")
                        auto_scaler.record_scale_event(decision, success=False, error=str(e))

                elif decision.action.value == "scale_down":
                    logger.info(f"Auto-scaling: scale_down {len(decision.node_ids)} instances ({decision.reason})")
                    try:
                        # Get mapping from node_id to vast_instance_id
                        from scripts.vast_p2p_sync import (
                            deprovision_instances_async,
                            get_node_to_vast_mapping_async,
                        )
                        node_to_vast = await get_node_to_vast_mapping_async()

                        # Translate node_ids to vast_instance_ids
                        vast_ids_to_remove = []
                        for node_id in decision.node_ids:
                            if node_id in node_to_vast:
                                vast_ids_to_remove.append(node_to_vast[node_id])
                            else:
                                logger.warning(f"Auto-scaling: node {node_id} not found in Vast mapping")

                        if vast_ids_to_remove:
                            # Deprovision (stop, not destroy - safer for cost saving)
                            removed = await deprovision_instances_async(
                                vast_ids_to_remove,
                                destroy=False,  # Stop instead of destroy for safety
                            )
                            logger.info(f"Auto-scaling: stopped {removed} instances")
                            auto_scaler.record_scale_event(decision, success=True)
                            # Notify via webhook
                            await self.notifier.send(
                                f" Auto-scaled DOWN: Stopped {removed} idle Vast.ai instances",
                                severity="info",
                                context={"reason": decision.reason, "instances": removed}
                            )
                        else:
                            logger.warning("Auto-scaling: scale_down requested but no Vast instances mapped")
                            auto_scaler.record_scale_event(decision, success=False, error="no_vast_mapping")
                    except Exception as e:
                        logger.error(f"Auto-scaling deprovision failed: {e}")
                        auto_scaler.record_scale_event(decision, success=False, error=str(e))

                await asyncio.sleep(SCALING_INTERVAL)

            except Exception as e:
                logger.error(f"Auto-scaling loop error: {e}")
                await asyncio.sleep(SCALING_INTERVAL)

    async def _predictive_monitoring_loop(self):
        """Background loop for proactive monitoring and alerting.

        Predicts issues before they occur and sends proactive alerts.
        Only runs on the leader node.
        """
        MONITOR_INTERVAL = 300  # 5 minutes
        await asyncio.sleep(90)  # Initial delay

        logger.info("Predictive monitoring loop started")

        while self.running:
            try:
                # Only leader performs monitoring
                if self.role != NodeRole.LEADER:
                    await asyncio.sleep(MONITOR_INTERVAL)
                    continue

                alert_manager = get_predictive_alerts()
                if alert_manager is None:
                    await asyncio.sleep(MONITOR_INTERVAL)
                    continue

                # Collect metrics from peers
                with self.peers_lock:
                    for peer in self.peers.values():
                        if not peer.is_alive():
                            continue

                        # Record disk usage
                        disk_pct = float(getattr(peer, "disk_percent", 0) or 0)
                        if disk_pct > 0:
                            alert_manager.record_disk_usage(peer.node_id, disk_pct)

                        # Record memory usage
                        mem_pct = float(getattr(peer, "mem_percent", 0) or 0)
                        if mem_pct > 0:
                            alert_manager.record_memory_usage(peer.node_id, mem_pct)

                # Record queue depth
                wq = get_work_queue()
                if wq is not None:
                    status = wq.get_queue_status()
                    pending = status.get("by_status", {}).get("pending", 0)
                    alert_manager.record_queue_depth(pending)

                # Check for alerts
                node_ids = [p.node_id for p in self.peers.values() if p.is_alive()]

                # Get production models from registry (2025-12-18)
                model_ids = []
                last_training = time.time() - 3600  # Default to 1 hour ago
                try:
                    from app.training.model_registry import ModelRegistry, ModelStage
                    registry = ModelRegistry()
                    production_models = registry.get_versions_by_stage(ModelStage.PRODUCTION)
                    model_ids = [f"{m['model_id']}_v{m['version']}" for m in production_models]

                    # Get last training time from most recently updated model
                    if production_models:
                        # updated_at is ISO format string
                        from datetime import datetime
                        latest_update = max(
                            datetime.fromisoformat(m['updated_at'].replace('Z', '+00:00'))
                            for m in production_models
                            if m.get('updated_at')
                        )
                        last_training = latest_update.timestamp()
                except Exception as e:
                    logger.debug(f"Model registry lookup failed, using defaults: {e}")

                alerts = await alert_manager.run_all_checks(
                    node_ids=node_ids,
                    model_ids=model_ids,
                    last_training_time=last_training,
                )

                # Send alerts via webhook notifier
                for alert in alerts:
                    try:
                        await self.notifier.send(
                            title=f"Proactive Alert: {alert.alert_type.value}",
                            message=alert.message,
                            level="warning" if alert.severity.value == "warning" else "error",
                            fields={
                                "action": alert.action,
                                "target": alert.target_id,
                            },
                            node_id=alert.target_id,
                        )
                    except Exception as e:
                        logger.warning(f"Failed to send proactive alert: {e}")

                await asyncio.sleep(MONITOR_INTERVAL)

            except Exception as e:
                logger.error(f"Predictive monitoring loop error: {e}")
                await asyncio.sleep(MONITOR_INTERVAL)

    async def _self_healing_loop(self):
        """Background loop for self-healing: recover stuck jobs and unhealthy nodes.

        Detects jobs that have exceeded their expected timeout and automatically
        terminates and reschedules them. Stuck job recovery only runs on the leader,
        but stale process cleanup runs on all nodes.
        """
        HEALING_INTERVAL = 60  # 1 minute
        await asyncio.sleep(45)  # Initial delay

        logger.info("Self-healing loop started")
        last_stale_check = 0

        while self.running:
            try:
                # Stale process cleanup runs on ALL nodes (not just leader)
                now = time.time()
                if now - last_stale_check >= STALE_PROCESS_CHECK_INTERVAL:
                    try:
                        killed = self._cleanup_stale_processes()
                        if killed > 0:
                            logger.info(f"Cleaned up {killed} stale processes")
                    except Exception as e:
                        logger.debug(f"Stale process cleanup error: {e}")
                    last_stale_check = now

                # Only leader performs job recovery
                if self.role != NodeRole.LEADER:
                    await asyncio.sleep(HEALING_INTERVAL)
                    continue

                # December 2025: Use UnifiedHealthManager (consolidated from recovery_manager)
                health_manager = get_health_manager()
                if health_manager is None:
                    await asyncio.sleep(HEALING_INTERVAL)
                    continue

                # Wire up work queue
                wq = get_work_queue()
                if wq is not None:
                    health_manager.set_work_queue(wq)

                    # Get running work items
                    status = wq.get_queue_status()
                    running_items = status.get("running", [])

                    # Convert to WorkItem objects for stuck job detection
                    from app.coordination.work_queue import WorkItem

                    work_items = []
                    for item_dict in running_items:
                        with contextlib.suppress(Exception):
                            work_items.append(WorkItem.from_dict(item_dict))

                    # Find stuck jobs
                    stuck_jobs = health_manager.find_stuck_jobs(work_items)

                    for work_item, expected_timeout in stuck_jobs:
                        logger.warning(
                            f"Detected stuck job {work_item.work_id} on {work_item.claimed_by} "
                            f"(running {time.time() - work_item.started_at:.0f}s > expected {expected_timeout * 1.5:.0f}s)"
                        )
                        result = await health_manager.recover_stuck_job(work_item, expected_timeout)
                        if result.value == "success":
                            logger.info(f"Recovered stuck job {work_item.work_id}")

                await asyncio.sleep(HEALING_INTERVAL)

            except Exception as e:
                logger.error(f"Self-healing loop error: {e}")
                await asyncio.sleep(HEALING_INTERVAL)

    async def _job_reaper_loop(self):
        """Background loop for job timeout enforcement (leader-only).

        The JobReaperDaemon provides:
        1. Detection of jobs that exceeded their timeout
        2. SSH-based process termination on remote nodes
        3. Marking timed-out jobs in the work queue
        4. Automatic reassignment of failed work
        5. Temporary blacklisting of repeatedly failing nodes
        """
        await asyncio.sleep(60)  # Initial delay

        logger.info("Job reaper loop starting...")

        while self.running:
            try:
                # Only leader runs the job reaper
                if self.role != NodeRole.LEADER:
                    await asyncio.sleep(30)
                    continue

                # Get work queue
                wq = get_work_queue()
                if wq is None:
                    await asyncio.sleep(30)
                    continue

                # Get SSH config from cluster configuration
                ssh_config = self._get_ssh_config_for_reaper()

                # Get or create job reaper
                reaper = get_job_reaper(work_queue=wq, ssh_config=ssh_config)
                if reaper is None:
                    logger.debug("JobReaperDaemon not available")
                    await asyncio.sleep(60)
                    continue

                # Run one iteration of the reaper (it handles its own sleeping)
                if not reaper.running:
                    # Start the reaper - it will run its own loop
                    logger.info("Starting JobReaperDaemon")
                    asyncio.create_task(reaper.run())

                # Check reaper stats periodically
                stats = reaper.get_stats()
                if stats.get("jobs_reaped", 0) > 0 or stats.get("jobs_reassigned", 0) > 0:
                    logger.info(
                        f"JobReaper stats: reaped={stats['jobs_reaped']}, "
                        f"reassigned={stats['jobs_reassigned']}, "
                        f"blacklisted={stats['currently_blacklisted']}"
                    )

                await asyncio.sleep(60)  # Check every minute

            except Exception as e:
                logger.error(f"Job reaper loop error: {e}")
                await asyncio.sleep(60)

    def _get_ssh_config_for_reaper(self) -> dict[str, Any]:
        """Get SSH configuration for the job reaper from cluster config."""
        ssh_config = {}
        try:
            # Try to load from cluster_nodes.yaml
            config_path = Path(self.ringrift_path) / "ai-service" / "config" / "cluster_nodes.yaml"
            if config_path.exists():
                import yaml
                with open(config_path) as f:
                    config = yaml.safe_load(f)

                for node_id, node_cfg in config.get("nodes", {}).items():
                    ssh = node_cfg.get("ssh", {})
                    if ssh:
                        ssh_config[node_id] = {
                            "host": ssh.get("host", node_id),
                            "user": ssh.get("user", "ubuntu"),
                            "key": ssh.get("key", ""),
                        }
        except Exception as e:
            logger.debug(f"Error loading SSH config for reaper: {e}")

        return ssh_config

    async def _validation_loop(self):
        """Background loop for automatic model validation.

        Finds newly trained models that need validation and queues validation
        work items for them. Only runs on the leader node.
        """
        VALIDATION_INTERVAL = 300  # 5 minutes
        await asyncio.sleep(120)  # Initial delay

        logger.info("Validation loop started")

        while self.running:
            try:
                # Only leader performs validation scheduling
                if self.role != NodeRole.LEADER:
                    await asyncio.sleep(VALIDATION_INTERVAL)
                    continue

                # Get model registry
                try:
                    from app.training.model_registry import ModelRegistry
                    registry = ModelRegistry()
                except Exception as e:
                    logger.debug(f"Model registry not available: {e}")
                    await asyncio.sleep(VALIDATION_INTERVAL)
                    continue

                # Get work queue
                wq = get_work_queue()
                if wq is None:
                    await asyncio.sleep(VALIDATION_INTERVAL)
                    continue

                # Find models needing validation
                models_pending = registry.get_models_needing_validation()
                logger.debug(f"Found {len(models_pending)} models pending validation")

                for model_info in models_pending[:3]:  # Process up to 3 per cycle
                    model_id = model_info['model_id']
                    version = model_info['version']
                    baselines = model_info.get('baselines', [])
                    games_per = model_info.get('games_per_matchup', 50)

                    # Default baselines if none specified
                    if not baselines:
                        baselines = ["mcts_500"]

                    # Create validation work item
                    from app.coordination.work_queue import WorkItem, WorkType
                    work_id = f"validation_{model_id}_v{version}_{int(time.time())}"

                    work_item = WorkItem(
                        work_id=work_id,
                        work_type=WorkType.VALIDATION,
                        priority=80,  # High priority
                        config={
                            "model_id": model_id,
                            "version": version,
                            "baselines": baselines,
                            "games_per_matchup": games_per,
                            "file_path": model_info.get('file_path'),
                        },
                    )

                    try:
                        wq.add_work(work_item)
                        registry.set_validation_queued(model_id, version, work_id)
                        logger.info(f"Queued validation for {model_id}:v{version} ({work_id})")

                        # Notify via webhook
                        await self.notifier.send(
                            f" Model validation queued: {model_id}:v{version}",
                            severity="info",
                            context={"baselines": baselines, "games_per_matchup": games_per}
                        )
                    except Exception as e:
                        logger.error(f"Failed to queue validation for {model_id}:v{version}: {e}")

                # Also check for models without validation entries
                unvalidated = registry.get_unvalidated_models()
                for model_info in unvalidated[:2]:  # Process up to 2 per cycle
                    model_id = model_info['model_id']
                    version = model_info['version']
                    # Create validation entry (will be picked up next cycle)
                    registry.create_validation(model_id, version)
                    logger.info(f"Created validation entry for {model_id}:v{version}")

                await asyncio.sleep(VALIDATION_INTERVAL)

            except Exception as e:
                logger.error(f"Validation loop error: {e}")
                await asyncio.sleep(VALIDATION_INTERVAL)

    async def _queue_populator_loop(self):
        """Background loop to maintain minimum work queue depth.

        Ensures there are always at least 50 work items in the queue until
        all board/player configurations reach 2000 Elo. Only runs on leader.
        """
        POPULATOR_INTERVAL = 60  # 1 minute
        await asyncio.sleep(30)  # Initial delay

        logger.info("Queue populator loop started")

        # Initialize populator
        try:
            import yaml

            from app.coordination.queue_populator import QueuePopulator, load_populator_config_from_yaml

            # Load config from YAML
            config_path = Path(__file__).parent.parent / "config" / "unified_loop.yaml"
            if config_path.exists():
                with open(config_path) as f:
                    yaml_config = yaml.safe_load(f)
                populator_config = load_populator_config_from_yaml(yaml_config)
            else:
                populator_config = None

            populator = QueuePopulator(config=populator_config)
            populator.set_work_queue(get_work_queue())
            self._queue_populator = populator
        except Exception as e:
            logger.error(f"Failed to initialize queue populator: {e}")
            return

        while self.running:
            try:
                # Only leader populates work queue
                if self.role != NodeRole.LEADER:
                    await asyncio.sleep(POPULATOR_INTERVAL)
                    continue

                # Check if populator is enabled
                if not populator.config.enabled:
                    await asyncio.sleep(POPULATOR_INTERVAL)
                    continue

                # Check if all targets are met
                if populator.all_targets_met():
                    logger.info("All Elo targets met (2000+), queue population paused")
                    await asyncio.sleep(POPULATOR_INTERVAL * 5)  # Check less often
                    continue

                # Populate the queue
                items_added = populator.populate()
                if items_added > 0:
                    status = populator.get_status()
                    logger.info(
                        f"Queue populated: +{items_added} items, "
                        f"depth={status['current_queue_depth']}, "
                        f"unmet={status['configs_unmet']}/{status['total_configs']}"
                    )

                    # Notify if significant change
                    if items_added >= 10:
                        await self.notifier.send(
                            f" Queue populated: +{items_added} work items",
                            severity="info",
                            context={
                                "queue_depth": status['current_queue_depth'],
                                "configs_unmet": status['configs_unmet'],
                            }
                        )

                await asyncio.sleep(POPULATOR_INTERVAL)

            except Exception as e:
                logger.error(f"Queue populator loop error: {e}")
                await asyncio.sleep(POPULATOR_INTERVAL)

    async def handle_games_analytics(self, request: web.Request) -> web.Response:
        """GET /games/analytics - Game statistics for dashboards.

        Returns aggregated game analytics including:
        - Average game length by config
        - Victory type distribution
        - Games per hour throughput
        - Opening move diversity
        """
        import json
        from collections import defaultdict

        try:
            # Skip JSONL scanning during startup grace period
            if self._is_in_startup_grace_period():
                return web.json_response({"configs": {}, "message": "Startup in progress"})

            hours = int(request.query.get("hours", "24"))
            cutoff = time.time() - (hours * 3600)

            ai_root = Path(self.ringrift_path) / "ai-service"
            data_dirs = [
                ai_root / "data" / "games" / "daemon_sync",
                ai_root / "data" / "selfplay",
            ]

            # Aggregation containers
            game_lengths: dict[str, list[int]] = defaultdict(list)
            victory_types: dict[str, dict[str, int]] = defaultdict(lambda: defaultdict(int))
            games_by_hour: dict[str, dict[int, int]] = defaultdict(lambda: defaultdict(int))
            opening_moves: dict[str, dict[str, int]] = defaultdict(lambda: defaultdict(int))
            total_games = 0

            for data_dir in data_dirs:
                if not data_dir.exists():
                    continue
                for jsonl_path in data_dir.rglob("*.jsonl"):
                    try:
                        if jsonl_path.stat().st_mtime < cutoff:
                            continue
                        with open_jsonl_file(jsonl_path) as f:
                            for line in f:
                                try:
                                    game = json.loads(line)
                                    board_type = game.get("board_type", "unknown")
                                    num_players = game.get("num_players", 0)
                                    config = f"{board_type}_{num_players}p"

                                    # Game length
                                    length = game.get("length", 0)
                                    if length > 0:
                                        game_lengths[config].append(length)

                                    # Victory type
                                    vt = game.get("victory_type", "unknown")
                                    if vt:
                                        victory_types[config][vt] += 1

                                    # Games by hour (for throughput)
                                    moves = game.get("moves", [])
                                    if moves and len(moves) > 0:
                                        # Use first move timestamp or file mtime
                                        hour_bucket = int(jsonl_path.stat().st_mtime // 3600)
                                        games_by_hour[config][hour_bucket] += 1

                                    # Opening moves (first 3 moves)
                                    if moves and len(moves) >= 1:
                                        first_move = str(moves[0].get("action", ""))[:20]
                                        if first_move:
                                            opening_moves[config][first_move] += 1

                                    total_games += 1
                                except json.JSONDecodeError:
                                    continue
                    except Exception:
                        continue

            # Build response
            analytics = {
                "period_hours": hours,
                "total_games": total_games,
                "configs": {}
            }

            for config in set(list(game_lengths.keys()) + list(victory_types.keys())):
                lengths = game_lengths.get(config, [])
                vt = dict(victory_types.get(config, {}))
                openings = dict(opening_moves.get(config, {}))

                # Calculate throughput (games/hour)
                hourly = games_by_hour.get(config, {})
                throughput = sum(hourly.values()) / max(len(hourly), 1) if hourly else 0

                analytics["configs"][config] = {
                    "games": len(lengths),
                    "avg_length": round(sum(lengths) / len(lengths), 1) if lengths else 0,
                    "min_length": min(lengths) if lengths else 0,
                    "max_length": max(lengths) if lengths else 0,
                    "victory_types": vt,
                    "throughput_per_hour": round(throughput, 1),
                    "opening_diversity": len(openings),
                    "top_openings": dict(sorted(openings.items(), key=lambda x: -x[1])[:5]),
                }

            return web.json_response(analytics)

        except Exception as e:
            return web.json_response({"error": str(e)})

    async def handle_training_metrics(self, request: web.Request) -> web.Response:
        """GET /training/metrics - Training loss and accuracy metrics.

        Returns recent training metrics from log files.
        """
        import re

        try:
            ai_root = Path(self.ringrift_path) / "ai-service"
            logs_dir = ai_root / "logs" / "training"

            metrics = {
                "configs": {},
                "latest_training": None,
            }

            if not logs_dir.exists():
                return web.json_response(metrics)

            # Find recent training logs
            log_files = sorted(logs_dir.glob("*.log"), key=lambda f: f.stat().st_mtime, reverse=True)[:10]

            for log_file in log_files:
                try:
                    content = log_file.read_text()

                    # Extract config from filename (e.g., train_square8_2p_20251214.log)
                    config_match = re.search(r"(square\d+|hexagonal|hex)_(\d+)p", log_file.name)
                    if not config_match:
                        continue
                    config = f"{config_match.group(1)}_{config_match.group(2)}p"

                    # Parse training metrics from log
                    # Look for patterns like: "Epoch 5: loss=0.423, policy_loss=0.312, value_loss=0.111"
                    loss_pattern = re.compile(
                        r"[Ee]poch\s+(\d+).*?loss[=:]\s*([\d.]+).*?"
                        r"(?:policy[_\s]?loss[=:]\s*([\d.]+))?.*?"
                        r"(?:value[_\s]?loss[=:]\s*([\d.]+))?"
                    )

                    epochs = []
                    for match in loss_pattern.finditer(content):
                        epoch = int(match.group(1))
                        total_loss = float(match.group(2))
                        policy_loss = float(match.group(3)) if match.group(3) else None
                        value_loss = float(match.group(4)) if match.group(4) else None
                        epochs.append({
                            "epoch": epoch,
                            "loss": total_loss,
                            "policy_loss": policy_loss,
                            "value_loss": value_loss,
                        })

                    if epochs:
                        metrics["configs"][config] = {
                            "log_file": log_file.name,
                            "epochs": epochs[-20:],  # Last 20 epochs
                            "latest_loss": epochs[-1]["loss"] if epochs else None,
                            "latest_epoch": epochs[-1]["epoch"] if epochs else None,
                        }
                        if not metrics["latest_training"]:
                            metrics["latest_training"] = {
                                "config": config,
                                "file": log_file.name,
                                "mtime": log_file.stat().st_mtime,
                            }

                except Exception:
                    continue

            return web.json_response(metrics)

        except Exception as e:
            return web.json_response({"error": str(e)})

    async def handle_holdout_metrics(self, request: web.Request) -> web.Response:
        """GET /holdout/metrics - Holdout validation metrics.

        Returns holdout set statistics and evaluation results for overfitting detection.
        Supports optional query params:
            - config: Filter by config (e.g., square8_2p)
        """
        try:
            config_filter = request.query.get("config")
            metrics = await self._get_holdout_metrics_cached()

            if config_filter:
                # Filter to specific config
                filtered = {
                    "configs": {k: v for k, v in metrics.get("configs", {}).items() if k == config_filter},
                    "evaluations": [e for e in metrics.get("evaluations", []) if e.get("config") == config_filter],
                    "summary": metrics.get("summary", {}),
                }
                return web.json_response(filtered)

            return web.json_response(metrics)

        except Exception as e:
            return web.json_response({"error": str(e)})

    async def handle_holdout_table(self, request: web.Request) -> web.Response:
        """GET /holdout/table - Holdout validation data in table format for Grafana Infinity.

        Returns holdout metrics as flat table rows.
        """
        try:
            metrics = await self._get_holdout_metrics_cached()

            table_data = []
            for config, data in metrics.get("configs", {}).items():
                row = {
                    "Config": config,
                    "HoldoutGames": data.get("holdout_games", 0),
                    "HoldoutPositions": data.get("holdout_positions", 0),
                    "HoldoutLoss": round(data.get("holdout_loss", 0), 4) if data.get("holdout_loss") else None,
                    "HoldoutAccuracy": round(data.get("holdout_accuracy", 0) * 100, 1) if data.get("holdout_accuracy") else None,
                    "OverfitGap": round(data.get("overfit_gap", 0), 4) if data.get("overfit_gap") else None,
                    "Status": "OK" if (data.get("overfit_gap") or 0) < 0.15 else "OVERFITTING",
                }
                table_data.append(row)

            return web.json_response(table_data)

        except Exception as e:
            return web.json_response([{"error": str(e)}])

    async def handle_mcts_stats(self, request: web.Request) -> web.Response:
        """GET /mcts/stats - MCTS search statistics.

        Returns MCTS performance metrics including nodes/move, search depth, and timing.
        """
        try:
            stats = await self._get_mcts_stats_cached()
            return web.json_response(stats)

        except Exception as e:
            return web.json_response({"error": str(e)})

    async def handle_mcts_table(self, request: web.Request) -> web.Response:
        """GET /mcts/table - MCTS stats in table format for Grafana Infinity.

        Returns MCTS statistics as flat table rows.
        """
        try:
            stats = await self._get_mcts_stats_cached()

            table_data = []
            # Add summary row
            summary = stats.get("summary", {})
            if summary:
                table_data.append({
                    "Config": "CLUSTER AVERAGE",
                    "AvgNodes": round(summary.get("avg_nodes_per_move", 0), 0),
                    "MaxNodes": summary.get("max_nodes_per_move", 0),
                    "AvgDepth": round(summary.get("avg_search_depth", 0), 1),
                    "MaxDepth": summary.get("max_search_depth", 0),
                    "AvgTime": round(summary.get("avg_time_per_move", 0), 3) if summary.get("avg_time_per_move") else None,
                })

            # Add per-config rows
            for config, data in stats.get("configs", {}).items():
                table_data.append({
                    "Config": config,
                    "AvgNodes": round(data.get("avg_nodes", 0), 0) if data.get("avg_nodes") else None,
                    "MaxNodes": None,
                    "AvgDepth": round(data.get("avg_depth", 0), 1) if data.get("avg_depth") else None,
                    "MaxDepth": None,
                    "AvgTime": None,
                })

            return web.json_response(table_data)

        except Exception as e:
            return web.json_response([{"error": str(e)}])

    # =========================================================================
    # Feature Endpoints
    # =========================================================================

    async def handle_matchup_matrix(self, request: web.Request) -> web.Response:
        """GET /matchups/matrix - Head-to-head matchup statistics."""
        try:
            matrix = await self._get_matchup_matrix_cached()
            return web.json_response(matrix)
        except Exception as e:
            return web.json_response({"error": str(e)})

    async def handle_matchup_table(self, request: web.Request) -> web.Response:
        """GET /matchups/table - Matchups in table format for Grafana Infinity."""
        try:
            matrix = await self._get_matchup_matrix_cached()
            table_data = []
            for matchup in matrix.get("matchups", []):
                table_data.append({
                    "ModelA": matchup["model_a"],
                    "ModelB": matchup["model_b"],
                    "AWins": matchup["a_wins"],
                    "BWins": matchup["b_wins"],
                    "Draws": matchup["draws"],
                    "Total": matchup["total"],
                    "AWinRate": round(matchup["a_win_rate"] * 100, 1),
                })
            return web.json_response(table_data)
        except Exception as e:
            return web.json_response([{"error": str(e)}])

    async def handle_model_lineage(self, request: web.Request) -> web.Response:
        """GET /models/lineage - Model ancestry and generation tracking."""
        try:
            lineage = await self._get_model_lineage_cached()
            return web.json_response(lineage)
        except Exception as e:
            return web.json_response({"error": str(e)})

    async def handle_model_lineage_table(self, request: web.Request) -> web.Response:
        """GET /models/lineage/table - Model lineage in table format for Grafana Infinity."""
        try:
            lineage = await self._get_model_lineage_cached()
            table_data = []
            for model in lineage.get("models", []):
                table_data.append({
                    "Name": model["name"],
                    "Config": model["config"],
                    "Generation": model["generation"],
                    "SizeMB": model["size_mb"],
                    "AgeHours": model["age_hours"],
                })
            return web.json_response(sorted(table_data, key=lambda x: (-x["Generation"], x["Config"])))
        except Exception as e:
            return web.json_response([{"error": str(e)}])

    async def handle_data_quality(self, request: web.Request) -> web.Response:
        """GET /data/quality - Data quality metrics and issue detection."""
        try:
            quality = await self._get_data_quality_cached()
            return web.json_response(quality)
        except Exception as e:
            return web.json_response({"error": str(e)})

    async def handle_data_quality_table(self, request: web.Request) -> web.Response:
        """GET /data/quality/table - Data quality in table format for Grafana Infinity."""
        try:
            quality = await self._get_data_quality_cached()
            table_data = []
            for config, metrics in quality.get("configs", {}).items():
                status = "OK"
                for issue in quality.get("issues", []):
                    if issue["config"] == config and issue["severity"] == "warning":
                        status = "WARNING"
                        break
                table_data.append({
                    "Config": config,
                    "Games": metrics["total_games"],
                    "AvgLength": metrics["avg_length"],
                    "ShortRate": metrics["short_game_rate"],
                    "StalemateRate": metrics["stalemate_rate"],
                    "OpeningDiv": metrics["opening_diversity"],
                    "Status": status,
                })
            return web.json_response(table_data)
        except Exception as e:
            return web.json_response([{"error": str(e)}])

    async def handle_data_quality_issues(self, request: web.Request) -> web.Response:
        """GET /data/quality/issues - Data quality issues in table format."""
        try:
            quality = await self._get_data_quality_cached()
            return web.json_response(quality.get("issues", []))
        except Exception as e:
            return web.json_response([{"error": str(e)}])

    async def handle_training_efficiency(self, request: web.Request) -> web.Response:
        """GET /training/efficiency - Training efficiency and cost metrics."""
        try:
            efficiency = await self._get_training_efficiency_cached()
            return web.json_response(efficiency)
        except Exception as e:
            return web.json_response({"error": str(e)})

    async def handle_training_efficiency_table(self, request: web.Request) -> web.Response:
        """GET /training/efficiency/table - Efficiency in table format for Grafana Infinity."""
        try:
            efficiency = await self._get_training_efficiency_cached()
            table_data = []
            for config, metrics in efficiency.get("configs", {}).items():
                table_data.append({
                    "Config": config,
                    "GPUHours": metrics["gpu_hours"],
                    "EloGain": metrics["elo_gain"],
                    "EloPerHour": metrics["elo_per_gpu_hour"],
                    "CostUSD": metrics["estimated_cost_usd"],
                    "CostPerElo": metrics["cost_per_elo_point"],
                })
            return web.json_response(table_data)
        except Exception as e:
            return web.json_response([{"error": str(e)}])

    async def handle_rollback_status(self, request: web.Request) -> web.Response:
        """GET /rollback/status - Model rollback status and recommendations."""
        try:
            status = await self._check_rollback_conditions()
            return web.json_response(status)
        except Exception as e:
            return web.json_response({"error": str(e)})

    async def handle_rollback_candidates(self, request: web.Request) -> web.Response:
        """GET /rollback/candidates - Rollback candidates in table format."""
        try:
            status = await self._check_rollback_conditions()
            table_data = []
            for candidate in status.get("candidates", []):
                table_data.append({
                    "Config": candidate["config"],
                    "Reasons": ", ".join(candidate["reasons"]),
                    "Recommended": "YES" if candidate["rollback_recommended"] else "NO",
                })
            return web.json_response(table_data)
        except Exception as e:
            return web.json_response([{"error": str(e)}])

    async def handle_rollback_execute(self, request: web.Request) -> web.Response:
        """POST /rollback/execute - Execute a model rollback.

        Query params:
            config: Config string like "square8_2p" (required)
            dry_run: If "true", only simulate the rollback (default: false)
        """
        try:
            config = request.query.get("config")
            if not config:
                return web.json_response({"error": "Missing required parameter: config"}, status=400)

            dry_run = request.query.get("dry_run", "").lower() in ("true", "1", "yes")

            result = await self._execute_rollback(config, dry_run=dry_run)
            status_code = 200 if result["success"] else 400
            return web.json_response(result, status=status_code)
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_rollback_auto(self, request: web.Request) -> web.Response:
        """POST /rollback/auto - Trigger automatic rollback check for all configs.

        This will check all configs for rollback conditions and execute rollbacks
        for any that meet the criteria.
        """
        try:
            # Temporarily enable auto-rollback for this request
            original_env = os.environ.get("RINGRIFT_AUTO_ROLLBACK", "")
            os.environ["RINGRIFT_AUTO_ROLLBACK"] = "true"

            executed = await self._auto_rollback_check()

            # Restore original env
            if original_env:
                os.environ["RINGRIFT_AUTO_ROLLBACK"] = original_env
            else:
                os.environ.pop("RINGRIFT_AUTO_ROLLBACK", None)

            return web.json_response({
                "executed_rollbacks": executed,
                "count": len(executed),
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_autoscale_metrics(self, request: web.Request) -> web.Response:
        """GET /autoscale/metrics - Autoscaling metrics and recommendations."""
        try:
            metrics = await self._get_autoscaling_metrics()
            return web.json_response(metrics)
        except Exception as e:
            return web.json_response({"error": str(e)})

    async def handle_autoscale_recommendations(self, request: web.Request) -> web.Response:
        """GET /autoscale/recommendations - Autoscaling recommendations table."""
        try:
            metrics = await self._get_autoscaling_metrics()
            table_data = []
            for rec in metrics.get("recommendations", []):
                table_data.append({
                    "Action": rec["action"].upper(),
                    "Reason": rec["reason"],
                    "SuggestedWorkers": rec["suggested_workers"],
                })
            if not table_data:
                table_data.append({
                    "Action": "NONE",
                    "Reason": "Current scaling is optimal",
                    "SuggestedWorkers": metrics.get("current_state", {}).get("total_nodes", 1),
                })
            return web.json_response(table_data)
        except Exception as e:
            return web.json_response([{"error": str(e)}])

    async def handle_resource_optimizer(self, request: web.Request) -> web.Response:
        """GET /resource/optimizer - Resource optimizer state and recommendations.

        Returns cluster-wide utilization state, PID-controlled optimization
        recommendations, and target utilization ranges (60-80%).
        """
        try:
            if not HAS_NEW_COORDINATION:
                return web.json_response({
                    "error": "Resource optimizer not available",
                    "available": False,
                })

            optimizer = get_resource_optimizer()
            cluster_state = optimizer.get_cluster_state()
            recommendation = optimizer.get_optimization_recommendation()
            metrics = optimizer.get_metrics_dict()

            return web.json_response({
                "available": True,
                "cluster_state": {
                    "total_cpu_util": round(cluster_state.total_cpu_util, 1),
                    "total_gpu_util": round(cluster_state.total_gpu_util, 1),
                    "total_memory_util": round(cluster_state.total_memory_util, 1),
                    "gpu_node_count": cluster_state.gpu_node_count,
                    "cpu_node_count": cluster_state.cpu_node_count,
                    "total_jobs": cluster_state.total_jobs,
                    "nodes": [n.to_dict() for n in cluster_state.nodes],
                },
                "recommendation": recommendation.to_dict(),
                "targets": {
                    "min": TARGET_GPU_UTIL_MIN,
                    "max": TARGET_GPU_UTIL_MAX,
                    "optimal": (TARGET_GPU_UTIL_MIN + TARGET_GPU_UTIL_MAX) // 2,
                },
                "metrics": metrics,
                "in_target_range": {
                    "cpu": TARGET_GPU_UTIL_MIN <= cluster_state.total_cpu_util <= TARGET_GPU_UTIL_MAX,
                    "gpu": TARGET_GPU_UTIL_MIN <= cluster_state.total_gpu_util <= TARGET_GPU_UTIL_MAX
                           if cluster_state.gpu_node_count > 0 else True,
                },
            })
        except Exception as e:
            return web.json_response({"error": str(e), "available": False}, status=500)

    async def handle_resource_utilization_history(self, request: web.Request) -> web.Response:
        """GET /resource/history - Resource utilization history for graphing.

        Query params:
            node_id: Specific node (optional, defaults to cluster average)
            hours: Hours of history (default: 1)
        """
        try:
            if not HAS_NEW_COORDINATION:
                return web.json_response([])

            node_id = request.query.get("node_id")
            hours = float(request.query.get("hours", "1"))

            optimizer = get_resource_optimizer()
            history = optimizer.get_utilization_history(node_id=node_id, hours=hours)
            return web.json_response(history)
        except Exception:
            return web.json_response([])

    async def handle_webhook_test(self, request: web.Request) -> web.Response:
        """POST /webhook/test - Test webhook notification.

        Query params:
            level: debug/info/warning/error (default: info)
            message: Custom message (default: "Test notification")
        """
        try:
            level = request.query.get("level", "info")
            message = request.query.get("message", "Test notification from RingRift AI orchestrator")

            has_slack = bool(self.notifier.slack_webhook)
            has_discord = bool(self.notifier.discord_webhook)

            if not has_slack and not has_discord:
                return web.json_response({
                    "success": False,
                    "message": "No webhooks configured. Set RINGRIFT_SLACK_WEBHOOK and/or RINGRIFT_DISCORD_WEBHOOK",
                })

            await self.notifier.send(
                title="Webhook Test",
                message=message,
                level=level,
                fields={
                    "Node": self.node_id,
                    "Timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "Level": level.upper(),
                },
                node_id=self.node_id,
            )

            return web.json_response({
                "success": True,
                "message": f"Test notification sent to {'Slack' if has_slack else ''}{' and ' if has_slack and has_discord else ''}{'Discord' if has_discord else ''}",
                "level": level,
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_trends_summary(self, request: web.Request) -> web.Response:
        """GET /trends/summary - Get summary of metrics over time period.

        Query params:
            hours: Time period in hours (default: 24)
        """
        try:
            hours = float(request.query.get("hours", "24"))
            summary = self.get_metrics_summary(hours)
            return web.json_response(summary)
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_trends_history(self, request: web.Request) -> web.Response:
        """GET /trends/history - Get historical metrics data.

        Query params:
            metric: Metric type (required) - e.g., "best_elo", "games_generated", "training_loss"
            hours: Time period in hours (default: 24)
            board: Board type filter (optional) - e.g., "square8"
            players: Number of players filter (optional) - e.g., 2
            limit: Max records to return (default: 1000)
        """
        try:
            metric_type = request.query.get("metric")
            if not metric_type:
                return web.json_response({"error": "Missing required parameter: metric"}, status=400)

            hours = float(request.query.get("hours", "24"))
            board_type = request.query.get("board")
            num_players = int(request.query.get("players")) if request.query.get("players") else None
            limit = int(request.query.get("limit", "1000"))

            history = self.get_metrics_history(
                metric_type=metric_type,
                board_type=board_type,
                num_players=num_players,
                hours=hours,
                limit=limit,
            )

            return web.json_response({
                "metric": metric_type,
                "period_hours": hours,
                "count": len(history),
                "data": history,
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_trends_table(self, request: web.Request) -> web.Response:
        """GET /trends/table - Historical trends in table format for Grafana Infinity.

        Query params:
            metric: Metric type (required)
            hours: Time period (default: 168 = 7 days)
        """
        try:
            metric_type = request.query.get("metric")
            if not metric_type:
                return web.json_response([{"error": "Missing metric parameter"}])

            hours = float(request.query.get("hours", "168"))
            history = self.get_metrics_history(metric_type=metric_type, hours=hours, limit=500)

            table_data = []
            for record in history:
                from datetime import datetime
                ts = datetime.fromtimestamp(record["timestamp"]).strftime("%Y-%m-%d %H:%M")
                config = f"{record.get('board_type', '')}_{record.get('num_players', '')}p" if record.get('board_type') else "global"
                table_data.append({
                    "Timestamp": ts,
                    "Config": config,
                    "Value": round(record["value"], 3),
                    "Metric": metric_type,
                })

            return web.json_response(table_data)
        except Exception as e:
            return web.json_response([{"error": str(e)}])

    # ==================== A/B Testing Framework ====================

    def _calculate_ab_test_stats(self, test_id: str) -> dict[str, Any]:
        """Calculate statistical significance for an A/B test."""
        import math

        try:
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()

            # Get game results
            cursor.execute("""
                SELECT model_a_result, model_a_score, model_b_score, game_length
                FROM ab_test_games WHERE test_id = ?
            """, (test_id,))
            games = cursor.fetchall()
            conn.close()

            if not games:
                return {
                    "games_played": 0,
                    "model_a_wins": 0,
                    "model_b_wins": 0,
                    "draws": 0,
                    "model_a_score": 0.0,
                    "model_b_score": 0.0,
                    "model_a_winrate": 0.0,
                    "model_b_winrate": 0.0,
                    "confidence": 0.0,
                    "likely_winner": None,
                    "statistically_significant": False,
                }

            # Count results
            model_a_wins = sum(1 for g in games if g[0] == "win")
            model_b_wins = sum(1 for g in games if g[0] == "loss")
            draws = sum(1 for g in games if g[0] == "draw")
            total = len(games)

            model_a_score = sum(g[1] for g in games)
            model_b_score = sum(g[2] for g in games)

            # Winrate (using score, e.g., 1 for win, 0.5 for draw, 0 for loss)
            model_a_winrate = model_a_score / total if total > 0 else 0.0
            model_b_winrate = model_b_score / total if total > 0 else 0.0

            # Wilson score confidence interval for statistical significance
            # Using normal approximation for simplicity
            def wilson_ci(wins: int, n: int, z: float = 1.96) -> tuple[float, float]:
                if n == 0:
                    return (0.0, 1.0)
                p = wins / n
                denominator = 1 + z * z / n
                center = (p + z * z / (2 * n)) / denominator
                spread = z * math.sqrt((p * (1 - p) + z * z / (4 * n)) / n) / denominator
                return (max(0, center - spread), min(1, center + spread))

            # Calculate confidence intervals
            a_lo, a_hi = wilson_ci(model_a_wins + draws // 2, total)
            b_lo, b_hi = wilson_ci(model_b_wins + draws // 2, total)

            # Determine if statistically significant (non-overlapping CIs)
            statistically_significant = a_hi < b_lo or b_hi < a_lo

            # Estimate confidence based on score difference and sample size
            if total > 0:
                score_diff = abs(model_a_winrate - model_b_winrate)
                # Rough confidence estimate (higher with more games and larger diff)
                confidence = min(0.99, 1 - math.exp(-total * score_diff * 2))
            else:
                confidence = 0.0

            # Determine likely winner
            likely_winner = None
            if model_a_winrate > model_b_winrate + 0.05:
                likely_winner = "model_a"
            elif model_b_winrate > model_a_winrate + 0.05:
                likely_winner = "model_b"

            avg_game_length = sum(g[3] for g in games if g[3]) / max(1, sum(1 for g in games if g[3]))

            return {
                "games_played": total,
                "model_a_wins": model_a_wins,
                "model_b_wins": model_b_wins,
                "draws": draws,
                "model_a_score": model_a_score,
                "model_b_score": model_b_score,
                "model_a_winrate": round(model_a_winrate, 4),
                "model_b_winrate": round(model_b_winrate, 4),
                "confidence": round(confidence, 4),
                "likely_winner": likely_winner,
                "statistically_significant": statistically_significant,
                "avg_game_length": round(avg_game_length, 1),
            }
        except Exception as e:
            return {"error": str(e)}

    async def handle_abtest_create(self, request: web.Request) -> web.Response:
        """POST /abtest/create - Create a new A/B test between two models.

        JSON body:
            name: Test name (required)
            description: Test description (optional)
            board_type: Board type (required) - e.g., "square8"
            num_players: Number of players (required) - e.g., 2
            model_a: Path or ID of first model (required)
            model_b: Path or ID of second model (required)
            target_games: Number of games to play (default: 100)
            confidence_threshold: Confidence level to conclude (default: 0.95)
        """
        try:
            data = await request.json()

            # Validate required fields
            required = ["name", "board_type", "num_players", "model_a", "model_b"]
            for field in required:
                if field not in data:
                    return web.json_response({"error": f"Missing required field: {field}"}, status=400)

            test_id = str(uuid.uuid4())
            now = time.time()

            test_data = {
                "test_id": test_id,
                "name": data["name"],
                "description": data.get("description", ""),
                "board_type": data["board_type"],
                "num_players": int(data["num_players"]),
                "model_a": data["model_a"],
                "model_b": data["model_b"],
                "target_games": int(data.get("target_games", 100)),
                "confidence_threshold": float(data.get("confidence_threshold", 0.95)),
                "status": "running",
                "winner": None,
                "created_at": now,
                "completed_at": None,
                "metadata": json.dumps(data.get("metadata", {})),
            }

            # Store in database
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()
            cursor.execute("""
                INSERT INTO ab_tests (
                    test_id, name, description, board_type, num_players,
                    model_a, model_b, target_games, confidence_threshold,
                    status, winner, created_at, completed_at, metadata
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                test_data["test_id"], test_data["name"], test_data["description"],
                test_data["board_type"], test_data["num_players"],
                test_data["model_a"], test_data["model_b"],
                test_data["target_games"], test_data["confidence_threshold"],
                test_data["status"], test_data["winner"],
                test_data["created_at"], test_data["completed_at"],
                test_data["metadata"],
            ))
            conn.commit()
            conn.close()

            # Store in memory
            with self.ab_test_lock:
                self.ab_tests[test_id] = test_data

            return web.json_response({
                "test_id": test_id,
                "status": "created",
                "message": f"A/B test '{data['name']}' created. Submit game results via POST /abtest/result",
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_abtest_result(self, request: web.Request) -> web.Response:
        """POST /abtest/result - Submit a game result for an A/B test.

        JSON body:
            test_id: A/B test ID (required)
            game_id: Unique game ID (required)
            winner: "model_a", "model_b", or "draw" (required)
            game_length: Number of moves in the game (optional)
            metadata: Additional game metadata (optional)
        """
        try:
            data = await request.json()

            test_id = data.get("test_id")
            if not test_id:
                return web.json_response({"error": "Missing test_id"}, status=400)

            game_id = data.get("game_id") or str(uuid.uuid4())
            winner = data.get("winner")
            if winner not in ["model_a", "model_b", "draw"]:
                return web.json_response({"error": "winner must be 'model_a', 'model_b', or 'draw'"}, status=400)

            # Calculate scores
            if winner == "model_a":
                model_a_result = "win"
                model_a_score = 1.0
                model_b_score = 0.0
            elif winner == "model_b":
                model_a_result = "loss"
                model_a_score = 0.0
                model_b_score = 1.0
            else:
                model_a_result = "draw"
                model_a_score = 0.5
                model_b_score = 0.5

            now = time.time()

            # Store game result
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()

            # Verify test exists
            cursor.execute("SELECT status, target_games, confidence_threshold FROM ab_tests WHERE test_id = ?", (test_id,))
            row = cursor.fetchone()
            if not row:
                conn.close()
                return web.json_response({"error": f"Test {test_id} not found"}, status=404)

            test_status, target_games, confidence_threshold = row
            if test_status != "running":
                conn.close()
                return web.json_response({"error": f"Test {test_id} is {test_status}, not running"}, status=400)

            # Insert game result
            cursor.execute("""
                INSERT INTO ab_test_games (
                    test_id, game_id, model_a_result, model_a_score, model_b_score,
                    game_length, played_at, metadata
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                test_id, game_id, model_a_result, model_a_score, model_b_score,
                data.get("game_length"), now, json.dumps(data.get("metadata", {})),
            ))
            conn.commit()
            conn.close()

            # Calculate updated stats
            stats = self._calculate_ab_test_stats(test_id)

            # Check if test should conclude
            should_conclude = False
            if stats.get("games_played", 0) >= target_games or (stats.get("statistically_significant") and stats.get("confidence", 0) >= confidence_threshold):
                should_conclude = True

            if should_conclude:
                winner_model = stats.get("likely_winner")
                conn = sqlite3.connect(str(self.db_path))
                cursor = conn.cursor()
                cursor.execute("""
                    UPDATE ab_tests SET status = 'completed', winner = ?, completed_at = ?
                    WHERE test_id = ?
                """, (winner_model, time.time(), test_id))
                conn.commit()
                conn.close()

                # Notify
                self.notifier.notify(
                    f"A/B Test Complete: {test_id}",
                    f"Winner: {winner_model or 'inconclusive'}\n"
                    f"Games: {stats['games_played']}, Confidence: {stats['confidence']:.1%}"
                )

            return web.json_response({
                "test_id": test_id,
                "game_id": game_id,
                "recorded": True,
                "stats": stats,
                "concluded": should_conclude,
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_abtest_status(self, request: web.Request) -> web.Response:
        """GET /abtest/status - Get status of an A/B test.

        Query params:
            test_id: A/B test ID (required)
        """
        try:
            test_id = request.query.get("test_id")
            if not test_id:
                return web.json_response({"error": "Missing test_id parameter"}, status=400)

            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM ab_tests WHERE test_id = ?", (test_id,))
            row = cursor.fetchone()
            conn.close()

            if not row:
                return web.json_response({"error": f"Test {test_id} not found"}, status=404)

            test_data = {
                "test_id": row[0],
                "name": row[1],
                "description": row[2],
                "board_type": row[3],
                "num_players": row[4],
                "model_a": row[5],
                "model_b": row[6],
                "target_games": row[7],
                "confidence_threshold": row[8],
                "status": row[9],
                "winner": row[10],
                "created_at": row[11],
                "completed_at": row[12],
                "metadata": json.loads(row[13]) if row[13] else {},
            }

            # Add current stats
            test_data["stats"] = self._calculate_ab_test_stats(test_id)

            return web.json_response(test_data)
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_abtest_list(self, request: web.Request) -> web.Response:
        """GET /abtest/list - List all A/B tests.

        Query params:
            status: Filter by status (optional) - "running", "completed", "cancelled"
            limit: Max results (default: 50)
        """
        try:
            status_filter = request.query.get("status")
            limit = int(request.query.get("limit", "50"))

            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()

            if status_filter:
                cursor.execute(
                    "SELECT test_id, name, board_type, num_players, model_a, model_b, status, winner, created_at "
                    "FROM ab_tests WHERE status = ? ORDER BY created_at DESC LIMIT ?",
                    (status_filter, limit)
                )
            else:
                cursor.execute(
                    "SELECT test_id, name, board_type, num_players, model_a, model_b, status, winner, created_at "
                    "FROM ab_tests ORDER BY created_at DESC LIMIT ?",
                    (limit,)
                )

            rows = cursor.fetchall()
            conn.close()

            tests = []
            for row in rows:
                test_id = row[0]
                stats = self._calculate_ab_test_stats(test_id)
                tests.append({
                    "test_id": test_id,
                    "name": row[1],
                    "board_type": row[2],
                    "num_players": row[3],
                    "model_a": row[4],
                    "model_b": row[5],
                    "status": row[6],
                    "winner": row[7],
                    "created_at": row[8],
                    "games_played": stats.get("games_played", 0),
                    "model_a_winrate": stats.get("model_a_winrate", 0),
                    "model_b_winrate": stats.get("model_b_winrate", 0),
                    "confidence": stats.get("confidence", 0),
                })

            return web.json_response({"tests": tests, "count": len(tests)})
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_abtest_cancel(self, request: web.Request) -> web.Response:
        """POST /abtest/cancel - Cancel a running A/B test.

        JSON body:
            test_id: A/B test ID (required)
            reason: Cancellation reason (optional)
        """
        try:
            data = await request.json()
            test_id = data.get("test_id")
            if not test_id:
                return web.json_response({"error": "Missing test_id"}, status=400)

            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()
            cursor.execute("SELECT status FROM ab_tests WHERE test_id = ?", (test_id,))
            row = cursor.fetchone()

            if not row:
                conn.close()
                return web.json_response({"error": f"Test {test_id} not found"}, status=404)

            if row[0] != "running":
                conn.close()
                return web.json_response({"error": f"Test {test_id} is already {row[0]}"}, status=400)

            cursor.execute(
                "UPDATE ab_tests SET status = 'cancelled', completed_at = ? WHERE test_id = ?",
                (time.time(), test_id)
            )
            conn.commit()
            conn.close()

            return web.json_response({"test_id": test_id, "status": "cancelled"})
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_abtest_table(self, request: web.Request) -> web.Response:
        """GET /abtest/table - A/B tests in table format for Grafana Infinity.

        Query params:
            status: Filter by status (optional)
        """
        try:
            status_filter = request.query.get("status")

            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()

            if status_filter:
                cursor.execute(
                    "SELECT test_id, name, board_type, num_players, model_a, model_b, status, winner, created_at "
                    "FROM ab_tests WHERE status = ? ORDER BY created_at DESC LIMIT 100",
                    (status_filter,)
                )
            else:
                cursor.execute(
                    "SELECT test_id, name, board_type, num_players, model_a, model_b, status, winner, created_at "
                    "FROM ab_tests ORDER BY created_at DESC LIMIT 100"
                )

            rows = cursor.fetchall()
            conn.close()

            table_data = []
            for row in rows:
                test_id = row[0]
                stats = self._calculate_ab_test_stats(test_id)
                from datetime import datetime
                created = datetime.fromtimestamp(row[8]).strftime("%Y-%m-%d %H:%M") if row[8] else ""

                table_data.append({
                    "Test ID": test_id[:8],
                    "Name": row[1],
                    "Config": f"{row[2]}_{row[3]}p",
                    "Model A": row[4].split("/")[-1] if "/" in row[4] else row[4],
                    "Model B": row[5].split("/")[-1] if "/" in row[5] else row[5],
                    "Games": stats.get("games_played", 0),
                    "A Win%": f"{stats.get('model_a_winrate', 0):.1%}",
                    "B Win%": f"{stats.get('model_b_winrate', 0):.1%}",
                    "Confidence": f"{stats.get('confidence', 0):.1%}",
                    "Status": row[6],
                    "Winner": row[7] or "-",
                    "Created": created,
                })

            return web.json_response(table_data)
        except Exception as e:
            return web.json_response([{"error": str(e)}])

    async def handle_abtest_run(self, request: web.Request) -> web.Response:
        """POST /abtest/run - Start running games for an A/B test using the cluster.

        This schedules games to be played between model_a and model_b on available nodes.

        JSON body:
            test_id: A/B test ID (required)
            parallel_games: Number of games to run in parallel (default: 4)
            think_time_ms: AI think time in ms (default: 100)
        """
        try:
            data = await request.json()
            test_id = data.get("test_id")
            if not test_id:
                return web.json_response({"error": "Missing test_id"}, status=400)

            # Get test info
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()
            cursor.execute(
                "SELECT board_type, num_players, model_a, model_b, target_games, status "
                "FROM ab_tests WHERE test_id = ?",
                (test_id,)
            )
            row = cursor.fetchone()
            conn.close()

            if not row:
                return web.json_response({"error": f"Test {test_id} not found"}, status=404)

            _board_type, _num_players, _model_a, _model_b, target_games, status = row
            if status != "running":
                return web.json_response({"error": f"Test is {status}, not running"}, status=400)

            # Get current game count
            stats = self._calculate_ab_test_stats(test_id)
            games_remaining = target_games - stats.get("games_played", 0)

            if games_remaining <= 0:
                return web.json_response({
                    "test_id": test_id,
                    "message": "Test has reached target games",
                    "stats": stats,
                })

            parallel = int(data.get("parallel_games", 4))
            think_time = int(data.get("think_time_ms", 100))

            # Schedule games via existing tournament infrastructure
            # This creates a mini-tournament between the two models
            f"abtest_{test_id[:8]}"

            return web.json_response({
                "test_id": test_id,
                "status": "scheduled",
                "games_remaining": games_remaining,
                "parallel_games": parallel,
                "think_time_ms": think_time,
                "message": f"Use tournament infrastructure to run {games_remaining} games between models",
                "hint": "Games should be submitted via POST /abtest/result as they complete",
            })
        except Exception as e:
            return web.json_response({"error": str(e)}, status=500)

    async def handle_api_training_status(self, request: web.Request) -> web.Response:
        """Get training pipeline status including NNUE, CMAES, and auto-promotion state.

        Returns daemon state for NNUE training, CMAES optimization, and model promotion.
        """
        try:
            from datetime import datetime

            ai_root = Path(self.ringrift_path) / "ai-service"

            # Load daemon state (from continuous_improvement_daemon.py)
            daemon_state_path = ai_root / "logs" / "improvement_daemon" / "state.json"
            daemon_state = {}
            daemon_running = False
            daemon_pid = None
            daemon_uptime = 0

            # Check if daemon is running
            pid_file = ai_root / "logs" / "improvement_daemon" / "daemon.pid"
            if pid_file.exists():
                try:
                    daemon_pid = int(pid_file.read_text().strip())
                    # Check if process is running
                    import os
                    os.kill(daemon_pid, 0)  # Doesn't kill, just checks
                    daemon_running = True
                except (ValueError, ProcessLookupError, PermissionError):
                    daemon_running = False

            if daemon_state_path.exists():
                try:
                    daemon_state = json.loads(daemon_state_path.read_text())
                    # Calculate uptime if daemon is running
                    if daemon_running and daemon_state.get("started_at"):
                        started = datetime.fromisoformat(daemon_state["started_at"])
                        daemon_uptime = (datetime.now() - started).total_seconds()
                except (json.JSONDecodeError, ValueError, OSError):
                    pass

            # Load runtime overrides (promoted models)
            overrides_path = ai_root / "data" / "ladder_runtime_overrides.json"
            runtime_overrides = {}
            if overrides_path.exists():
                with contextlib.suppress(json.JSONDecodeError, ValueError, OSError):
                    runtime_overrides = json.loads(overrides_path.read_text())

            # Load auto-promotion log
            promotion_log_path = (
                ai_root / "runs" / "promotion" / "model_promotion_history.json"
                if (ai_root / "runs" / "promotion" / "model_promotion_history.json").exists()
                else (ai_root / "data" / "auto_promotion_log.json")
            )
            promotion_log = []
            if promotion_log_path.exists():
                try:
                    promotion_log = json.loads(promotion_log_path.read_text())
                    if isinstance(promotion_log, list):
                        promotion_log = promotion_log[-10:]  # Last 10 entries
                except (json.JSONDecodeError, ValueError, OSError):
                    pass

            # Check NNUE model timestamps
            nnue_models = {}
            nnue_dir = ai_root / "models" / "nnue"
            if nnue_dir.exists():
                for model_file in nnue_dir.glob("*.pt"):
                    if "_prev" not in model_file.name:
                        stat = model_file.stat()
                        nnue_models[model_file.stem] = {
                            "path": str(model_file),
                            "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                            "size_mb": round(stat.st_size / 1024 / 1024, 2),
                        }

            # Check trained heuristic profiles
            profiles_path = ai_root / "data" / "trained_heuristic_profiles.json"
            heuristic_profiles = {}
            if profiles_path.exists():
                try:
                    profiles_data = json.loads(profiles_path.read_text())
                    heuristic_profiles = {
                        "count": len(profiles_data),
                        "profiles": list(profiles_data.keys())[:20],
                    }
                except (json.JSONDecodeError, ValueError, OSError):
                    pass

            return web.json_response({
                "success": True,
                "daemon": {
                    "running": daemon_running,
                    "pid": daemon_pid,
                    "uptime_seconds": daemon_uptime,
                    "current_cycle": daemon_state.get("total_cycles", 0),
                    "last_cycle_at": daemon_state.get("last_cycle_at", ""),
                    "total_games_generated": daemon_state.get("total_games_generated", 0),
                    "total_training_runs": daemon_state.get("total_training_runs", 0),
                    "total_tournaments": daemon_state.get("total_tournaments", 0),
                    "total_auto_promotions": daemon_state.get("total_auto_promotions", 0),
                    "last_auto_promote_time": daemon_state.get("last_auto_promote_time", 0),
                    "consecutive_failures": daemon_state.get("consecutive_failures", 0),
                },
                "nnue": {
                    "state": "idle" if not daemon_state.get("nnue_state") else "active",
                    "models": list(nnue_models.keys()),
                    "model_details": nnue_models,
                    "per_config_state": daemon_state.get("nnue_state", {}),
                    "last_gate_result": daemon_state.get("last_nnue_gate_result", None),
                },
                "cmaes": {
                    "state": "idle" if not daemon_state.get("cmaes_state") else "active",
                    "profiles": heuristic_profiles.get("profiles", []) if heuristic_profiles else [],
                    "profile_count": heuristic_profiles.get("count", 0) if heuristic_profiles else 0,
                    "per_config_state": daemon_state.get("cmaes_state", {}),
                    "generations": sum(s.get("generations", 0) for s in daemon_state.get("cmaes_state", {}).values()),
                },
                "promotion": {
                    "runtime_overrides": runtime_overrides,
                    "recent_promotions": promotion_log,
                },
                "timestamp": time.time(),
            })

        except Exception as e:
            return web.json_response({"success": False, "error": str(e)}, status=500)

    def _canonical_slug_for_board(self, board_type: str) -> str:
        return {
            "square8": "square8",
            "square19": "square19",
            "hex8": "hex8",
            "hexagonal": "hex",
        }.get(board_type, board_type)

    def _canonical_gate_paths(self, board_type: str, num_players: int) -> tuple[Path, Path]:
        """Compute canonical DB + gate summary paths (leader-side conventions)."""
        slug = self._canonical_slug_for_board(board_type)
        suffix = "" if int(num_players) == 2 else f"_{int(num_players)}p"
        ai_root = Path(self.ringrift_path) / "ai-service"
        db_path = (ai_root / "data" / "games" / f"canonical_{slug}{suffix}.db").resolve()
        summary_path = (ai_root / "data" / "games" / f"db_health.canonical_{slug}{suffix}.json").resolve()
        return db_path, summary_path

    def _tail_text_file(self, path: Path, *, max_lines: int = 200, max_bytes: int = 256_000) -> str:
        """Best-effort tail of a potentially large log file."""
        try:
            if not path.exists():
                return ""
            with path.open("rb") as f:
                f.seek(0, os.SEEK_END)
                size = f.tell()
                seek = max(0, size - int(max_bytes))
                f.seek(seek)
                data = f.read().decode("utf-8", errors="replace")
            lines = data.splitlines()
            return "\n".join(lines[-int(max_lines) :])
        except Exception as e:
            return f"[tail_error] {e}"

    async def handle_api_canonical_health(self, request: web.Request) -> web.Response:
        """List canonical gate summary JSONs found on this node."""
        try:
            if not self._is_leader() and request.query.get("local") != "1":
                return await self._proxy_to_leader(request)

            ai_root = Path(self.ringrift_path) / "ai-service"
            games_dir = (ai_root / "data" / "games").resolve()
            summaries: list[dict[str, Any]] = []

            for path in sorted(games_dir.glob("db_health.canonical_*.json"), key=lambda p: p.stat().st_mtime, reverse=True):
                try:
                    payload = json.loads(path.read_text(encoding="utf-8"))
                except Exception as exc:
                    payload = {"error": "failed_to_parse_json", "message": str(exc)}

                mtime = 0.0
                try:
                    mtime = float(path.stat().st_mtime)
                except Exception:
                    mtime = 0.0

                db_path_str = str(payload.get("db_path") or "")
                db_size_bytes = None
                if db_path_str:
                    try:
                        db_path = Path(db_path_str)
                        if not db_path.is_absolute():
                            db_path = (games_dir / db_path).resolve()
                        db_size_bytes = int(db_path.stat().st_size)
                    except Exception:
                        db_size_bytes = None

                summaries.append(
                    {
                        "path": str(path),
                        "modified_time": mtime,
                        "db_size_bytes": db_size_bytes,
                        "summary": payload,
                    }
                )

            return web.json_response(
                {
                    "success": True,
                    "node_id": self.node_id,
                    "is_leader": self._is_leader(),
                    "summaries": summaries,
                    "timestamp": time.time(),
                }
            )
        except Exception as e:
            return web.json_response({"success": False, "error": str(e)}, status=500)

    async def handle_api_canonical_jobs_list(self, request: web.Request) -> web.Response:
        """List canonical gate jobs started from this node."""
        try:
            if not self._is_leader() and request.query.get("local") != "1":
                return await self._proxy_to_leader(request)

            with self.canonical_gate_jobs_lock:
                jobs = list(self.canonical_gate_jobs.values())
            jobs.sort(key=lambda j: float(j.get("started_at", 0.0) or 0.0), reverse=True)
            return web.json_response(
                {
                    "success": True,
                    "node_id": self.node_id,
                    "is_leader": self._is_leader(),
                    "jobs": jobs[:100],
                    "timestamp": time.time(),
                }
            )
        except Exception as e:
            return web.json_response({"success": False, "error": str(e)}, status=500)

    async def handle_api_canonical_job_get(self, request: web.Request) -> web.Response:
        """Get details for a canonical gate job."""
        try:
            if not self._is_leader() and request.query.get("local") != "1":
                return await self._proxy_to_leader(request)

            job_id = (request.match_info.get("job_id") or "").strip()
            if not job_id:
                return web.json_response({"success": False, "error": "job_id is required"}, status=400)
            with self.canonical_gate_jobs_lock:
                job = self.canonical_gate_jobs.get(job_id)
            if not job:
                return web.json_response({"success": False, "error": f"Job {job_id} not found"}, status=404)
            return web.json_response({"success": True, "job": job})
        except Exception as e:
            return web.json_response({"success": False, "error": str(e)}, status=500)

    async def handle_api_canonical_job_log(self, request: web.Request) -> web.Response:
        """Tail the log file for a canonical gate job."""
        try:
            if not self._is_leader() and request.query.get("local") != "1":
                return await self._proxy_to_leader(request)

            job_id = (request.match_info.get("job_id") or "").strip()
            if not job_id:
                return web.json_response({"success": False, "error": "job_id is required"}, status=400)
            tail_lines = int(request.query.get("tail", 200))
            tail_lines = max(10, min(tail_lines, 1000))

            with self.canonical_gate_jobs_lock:
                job = self.canonical_gate_jobs.get(job_id)
            if not job:
                return web.json_response({"success": False, "error": f"Job {job_id} not found"}, status=404)

            log_path = Path(str(job.get("log_path") or ""))
            text = self._tail_text_file(log_path, max_lines=tail_lines)
            return web.json_response({"success": True, "job_id": job_id, "log_tail": text})
        except Exception as e:
            return web.json_response({"success": False, "error": str(e)}, status=500)

    def _canonical_gate_log_dir(self) -> Path:
        return (Path(self.ringrift_path) / "ai-service" / "logs" / "canonical_gate").resolve()

    async def handle_api_canonical_logs_list(self, request: web.Request) -> web.Response:
        """List canonical gate log files on this node (use ?local=1 to avoid proxying to the leader)."""
        try:
            if not self._is_leader() and request.query.get("local") != "1":
                return await self._proxy_to_leader(request)

            logs_dir = self._canonical_gate_log_dir()
            entries: list[dict[str, Any]] = []
            if logs_dir.exists():
                paths = sorted(
                    logs_dir.glob("*.log"),
                    key=lambda p: float(p.stat().st_mtime),
                    reverse=True,
                )
                for path in paths[:200]:
                    try:
                        st = path.stat()
                        entries.append(
                            {
                                "name": path.name,
                                "path": str(path),
                                "size_bytes": int(st.st_size),
                                "modified_time": float(st.st_mtime),
                            }
                        )
                    except Exception:
                        continue

            return web.json_response(
                {
                    "success": True,
                    "node_id": self.node_id,
                    "is_leader": self._is_leader(),
                    "log_dir": str(logs_dir),
                    "logs": entries,
                    "timestamp": time.time(),
                }
            )
        except Exception as e:
            return web.json_response({"success": False, "error": str(e)}, status=500)

    async def handle_api_canonical_log_tail(self, request: web.Request) -> web.Response:
        """Tail a specific canonical gate log file by name."""
        try:
            if not self._is_leader() and request.query.get("local") != "1":
                return await self._proxy_to_leader(request)

            log_name = (request.match_info.get("log_name") or "").strip()
            if not log_name:
                return web.json_response({"success": False, "error": "log_name is required"}, status=400)
            if any(token in log_name for token in ("..", "/", "\\")):
                return web.json_response({"success": False, "error": "Invalid log_name"}, status=400)

            tail_lines = int(request.query.get("tail", 200))
            tail_lines = max(10, min(tail_lines, 2000))

            logs_dir = self._canonical_gate_log_dir()
            log_path = (logs_dir / log_name).resolve()
            if log_path.parent != logs_dir:
                return web.json_response({"success": False, "error": "Invalid log_name"}, status=400)
            if not log_path.exists() or not log_path.is_file():
                return web.json_response({"success": False, "error": f"Log {log_name} not found"}, status=404)

            text = self._tail_text_file(log_path, max_lines=tail_lines)
            return web.json_response(
                {
                    "success": True,
                    "node_id": self.node_id,
                    "is_leader": self._is_leader(),
                    "log_name": log_name,
                    "log_tail": text,
                    "timestamp": time.time(),
                }
            )
        except Exception as e:
            return web.json_response({"success": False, "error": str(e)}, status=500)

    async def _monitor_canonical_gate_job(self, job_id: str, proc: asyncio.subprocess.Process, summary_path: Path) -> None:
        """Background task: wait for canonical gate to finish and record summary."""
        try:
            returncode = await proc.wait()
        except Exception:
            returncode = -1

        finished_at = time.time()
        gate_summary: dict[str, Any] | None = None
        try:
            if summary_path.exists():
                gate_summary = json.loads(summary_path.read_text(encoding="utf-8"))
        except Exception:
            gate_summary = None

        with self.canonical_gate_jobs_lock:
            job = self.canonical_gate_jobs.get(job_id, {})
            prior_status = str(job.get("status") or "")
            if prior_status == "cancelling":
                status = "cancelled"
            else:
                status = "completed" if int(returncode) == 0 else "failed"
            job.update(
                {
                    "status": status,
                    "returncode": int(returncode),
                    "completed_at": finished_at,
                    "gate_summary": gate_summary,
                }
            )
            self.canonical_gate_jobs[job_id] = job

    async def handle_api_canonical_generate(self, request: web.Request) -> web.Response:
        """Start a canonical selfplay+gate run (leader-only, dashboard-triggered)."""
        if not self._is_leader() and request.query.get("local") != "1":
            return await self._proxy_to_leader(request)
        if not self._is_leader():
            return web.json_response(
                {"success": False, "error": "Only leader can start canonical gate runs", "leader_id": self.leader_id},
                status=403,
            )

        try:
            data = await request.json()
            board_type = str(data.get("board_type") or "square8")
            num_players = int(data.get("num_players") or 2)
            num_games = int(data.get("num_games") or 0)
            difficulty_band = str(data.get("difficulty_band") or "light")
            reset_db = bool(data.get("reset_db") or False)
            hosts = (str(data.get("hosts") or "").strip()) or None
            distributed_job_timeout_seconds = int(data.get("distributed_job_timeout_seconds") or 0)
            distributed_fetch_timeout_seconds = int(data.get("distributed_fetch_timeout_seconds") or 0)

            if board_type not in ("square8", "square19", "hex8", "hexagonal"):
                return web.json_response({"success": False, "error": f"Unsupported board_type: {board_type}"}, status=400)
            if num_players not in (2, 3, 4):
                return web.json_response({"success": False, "error": f"Unsupported num_players: {num_players}"}, status=400)
            if num_games < 0 or num_games > 250_000:
                return web.json_response({"success": False, "error": f"num_games out of range: {num_games}"}, status=400)
            if difficulty_band not in ("light", "canonical"):
                return web.json_response({"success": False, "error": f"Unsupported difficulty_band: {difficulty_band}"}, status=400)
            if hosts and any(c.isspace() for c in hosts):
                return web.json_response({"success": False, "error": "hosts must be comma-separated with no spaces"}, status=400)
            if distributed_job_timeout_seconds < 0 or distributed_job_timeout_seconds > 604_800:
                return web.json_response(
                    {"success": False, "error": f"distributed_job_timeout_seconds out of range: {distributed_job_timeout_seconds}"},
                    status=400,
                )
            if distributed_fetch_timeout_seconds < 0 or distributed_fetch_timeout_seconds > 86_400:
                return web.json_response(
                    {"success": False, "error": f"distributed_fetch_timeout_seconds out of range: {distributed_fetch_timeout_seconds}"},
                    status=400,
                )

            db_path, summary_path = self._canonical_gate_paths(board_type, num_players)

            job_id = f"canon_gate_{board_type}_{num_players}p_{int(time.time())}_{secrets.token_hex(4)}"
            ai_root = Path(self.ringrift_path) / "ai-service"
            log_dir = (ai_root / "logs" / "canonical_gate").resolve()
            log_dir.mkdir(parents=True, exist_ok=True)
            log_path = (log_dir / f"{job_id}.log").resolve()

            cmd = [
                sys.executable,
                "scripts/generate_canonical_selfplay.py",
                "--board-type", board_type,
                "--num-players", str(num_players),
                "--num-games", str(num_games),
                "--difficulty-band", difficulty_band,
                "--db", str(db_path),
                "--summary", str(summary_path),
            ]
            if hosts:
                cmd.extend(["--hosts", hosts])
                if distributed_job_timeout_seconds > 0:
                    cmd.extend(
                        [
                            "--distributed-job-timeout-seconds",
                            str(distributed_job_timeout_seconds),
                        ]
                    )
                if distributed_fetch_timeout_seconds > 0:
                    cmd.extend(
                        [
                            "--distributed-fetch-timeout-seconds",
                            str(distributed_fetch_timeout_seconds),
                        ]
                    )
            if reset_db:
                cmd.append("--reset-db")

            env = os.environ.copy()
            env["PYTHONPATH"] = str(ai_root)
            env.setdefault("RINGRIFT_JOB_ORIGIN", "dashboard")
            env.setdefault("PYTHONUNBUFFERED", "1")

            with log_path.open("a", encoding="utf-8") as log_handle:
                proc = await asyncio.create_subprocess_exec(
                    *cmd,
                    cwd=str(ai_root),
                    env=env,
                    stdout=log_handle,
                    stderr=log_handle,
                )

            job = {
                "job_id": job_id,
                "status": "running",
                "board_type": board_type,
                "num_players": num_players,
                "num_games": num_games,
                "difficulty_band": difficulty_band,
                "hosts": hosts,
                "reset_db": reset_db,
                "distributed_job_timeout_seconds": distributed_job_timeout_seconds,
                "distributed_fetch_timeout_seconds": distributed_fetch_timeout_seconds,
                "db_path": str(db_path),
                "summary_path": str(summary_path),
                "log_path": str(log_path),
                "pid": int(proc.pid),
                "started_at": time.time(),
            }

            with self.canonical_gate_jobs_lock:
                self.canonical_gate_jobs[job_id] = job

            asyncio.create_task(self._monitor_canonical_gate_job(job_id, proc, summary_path))

            return web.json_response({"success": True, "job": job})
        except Exception as e:
            return web.json_response({"success": False, "error": str(e)}, status=500)

    async def handle_api_canonical_job_cancel(self, request: web.Request) -> web.Response:
        """Cancel a running canonical gate job."""
        if not self._is_leader() and request.query.get("local") != "1":
            return await self._proxy_to_leader(request)
        if not self._is_leader():
            return web.json_response(
                {"success": False, "error": "Only leader can cancel canonical gate runs", "leader_id": self.leader_id},
                status=403,
            )

        try:
            job_id = (request.match_info.get("job_id") or "").strip()
            if not job_id:
                return web.json_response({"success": False, "error": "job_id is required"}, status=400)

            with self.canonical_gate_jobs_lock:
                job = self.canonical_gate_jobs.get(job_id)
            if not job:
                return web.json_response({"success": False, "error": f"Job {job_id} not found"}, status=404)

            pid = int(job.get("pid") or 0)
            if pid <= 0:
                return web.json_response({"success": False, "error": "No pid recorded for job"}, status=400)

            try:
                os.kill(pid, signal.SIGTERM)
            except Exception as exc:
                return web.json_response({"success": False, "error": f"Failed to signal pid {pid}: {exc}"}, status=500)

            with self.canonical_gate_jobs_lock:
                job = self.canonical_gate_jobs.get(job_id, job)
                job["status"] = "cancelling"
                job["cancel_requested_at"] = time.time()
                self.canonical_gate_jobs[job_id] = job

            return web.json_response({"success": True, "message": f"Cancel signaled for {job_id}", "job": job})
        except Exception as e:
            return web.json_response({"success": False, "error": str(e)}, status=500)

    async def handle_api_jobs_list(self, request: web.Request) -> web.Response:
        """List all jobs with optional filtering."""
        try:
            if not self._is_leader() and request.query.get("local") != "1":
                proxied = await self._proxy_to_leader(request)
                if proxied.status not in (502, 503):
                    return proxied

            job_type = request.query.get("type")
            status = request.query.get("status")
            limit = int(request.query.get("limit", 100))

            # Collect all jobs (local + training + ssh tournament runs)
            all_jobs = []

            with self.jobs_lock:
                local_jobs_snapshot = list(self.local_jobs.values())
            for job in local_jobs_snapshot:
                jt = job.job_type.value if hasattr(job.job_type, "value") else str(job.job_type)
                if job_type and jt != job_type:
                    continue
                if status and job.status != status:
                    continue
                all_jobs.append(
                    {
                        "job_id": job.job_id,
                        "job_type": jt,
                        "status": job.status,
                        "assigned_to": job.node_id,
                        "created_at": job.started_at,
                        "board_type": job.board_type,
                        "num_players": job.num_players,
                        "category": "local",
                    }
                )

            with self.training_lock:
                for job_id, job in self.training_jobs.items():
                    if job_type and job.job_type != job_type:
                        continue
                    if status and job.status != status:
                        continue
                    all_jobs.append({
                        "job_id": job_id,
                        "job_type": job.job_type,
                        "status": job.status,
                        "assigned_to": job.worker_node,
                        "created_at": job.created_at,
                        "board_type": job.board_type,
                        "num_players": job.num_players,
                        "category": "training",
                    })

            with self.ssh_tournament_lock:
                ssh_runs_snapshot = list(self.ssh_tournament_runs.values())
            for run in ssh_runs_snapshot:
                if job_type and job_type != "ssh_tournament":
                    continue
                if status and run.status != status:
                    continue
                all_jobs.append(
                    {
                        "job_id": run.job_id,
                        "job_type": "ssh_tournament",
                        "status": run.status,
                        "assigned_to": self.node_id,
                        "created_at": run.started_at,
                        "board_type": run.board,
                        "num_players": 2,
                        "category": "ssh_tournament",
                    }
                )

            # Sort by created_at descending and limit
            all_jobs.sort(key=lambda x: x.get("created_at", 0), reverse=True)
            all_jobs = all_jobs[:limit]

            return web.json_response({
                "success": True,
                "jobs": all_jobs,
                "total": len(all_jobs),
            })
        except Exception as e:
            return web.json_response({"success": False, "error": str(e)}, status=500)

    async def handle_api_jobs_submit(self, request: web.Request) -> web.Response:
        """Submit a new job via REST API."""
        if not self._is_leader() and request.query.get("local") != "1":
            return await self._proxy_to_leader(request)
        if not self._is_leader():
            return web.json_response(
                {
                    "success": False,
                    "error": "Not the leader. Please submit to leader node.",
                    "leader_id": self.leader_id,
                },
                status=400,
            )

        try:
            data = await request.json()
            job_type = data.get("job_type")
            if not job_type:
                return web.json_response({
                    "success": False,
                    "error": "job_type is required",
                }, status=400)

            if job_type in ["nnue", "cmaes"]:
                board_type = data.get("board_type", "square8")
                num_players = int(data.get("num_players", 2))
                job_config = {
                    "job_type": job_type,
                    "board_type": board_type,
                    "num_players": num_players,
                    "config_key": f"{board_type}_{num_players}p",
                    "total_games": int(data.get("total_games", 0)),
                }
                job = await self._dispatch_training_job(job_config)
                if not job:
                    return web.json_response(
                        {"success": False, "error": "No suitable worker available"},
                        status=400,
                    )
                return web.json_response(
                    {
                        "success": True,
                        "job_id": job.job_id,
                        "job_type": job.job_type,
                        "status": job.status,
                        "message": f"Training job {job.job_id} created",
                    }
                )

            if job_type in ["selfplay", "gpu_selfplay", "hybrid_selfplay"]:
                board_type = data.get("board_type", "square8")
                num_players = int(data.get("num_players", 2))
                engine_mode = data.get("engine_mode", "heuristic-only")

                # Map job type string to enum
                if job_type == "gpu_selfplay":
                    jt = JobType.GPU_SELFPLAY
                elif job_type == "hybrid_selfplay":
                    jt = JobType.HYBRID_SELFPLAY
                else:
                    jt = JobType.SELFPLAY

                job = await self._start_local_job(jt, board_type, num_players, engine_mode)
                if not job:
                    return web.json_response(
                        {"success": False, "error": "Failed to start local job"},
                        status=500,
                    )
                return web.json_response(
                    {
                        "success": True,
                        "job_id": job.job_id,
                        "job_type": job.job_type.value,
                        "status": job.status,
                        "message": f"Job {job.job_id} started",
                    }
                )

            return web.json_response(
                {
                    "success": False,
                    "error": f"Unknown job type: {job_type}. Supported: nnue, cmaes, selfplay, gpu_selfplay, hybrid_selfplay",
                },
                status=400,
            )

        except Exception as e:
            return web.json_response({"success": False, "error": str(e)}, status=500)

    async def handle_api_job_get(self, request: web.Request) -> web.Response:
        """Get details for a specific job."""
        try:
            job_id = request.match_info.get("job_id")
            if not job_id:
                return web.json_response({
                    "success": False,
                    "error": "job_id is required",
                }, status=400)

            with self.jobs_lock:
                local_job = self.local_jobs.get(job_id)
            if local_job:
                return web.json_response(
                    {
                        "success": True,
                        "job": {
                            "job_id": job_id,
                            "job_type": local_job.job_type.value if hasattr(local_job.job_type, "value") else str(local_job.job_type),
                            "status": local_job.status,
                            "assigned_to": local_job.node_id,
                            "created_at": local_job.started_at,
                            "board_type": local_job.board_type,
                            "num_players": local_job.num_players,
                            "engine_mode": local_job.engine_mode,
                            "pid": local_job.pid,
                            "category": "local",
                        },
                    }
                )

            with self.ssh_tournament_lock:
                ssh_run = self.ssh_tournament_runs.get(job_id)
            if ssh_run:
                return web.json_response(
                    {
                        "success": True,
                        "job": {
                            "job_id": ssh_run.job_id,
                            "job_type": "ssh_tournament",
                            "status": ssh_run.status,
                            "assigned_to": self.node_id,
                            "created_at": ssh_run.started_at,
                            "run_id": ssh_run.run_id,
                            "tiers": ssh_run.tiers,
                            "board_type": ssh_run.board,
                            "games_per_matchup": ssh_run.games_per_matchup,
                            "output_root": ssh_run.output_root,
                            "manifest_path": ssh_run.manifest_path,
                            "checkpoint_path": ssh_run.checkpoint_path,
                            "report_path": ssh_run.report_path,
                            "log_path": ssh_run.log_path,
                            "category": "ssh_tournament",
                        },
                    }
                )

            # Check training jobs
            with self.training_lock:
                if job_id in self.training_jobs:
                    job = self.training_jobs[job_id]
                    return web.json_response({
                        "success": True,
                        "job": {
                            "job_id": job_id,
                            "job_type": job.job_type,
                            "status": job.status,
                            "board_type": job.board_type,
                            "num_players": job.num_players,
                            "assigned_worker": job.worker_node,
                            "created_at": job.created_at,
                            "started_at": job.started_at,
                            "completed_at": job.completed_at,
                            "output_model_path": job.output_model_path,
                            "error_message": job.error_message,
                            "category": "training",
                        },
                    })

            return web.json_response({
                "success": False,
                "error": f"Job {job_id} not found",
            }, status=404)

        except Exception as e:
            return web.json_response({"success": False, "error": str(e)}, status=500)

    async def handle_api_job_cancel(self, request: web.Request) -> web.Response:
        """Cancel a pending or running job."""
        if not self._is_leader() and request.query.get("local") != "1":
            return await self._proxy_to_leader(request)
        if not self._is_leader():
            return web.json_response(
                {
                    "success": False,
                    "error": "Not the leader",
                },
                status=400,
            )

        try:
            job_id = request.match_info.get("job_id")
            if not job_id:
                return web.json_response({
                    "success": False,
                    "error": "job_id is required",
                }, status=400)

            with self.jobs_lock:
                local_job = self.local_jobs.get(job_id)
            if local_job:
                with contextlib.suppress(Exception):
                    os.kill(local_job.pid, signal.SIGTERM)
                with self.jobs_lock:
                    local_job.status = "stopped"
                    self.local_jobs[job_id] = local_job
                self._save_state()
                return web.json_response({"success": True, "message": f"Job {job_id} stopped"})

            with self.ssh_tournament_lock:
                ssh_run = self.ssh_tournament_runs.get(job_id)
            if ssh_run:
                if ssh_run.pid:
                    with contextlib.suppress(Exception):
                        os.kill(ssh_run.pid, signal.SIGTERM)
                with self.ssh_tournament_lock:
                    ssh_run.status = "cancelled"
                    ssh_run.completed_at = time.time()
                    self.ssh_tournament_runs[job_id] = ssh_run
                return web.json_response({"success": True, "message": f"SSH tournament {job_id} cancelled"})

            # Check training jobs
            with self.training_lock:
                if job_id in self.training_jobs:
                    job = self.training_jobs[job_id]
                    if job.status in ["pending", "queued"]:
                        job.status = "cancelled"
                        return web.json_response({
                            "success": True,
                            "message": f"Training job {job_id} cancelled",
                        })
                    else:
                        return web.json_response({
                            "success": False,
                            "error": f"Cannot cancel job in status: {job.status}",
                        }, status=400)

            return web.json_response({
                "success": False,
                "error": f"Job {job_id} not found",
            }, status=404)

        except Exception as e:
            return web.json_response({"success": False, "error": str(e)}, status=500)

    async def handle_dashboard(self, request: web.Request) -> web.Response:
        """Serve the web dashboard HTML."""
        dashboard_path = Path(__file__).resolve().parent / "dashboard_assets" / "dashboard.html"
        try:
            html = dashboard_path.read_text(encoding="utf-8")
        except Exception as e:
            html = (
                "<!doctype html><html><body style='font-family:monospace'>"
                f"<h3>Dashboard asset unavailable</h3><pre>{e}</pre>"
                f"<pre>Expected: {dashboard_path}</pre>"
                "</body></html>"
            )
        headers = {
            # Avoid stale HTML across load balancers / browsers during rapid iteration.
            "Cache-Control": "no-store, max-age=0",
            "Pragma": "no-cache",
            "Expires": "0",
            # Simple diagnostics (no secrets).
            "X-RingRift-Node-Id": str(self.node_id or ""),
            "X-RingRift-Build-Version": str(getattr(self, "build_version", "") or ""),
        }
        return web.Response(text=html, content_type="text/html", headers=headers)

    async def handle_work_queue_dashboard(self, request: web.Request) -> web.Response:
        """Serve the work queue dashboard HTML."""
        dashboard_path = Path(__file__).resolve().parent / "dashboard_assets" / "work_queue_dashboard.html"
        try:
            html = dashboard_path.read_text(encoding="utf-8")
        except Exception as e:
            html = (
                "<!doctype html><html><body style='font-family:monospace'>"
                f"<h3>Work Queue Dashboard unavailable</h3><pre>{e}</pre>"
                f"<pre>Expected: {dashboard_path}</pre>"
                "</body></html>"
            )
        headers = {
            "Cache-Control": "no-store, max-age=0",
            "Pragma": "no-cache",
            "Expires": "0",
            "X-RingRift-Node-Id": str(self.node_id or ""),
        }
        return web.Response(text=html, content_type="text/html", headers=headers)

    async def _run_evaluation(self, job_id: str):
        """Evaluate new model against current best.

        Runs evaluation games between the candidate model and the best model.
        Reports win rate for the candidate.
        """
        import json as json_module
        import sys

        state = self.improvement_loop_state.get(job_id)
        if not state:
            return

        logger.info(f"Running evaluation for job {job_id}, iteration {state.current_iteration}")

        getattr(state, 'candidate_model_path', None)

        # Number of evaluation games
        eval_games = 100

        eval_script = f"""
import sys
sys.path.insert(0, '{self.ringrift_path}/ai-service')
from app.game_engine import GameEngine
from app.agents.heuristic_agent import HeuristicAgent
import json

# Run evaluation games
candidate_wins = 0
best_wins = 0
draws = 0

for game_idx in range({eval_games}):
    engine = GameEngine(board_type='{state.board_type}', num_players={state.num_players})

    # Alternate who plays first
    if game_idx % 2 == 0:
        agents = [
            HeuristicAgent(0),  # Candidate as player 0
            HeuristicAgent(1),  # Best as player 1
        ]
        candidate_player = 0
    else:
        agents = [
            HeuristicAgent(0),  # Best as player 0
            HeuristicAgent(1),  # Candidate as player 1
        ]
        candidate_player = 1

    # Play game
    max_moves = 10000
    move_count = 0
    while not engine.is_game_over() and move_count < max_moves:
        current_player = engine.current_player
        agent = agents[current_player]
        legal_moves = engine.get_legal_moves()
        if not legal_moves:
            break
        move = agent.select_move(engine.get_state(), legal_moves)
        engine.apply_move(move)
        move_count += 1

    outcome = engine.get_outcome()
    winner = outcome.get('winner')

    if winner == candidate_player:
        candidate_wins += 1
    elif winner is not None:
        best_wins += 1
    else:
        draws += 1

# Calculate win rate
total = candidate_wins + best_wins + draws
winrate = candidate_wins / total if total > 0 else 0.5

print(json.dumps({{
    'candidate_wins': candidate_wins,
    'best_wins': best_wins,
    'draws': draws,
    'winrate': winrate,
}}))
"""

        cmd = [sys.executable, "-c", eval_script]
        env = os.environ.copy()
        env["PYTHONPATH"] = os.path.join(self.ringrift_path, "ai-service")
        env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

        try:
            proc = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env,
            )

            stdout, stderr = await asyncio.wait_for(
                proc.communicate(),
                timeout=3600  # 1 hour max
            )

            if proc.returncode == 0:
                output_lines = stdout.decode().strip().split('\n')
                result_line = output_lines[-1] if output_lines else '{}'
                result = json_module.loads(result_line)

                state.evaluation_winrate = result.get('winrate', 0.5)
                logger.info(f"Evaluation result: winrate={state.evaluation_winrate:.2%}")
                logger.info("  Candidate")
            else:
                logger.info(f"Evaluation failed: {stderr.decode()[:500]}")
                state.evaluation_winrate = 0.5

        except asyncio.TimeoutError:
            logger.info("Evaluation timed out")
            state.evaluation_winrate = 0.5
        except Exception as e:
            logger.info(f"Evaluation error: {e}")
            state.evaluation_winrate = 0.5

    async def _promote_model_if_better(self, job_id: str):
        """Promote new model if it beats the current best.

        Promotion threshold: candidate must win >= 55% of evaluation games.
        """
        state = self.improvement_loop_state.get(job_id)
        if not state:
            return

        PROMOTION_THRESHOLD = 0.55  # 55% win rate required

        winrate = getattr(state, 'evaluation_winrate', 0.5)
        candidate_path = getattr(state, 'candidate_model_path', None)

        logger.info(f"Checking model promotion for job {job_id}")
        logger.info("  Current")
        logger.info("  Candidate")
        logger.info("  Threshold")

        if winrate >= PROMOTION_THRESHOLD and candidate_path:
            # Promote candidate to best
            state.best_model_path = candidate_path
            state.best_winrate = winrate

            # Save best model to well-known location
            best_model_dir = os.path.join(
                self.ringrift_path, "ai-service", "models", "best"
            )
            os.makedirs(best_model_dir, exist_ok=True)

            import shutil
            best_path = os.path.join(best_model_dir, f"{state.board_type}_{state.num_players}p.pt")
            if os.path.exists(candidate_path):
                shutil.copy2(candidate_path, best_path)
                logger.info(f"PROMOTED: New best model at {best_path}")
                logger.info(f"  Win rate: {winrate:.2%}")
            else:
                logger.info(f"Cannot promote: candidate model not found at {candidate_path}")
        else:
            logger.info(f"No promotion: candidate ({winrate:.2%}) below threshold ({PROMOTION_THRESHOLD:.0%})")

    # ============================================
    # Core Logic
    # ============================================

    def _update_self_info(self):
        """Update self info with current resource usage."""
        usage = self._get_resource_usage()
        selfplay, training = self._count_local_jobs()

        # NAT/relay detection: if we haven't received any inbound heartbeats for a
        # while (but we do know about other peers), assume we're not reachable
        # inbound and must poll a relay for commands.
        now = time.time()
        if self.known_peers or self.peers:
            last_inbound = self.last_inbound_heartbeat or self.start_time
            self.self_info.nat_blocked = (now - last_inbound) >= NAT_INBOUND_HEARTBEAT_STALE_SECONDS
        else:
            self.self_info.nat_blocked = False

        if not self.self_info.nat_blocked:
            self.self_info.relay_via = ""
        elif self.leader_id and self.leader_id != self.node_id:
            self.self_info.relay_via = self.leader_id

        self.self_info.cpu_percent = usage["cpu_percent"]
        self.self_info.memory_percent = usage["memory_percent"]
        self.self_info.disk_percent = usage["disk_percent"]
        self.self_info.gpu_percent = usage["gpu_percent"]
        self.self_info.gpu_memory_percent = usage["gpu_memory_percent"]
        self.self_info.selfplay_jobs = selfplay
        self.self_info.training_jobs = training
        self.self_info.role = self.role
        self.self_info.last_heartbeat = time.time()

        # Detect external work (running outside P2P orchestrator tracking)
        external = self._detect_local_external_work()
        self.self_info.cmaes_running = external.get('cmaes_running', False)
        self.self_info.gauntlet_running = external.get('gauntlet_running', False)
        self.self_info.tournament_running = external.get('tournament_running', False)
        self.self_info.data_merge_running = external.get('data_merge_running', False)

        # Phase 6: Health broadcasting - additional health metrics
        self.self_info.nfs_accessible = self._check_nfs_accessible()
        self.self_info.code_version = self.build_version
        self.self_info.errors_last_hour = getattr(self, '_error_count_last_hour', 0)
        self.self_info.disk_free_gb = usage.get("disk_free_gb", 0.0)
        self.self_info.active_job_count = (
            selfplay + training +
            (1 if self.self_info.cmaes_running else 0) +
            (1 if self.self_info.gauntlet_running else 0) +
            (1 if self.self_info.tournament_running else 0)
        )

        # Report to unified resource optimizer for cluster-wide coordination
        if HAS_NEW_COORDINATION:
            try:
                optimizer = get_resource_optimizer()
                node_resources = NodeResources(
                    node_id=self.node_id,
                    cpu_percent=usage["cpu_percent"],
                    gpu_percent=usage["gpu_percent"],
                    memory_percent=usage["memory_percent"],
                    disk_percent=usage["disk_percent"],
                    gpu_memory_percent=usage["gpu_memory_percent"],
                    cpu_count=int(getattr(self.self_info, "cpu_count", 0) or 0),
                    memory_gb=float(getattr(self.self_info, "memory_gb", 0) or 0),
                    has_gpu=bool(getattr(self.self_info, "has_gpu", False)),
                    gpu_name=str(getattr(self.self_info, "gpu_name", "") or ""),
                    active_jobs=selfplay + training,
                    selfplay_jobs=selfplay,
                    training_jobs=training,
                    orchestrator="p2p_orchestrator",
                )
                optimizer.report_node_resources(node_resources)
            except Exception:
                pass  # Don't fail heartbeat if optimizer unavailable

    async def _send_heartbeat_to_peer(self, peer_host: str, peer_port: int, scheme: str = "http", timeout: int = 10) -> NodeInfo | None:
        """Send heartbeat to a peer and return their info.

        Args:
            peer_host: Target peer hostname or IP
            peer_port: Target peer port
            scheme: HTTP or HTTPS scheme
            timeout: Request timeout in seconds (default 10, use smaller for voter heartbeats)
        """
        target = f"{peer_host}:{peer_port}"
        breaker = self._circuit_registry.get_breaker("p2p")

        # Check circuit breaker before attempting request
        if not breaker.can_execute(target):
            state = breaker.get_state(target)
            if state == CircuitState.OPEN:
                # Circuit is open - skip this peer temporarily
                return None

        try:
            self._update_self_info()
            payload = self.self_info.to_dict()
            voter_node_ids = list(getattr(self, "voter_node_ids", []) or [])
            if voter_node_ids:
                payload["voter_node_ids"] = voter_node_ids
                payload["voter_quorum_size"] = int(getattr(self, "voter_quorum_size", 0) or 0)
                payload["voter_config_source"] = str(getattr(self, "voter_config_source", "") or "")

            # Adjust timeout based on circuit state (shorter for half-open probing)
            effective_timeout = self._circuit_registry.get_timeout("p2p", target, float(timeout))
            client_timeout = ClientTimeout(total=effective_timeout)

            async with get_client_session(client_timeout) as session:
                scheme = (scheme or "http").lower()
                url = f"{scheme}://{peer_host}:{peer_port}/heartbeat"
                async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        incoming_voters = data.get("voter_node_ids") or data.get("voters") or None
                        if incoming_voters:
                            voters_list: list[str] = []
                            if isinstance(incoming_voters, list):
                                voters_list = [str(v).strip() for v in incoming_voters if str(v).strip()]
                            elif isinstance(incoming_voters, str):
                                voters_list = [t.strip() for t in incoming_voters.split(",") if t.strip()]
                            if voters_list:
                                self._maybe_adopt_voter_node_ids(voters_list, source="learned")
                        info = NodeInfo.from_dict(data)
                        if not info.reported_host:
                            info.reported_host = info.host
                        if not info.reported_port:
                            info.reported_port = info.port
                        # Use the address we successfully reached instead of any
                        # self-reported interface address.
                        info.scheme = scheme
                        info.host = peer_host
                        info.port = peer_port
                        # Record success with circuit breaker
                        breaker.record_success(target)

                        # Phase 27: Cache peer for persistence across restarts
                        self._save_peer_to_cache(
                            info.node_id,
                            peer_host,
                            peer_port,
                            str(getattr(info, "tailscale_ip", "") or "")
                        )
                        self._update_peer_reputation(info.node_id, success=True)

                        return info
                    else:
                        # Non-200 response is a failure
                        breaker.record_failure(target)
        except Exception:
            # Record failure with circuit breaker
            breaker.record_failure(target)
        return None

    async def _bootstrap_from_known_peers(self) -> bool:
        """Import cluster membership from seed peers via `/relay/peers`.

        Heartbeats intentionally return only a single peer's NodeInfo, which
        makes initial convergence slow when only one seed peer is configured
        (common for cloud nodes). `/relay/peers` returns a snapshot of the
        sender's full peer list, allowing new nodes to quickly learn about the
        leader and other cluster members.
        """
        # Seed peers are configured via `--peers`, but relying on a single
        # coordinator makes clusters brittle. Also bootstrap from any
        # previously-seen directly-reachable peers so nodes can re-join after
        # restarts even if the original seed goes offline.
        known_seed_peers: list[str] = [p for p in (self.known_peers or []) if p]
        discovered_seed_peers: list[str] = []

        with self.peers_lock:
            peers_snapshot = [p for p in self.peers.values() if p.node_id != self.node_id]
        peers_snapshot.sort(key=lambda p: str(getattr(p, "node_id", "") or ""))

        for peer in peers_snapshot:
            if getattr(peer, "nat_blocked", False):
                # NAT-blocked nodes cannot serve as inbound seeds.
                continue
            if not peer.should_retry():
                continue

            scheme = (getattr(peer, "scheme", "http") or "http").lower()
            host = str(getattr(peer, "host", "") or "").strip()
            try:
                port = int(getattr(peer, "port", DEFAULT_PORT) or DEFAULT_PORT)
            except Exception:
                port = DEFAULT_PORT
            if host:
                discovered_seed_peers.append(f"{scheme}://{host}:{port}")

            rh = str(getattr(peer, "reported_host", "") or "").strip()
            try:
                rp = int(getattr(peer, "reported_port", 0) or 0)
            except Exception:
                rp = 0
            if rh and rp:
                discovered_seed_peers.append(f"{scheme}://{rh}:{rp}")

        seen: set[str] = set()
        seed_peers: list[str] = []
        ki = 0
        di = 0
        while ki < len(known_seed_peers) or di < len(discovered_seed_peers):
            if ki < len(known_seed_peers):
                candidate = known_seed_peers[ki]
                ki += 1
                if candidate and candidate not in seen:
                    seen.add(candidate)
                    seed_peers.append(candidate)
            if di < len(discovered_seed_peers):
                candidate = discovered_seed_peers[di]
                di += 1
                if candidate and candidate not in seen:
                    seen.add(candidate)
                    seed_peers.append(candidate)
        if not seed_peers:
            return False

        now = time.time()
        if now - self.last_peer_bootstrap < PEER_BOOTSTRAP_INTERVAL:
            return False

        max_seeds = int(os.environ.get("RINGRIFT_P2P_BOOTSTRAP_MAX_SEEDS_PER_RUN", "8") or 8)
        max_seeds = max(1, min(max_seeds, 32))

        timeout = ClientTimeout(total=8)
        bootstrapped = False
        imported_any = False

        async with get_client_session(timeout) as session:
            for idx, peer_addr in enumerate(seed_peers):
                if idx >= max_seeds:
                    break
                try:
                    scheme, host, port = self._parse_peer_address(peer_addr)
                    scheme = (scheme or "http").lower()
                    url = f"{scheme}://{host}:{port}/relay/peers"
                    async with session.get(url, headers=self._auth_headers()) as resp:
                        if resp.status != 200:
                            continue
                        data = await resp.json()

                    if not isinstance(data, dict) or not data.get("success"):
                        continue

                    bootstrapped = True

                    incoming_voters = data.get("voter_node_ids") or data.get("voters") or None
                    if incoming_voters:
                        voters_list: list[str] = []
                        if isinstance(incoming_voters, list):
                            voters_list = [str(v).strip() for v in incoming_voters if str(v).strip()]
                        elif isinstance(incoming_voters, str):
                            voters_list = [t.strip() for t in incoming_voters.split(",") if t.strip()]
                        if voters_list:
                            self._maybe_adopt_voter_node_ids(voters_list, source="learned")

                    peers_data = data.get("peers") or {}
                    if not isinstance(peers_data, dict):
                        continue

                    with self.peers_lock:
                        before = len(self.peers)
                        for node_id, peer_dict in peers_data.items():
                            if not node_id or node_id == self.node_id:
                                continue
                            try:
                                info = NodeInfo.from_dict(peer_dict)
                            except Exception:
                                continue
                            existing = self.peers.get(info.node_id)
                            if existing:
                                # Preserve relay/NAT routing and retirement state when merging peer snapshots.
                                if getattr(existing, "nat_blocked", False) and not getattr(info, "nat_blocked", False):
                                    info.nat_blocked = True
                                    info.nat_blocked_since = float(getattr(existing, "nat_blocked_since", 0.0) or 0.0) or time.time()
                                    info.last_nat_probe = float(getattr(existing, "last_nat_probe", 0.0) or 0.0)
                                if (getattr(existing, "relay_via", "") or "") and not (getattr(info, "relay_via", "") or ""):
                                    info.relay_via = str(getattr(existing, "relay_via", "") or "")
                                if getattr(existing, "retired", False):
                                    info.retired = True
                                    info.retired_at = float(getattr(existing, "retired_at", 0.0) or 0.0)
                                # Preserve local reachability diagnostics.
                                info.consecutive_failures = int(getattr(existing, "consecutive_failures", 0) or 0)
                                info.last_failure_time = float(getattr(existing, "last_failure_time", 0.0) or 0.0)

                            self.peers[info.node_id] = info
                        after = len(self.peers)

                    new = max(0, after - before)
                    if new:
                        imported_any = True
                        logger.info(f"Bootstrap: imported {new} new peers from {host}:{port}")

                    leader_id = str(data.get("leader_id") or "").strip()
                    if leader_id and leader_id != self.node_id:
                        # If we're currently leader but the seed reports a higher-priority
                        # leader, step down to converge quickly.
                        if self.role == NodeRole.LEADER and leader_id > self.node_id:
                            logger.info(f"Bootstrap: stepping down for leader {leader_id}")
                            self.role = NodeRole.FOLLOWER
                        self.leader_id = leader_id
                except Exception:
                    continue

        self.last_peer_bootstrap = now
        if bootstrapped:
            self._maybe_adopt_leader_from_peers()
            self._save_state()
        return imported_any

    async def _continuous_bootstrap_loop(self) -> None:
        """Phase 26.3: Continuously attempt to join cluster when isolated.

        This loop runs on ALL nodes (not just leader) and ensures that
        isolated nodes can rejoin the cluster without manual intervention.

        Triggers when:
        - Less than MIN_CONNECTED_PEERS alive peers
        - No leader known

        Uses multi-seed bootstrap with:
        1. Cached peers (highest reputation first)
        2. CLI-provided peers
        3. Hardcoded BOOTSTRAP_SEEDS
        4. Tailscale network scan (fallback)
        """
        # Wait for initial startup before checking isolation
        await asyncio.sleep(60)

        while self.running:
            try:
                await asyncio.sleep(ISOLATED_BOOTSTRAP_INTERVAL)

                # Count alive peers
                with self.peers_lock:
                    peers_alive = sum(
                        1 for p in self.peers.values()
                        if p.node_id != self.node_id and p.is_alive()
                    )

                # Check if we're isolated (few peers or no leader)
                is_isolated = peers_alive < MIN_CONNECTED_PEERS
                no_leader = self.leader_id is None or (
                    self.leader_id != self.node_id and
                    self.leader_id not in self.peers
                )

                if is_isolated or no_leader:
                    if is_isolated:
                        logger.warning(
                            f"Isolated: only {peers_alive} alive peers "
                            f"(need {MIN_CONNECTED_PEERS}), attempting bootstrap..."
                        )
                    elif no_leader:
                        logger.warning(
                            f"No valid leader (current: {self.leader_id}), "
                            f"attempting bootstrap..."
                        )

                    # Try bootstrap from multiple sources
                    bootstrapped = await self._bootstrap_from_multiple_seeds()

                    if bootstrapped:
                        logger.info(
                            f"Bootstrap successful! "
                            f"Now have {len([p for p in self.peers.values() if p.is_alive()])} alive peers"
                        )
                        # Try to adopt leader from newly discovered peers
                        self._maybe_adopt_leader_from_peers()

                        # If still no leader, start election
                        if not self.leader_id:
                            await self._start_election()
                    else:
                        # Fallback: try Tailscale peer discovery
                        logger.info("Bootstrap from seeds failed, trying Tailscale discovery...")
                        await self._discover_tailscale_peers()

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Error in continuous bootstrap loop: {e}")
                await asyncio.sleep(30)  # Back off on errors

    async def _bootstrap_from_multiple_seeds(self) -> bool:
        """Phase 26.3: Try multiple seeds until we join the cluster.

        Priority order:
        1. Cached peers with high reputation (from peer_cache table)
        2. CLI --peers (self.known_peers)
        3. Hardcoded BOOTSTRAP_SEEDS

        Returns True if we successfully connected to any peer.
        """
        # Build seed list with priority ordering
        all_seeds: list[str] = []
        seen: set[str] = set()

        # 1. First, try cached peers by reputation (if available)
        cached_peers = self._get_bootstrap_peers_by_reputation(limit=3)
        for seed in cached_peers:
            if seed and seed not in seen:
                seen.add(seed)
                all_seeds.append(seed)

        # 2. Then, CLI peers and hardcoded seeds (already merged in self.known_peers)
        for seed in self.known_peers:
            if seed and seed not in seen:
                seen.add(seed)
                all_seeds.append(seed)

        if not all_seeds:
            logger.warning("No bootstrap seeds available")
            return False

        # Limit attempts per cycle
        max_attempts = min(MIN_BOOTSTRAP_ATTEMPTS * 2, len(all_seeds))
        timeout = ClientTimeout(total=10)
        success = False

        async with get_client_session(timeout) as session:
            for idx, seed_addr in enumerate(all_seeds[:max_attempts]):
                try:
                    scheme, host, port = self._parse_peer_address(seed_addr)
                    scheme = (scheme or "http").lower()
                    url = f"{scheme}://{host}:{port}/relay/peers"

                    async with session.get(url, headers=self._auth_headers()) as resp:
                        if resp.status != 200:
                            self._update_peer_reputation(seed_addr, success=False)
                            continue

                        data = await resp.json()
                        if not isinstance(data, dict) or not data.get("success"):
                            self._update_peer_reputation(seed_addr, success=False)
                            continue

                    # Successfully got peer list
                    self._update_peer_reputation(seed_addr, success=True)
                    success = True

                    # Import peers
                    peers_data = data.get("peers") or {}
                    if isinstance(peers_data, dict):
                        with self.peers_lock:
                            for node_id, peer_dict in peers_data.items():
                                if node_id and node_id != self.node_id:
                                    try:
                                        info = NodeInfo.from_dict(peer_dict)
                                        self.peers[info.node_id] = info
                                        # Cache the peer for future restarts
                                        self._save_peer_to_cache(
                                            info.node_id,
                                            str(getattr(info, "host", "") or ""),
                                            int(getattr(info, "port", DEFAULT_PORT) or DEFAULT_PORT),
                                            str(getattr(info, "tailscale_ip", "") or "")
                                        )
                                    except Exception:
                                        continue

                    # Adopt leader if provided
                    leader_id = str(data.get("leader_id") or "").strip()
                    if leader_id and leader_id != self.node_id:
                        self.leader_id = leader_id
                        if self.role == NodeRole.LEADER:
                            logger.info(f"Stepping down for discovered leader: {leader_id}")
                            self.role = NodeRole.FOLLOWER

                    # Handle cluster epoch (Phase 29)
                    incoming_epoch = data.get("cluster_epoch")
                    if incoming_epoch is not None:
                        try:
                            epoch = int(incoming_epoch)
                            if epoch > self._cluster_epoch:
                                logger.info(f"Adopting higher cluster epoch: {epoch} (was {self._cluster_epoch})")
                                self._cluster_epoch = epoch
                                self._save_cluster_epoch()
                        except (ValueError, TypeError):
                            pass

                    # Import voter config if provided
                    incoming_voters = data.get("voter_node_ids") or data.get("voters")
                    if incoming_voters:
                        voters_list = []
                        if isinstance(incoming_voters, list):
                            voters_list = [str(v).strip() for v in incoming_voters if str(v).strip()]
                        elif isinstance(incoming_voters, str):
                            voters_list = [t.strip() for t in incoming_voters.split(",") if t.strip()]
                        if voters_list:
                            self._maybe_adopt_voter_node_ids(voters_list, source="learned")

                    self._save_state()
                    logger.info(f"Bootstrap from {host}:{port}: imported {len(peers_data)} peers")
                    break  # Success, no need to try more seeds

                except asyncio.TimeoutError:
                    self._update_peer_reputation(seed_addr, success=False)
                    continue
                except Exception as e:
                    self._update_peer_reputation(seed_addr, success=False)
                    if self.verbose:
                        logger.debug(f"Bootstrap seed {seed_addr} failed: {e}")
                    continue

        return success

    async def _follower_discovery_loop(self) -> None:
        """Phase 30.2: Background discovery for non-leader nodes.

        This loop ensures followers actively discover peers, not just the leader.
        It probes gossip-learned endpoints and cached peers that we haven't
        connected to yet.
        """
        # Wait for initial startup
        await asyncio.sleep(90)

        while self.running:
            try:
                await asyncio.sleep(300)  # Every 5 minutes

                # Skip if we're the leader (leader has its own discovery loops)
                if self.role == NodeRole.LEADER:
                    continue

                # Check if we have enough peers
                with self.peers_lock:
                    alive_count = sum(1 for p in self.peers.values() if p.is_alive())

                if alive_count >= MIN_CONNECTED_PEERS * 2:
                    # We have plenty of peers, less aggressive discovery
                    continue

                # 1. Probe gossip-learned endpoints we haven't connected to
                endpoints_to_try = []
                now = time.time()
                for node_id, endpoint in list(self._gossip_learned_endpoints.items()):
                    if node_id == self.node_id:
                        continue
                    if node_id in self.peers and self.peers[node_id].is_alive():
                        continue

                    # Only try endpoints learned in the last hour
                    if now - endpoint.get("learned_at", 0) > 3600:
                        continue

                    endpoints_to_try.append((node_id, endpoint))

                # Limit attempts per cycle
                for node_id, endpoint in endpoints_to_try[:5]:
                    host = endpoint.get("host")
                    port = endpoint.get("port", DEFAULT_PORT)
                    if host and port:
                        try:
                            logger.debug(f"Follower discovery: trying {node_id} at {host}:{port}")
                            info = await self._send_heartbeat_to_peer(host, port)
                            if info:
                                with self.peers_lock:
                                    self.peers[info.node_id] = info
                                logger.info(f"Follower discovery: connected to {info.node_id}")
                        except Exception:
                            pass

                # 2. Bootstrap from any reachable cached peer
                await self._bootstrap_from_known_peers()

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Error in follower discovery loop: {e}")
                await asyncio.sleep(60)

    async def _send_relay_heartbeat(self, relay_url: str) -> dict[str, Any]:
        """Send heartbeat via relay endpoint for NAT-blocked nodes.

        This is used when the peer URL is HTTPS (indicating a relay/proxy endpoint)
        or when direct heartbeats fail consistently.

        Returns dict with:
        - success: bool
        - peers: dict of all cluster peers
        - leader_id: current leader
        """
        try:
            self._update_self_info()

            timeout = ClientTimeout(total=15)
            async with get_client_session(timeout) as session:
                # Use /relay/heartbeat endpoint
                url = f"{relay_url.rstrip('/')}/relay/heartbeat"
                payload = self.self_info.to_dict()
                if self.pending_relay_acks:
                    payload["relay_ack"] = sorted(self.pending_relay_acks)
                if self.pending_relay_results:
                    payload["relay_results"] = list(self.pending_relay_results)
                async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                    if resp.status != 200:
                        return {"success": False, "error": f"HTTP {resp.status}"}

                    data = await resp.json()
                    if not data.get("success"):
                        return {"success": False, "error": data.get("error", "Unknown error")}

                    incoming_voters = data.get("voter_node_ids") or data.get("voters") or None
                    if incoming_voters:
                        voters_list: list[str] = []
                        if isinstance(incoming_voters, list):
                            voters_list = [str(v).strip() for v in incoming_voters if str(v).strip()]
                        elif isinstance(incoming_voters, str):
                            voters_list = [t.strip() for t in incoming_voters.split(",") if t.strip()]
                        if voters_list:
                            self._maybe_adopt_voter_node_ids(voters_list, source="learned")

                    # Clear pending acks/results only after a successful round-trip.
                    self.pending_relay_acks.clear()
                    self.pending_relay_results.clear()

                    # Update our peer list with all peers from relay
                    peers_data = data.get("peers", {})
                    with self.peers_lock:
                        for node_id, peer_dict in peers_data.items():
                            if node_id != self.node_id:
                                peer_info = NodeInfo.from_dict(peer_dict)
                                existing = self.peers.get(node_id)
                                if existing:
                                    if getattr(existing, "nat_blocked", False) and not getattr(peer_info, "nat_blocked", False):
                                        peer_info.nat_blocked = True
                                        peer_info.nat_blocked_since = float(getattr(existing, "nat_blocked_since", 0.0) or 0.0) or time.time()
                                        peer_info.last_nat_probe = float(getattr(existing, "last_nat_probe", 0.0) or 0.0)
                                    if (getattr(existing, "relay_via", "") or "") and not (getattr(peer_info, "relay_via", "") or ""):
                                        peer_info.relay_via = str(getattr(existing, "relay_via", "") or "")
                                    if getattr(existing, "retired", False):
                                        peer_info.retired = True
                                        peer_info.retired_at = float(getattr(existing, "retired_at", 0.0) or 0.0)
                                    peer_info.consecutive_failures = int(getattr(existing, "consecutive_failures", 0) or 0)
                                    peer_info.last_failure_time = float(getattr(existing, "last_failure_time", 0.0) or 0.0)
                                self.peers[node_id] = peer_info

                    # Execute any queued commands addressed to us.
                    commands = data.get("commands") or []
                    if isinstance(commands, list) and commands:
                        await self._execute_relay_commands(commands)

                    # Update leader if provided
                    leader_id = data.get("leader_id")
                    if leader_id and leader_id != self.node_id:
                        if self.leader_id != leader_id:
                            logger.info(f"Adopted leader from relay: {leader_id}")
                        self.leader_id = leader_id
                        self.role = NodeRole.FOLLOWER

                    return {
                        "success": True,
                        "peers_received": len(peers_data) if isinstance(peers_data, dict) else 0,
                        "leader_id": leader_id,
                        "commands_received": len(commands) if isinstance(commands, list) else 0,
                    }
        except Exception as e:
            return {"success": False, "error": str(e)}

    async def _execute_relay_commands(self, commands: list[dict[str, Any]]) -> None:
        """Execute relay commands (polling mode for NAT-blocked nodes)."""
        now = time.time()
        for cmd in commands:
            try:
                cmd_id = str(cmd.get("id") or "")
                cmd_type = str(cmd.get("type") or "")
                payload = cmd.get("payload") or {}
                if not cmd_id or not cmd_type:
                    continue

                # Check for stale commands (>5 min old indicates relay/polling issues)
                cmd_ts = cmd.get("ts") or cmd.get("timestamp") or now
                cmd_age_secs = now - float(cmd_ts)
                if cmd_age_secs > 300:
                    logger.info(f"WARNING: Relay command {cmd_id} ({cmd_type}) is {cmd_age_secs:.0f}s old - relay delivery may be delayed")

                attempts = int(self.relay_command_attempts.get(cmd_id, 0) or 0) + 1
                self.relay_command_attempts[cmd_id] = attempts

                ok = False
                err = ""
                if cmd_type == "start_job":
                    job_type = JobType(str(payload.get("job_type") or "selfplay"))
                    board_type = str(payload.get("board_type") or "square8")
                    num_players = int(payload.get("num_players") or 2)
                    engine_mode = str(payload.get("engine_mode") or "mixed")
                    job_id = str(payload.get("job_id") or "")

                    if job_id:
                        with self.jobs_lock:
                            existing = self.local_jobs.get(job_id)
                        if existing and existing.status == "running":
                            ok = True
                        else:
                            job = await self._start_local_job(
                                job_type,
                                board_type=board_type,
                                num_players=num_players,
                                engine_mode=engine_mode,
                                job_id=job_id,
                            )
                            ok = job is not None
                    else:
                        job = await self._start_local_job(
                            job_type,
                            board_type=board_type,
                            num_players=num_players,
                            engine_mode=engine_mode,
                        )
                        ok = job is not None
                elif cmd_type == "cleanup":
                    asyncio.create_task(self._cleanup_local_disk())
                    ok = True
                elif cmd_type == "restart_stuck_jobs":
                    asyncio.create_task(self._restart_local_stuck_jobs())
                    ok = True
                elif cmd_type == "reduce_selfplay":
                    target = payload.get("target_selfplay_jobs", payload.get("target", 0))
                    reason = str(payload.get("reason") or "relay")
                    try:
                        target_jobs = int(target)
                    except Exception:
                        target_jobs = 0
                    await self._reduce_local_selfplay_jobs(target_jobs, reason=reason)
                    ok = True
                elif cmd_type == "cleanup_files":
                    files = payload.get("files", []) or []
                    reason = str(payload.get("reason") or "relay")
                    if not isinstance(files, list) or not files:
                        ok = False
                        err = "no_files"
                    else:
                        data_dir = self.get_data_directory()
                        freed_bytes = 0
                        deleted_count = 0
                        data_root = data_dir.resolve()
                        for file_path in files:
                            full_path = data_dir / (str(file_path or "").lstrip("/"))
                            try:
                                resolved = full_path.resolve()
                                resolved.relative_to(data_root)
                            except Exception:
                                continue
                            if not resolved.exists():
                                continue
                            try:
                                size = resolved.stat().st_size
                                resolved.unlink()
                                freed_bytes += size
                                deleted_count += 1
                            except Exception:
                                continue
                        print(
                            f"[P2P] Relay cleanup_files: {deleted_count} files deleted, "
                            f"{freed_bytes / 1e6:.1f}MB freed (reason={reason})"
                        )
                        ok = True
                elif cmd_type == "canonical_selfplay":
                    job_id = str(payload.get("job_id") or "")
                    board_type = str(payload.get("board_type") or "square8")
                    num_players = int(payload.get("num_players") or 2)
                    num_games = int(payload.get("num_games") or payload.get("games_per_node") or 500)
                    seed = int(payload.get("seed") or 0)
                    if not job_id:
                        ok = False
                        err = "missing_job_id"
                    else:
                        asyncio.create_task(
                            self._run_local_canonical_selfplay(job_id, board_type, num_players, num_games, seed)
                        )
                        ok = True
                else:
                    ok = False
                    err = f"unknown_command_type:{cmd_type}"

                if ok:
                    self.pending_relay_acks.add(cmd_id)
                    self.pending_relay_results.append({"id": cmd_id, "ok": True})
                    self.relay_command_attempts.pop(cmd_id, None)
                else:
                    if not err:
                        err = "command_failed"
                    if attempts >= RELAY_COMMAND_MAX_ATTEMPTS:
                        self.pending_relay_acks.add(cmd_id)
                        self.pending_relay_results.append({"id": cmd_id, "ok": False, "error": err})
                        self.relay_command_attempts.pop(cmd_id, None)
            except Exception as exc:
                try:
                    cmd_id = str(cmd.get("id") or "")
                    if cmd_id:
                        attempts = int(self.relay_command_attempts.get(cmd_id, 0) or 0)
                        if attempts >= RELAY_COMMAND_MAX_ATTEMPTS:
                            self.pending_relay_acks.add(cmd_id)
                            self.pending_relay_results.append({"id": cmd_id, "ok": False, "error": str(exc)})
                            self.relay_command_attempts.pop(cmd_id, None)
                except Exception:
                    continue

    async def _heartbeat_loop(self):
        """Send heartbeats to all known peers."""
        while self.running:
            try:
                # Send to known peers from config
                for peer_addr in self.known_peers:
                    try:
                        scheme, host, port = self._parse_peer_address(peer_addr)
                    except Exception:
                        continue

                    # Use relay heartbeat for HTTPS endpoints (they're proxies/relays)
                    # or for explicitly configured relay peers (--relay-peers flag)
                    use_relay = scheme == "https" or peer_addr in self.relay_peers
                    if use_relay:
                        # Relay/proxy endpoint, use relay heartbeat
                        relay_url = f"{scheme}://{host}" if port in (80, 443) else f"{scheme}://{host}:{port}"
                        result = await self._send_relay_heartbeat(relay_url)
                        if result.get("success"):
                            # Relay heartbeat already updates peers and leader
                            continue

                    info = await self._send_heartbeat_to_peer(host, port, scheme=scheme)
                    if info:
                        if info.node_id == self.node_id:
                            continue
                        async with AsyncLockWrapper(self.peers_lock):
                            info.last_heartbeat = time.time()
                            self.peers[info.node_id] = info
                        if info.role == NodeRole.LEADER and info.node_id != self.node_id:
                            async with AsyncLockWrapper(self.peers_lock):
                                peers_snapshot = list(self.peers.values())
                            conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])
                            if not self._is_leader_eligible(info, conflict_keys, require_alive=False):
                                continue
                            if self.role == NodeRole.LEADER and info.node_id <= self.node_id:
                                continue
                            if (
                                self.leader_id
                                and self.leader_id != info.node_id
                                and self._is_leader_lease_valid()
                                and info.node_id <= self.leader_id
                            ):
                                continue
                            if self.leader_id != info.node_id or self.role != NodeRole.FOLLOWER:
                                logger.info(f"Following configured leader from heartbeat: {info.node_id}")
                            prev_leader = self.leader_id
                            self.leader_id = info.node_id
                            # Provisional lease: allow time for the leader to send
                            # a /coordinator lease renewal after we discover it via
                            # heartbeat (prevents leaderless oscillation right after
                            # restarts/partitions).
                            if prev_leader != info.node_id or not self._is_leader_lease_valid():
                                self.leader_lease_id = ""
                                self.leader_lease_expires = time.time() + LEADER_LEASE_DURATION
                            self.role = NodeRole.FOLLOWER

                # Send to discovered peers (skip NAT-blocked peers and ambiguous endpoints).
                async with AsyncLockWrapper(self.peers_lock):
                    peers_snapshot = list(self.peers.values())
                conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])
                peer_list = [
                    p for p in peers_snapshot
                    if (
                        not p.nat_blocked
                        and self._endpoint_key(p) not in conflict_keys
                    )
                ]

                for peer in peer_list:
                    if peer.node_id != self.node_id:
                        if not peer.should_retry():
                            continue
                        peer_scheme = getattr(peer, "scheme", "http") or "http"
                        info = await self._send_heartbeat_to_peer(peer.host, peer.port, scheme=peer_scheme)
                        if not info and getattr(peer, "reported_host", "") and getattr(peer, "reported_port", 0):
                            # Multi-path retry: fall back to self-reported endpoint when the
                            # observed reachable endpoint fails (e.g., mixed overlays).
                            try:
                                rh = str(getattr(peer, "reported_host", "") or "").strip()
                                rp = int(getattr(peer, "reported_port", 0) or 0)
                            except Exception:
                                rh, rp = "", 0
                            if rh and rp and (rh != peer.host or rp != peer.port):
                                info = await self._send_heartbeat_to_peer(rh, rp, scheme=peer_scheme)
                        # Self-healing: Tailscale IP fallback when both primary and reported fail
                        if not info:
                            ts_ip = self._get_tailscale_ip_for_peer(peer.node_id)
                            if ts_ip and ts_ip != peer.host:
                                # Try Tailscale mesh IP (100.x.x.x)
                                info = await self._send_heartbeat_to_peer(ts_ip, peer.port, scheme=peer_scheme)
                                if info:
                                    logger.info(f"Reached {peer.node_id} via Tailscale ({ts_ip})")
                        if info:
                            info.consecutive_failures = 0
                            info.last_failure_time = 0.0
                            async with AsyncLockWrapper(self.peers_lock):
                                info.last_heartbeat = time.time()
                                self.peers[info.node_id] = info
                            if info.role == NodeRole.LEADER and self.role != NodeRole.LEADER:
                                if not self._is_leader_eligible(info, conflict_keys, require_alive=False):
                                    continue
                                if (
                                    self.leader_id
                                    and self.leader_id != info.node_id
                                    and self._is_leader_lease_valid()
                                    and info.node_id <= self.leader_id
                                ):
                                    continue
                                if self.leader_id != info.node_id:
                                    logger.info(f"Adopted leader from heartbeat: {info.node_id}")
                                prev_leader = self.leader_id
                                self.leader_id = info.node_id
                                if prev_leader != info.node_id or not self._is_leader_lease_valid():
                                    self.leader_lease_id = ""
                                    self.leader_lease_expires = time.time() + LEADER_LEASE_DURATION
                                self.role = NodeRole.FOLLOWER
                        else:
                            async with AsyncLockWrapper(self.peers_lock):
                                existing = self.peers.get(peer.node_id)
                                if existing:
                                    existing.consecutive_failures = int(getattr(existing, "consecutive_failures", 0) or 0) + 1
                                    existing.last_failure_time = time.time()

                # If we're only connected to a seed peer (or lost cluster membership),
                # pull a fresh peer snapshot so leader election converges quickly.
                await self._bootstrap_from_known_peers()

                # NAT-blocked nodes: poll a relay endpoint for peer snapshots + commands.
                if getattr(self.self_info, "nat_blocked", False):
                    now = time.time()
                    if now - self.last_relay_heartbeat >= RELAY_HEARTBEAT_INTERVAL:
                        relay_urls: list[str] = []
                        leader_peer = self._get_leader_peer()
                        if leader_peer and leader_peer.node_id != self.node_id:
                            relay_urls.append(f"{leader_peer.scheme}://{leader_peer.host}:{leader_peer.port}")
                        for peer_addr in self.known_peers:
                            try:
                                scheme, host, port = self._parse_peer_address(peer_addr)
                            except Exception:
                                continue
                            relay_urls.append(f"{scheme}://{host}:{port}")
                        seen: set[str] = set()
                        relay_urls = [u for u in relay_urls if not (u in seen or seen.add(u))]

                        for relay_url in relay_urls:
                            result = await self._send_relay_heartbeat(relay_url)
                            if result.get("success"):
                                self.last_relay_heartbeat = now
                                break

                # Check for dead peers
                await self._check_dead_peers_async()

                # Self-healing: detect network partition and trigger Tailscale-priority mode
                if self._detect_network_partition():
                    self._enable_tailscale_priority()
                    # Also enable partition-local election if no voters reachable
                    if not self._has_voter_quorum():
                        self._enable_partition_local_election()
                    # Force refresh all IP sources to discover alternative paths
                    last_refresh = getattr(self, "_last_partition_ip_refresh", 0)
                    if time.time() - last_refresh > 60:  # Refresh at most once per minute
                        self._last_partition_ip_refresh = time.time()
                        asyncio.create_task(self._force_ip_refresh_all_sources())
                elif getattr(self, "_tailscale_priority", False):
                    # Check if priority mode should expire
                    if time.time() > getattr(self, "_tailscale_priority_until", 0):
                        # Check if connectivity recovered
                        alive_count = sum(1 for p in self.peers.values() if p.is_alive() and p.node_id != self.node_id)
                        if alive_count > 0:
                            self._disable_tailscale_priority()

                # Self-healing: check if partition healed and restore original voters
                if hasattr(self, "_original_voters"):
                    self._restore_original_voters()

                # Dynamic voter management: promote/demote voters based on health
                # Only the leader manages voters to ensure consistency
                if self.role == NodeRole.LEADER:
                    self._manage_dynamic_voters()

                # Health-based leadership: step down if we can't reach enough peers
                if self.role == NodeRole.LEADER and not self._check_leader_health():
                    logger.info("Stepping down due to degraded health")
                    self.role = NodeRole.FOLLOWER
                    self.leader_id = None
                    self.leader_lease_id = ""
                    self.leader_lease_expires = 0.0
                    self._release_voter_grant_if_self()
                    self._save_state()
                    continue  # Skip leader duties this cycle

                # LEARNED LESSONS - Lease renewal to maintain leadership
                if self.role == NodeRole.LEADER:
                    await self._renew_leader_lease()

                # P2P monitoring: start/stop services based on leadership
                await self._stop_monitoring_if_not_leader()
                if self.role == NodeRole.LEADER:
                    await self._start_monitoring_if_leader()

                # P2P auto-deployer: start/stop based on leadership
                if self.role != NodeRole.LEADER and self._auto_deployer_task:
                    await self._stop_p2p_auto_deployer()
                elif self.role == NodeRole.LEADER and not self._auto_deployer_task:
                    await self._start_p2p_auto_deployer()

                # Report node resources to resource_optimizer for cluster-wide utilization tracking
                # This enables cooperative 60-80% utilization targeting across orchestrators
                if HAS_NEW_COORDINATION and get_resource_optimizer is not None:
                    try:
                        optimizer = get_resource_optimizer()
                        self._update_self_info()
                        node_resources = NodeResources(
                            node_id=self.node_id,
                            cpu_percent=self.self_info.cpu_percent,
                            memory_percent=self.self_info.memory_percent,
                            active_jobs=self.self_info.selfplay_jobs + self.self_info.training_jobs,
                            has_gpu=self.self_info.has_gpu,
                            gpu_name=self.self_info.gpu_type or "",
                        )
                        optimizer.report_node_resources(node_resources)
                    except Exception:
                        pass  # Non-critical, don't disrupt heartbeat

                # Save state periodically
                self._save_state()

            except Exception as e:
                logger.info(f"Heartbeat error: {e}")

            # Notify systemd watchdog that we're still alive
            systemd_notify_watchdog()

            await asyncio.sleep(HEARTBEAT_INTERVAL)

    async def _voter_heartbeat_loop(self):
        """
        Dedicated high-frequency heartbeat loop for voter nodes.

        IMPROVEMENTS:
        - Faster heartbeat interval (10s vs 30s) for voter nodes
        - Aggressively clears NAT-blocked status on successful heartbeats
        - Maintains full mesh connectivity between all voters
        - Propagates voter list to ensure consistent quorum
        """
        # Only run if this node is a voter
        if self.node_id not in self.voter_node_ids:
            return

        logger.info(f"Starting voter heartbeat loop (interval={VOTER_HEARTBEAT_INTERVAL}s)")
        last_voter_mesh_refresh = 0.0

        while self.running:
            try:
                now = time.time()

                # Get all other voters
                other_voters = [v for v in self.voter_node_ids if v != self.node_id]

                for voter_id in other_voters:
                    # Find voter peer info
                    async with AsyncLockWrapper(self.peers_lock):
                        voter_peer = self.peers.get(voter_id)

                    if not voter_peer:
                        # Try to discover voter from known peers
                        await self._discover_voter_peer(voter_id)
                        continue

                    # Attempt heartbeat to voter
                    success = await self._send_voter_heartbeat(voter_peer)

                    if success:
                        # AGGRESSIVE NAT RECOVERY: Clear NAT-blocked immediately on success
                        if VOTER_NAT_RECOVERY_AGGRESSIVE and voter_peer.nat_blocked:
                            logger.info(f"Voter {voter_id} NAT-blocked status cleared (heartbeat succeeded)")
                            async with AsyncLockWrapper(self.peers_lock):
                                if voter_id in self.peers:
                                    self.peers[voter_id].nat_blocked = False
                                    self.peers[voter_id].nat_blocked_since = 0.0
                                    self.peers[voter_id].consecutive_failures = 0
                    else:
                        # Try alternative endpoints
                        success = await self._try_voter_alternative_endpoints(voter_peer)

                        if not success:
                            # Increment failure count but don't mark NAT-blocked yet
                            with self.peers_lock:
                                if voter_id in self.peers:
                                    self.peers[voter_id].consecutive_failures = \
                                        int(getattr(self.peers[voter_id], "consecutive_failures", 0) or 0) + 1

                # Periodic voter mesh refresh - ensure all voters know about each other
                if now - last_voter_mesh_refresh > VOTER_MESH_REFRESH_INTERVAL:
                    last_voter_mesh_refresh = now
                    await self._refresh_voter_mesh()

            except Exception as e:
                logger.info(f"Voter heartbeat error: {e}")

            await asyncio.sleep(VOTER_HEARTBEAT_INTERVAL)

    async def _send_voter_heartbeat(self, voter_peer) -> bool:
        """Send a heartbeat to a voter peer with shorter timeout."""
        try:
            peer_scheme = getattr(voter_peer, "scheme", "http") or "http"

            # Use Tailscale IP if available (more reliable for cross-provider)
            target_host = voter_peer.host
            ts_ip = self._get_tailscale_ip_for_peer(voter_peer.node_id)
            if ts_ip:
                target_host = ts_ip

            info = await self._send_heartbeat_to_peer(
                target_host,
                voter_peer.port,
                scheme=peer_scheme,
                timeout=VOTER_HEARTBEAT_TIMEOUT
            )

            if info:
                with self.peers_lock:
                    info.last_heartbeat = time.time()
                    info.consecutive_failures = 0
                    self.peers[info.node_id] = info

                # Update leader if this voter claims leadership
                if info.role == NodeRole.LEADER and info.node_id != self.node_id and self.leader_id != info.node_id:
                    logger.info(f"Discovered leader from voter heartbeat: {info.node_id}")
                    self.leader_id = info.node_id
                    self.role = NodeRole.FOLLOWER
                    self.leader_lease_expires = time.time() + LEADER_LEASE_DURATION

                return True
        except Exception:
            pass
        return False

    async def _try_voter_alternative_endpoints(self, voter_peer) -> bool:
        """Try alternative endpoints for a voter peer."""
        peer_scheme = getattr(voter_peer, "scheme", "http") or "http"

        # Try 1: Tailscale IP
        ts_ip = self._get_tailscale_ip_for_peer(voter_peer.node_id)
        if ts_ip and ts_ip != voter_peer.host:
            info = await self._send_heartbeat_to_peer(ts_ip, voter_peer.port, scheme=peer_scheme, timeout=VOTER_HEARTBEAT_TIMEOUT)
            if info:
                logger.info(f"Reached voter {voter_peer.node_id} via Tailscale ({ts_ip})")
                with self.peers_lock:
                    info.last_heartbeat = time.time()
                    info.consecutive_failures = 0
                    self.peers[info.node_id] = info
                return True

        # Try 2: Reported host/port
        rh = str(getattr(voter_peer, "reported_host", "") or "").strip()
        rp = int(getattr(voter_peer, "reported_port", 0) or 0)
        if rh and rp and (rh != voter_peer.host or rp != voter_peer.port):
            info = await self._send_heartbeat_to_peer(rh, rp, scheme=peer_scheme, timeout=VOTER_HEARTBEAT_TIMEOUT)
            if info:
                logger.info(f"Reached voter {voter_peer.node_id} via reported endpoint ({rh}:{rp})")
                with self.peers_lock:
                    info.last_heartbeat = time.time()
                    info.consecutive_failures = 0
                    self.peers[info.node_id] = info
                return True

        return False

    async def _discover_voter_peer(self, voter_id: str):
        """Discover a voter peer from known peers."""
        # Ask known peers for the voter's endpoint
        for peer_addr in self.known_peers:
            try:
                scheme, host, port = self._parse_peer_address(peer_addr)
                async with aiohttp.ClientSession() as session, session.get(
                    f"{scheme}://{host}:{port}/relay/peers",
                    timeout=aiohttp.ClientTimeout(total=5),
                    headers=self._auth_headers()
                ) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        peers_data = data.get("peers", {})
                        if voter_id in peers_data:
                            peer_info = NodeInfo.from_dict(peers_data[voter_id])
                            with self.peers_lock:
                                self.peers[voter_id] = peer_info
                            logger.info(f"Discovered voter {voter_id} from {host}")
                            return
            except Exception:
                continue

    async def _refresh_voter_mesh(self):
        """Ensure all voters have knowledge of each other."""
        if not self.voter_node_ids:
            return

        # Check how many voters we know about
        with self.peers_lock:
            known_voters = [v for v in self.voter_node_ids if v in self.peers or v == self.node_id]

        if len(known_voters) < len(self.voter_node_ids):
            missing_voters = [v for v in self.voter_node_ids if v not in known_voters]
            logger.info(f"Voter mesh incomplete, missing: {missing_voters}")

            # Try to discover missing voters
            for voter_id in missing_voters:
                await self._discover_voter_peer(voter_id)

    async def _nat_management_loop(self):
        """
        Advanced NAT management loop.

        IMPROVEMENTS:
        - STUN-like probing to detect NAT type
        - Symmetric NAT detection (which breaks direct connectivity)
        - Intelligent relay selection
        - Hole-punch coordination
        """
        logger.info("Starting advanced NAT management loop")
        last_stun_probe = 0.0

        while self.running:
            try:
                now = time.time()

                # Periodic STUN-like probe to detect external IP and NAT type
                if NAT_SYMMETRIC_DETECTION_ENABLED and now - last_stun_probe > NAT_STUN_LIKE_PROBE_INTERVAL:
                    last_stun_probe = now
                    await self._detect_nat_type()

                # Probe NAT-blocked peers for recovery
                await self._probe_nat_blocked_peers()

                # Update relay preferences based on connectivity
                await self._update_relay_preferences()

            except Exception as e:
                logger.info(f"NAT management error: {e}")

            await asyncio.sleep(NAT_BLOCKED_PROBE_INTERVAL)

    async def _detect_nat_type(self):
        """
        Detect NAT type using STUN-like probing.

        NAT Types:
        - Full Cone: Any external host can send packets to internal host
        - Restricted Cone: Only hosts that internal has contacted can respond
        - Port Restricted: Only hosts+ports that internal has contacted can respond
        - Symmetric: Different external IP:port for each destination (breaks P2P)
        """
        external_ips = set()

        # Probe multiple peers to detect if we get different external IPs
        with self.peers_lock:
            alive_peers = [p for p in self.peers.values() if p.is_alive() and p.node_id != self.node_id]

        for peer in alive_peers[:5]:  # Probe up to 5 peers
            try:
                peer_scheme = getattr(peer, "scheme", "http") or "http"
                async with aiohttp.ClientSession() as session, session.get(
                    f"{peer_scheme}://{peer.host}:{peer.port}/health",
                    timeout=aiohttp.ClientTimeout(total=5),
                    headers=self._auth_headers()
                ) as resp:
                    if resp.status == 200:
                        # The peer would report our external IP if we had an endpoint for it
                        # For now, just track connectivity
                        await resp.json()
                        external_ips.add(peer.host)  # Track which peers we can reach
            except Exception:
                continue

        # If we see ourselves with different IPs from different vantage points,
        # we likely have symmetric NAT
        if len(external_ips) > 1:
            self._nat_type = "symmetric"
            logger.info("Detected symmetric NAT (multiple external IPs seen)")
        elif len(external_ips) == 1:
            self._nat_type = "cone"
        else:
            self._nat_type = "unknown"

    async def _probe_nat_blocked_peers(self):
        """Probe NAT-blocked peers to see if they've become reachable."""
        with self.peers_lock:
            nat_blocked_peers = [
                p for p in self.peers.values()
                if p.nat_blocked and p.node_id != self.node_id
            ]

        for peer in nat_blocked_peers:
            # Check if enough time has passed since blocking
            blocked_duration = time.time() - (peer.nat_blocked_since or 0)
            if blocked_duration < NAT_BLOCKED_RECOVERY_TIMEOUT:
                continue

            # Try to reach the peer
            peer_scheme = getattr(peer, "scheme", "http") or "http"

            # Try multiple endpoints
            endpoints_to_try = [(peer.host, peer.port)]

            # Add Tailscale IP
            ts_ip = self._get_tailscale_ip_for_peer(peer.node_id)
            if ts_ip and ts_ip != peer.host:
                endpoints_to_try.insert(0, (ts_ip, peer.port))  # Prefer Tailscale

            # Add reported endpoint
            rh = str(getattr(peer, "reported_host", "") or "").strip()
            rp = int(getattr(peer, "reported_port", 0) or 0)
            if rh and rp:
                endpoints_to_try.append((rh, rp))

            for host, port in endpoints_to_try:
                try:
                    async with aiohttp.ClientSession() as session, session.get(
                        f"{peer_scheme}://{host}:{port}/health",
                        timeout=aiohttp.ClientTimeout(total=NAT_BLOCKED_PROBE_TIMEOUT),
                        headers=self._auth_headers()
                    ) as resp:
                        if resp.status == 200:
                            # Peer is reachable! Clear NAT-blocked status
                            logger.info(f"NAT-blocked peer {peer.node_id} is now reachable at {host}:{port}")
                            with self.peers_lock:
                                if peer.node_id in self.peers:
                                    self.peers[peer.node_id].nat_blocked = False
                                    self.peers[peer.node_id].nat_blocked_since = 0.0
                                    self.peers[peer.node_id].host = host  # Update to working endpoint
                                    self.peers[peer.node_id].consecutive_failures = 0
                            break
                except Exception:
                    continue

    async def _update_relay_preferences(self):
        """Update relay preferences based on connectivity patterns."""
        # Identify peers that consistently fail direct connections
        with self.peers_lock:
            peers_needing_relay = [
                p for p in self.peers.values()
                if (getattr(p, "consecutive_failures", 0) or 0) >= NAT_RELAY_PREFERENCE_THRESHOLD
                and not p.nat_blocked
                and p.node_id != self.node_id
            ]

        for peer in peers_needing_relay:
            # Mark as preferring relay
            if not peer.nat_blocked:
                logger.info(f"Peer {peer.node_id} has {peer.consecutive_failures} consecutive failures, marking as NAT-blocked")
                with self.peers_lock:
                    if peer.node_id in self.peers:
                        self.peers[peer.node_id].nat_blocked = True
                        self.peers[peer.node_id].nat_blocked_since = time.time()
                        # Set relay to best available relay node
                        relay_node = self._select_best_relay()
                        if relay_node:
                            self.peers[peer.node_id].relay_via = relay_node

    def _select_best_relay(self) -> str:
        """Select the best relay node based on connectivity and load."""
        with self.peers_lock:
            candidates = [
                p for p in self.peers.values()
                if p.is_alive()
                and not p.nat_blocked
                and p.node_id != self.node_id
                and (getattr(p, "consecutive_failures", 0) or 0) < 2
            ]

        if not candidates:
            return ""

        # Prefer leader, then voters, then lowest load
        leader_peer = next((p for p in candidates if p.node_id == self.leader_id), None)
        if leader_peer:
            return leader_peer.node_id

        voter_peer = next((p for p in candidates if p.node_id in self.voter_node_ids), None)
        if voter_peer:
            return voter_peer.node_id

        # Lowest load
        candidates.sort(key=lambda p: getattr(p, "load_score", 100))
        return candidates[0].node_id if candidates else ""

    async def _manifest_collection_loop(self):
        """Periodically collect manifests for dashboard/training/sync decisions."""
        # IMPORTANT: Wait longer before first manifest collection to ensure HTTP server
        # is fully responsive. Initial manifest collection reads 700+ JSONL files and
        # can take several minutes, which can block health checks if started too early.
        await asyncio.sleep(60.0)  # Wait 60s before first manifest collection
        logger.info("Starting manifest collection loop (first collection in 60s)")
        while self.running:
            try:
                if self.role == NodeRole.LEADER:
                    cluster_manifest = await self._collect_cluster_manifest()
                    with self.manifest_lock:
                        self.cluster_data_manifest = cluster_manifest
                        self._record_selfplay_stats_sample(cluster_manifest)
                    if self.improvement_cycle_manager:
                        try:
                            self.improvement_cycle_manager.update_from_cluster_totals(
                                cluster_manifest.by_board_type
                            )
                        except Exception as e:
                            logger.info(f"ImprovementCycleManager update error: {e}")
                else:
                    local_manifest = await asyncio.to_thread(self._collect_local_data_manifest)
                    with self.manifest_lock:
                        self.local_data_manifest = local_manifest

                self.last_manifest_collection = time.time()

            except Exception as e:
                logger.info(f"Manifest collection error: {e}")

            await asyncio.sleep(self.manifest_collection_interval)

    def _record_selfplay_stats_sample(self, manifest: ClusterDataManifest) -> None:
        """Record a lightweight selfplay totals sample for dashboard charts."""
        try:
            sample = {
                "timestamp": time.time(),
                "manifest_collected_at": float(getattr(manifest, "collected_at", 0.0) or 0.0),
                "total_selfplay_games": int(getattr(manifest, "total_selfplay_games", 0) or 0),
                "by_board_type": manifest.by_board_type,
                "total_nodes": int(getattr(manifest, "total_nodes", 0) or 0),
            }
            self.selfplay_stats_history.append(sample)
            max_samples = int(getattr(self, "selfplay_stats_history_max_samples", 288) or 288)
            if max_samples > 0 and len(self.selfplay_stats_history) > max_samples:
                self.selfplay_stats_history = self.selfplay_stats_history[-max_samples:]
        except Exception:
            # Never let dashboard bookkeeping break manifest collection.
            return

    def _endpoint_key(self, info: NodeInfo) -> tuple[str, str, int] | None:
        """Return the normalized reachable endpoint key for a peer (scheme, host, port)."""
        host = str(getattr(info, "host", "") or "").strip()
        if not host:
            return None
        scheme = str(getattr(info, "scheme", "http") or "http").lower()
        try:
            port = int(getattr(info, "port", DEFAULT_PORT) or DEFAULT_PORT)
        except Exception:
            port = DEFAULT_PORT
        reported_host = str(getattr(info, "reported_host", "") or "").strip()
        try:
            reported_port = int(getattr(info, "reported_port", 0) or 0)
        except Exception:
            reported_port = 0

        if reported_host and reported_port > 0:
            # Reverse proxies / relays can cause inbound peer requests to appear as loopback.
            # Prefer the peer's self-reported advertised endpoint in that case so:
            # - endpoint conflict detection remains meaningful, and
            # - eligible leaders don't get filtered out as "conflicted".
            if host in {"127.0.0.1", "localhost", "0.0.0.0", "::1"} or self._is_tailscale_host(reported_host):
                host, port = reported_host, reported_port
        return (scheme, host, port)

    def _endpoint_conflict_keys(self, peers: list[NodeInfo]) -> set[tuple[str, str, int]]:
        """Compute endpoint keys that are shared by >1 node (NAT/port collisions)."""
        counts: dict[tuple[str, str, int], int] = {}
        for p in peers:
            # Ignore dead peers: stale node_ids can linger after restarts and would
            # otherwise permanently mark the live node as "conflicted".
            if not p.is_alive():
                continue
            key = self._endpoint_key(p)
            if not key:
                continue
            counts[key] = counts.get(key, 0) + 1
        return {k for k, v in counts.items() if v > 1}

    async def _probe_nat_blocked_peer(self, peer: NodeInfo) -> bool:
        """Probe a NAT-blocked peer to check if it's now directly reachable.

        Returns True if peer is reachable and NAT-blocked status was cleared.
        """
        if not peer.nat_blocked:
            return False

        now = time.time()
        nat_blocked_since = float(getattr(peer, "nat_blocked_since", 0.0) or 0.0)
        last_probe = float(getattr(peer, "last_nat_probe", 0.0) or 0.0)

        # Don't probe too frequently
        if now - last_probe < NAT_BLOCKED_PROBE_INTERVAL:
            return False

        # Don't probe if not blocked long enough
        if nat_blocked_since > 0 and (now - nat_blocked_since) < NAT_BLOCKED_RECOVERY_TIMEOUT:
            return False

        # Update last probe time
        with self.peers_lock:
            existing = self.peers.get(peer.node_id)
            if existing:
                existing.last_nat_probe = now

        try:
            url = self._url_for_peer(peer, "/status")
            timeout = ClientTimeout(total=NAT_BLOCKED_PROBE_TIMEOUT)
            async with get_client_session(timeout) as session:
                async with session.get(url, headers=self._auth_headers()) as resp:
                    if resp.status == 200:
                        # Peer is reachable! Clear NAT-blocked status
                        with self.peers_lock:
                            existing = self.peers.get(peer.node_id)
                            if existing and existing.nat_blocked:
                                existing.nat_blocked = False
                                existing.nat_blocked_since = 0.0
                                existing.relay_via = ""
                                logger.info(f"NAT recovery: {peer.node_id} is now directly reachable")
                                return True
        except Exception:
            # Probe failed - peer still not reachable
            pass

        return False

    async def _sweep_nat_recovery(self) -> int:
        """Periodically probe NAT-blocked peers to check if they've become reachable.

        Returns the number of peers that recovered from NAT-blocked state.
        """
        recovered = 0
        with self.peers_lock:
            nat_blocked_peers = [
                p for p in self.peers.values()
                if p.nat_blocked and p.is_alive()
            ]

        if not nat_blocked_peers:
            return 0

        # Probe in parallel but limit concurrency
        tasks = [self._probe_nat_blocked_peer(p) for p in nat_blocked_peers[:10]]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for result in results:
            if result is True:
                recovered += 1

        if recovered > 0:
            logger.info(f"NAT recovery sweep: {recovered} peer(s) recovered")

        return recovered

    def _is_leader_eligible(
        self,
        peer: NodeInfo,
        conflict_keys: set[tuple[str, str, int]],
        *,
        require_alive: bool = True,
    ) -> bool:
        """Heuristic: leaders must be directly reachable and uniquely addressable."""
        if require_alive and not peer.is_alive():
            return False
        voters = list(getattr(self, "voter_node_ids", []) or [])
        if voters and peer.node_id not in voters:
            return False
        if int(getattr(peer, "consecutive_failures", 0) or 0) >= MAX_CONSECUTIVE_FAILURES:
            return False
        if getattr(peer, "nat_blocked", False):
            return False
        key = self._endpoint_key(peer)
        return not (key and key in conflict_keys)

    def _maybe_adopt_leader_from_peers(self) -> bool:
        """If we can already see a healthy leader, adopt it and avoid elections."""
        if self.role == NodeRole.LEADER:
            return False

        with self.peers_lock:
            peers = [p for p in self.peers.values() if p.node_id != self.node_id]

        conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers])
        leaders = [
            p for p in peers
            if p.role == NodeRole.LEADER and self._is_leader_eligible(p, conflict_keys)
        ]

        voter_ids = list(getattr(self, "voter_node_ids", []) or [])
        if voter_ids:
            leaders = [p for p in leaders if p.node_id in voter_ids]

        if not leaders:
            return False

        # If multiple leaders exist (split brain), pick the lexicographically highest
        # ID (matches bully ordering) to converge.
        leader = sorted(leaders, key=lambda p: p.node_id)[-1]

        if self.leader_id != leader.node_id:
            logger.info(f"Adopted existing leader from peers: {leader.node_id}")
        self.leader_id = leader.node_id
        self.last_leader_seen = time.time()  # Track when we last had a functioning leader
        self.role = NodeRole.FOLLOWER
        self._save_state()
        return True

    async def _check_dead_peers_async(self):
        """Check for peers that have stopped responding (async version).

        This version uses AsyncLockWrapper to avoid blocking the event loop
        when acquiring the peers_lock.
        """
        now = time.time()
        dead_peers = []
        peers_to_purge = []

        async with AsyncLockWrapper(self.peers_lock):
            for node_id, info in self.peers.items():
                if not info.is_alive() and node_id != self.node_id:
                    dead_peers.append(node_id)
                    # Retire long-dead peers so they don't pollute active scheduling.
                    try:
                        dead_for = now - float(getattr(info, "last_heartbeat", 0.0) or 0.0)
                    except Exception:
                        dead_for = float("inf")
                    if not getattr(info, "retired", False) and dead_for >= PEER_RETIRE_AFTER_SECONDS:
                        info.retired = True
                        info.retired_at = now
                        logger.info(f"Retiring peer {node_id} (offline for {int(dead_for)}s)")
                elif info.is_alive() and getattr(info, "retired", False):
                    # Peer came back: clear retirement.
                    info.retired = False
                    info.retired_at = 0.0

            for node_id in dead_peers:
                info = self.peers.get(node_id)
                if info and getattr(info, "retired", False):
                    continue
                logger.info(f"Peer {node_id} is dead (no heartbeat for {PEER_TIMEOUT}s)")

            # Auto-purge very old retired peers
            for node_id, info in self.peers.items():
                if getattr(info, "retired", False):
                    retired_at = getattr(info, "retired_at", 0.0) or 0.0
                    if retired_at > 0 and (now - retired_at) >= PEER_PURGE_AFTER_SECONDS:
                        peers_to_purge.append(node_id)

            for node_id in peers_to_purge:
                del self.peers[node_id]
                logger.info(f"Auto-purged stale peer: {node_id} (retired for >{PEER_PURGE_AFTER_SECONDS}s)")

        # Clear stale leader IDs after restarts/partitions
        if self.leader_id and not self._is_leader_lease_valid():
            logger.info(f"Clearing stale/expired leader lease: leader_id={self.leader_id}")
            self.leader_id = None
            self.leader_lease_id = ""
            self.leader_lease_expires = 0.0
            self.last_lease_renewal = 0.0
            self.role = NodeRole.FOLLOWER
            asyncio.create_task(self._start_election())

        # If leader is dead, start election
        if self.leader_id and self.leader_id != self.node_id:
            async with AsyncLockWrapper(self.peers_lock):
                leader = self.peers.get(self.leader_id)
                peers_snapshot = [p for p in self.peers.values() if p.node_id != self.node_id]
            if leader:
                conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])
                if not self._is_leader_eligible(leader, conflict_keys):
                    reason = "dead" if not leader.is_alive() else "ineligible"
                    logger.info(f"Leader {self.leader_id} is {reason}, starting election")
                    self.leader_id = None
                    self.leader_lease_id = ""
                    self.leader_lease_expires = 0.0
                    self.last_lease_renewal = 0.0
                    self.role = NodeRole.FOLLOWER
                    asyncio.create_task(self._start_election())

        # If we're leaderless, periodically retry elections
        if not self.leader_id and not self.election_in_progress:
            now = time.time()
            backoff_seconds = max(LEADER_LEASE_RENEW_INTERVAL, ELECTION_TIMEOUT * 3)
            last_attempt = float(getattr(self, "last_election_attempt", 0.0) or 0.0)
            if now - last_attempt >= backoff_seconds:
                self.last_election_attempt = now
                asyncio.create_task(self._start_election())

    def _check_dead_peers(self):
        """Check for peers that have stopped responding."""
        now = time.time()
        with self.peers_lock:
            dead_peers = []
            for node_id, info in self.peers.items():
                if not info.is_alive() and node_id != self.node_id:
                    dead_peers.append(node_id)
                    # Retire long-dead peers so they don't pollute active scheduling.
                    try:
                        dead_for = now - float(getattr(info, "last_heartbeat", 0.0) or 0.0)
                    except Exception:
                        dead_for = float("inf")
                    if not getattr(info, "retired", False) and dead_for >= PEER_RETIRE_AFTER_SECONDS:
                        info.retired = True
                        info.retired_at = now
                        logger.info(f"Retiring peer {node_id} (offline for {int(dead_for)}s)")
                elif info.is_alive() and getattr(info, "retired", False):
                    # Peer came back: clear retirement.
                    info.retired = False
                    info.retired_at = 0.0

            for node_id in dead_peers:
                info = self.peers.get(node_id)
                if info and getattr(info, "retired", False):
                    continue
                logger.info(f"Peer {node_id} is dead (no heartbeat for {PEER_TIMEOUT}s)")
                # Don't remove immediately, just mark as dead for historical tracking

            # STABILITY FIX: Auto-purge very old retired peers to prevent unbounded list growth
            # This removes stale entries that would otherwise accumulate and cause confusion
            # (e.g., old leader role claims that don't match current elected leader)
            peers_to_purge = []
            for node_id, info in self.peers.items():
                if getattr(info, "retired", False):
                    retired_at = getattr(info, "retired_at", 0.0) or 0.0
                    if retired_at > 0 and (now - retired_at) >= PEER_PURGE_AFTER_SECONDS:
                        peers_to_purge.append(node_id)

            for node_id in peers_to_purge:
                del self.peers[node_id]
                logger.info(f"Auto-purged stale peer: {node_id} (retired for >{PEER_PURGE_AFTER_SECONDS}s)")

        # LEARNED LESSONS - Clear stale leader IDs after restarts/partitions.
        #
        # Nodes persist `leader_id` but not lease metadata. After a restart, it's
        # possible to have `leader_id` point at an alive peer that is no longer a
        # leader (or to a leader whose lease is expired). Without an explicit lease
        # validity check, the cluster can get stuck leaderless and stop dispatching
        # jobs (while still "thinking" it has a leader).
        if self.leader_id and not self._is_leader_lease_valid():
            logger.info(f"Clearing stale/expired leader lease: leader_id={self.leader_id}")
            self.leader_id = None
            self.leader_lease_id = ""
            self.leader_lease_expires = 0.0
            self.last_lease_renewal = 0.0
            self.role = NodeRole.FOLLOWER
            asyncio.create_task(self._start_election())

        # If leader is dead, start election
        if self.leader_id and self.leader_id != self.node_id:
            with self.peers_lock:
                leader = self.peers.get(self.leader_id)
                peers_snapshot = [p for p in self.peers.values() if p.node_id != self.node_id]
            if leader:
                conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])
                if not self._is_leader_eligible(leader, conflict_keys):
                    reason = "dead" if not leader.is_alive() else "ineligible"
                    logger.info(f"Leader {self.leader_id} is {reason}, starting election")
                    # Clear stale/ineligible leader to avoid proxy/relay selecting it.
                    self.leader_id = None
                    self.leader_lease_id = ""
                    self.leader_lease_expires = 0.0
                    self.last_lease_renewal = 0.0
                    self.role = NodeRole.FOLLOWER
                    asyncio.create_task(self._start_election())

        # If we're leaderless, periodically retry elections so the cluster can
        # recover without requiring manual restarts.
        if not self.leader_id and not self.election_in_progress:
            now = time.time()
            backoff_seconds = max(LEADER_LEASE_RENEW_INTERVAL, ELECTION_TIMEOUT * 3)
            last_attempt = float(getattr(self, "last_election_attempt", 0.0) or 0.0)
            if now - last_attempt >= backoff_seconds:
                self.last_election_attempt = now
                asyncio.create_task(self._start_election())

    async def _start_election(self):
        """Start leader election using Bully algorithm."""
        self._update_self_info()

        # NAT-blocked nodes cannot act as a leader because peers can't reach them.
        if getattr(self.self_info, "nat_blocked", False):
            return
        # Optional quorum gating: only configured voters may lead, and only when
        # a majority of voters are currently visible.
        if getattr(self, "voter_node_ids", []):
            if self.node_id not in self.voter_node_ids:
                return
            if not self._has_voter_quorum():
                return

        with self.peers_lock:
            peers_snapshot = [p for p in self.peers.values() if p.node_id != self.node_id]

        conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])

        if self.leader_id and self.leader_id != self.node_id:
            with self.peers_lock:
                leader = self.peers.get(self.leader_id)
            leader_ok = (
                leader is not None
                and leader.is_alive()
                and leader.role == NodeRole.LEADER
                and self._is_leader_eligible(leader, conflict_keys)
                and self._is_leader_lease_valid()
            )
            if leader_ok:
                return
            # Drop stale/ineligible leader so we don't keep advertising it.
            self.leader_id = None
            self.leader_lease_id = ""
            self.leader_lease_expires = 0.0
        if self._maybe_adopt_leader_from_peers():
            return

        if self.election_in_progress:
            return

        self.election_in_progress = True
        self.role = NodeRole.CANDIDATE
        logger.info(f"Starting election, my ID: {self.node_id}")

        try:
            # Send election message to all nodes with higher IDs
            with self.peers_lock:
                higher_nodes = [
                    p for p in self.peers.values()
                    if (
                        p.node_id > self.node_id
                        and self._is_leader_eligible(p, conflict_keys)
                    )
                ]
                voter_node_ids = list(getattr(self, "voter_node_ids", []) or [])
                if voter_node_ids:
                    higher_nodes = [p for p in higher_nodes if p.node_id in voter_node_ids]

            got_response = False

            timeout = ClientTimeout(total=ELECTION_TIMEOUT)
            async with get_client_session(timeout) as session:
                for peer in higher_nodes:
                    try:
                        url = self._url_for_peer(peer, "/election")
                        async with session.post(url, json={"candidate_id": self.node_id}, headers=self._auth_headers()) as resp:
                            if resp.status == 200:
                                data = await resp.json()
                                if data.get("response") == "ALIVE":
                                    got_response = True
                                    logger.info(f"Higher node {peer.node_id} responded")
                    except Exception:
                        pass  # Network errors expected during elections

            # If no higher node responded, we become leader
            if not got_response:
                # Only become leader if we're eligible (unique + directly reachable).
                if self._is_leader_eligible(self.self_info, conflict_keys):
                    await self._become_leader()
            else:
                # Wait for coordinator message
                await asyncio.sleep(ELECTION_TIMEOUT * 2)
                # If no coordinator arrives, fall back to adopting any eligible leader we can see.
                self._maybe_adopt_leader_from_peers()

        finally:
            self.election_in_progress = False
            if self.role == NodeRole.CANDIDATE:
                self.role = NodeRole.FOLLOWER

    async def _become_leader(self):
        """Become the cluster leader with lease-based leadership."""
        self._update_self_info()
        if getattr(self.self_info, "nat_blocked", False):
            logger.info(f"Refusing leadership while NAT-blocked: {self.node_id}")
            return
        if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
            logger.info(f"Refusing leadership without voter quorum: {self.node_id}")
            return
        import uuid
        lease_id = f"{self.node_id}_{int(time.time())}_{uuid.uuid4().hex[:8]}"
        lease_expires = await self._acquire_voter_lease_quorum(lease_id, int(LEADER_LEASE_DURATION))
        if getattr(self, "voter_node_ids", []) and not lease_expires:
            logger.error(f"Failed to obtain voter lease quorum; refusing leadership: {self.node_id}")
            self.role = NodeRole.FOLLOWER
            self.leader_id = None
            self.leader_lease_id = ""
            self.leader_lease_expires = 0.0
            self.last_lease_renewal = 0.0
            self._release_voter_grant_if_self()
            self._save_state()
            return

        logger.info(f"I am now the leader: {self.node_id}")
        self.role = NodeRole.LEADER
        self.leader_id = self.node_id
        self.last_leader_seen = time.time()  # Track when we last had a functioning leader

        # Phase 29: Increment cluster epoch on leadership change
        # This helps resolve split-brain when partitions merge
        self._increment_cluster_epoch()

        # Lease-based leadership (voter-backed when enabled).
        self.leader_lease_id = lease_id
        self.leader_lease_expires = float(lease_expires or (time.time() + LEADER_LEASE_DURATION))
        self.last_lease_renewal = time.time()

        # Announce to all peers with lease information
        with self.peers_lock:
            peers = list(self.peers.values())

        timeout = ClientTimeout(total=5)
        async with get_client_session(timeout) as session:
            for peer in peers:
                if peer.node_id != self.node_id:
                    try:
                        url = self._url_for_peer(peer, "/coordinator")
                        await session.post(url, json={
                            "leader_id": self.node_id,
                            "lease_id": self.leader_lease_id,
                            "lease_expires": self.leader_lease_expires,
                            "voter_node_ids": list(getattr(self, "voter_node_ids", []) or []),
                        }, headers=self._auth_headers())
                    except Exception:
                        pass  # Network errors expected during leader announcements

        self._save_state()

        # Start monitoring services when becoming leader
        await self._start_monitoring_if_leader()

        # Start P2P auto-deployer when becoming leader
        await self._start_p2p_auto_deployer()

    async def _check_emergency_coordinator_fallback(self):
        """DECENTRALIZED: When voter quorum is unreachable for >5 min, any GPU node can coordinate.

        EMERGENCY COORDINATOR: This is a last-resort fallback when the normal voter-based
        leadership cannot be established due to:
        - Too many voters being offline
        - Network partition isolating voters
        - Cluster-wide issues

        In this mode, the node acts as a temporary coordinator WITHOUT voter consensus.
        It will relinquish control once voter quorum is restored.
        """
        now = time.time()

        # Only check every 60 seconds
        last_check = getattr(self, "_last_emergency_coord_check", 0)
        if now - last_check < 60:
            return
        self._last_emergency_coord_check = now

        # Skip if we already are a leader
        if self.role == NodeRole.LEADER:
            return

        # Skip if we have a known leader
        if self.leader_id:
            self._emergency_coordinator_since = 0
            return

        # Check if we have voter quorum
        if self._has_voter_quorum():
            self._emergency_coordinator_since = 0
            return  # Normal election should work

        # Track how long we've been without voter quorum
        quorum_missing_since = getattr(self, "_quorum_missing_since", 0)
        if quorum_missing_since == 0:
            self._quorum_missing_since = now
            return

        EMERGENCY_THRESHOLD = 300  # 5 minutes without quorum triggers emergency
        quorum_missing_duration = now - quorum_missing_since

        if quorum_missing_duration < EMERGENCY_THRESHOLD:
            return

        # Check if we're eligible (must be GPU node, not NAT-blocked)
        self._update_self_info()
        if not getattr(self.self_info, "has_gpu", False):
            return
        if getattr(self.self_info, "nat_blocked", False):
            return

        # Use consistent hashing to determine which node should be emergency coordinator
        # This prevents multiple nodes from declaring themselves coordinator
        with self.peers_lock:
            candidates = [self.node_id]
            for peer in self.peers.values():
                if not peer.is_alive():
                    continue
                if not getattr(peer, "has_gpu", False):
                    continue
                if getattr(peer, "nat_blocked", False):
                    continue
                candidates.append(peer.node_id)

        if not candidates:
            return

        # Deterministic selection: highest node_id wins (simple, consistent)
        candidates.sort(reverse=True)
        designated_coordinator = candidates[0]

        if designated_coordinator != self.node_id:
            return  # Another node should be coordinator

        # Become emergency coordinator (without voter lease)
        logger.info(f"EMERGENCY COORDINATOR: Taking leadership without voter quorum "
              f"(quorum missing for {int(quorum_missing_duration)}s, {len(candidates)} candidates)")

        self.role = NodeRole.LEADER
        self.leader_id = self.node_id
        self.last_leader_seen = now
        self._emergency_coordinator_since = now

        # Use a special lease ID to mark emergency mode
        import uuid
        self.leader_lease_id = f"EMERGENCY_{self.node_id}_{uuid.uuid4().hex[:8]}"
        self.leader_lease_expires = now + 120  # Short lease - needs frequent renewal
        self.last_lease_renewal = now

        # Announce emergency leadership
        with self.peers_lock:
            peers = list(self.peers.values())

        timeout = ClientTimeout(total=5)
        async with get_client_session(timeout) as session:
            for peer in peers:
                if peer.node_id != self.node_id:
                    try:
                        url = self._url_for_peer(peer, "/coordinator")
                        await session.post(url, json={
                            "leader_id": self.node_id,
                            "lease_id": self.leader_lease_id,
                            "lease_expires": self.leader_lease_expires,
                            "emergency": True,
                        }, headers=self._auth_headers())
                    except Exception:
                        pass  # Network errors expected during emergency coordination

        self._save_state()
        logger.info(f"EMERGENCY COORDINATOR: {self.node_id} is now emergency leader")

    def _get_peer_health_score(self, peer_id: str) -> float:
        """Calculate health score for a peer (0-100, higher is healthier).

        HEALTH-BASED PEER SELECTION: Considers multiple factors to pick
        the best peer for data sync, avoiding overloaded or unreliable nodes.
        """
        with self.peers_lock:
            peer = self.peers.get(peer_id)
        if not peer or not peer.is_alive():
            return 0.0

        score = 100.0

        # Penalize high resource usage
        cpu = float(getattr(peer, "cpu_percent", 0) or 0)
        memory = float(getattr(peer, "memory_percent", 0) or 0)
        disk = float(getattr(peer, "disk_percent", 0) or 0)

        score -= cpu * 0.3  # CPU impact
        score -= memory * 0.2  # Memory impact
        score -= max(0, disk - 50) * 0.5  # Disk penalty above 50%

        # Penalize consecutive failures (circuit breaker input)
        failures = int(getattr(peer, "consecutive_failures", 0) or 0)
        score -= failures * 10

        # Penalize NAT-blocked peers (slower sync)
        if getattr(peer, "nat_blocked", False):
            score -= 20

        # Bonus for GPU nodes (typically more powerful)
        if getattr(peer, "has_gpu", False):
            score += 10

        # Check circuit breaker
        circuit_breaker = getattr(self, "_p2p_circuit_breaker", {})
        breaker_info = circuit_breaker.get(peer_id, {})
        if breaker_info.get("open_until", 0) > time.time():
            score = 0  # Circuit is open, don't use this peer

        return max(0.0, min(100.0, score))

    def _record_p2p_sync_result(self, peer_id: str, success: bool):
        """Record P2P sync result for circuit breaker, metrics, and reputation.

        CIRCUIT BREAKER: After 3 consecutive failures, open circuit for 5 minutes.
        This prevents wasting time on unreliable peers.

        PEER REPUTATION: Also records sync result for reputation tracking.
        """
        if not hasattr(self, "_p2p_circuit_breaker"):
            self._p2p_circuit_breaker = {}
        if not hasattr(self, "_p2p_sync_metrics"):
            self._p2p_sync_metrics = {"success": 0, "failure": 0, "bytes": 0}

        breaker = self._p2p_circuit_breaker.get(peer_id, {"failures": 0, "open_until": 0})

        # Record for reputation tracking
        self._record_peer_interaction(peer_id, success, "sync")

        if success:
            breaker["failures"] = 0
            breaker["open_until"] = 0
            self._p2p_sync_metrics["success"] += 1
        else:
            breaker["failures"] = breaker.get("failures", 0) + 1
            self._p2p_sync_metrics["failure"] += 1

            # Open circuit after 3 failures
            if breaker["failures"] >= 3:
                breaker["open_until"] = time.time() + 300  # 5 minute cooldown
                logger.info(f"CIRCUIT BREAKER: Opening circuit for {peer_id} (3 failures)")

        self._p2p_circuit_breaker[peer_id] = breaker

    async def _p2p_data_sync(self):
        """DECENTRALIZED: Nodes sync data directly with peers without leader coordination.

        P2P DATA SYNC with enhancements:
        - Health-based peer selection (avoids overloaded nodes)
        - Circuit breaker (skips unreliable peers)
        - Delta sync (only syncs files newer than last sync)
        - Model file prioritization (syncs models first)
        - ADAPTIVE INTERVALS: adjusts based on cluster activity and success rate
        """
        now = time.time()

        # ADAPTIVE INTERVAL: Uses activity-aware interval instead of fixed 5 min
        interval = self._get_adaptive_sync_interval("data")
        last_check = getattr(self, "_last_p2p_sync_check", 0)
        if now - last_check < interval:
            return
        self._last_p2p_sync_check = now

        # Skip if leader is actively managing sync (avoid conflicts)
        if self.role == NodeRole.LEADER:
            return  # Leader uses centralized sync

        # Skip if a sync is already in progress
        if getattr(self, "sync_in_progress", False):
            return

        # Skip if under disk pressure
        if getattr(self.self_info, "disk_percent", 0) > 85:
            return

        # Get our local manifest (use cache for speed)
        local_manifest = getattr(self, "local_data_manifest", None)
        if not local_manifest:
            try:
                local_manifest = self._collect_local_data_manifest_cached(max_cache_age=600)
                with self.manifest_lock:
                    self.local_data_manifest = local_manifest
            except Exception:
                return

        # Get local file set with timestamps for delta sync
        local_files = {}
        for file_info in getattr(local_manifest, "files", []) or []:
            rel_path = getattr(file_info, "relative_path", "")
            if rel_path:
                local_files[rel_path] = getattr(file_info, "modified_at", 0)

        # Check peer manifests from gossip cache
        peer_manifests = getattr(self, "_gossip_peer_manifests", {})
        if not peer_manifests:
            return

        # Find files we're missing that peers have (with prioritization)
        files_to_sync: dict[str, list[tuple]] = {}  # peer_id -> [(file, priority)]
        file_hashes: dict[str, str] = {}  # file_path -> hash (for dedup tracking)
        last_sync_time = getattr(self, "_last_successful_p2p_sync", 0)

        for peer_id, peer_manifest in peer_manifests.items():
            if peer_id == self.node_id:
                continue

            # Check circuit breaker
            health = self._get_peer_health_score(peer_id)
            if health <= 0:
                continue

            peer_files = getattr(peer_manifest, "files", []) or []
            for file_info in peer_files:
                rel_path = getattr(file_info, "relative_path", "")
                modified_at = getattr(file_info, "modified_at", 0)
                file_hash = getattr(file_info, "file_hash", "")
                file_size = getattr(file_info, "size_bytes", 0)

                if not rel_path:
                    continue

                # Skip if we have this file with same or newer timestamp
                if rel_path in local_files and local_files[rel_path] >= modified_at:
                    continue

                # Skip if file is older than last sync (delta optimization)
                if modified_at < last_sync_time and rel_path in local_files:
                    continue

                # DATA DEDUPLICATION: Skip if we already synced this file (by hash)
                if file_hash and self._is_file_already_synced(file_hash):
                    self._record_dedup_skip(file_count=1, bytes_saved=file_size)
                    continue

                # Calculate priority (models > ELO/training DBs > training data > selfplay)
                priority = 0
                if "models/" in rel_path or rel_path.endswith(".pt") or rel_path.endswith(".onnx"):
                    priority = 100  # Highest priority for models
                elif rel_path.endswith(".db") and ("unified_elo" in rel_path or "elo_ratings" in rel_path):
                    priority = 90  # Very high priority for ELO database
                elif rel_path.endswith(".db") and ("canonical_" in rel_path or "consolidated_training" in rel_path or "training_pool" in rel_path):
                    priority = 80  # High priority for training databases
                elif "training/" in rel_path:
                    priority = 50
                elif rel_path.endswith(".db"):
                    priority = 30  # Medium priority for other databases
                else:
                    priority = 10

                if peer_id not in files_to_sync:
                    files_to_sync[peer_id] = []
                files_to_sync[peer_id].append((rel_path, priority, health))

                # Track hash for dedup recording after sync
                if file_hash:
                    file_hashes[rel_path] = file_hash

        if not files_to_sync:
            return

        # Select best peer using health score AND file count
        def peer_score(peer_id):
            files = files_to_sync[peer_id]
            health = self._get_peer_health_score(peer_id)
            file_score = sum(f[1] for f in files)  # Sum of priorities
            return health * 0.4 + file_score * 0.6

        best_peer = max(files_to_sync.keys(), key=peer_score)
        files_with_priority = files_to_sync[best_peer]

        # Sort by priority (highest first) and take top 10
        files_with_priority.sort(key=lambda x: x[1], reverse=True)
        files_to_request = [f[0] for f in files_with_priority[:10]]

        # Check if peer is alive
        with self.peers_lock:
            peer = self.peers.get(best_peer)

        if not peer or not peer.is_alive():
            return

        # Log and initiate sync
        total_missing = sum(len(f) for f in files_to_sync.values())
        model_files = sum(1 for f in files_to_request if "models/" in f or f.endswith(".pt"))
        logger.info(f"P2P SYNC: Missing {total_missing} files, requesting {len(files_to_request)} "
              f"({model_files} models) from {best_peer} (health={self._get_peer_health_score(best_peer):.0f})")

        try:
            import uuid
            job = DataSyncJob(
                job_id=f"p2p_{uuid.uuid4().hex[:8]}",
                source_node=best_peer,
                target_node=self.node_id,
                files=files_to_request,
            )

            self.sync_in_progress = True
            try:
                success = await self._request_node_sync(job)
                self._record_p2p_sync_result(best_peer, success)
                self._record_sync_result_for_adaptive("data", success)  # ADAPTIVE INTERVAL

                if success:
                    logger.info(f"P2P SYNC: Completed {len(files_to_request)} files from {best_peer}")
                    self._last_successful_p2p_sync = now
                    # Invalidate manifest cache
                    cache_path = self._get_manifest_cache_path()
                    if cache_path.exists():
                        cache_path.unlink()
                    # Update metrics
                    if hasattr(self, "_p2p_sync_metrics"):
                        self._p2p_sync_metrics["bytes"] += job.bytes_transferred
                    # DATA DEDUPLICATION: Record synced file hashes
                    for fpath in files_to_request:
                        if fpath in file_hashes:
                            self._record_synced_file(file_hashes[fpath], 0)
                else:
                    logger.info(f"P2P SYNC: Failed from {best_peer}: {job.error_message}")
            finally:
                self.sync_in_progress = False

        except Exception as e:
            logger.info(f"P2P SYNC: Error: {e}")
            self._record_sync_result_for_adaptive("data", False)  # ADAPTIVE: record failure
            self._record_p2p_sync_result(best_peer, False)
            self.sync_in_progress = False

    async def _p2p_model_sync(self):
        """DECENTRALIZED: Sync model files via P2P for faster model distribution.

        MODEL P2P SYNC: Ensures all nodes have access to latest trained models
        without relying on leader-coordinated sync. Prioritizes:
        - Newer models (by timestamp)
        - Models for active board configurations
        - NNUE models (smaller, faster to sync)
        - ADAPTIVE INTERVALS: faster during training, slower when idle
        """
        now = time.time()

        # ADAPTIVE INTERVAL: Uses activity-aware interval (faster during training)
        interval = self._get_adaptive_sync_interval("model")
        last_check = getattr(self, "_last_p2p_model_sync", 0)
        if now - last_check < interval:
            return
        self._last_p2p_model_sync = now

        # Skip if sync in progress
        if getattr(self, "sync_in_progress", False):
            return

        # Get model files from local manifest
        local_manifest = getattr(self, "local_data_manifest", None)
        if not local_manifest:
            return

        local_models = set()
        for file_info in getattr(local_manifest, "files", []) or []:
            rel_path = getattr(file_info, "relative_path", "")
            if rel_path and ("models/" in rel_path or rel_path.endswith((".pt", ".onnx", ".bin"))):
                local_models.add(rel_path)

        # Check peer manifests for models we're missing
        peer_manifests = getattr(self, "_gossip_peer_manifests", {})
        missing_models: dict[str, list[str]] = {}

        for peer_id, peer_manifest in peer_manifests.items():
            if peer_id == self.node_id:
                continue

            health = self._get_peer_health_score(peer_id)
            if health <= 0:
                continue

            peer_files = getattr(peer_manifest, "files", []) or []
            for file_info in peer_files:
                rel_path = getattr(file_info, "relative_path", "")
                if not rel_path:
                    continue
                if not ("models/" in rel_path or rel_path.endswith((".pt", ".onnx", ".bin"))):
                    continue
                if rel_path in local_models:
                    continue

                if peer_id not in missing_models:
                    missing_models[peer_id] = []
                missing_models[peer_id].append(rel_path)

        if not missing_models:
            return

        # Pick healthiest peer with models
        best_peer = max(missing_models.keys(), key=lambda p: self._get_peer_health_score(p))
        models_to_sync = missing_models[best_peer][:5]  # Max 5 models per cycle

        with self.peers_lock:
            peer = self.peers.get(best_peer)
        if not peer or not peer.is_alive():
            return

        logger.info(f"MODEL SYNC: Requesting {len(models_to_sync)} models from {best_peer}")

        try:
            import uuid
            job = DataSyncJob(
                job_id=f"model_{uuid.uuid4().hex[:8]}",
                source_node=best_peer,
                target_node=self.node_id,
                files=models_to_sync,
            )

            self.sync_in_progress = True
            try:
                success = await self._request_node_sync(job)
                self._record_p2p_sync_result(best_peer, success)
                self._record_sync_result_for_adaptive("model", success)  # ADAPTIVE INTERVAL
                if success:
                    logger.info(f"MODEL SYNC: Got {len(models_to_sync)} models from {best_peer}")
            finally:
                self.sync_in_progress = False
        except Exception as e:
            logger.info(f"MODEL SYNC: Error: {e}")
            self._record_sync_result_for_adaptive("model", False)  # ADAPTIVE: record failure
            self.sync_in_progress = False

    async def _p2p_training_db_sync(self):
        """DECENTRALIZED: Sync training databases via P2P for improved training diversity.

        TRAINING DB P2P SYNC: Ensures all nodes have access to consolidated training
        data without relying on leader-coordinated sync. Prioritizes:
        - canonical_*.db (canonical training data)
        - consolidated_training*.db (merged training data)
        - training_pool*.db (training pool databases)

        This improves training diversity by ensuring all nodes can train on
        cluster-wide data, not just their local selfplay games.

        ADAPTIVE INTERVALS: faster during training, slower when idle.
        """
        now = time.time()

        # ADAPTIVE INTERVAL: Uses activity-aware interval (faster during training)
        interval = self._get_adaptive_sync_interval("training_db")
        last_check = getattr(self, "_last_p2p_training_db_sync", 0)
        if now - last_check < interval:
            return
        self._last_p2p_training_db_sync = now

        # Skip if sync is in progress
        if getattr(self, "sync_in_progress", False):
            return

        # Skip if under disk pressure
        if getattr(self.self_info, "disk_percent", 0) > 80:
            return

        # Get our local files
        local_manifest = getattr(self, "local_data_manifest", None)
        if not local_manifest:
            return

        local_dbs = set()
        local_db_sizes = {}
        for file_info in getattr(local_manifest, "files", []) or []:
            rel_path = getattr(file_info, "relative_path", "")
            if rel_path.endswith(".db"):
                local_dbs.add(rel_path)
                local_db_sizes[rel_path] = getattr(file_info, "size_bytes", 0)

        # Check peer manifests for training databases
        peer_manifests = getattr(self, "_gossip_peer_manifests", {})
        if not peer_manifests:
            return

        # Find training databases we're missing or have smaller versions of
        missing_dbs: dict[str, list[tuple]] = {}  # peer_id -> [(db_path, size)]

        for peer_id, peer_manifest in peer_manifests.items():
            if peer_id == self.node_id:
                continue

            # Check circuit breaker
            health = self._get_peer_health_score(peer_id)
            if health <= 0:
                continue

            peer_files = getattr(peer_manifest, "files", []) or []
            for file_info in peer_files:
                rel_path = getattr(file_info, "relative_path", "")
                size = getattr(file_info, "size_bytes", 0)

                # Only sync training-related databases and ELO database
                if not rel_path.endswith(".db"):
                    continue
                if not ("canonical_" in rel_path or "consolidated_training" in rel_path or
                        "training_pool" in rel_path or "unified_elo" in rel_path or
                        "elo_ratings" in rel_path):
                    continue

                # Skip empty databases
                if size < 1024:
                    continue

                # Check if we don't have it or have a smaller version
                local_size = local_db_sizes.get(rel_path, 0)
                if local_size >= size:
                    continue

                if peer_id not in missing_dbs:
                    missing_dbs[peer_id] = []
                missing_dbs[peer_id].append((rel_path, size, health))

        if not missing_dbs:
            return

        # Pick healthiest peer with training DBs
        best_peer = max(missing_dbs.keys(), key=lambda p: self._get_peer_health_score(p))
        dbs_to_sync = [db[0] for db in missing_dbs[best_peer][:3]]  # Max 3 DBs per cycle

        # Check if peer is alive
        with self.peers_lock:
            peer = self.peers.get(best_peer)
        if not peer or not peer.is_alive():
            return

        logger.info(f"TRAINING DB SYNC: Requesting {len(dbs_to_sync)} training DBs from {best_peer}")

        try:
            import uuid
            job = DataSyncJob(
                job_id=f"traindb_{uuid.uuid4().hex[:8]}",
                source_node=best_peer,
                target_node=self.node_id,
                files=dbs_to_sync,
            )

            self.sync_in_progress = True
            try:
                success = await self._request_node_sync(job)
                self._record_p2p_sync_result(best_peer, success)
                self._record_sync_result_for_adaptive("training_db", success)  # ADAPTIVE INTERVAL
                if success:
                    logger.info(f"TRAINING DB SYNC: Got {len(dbs_to_sync)} training DBs from {best_peer}")
            finally:
                self.sync_in_progress = False
        except Exception as e:
            logger.info(f"TRAINING DB SYNC: Error: {e}")
            self._record_sync_result_for_adaptive("training_db", False)  # ADAPTIVE: record failure
            self.sync_in_progress = False

    async def _gossip_state_to_peers(self):
        """DECENTRALIZED: Share node state with random peers using gossip protocol.

        GOSSIP PROTOCOL: Instead of relying solely on leader to collect state,
        nodes share information with neighbors, and it propagates through the cluster.

        Benefits:
        - Faster state propagation (O(log N) instead of O(N))
        - Works without a leader
        - Resilient to network partitions (state eventually converges)
        - Reduces load on leader

        Implementation:
        1. Each node maintains local state (jobs, resources, health)
        2. Periodically send state to K random peers (fanout)
        3. Receive state from peers and update local view
        4. Include version/timestamp to handle conflicts (last-write-wins)
        """
        now = time.time()

        # Rate limit: gossip every 30 seconds
        last_gossip = getattr(self, "_last_gossip_time", 0)
        if now - last_gossip < 30:
            return
        self._last_gossip_time = now

        # Prepare our state to share
        self._update_self_info()
        local_state = {
            "node_id": self.node_id,
            "timestamp": now,
            "version": int(now * 1000),  # Millisecond version for conflict resolution
            "role": self.role.value if hasattr(self.role, "value") else str(self.role),
            "leader_id": self.leader_id,
            "leader_lease_expires": getattr(self, "leader_lease_expires", 0),
            "selfplay_jobs": getattr(self.self_info, "selfplay_jobs", 0),
            "training_jobs": getattr(self.self_info, "training_jobs", 0),
            "gpu_percent": getattr(self.self_info, "gpu_percent", 0),
            "cpu_percent": getattr(self.self_info, "cpu_percent", 0),
            "memory_percent": getattr(self.self_info, "memory_percent", 0),
            "disk_percent": getattr(self.self_info, "disk_percent", 0),
            "has_gpu": getattr(self.self_info, "has_gpu", False),
            "gpu_name": getattr(self.self_info, "gpu_name", ""),
            "voter_quorum_ok": self._has_voter_quorum(),
        }

        # DISTRIBUTED TRAINING COORDINATION: Include active training configs
        # This allows nodes to coordinate training without a leader
        local_state["active_training_configs"] = self._get_local_active_training_configs()

        # DISTRIBUTED ELO: Include ELO summary for cluster-wide visibility
        local_state["elo_summary"] = self._get_local_elo_summary()

        # GOSSIP-BASED LEADER HINTS: Share leader preference for faster elections
        local_state["leader_hint"] = self._get_leader_hint()

        # PEER REPUTATION: Share peer reliability scores
        local_state["peer_reputation"] = self._get_peer_reputation_summary()

        # DISTRIBUTED TOURNAMENT: Share tournament proposals and active tournaments
        local_state["tournament"] = self._get_tournament_gossip_state()

        # Include manifest summary if available
        local_manifest = getattr(self, "local_data_manifest", None)
        if local_manifest:
            local_state["manifest_summary"] = {
                "total_files": getattr(local_manifest, "total_files", 0),
                "selfplay_games": getattr(local_manifest, "selfplay_games", 0),
                "collected_at": getattr(local_manifest, "collected_at", 0),
            }

        # Select K random peers to gossip with (fanout = 3)
        GOSSIP_FANOUT = 3
        with self.peers_lock:
            alive_peers = [
                p for p in self.peers.values()
                if p.is_alive() and not getattr(p, "retired", False)
            ]

        if not alive_peers:
            return

        import random
        peers_to_gossip = random.sample(alive_peers, min(GOSSIP_FANOUT, len(alive_peers)))

        # Send gossip to selected peers
        timeout = ClientTimeout(total=5)
        async with get_client_session(timeout) as session:
            for peer in peers_to_gossip:
                try:
                    # Include known state about other nodes (propagation)
                    gossip_payload = {
                        "sender": self.node_id,
                        "sender_state": local_state,
                        "known_states": self._get_gossip_known_states(),
                        # Phase 28: Peer-of-peer discovery - share peer endpoints
                        "peer_endpoints": self._get_peer_endpoints_for_gossip(),
                        # Phase 29: Cluster epoch for split-brain resolution
                        "cluster_epoch": self._cluster_epoch,
                    }

                    # GOSSIP COMPRESSION: Compress payload with gzip to reduce network transfer
                    json_bytes = json.dumps(gossip_payload).encode("utf-8")
                    original_size = len(json_bytes)
                    compressed_bytes = gzip.compress(json_bytes, compresslevel=6)
                    compressed_size = len(compressed_bytes)

                    # Track compression metrics
                    self._record_gossip_compression(original_size, compressed_size)

                    start_time = time.time()
                    for url in self._urls_for_peer(peer, "/gossip"):
                        try:
                            headers = self._auth_headers()
                            headers["Content-Encoding"] = "gzip"
                            headers["Content-Type"] = "application/json"
                            async with session.post(url, data=compressed_bytes, headers=headers) as resp:
                                if resp.status == 200:
                                    # Process response (peer shares their state back)
                                    # Check if response is compressed
                                    content_encoding = resp.headers.get("Content-Encoding", "")
                                    if content_encoding == "gzip":
                                        response_bytes = await resp.read()
                                        decompressed = gzip.decompress(response_bytes)
                                        response_data = json.loads(decompressed.decode("utf-8"))
                                    else:
                                        response_data = await resp.json()
                                    self._process_gossip_response(response_data)
                                    # Record metrics
                                    latency_ms = (time.time() - start_time) * 1000
                                    self._record_gossip_metrics("sent", peer.node_id)
                                    self._record_gossip_metrics("latency", peer.node_id, latency_ms)
                                    break
                        except Exception:
                            continue
                except Exception:
                    pass

    def _get_gossip_known_states(self) -> dict[str, dict]:
        """Get known states about other nodes to propagate via gossip."""
        known = {}
        gossip_states = getattr(self, "_gossip_peer_states", {})
        # Only share recent states (last 5 minutes)
        cutoff = time.time() - 300
        for node_id, state in gossip_states.items():
            if state.get("timestamp", 0) > cutoff:
                known[node_id] = state
        return known

    def _get_peer_endpoints_for_gossip(self) -> list[dict[str, Any]]:
        """Phase 28: Get peer endpoints to share via gossip for peer-of-peer discovery.

        Returns a list of alive peer endpoints with connection info.
        This enables nodes to discover peers they can't reach directly.
        """
        endpoints = []
        with self.peers_lock:
            # Get alive, non-retired peers
            alive_peers = [
                p for p in self.peers.values()
                if p.node_id != self.node_id and p.is_alive() and not getattr(p, "retired", False)
            ]

        # Limit to top N peers to avoid payload bloat
        for peer in alive_peers[:GOSSIP_MAX_PEER_ENDPOINTS]:
            endpoint = {
                "node_id": peer.node_id,
                "host": str(getattr(peer, "host", "") or ""),
                "port": int(getattr(peer, "port", DEFAULT_PORT) or DEFAULT_PORT),
                "tailscale_ip": str(getattr(peer, "tailscale_ip", "") or ""),
                "is_alive": True,
                "last_heartbeat": float(getattr(peer, "last_heartbeat", 0) or 0),
            }
            endpoints.append(endpoint)

        return endpoints

    def _process_gossip_response(self, response: dict):
        """Process gossip response from a peer, updating our view of the cluster."""
        if not response:
            return

        # Initialize gossip state storage if needed
        if not hasattr(self, "_gossip_peer_states"):
            self._gossip_peer_states = {}
        if not hasattr(self, "_gossip_peer_manifests"):
            self._gossip_peer_manifests = {}

        # Process sender's state
        sender_state = response.get("sender_state", {})
        if sender_state:
            sender_id = sender_state.get("node_id")
            if sender_id and sender_id != self.node_id:
                existing = self._gossip_peer_states.get(sender_id, {})
                # Last-write-wins conflict resolution
                if sender_state.get("version", 0) > existing.get("version", 0):
                    self._gossip_peer_states[sender_id] = sender_state

                    # Update leader info if sender claims to know a leader
                    if sender_state.get("leader_id") and not self.leader_id:
                        claimed_leader = sender_state.get("leader_id")
                        lease_expires = sender_state.get("leader_lease_expires", 0)
                        if lease_expires > time.time():
                            self.leader_id = claimed_leader
                            self.last_leader_seen = time.time()

        # Process known states (propagation)
        known_states = response.get("known_states", {})
        for node_id, state in known_states.items():
            if node_id == self.node_id:
                continue
            existing = self._gossip_peer_states.get(node_id, {})
            if state.get("version", 0) > existing.get("version", 0):
                self._gossip_peer_states[node_id] = state

        # Process manifest info for P2P sync
        peer_manifests = response.get("peer_manifests", {})
        for node_id, manifest_data in peer_manifests.items():
            if node_id != self.node_id:
                with contextlib.suppress(Exception):
                    self._gossip_peer_manifests[node_id] = NodeDataManifest.from_dict(manifest_data)

        # Process tournament gossip for distributed scheduling
        for node_id, state in known_states.items():
            if node_id == self.node_id:
                continue
            tournament_state = state.get("tournament")
            if tournament_state:
                with contextlib.suppress(Exception):
                    self._process_tournament_gossip(node_id, tournament_state)

        # Check for tournament consensus after processing gossip
        with contextlib.suppress(Exception):
            self._check_tournament_consensus()

        # Phase 28: Process peer endpoints for peer-of-peer discovery
        peer_endpoints = response.get("peer_endpoints") or []
        if peer_endpoints:
            self._process_gossip_peer_endpoints(peer_endpoints)

        # Phase 29: Process cluster epoch for split-brain resolution
        incoming_epoch = response.get("cluster_epoch")
        if incoming_epoch is not None:
            self._handle_incoming_cluster_epoch(incoming_epoch, response)

    def _process_gossip_peer_endpoints(self, peer_endpoints: list[dict]) -> None:
        """Phase 28: Process peer endpoints learned via gossip.

        Enables discovery of peers we can't reach directly through intermediaries.
        """
        for endpoint in peer_endpoints:
            node_id = endpoint.get("node_id")
            if not node_id or node_id == self.node_id:
                continue

            # Store in gossip-learned endpoints for later connection attempts
            host = endpoint.get("tailscale_ip") or endpoint.get("host")
            port = endpoint.get("port", DEFAULT_PORT)

            if host and port:
                self._gossip_learned_endpoints[node_id] = {
                    "host": host,
                    "port": port,
                    "tailscale_ip": endpoint.get("tailscale_ip", ""),
                    "last_heartbeat": endpoint.get("last_heartbeat", 0),
                    "learned_at": time.time(),
                }

                # If this is an unknown peer, try to connect
                if node_id not in self.peers:
                    # Queue for async connection attempt
                    asyncio.create_task(self._try_connect_gossip_peer(node_id, host, port))

    async def _try_connect_gossip_peer(self, node_id: str, host: str, port: int) -> None:
        """Phase 28: Attempt to connect to a peer learned via gossip."""
        try:
            # Check if already connected
            if node_id in self.peers and self.peers[node_id].is_alive():
                return

            logger.info(f"Attempting connection to gossip-learned peer: {node_id} at {host}:{port}")

            # Try to send heartbeat
            info = await self._send_heartbeat_to_peer(host, port)
            if info:
                with self.peers_lock:
                    self.peers[info.node_id] = info
                logger.info(f"Successfully connected to gossip-learned peer: {info.node_id}")

                # Save to cache for future restarts
                self._save_peer_to_cache(
                    info.node_id, host, port,
                    str(getattr(info, "tailscale_ip", "") or "")
                )
        except Exception as e:
            if self.verbose:
                logger.debug(f"Failed to connect to gossip-learned peer {node_id}: {e}")

    def _handle_incoming_cluster_epoch(self, incoming_epoch: Any, response: dict) -> None:
        """Phase 29: Handle incoming cluster epoch for split-brain resolution."""
        try:
            epoch = int(incoming_epoch)
        except (ValueError, TypeError):
            return

        if epoch > self._cluster_epoch:
            # Accept higher epoch - this cluster partition is more authoritative
            logger.info(f"Adopting higher cluster epoch: {epoch} (was {self._cluster_epoch})")
            self._cluster_epoch = epoch
            self._save_cluster_epoch()

            # If response includes a leader, adopt it
            sender_state = response.get("sender_state", {})
            incoming_leader = sender_state.get("leader_id")
            if incoming_leader and incoming_leader != self.node_id:
                if self.role == NodeRole.LEADER:
                    logger.info(f"Stepping down: higher epoch cluster has leader {incoming_leader}")
                    self.role = NodeRole.FOLLOWER
                self.leader_id = incoming_leader

    def _record_gossip_metrics(self, event: str, peer_id: str | None = None, latency_ms: float = 0):
        """Record gossip protocol metrics for monitoring.

        GOSSIP METRICS: Track propagation efficiency and protocol health.
        - message_sent: Gossip messages sent
        - message_received: Gossip messages received
        - state_updates: Number of state updates from gossip
        - propagation_delay_ms: Average latency for gossip messages
        - anti_entropy_repairs: Full state reconciliations triggered
        """
        if not hasattr(self, "_gossip_metrics"):
            self._gossip_metrics = {
                "message_sent": 0,
                "message_received": 0,
                "state_updates": 0,
                "propagation_delay_ms": [],
                "anti_entropy_repairs": 0,
                "stale_states_detected": 0,
                "last_reset": time.time(),
            }

        metrics = self._gossip_metrics

        if event == "sent":
            metrics["message_sent"] += 1
        elif event == "received":
            metrics["message_received"] += 1
        elif event == "update":
            metrics["state_updates"] += 1
        elif event == "anti_entropy":
            metrics["anti_entropy_repairs"] += 1
        elif event == "stale":
            metrics["stale_states_detected"] += 1
        elif event == "latency":
            # Keep last 100 latency measurements
            metrics["propagation_delay_ms"].append(latency_ms)
            if len(metrics["propagation_delay_ms"]) > 100:
                metrics["propagation_delay_ms"] = metrics["propagation_delay_ms"][-100:]

        # Reset metrics every hour
        if time.time() - metrics["last_reset"] > 3600:
            old_metrics = metrics.copy()
            self._gossip_metrics = {
                "message_sent": 0,
                "message_received": 0,
                "state_updates": 0,
                "propagation_delay_ms": [],
                "anti_entropy_repairs": 0,
                "stale_states_detected": 0,
                "last_reset": time.time(),
            }
            # Log metrics before reset
            avg_latency = sum(old_metrics["propagation_delay_ms"]) / max(1, len(old_metrics["propagation_delay_ms"]))
            logger.debug(f"[GOSSIP] Hourly: sent={old_metrics['message_sent']} recv={old_metrics['message_received']} "
                  f"updates={old_metrics['state_updates']} repairs={old_metrics['anti_entropy_repairs']} "
                  f"stale={old_metrics['stale_states_detected']} avg_latency={avg_latency:.1f}ms")
            # ALERTING: Notify on high gossip latency (>1000ms average)
            if avg_latency > 1000 and len(old_metrics["propagation_delay_ms"]) > 10:
                asyncio.create_task(self.notifier.send(
                    title="High Gossip Latency",
                    message=f"Average gossip latency {avg_latency:.0f}ms exceeds 1000ms threshold",
                    level="warning",
                    fields={
                        "Avg Latency": f"{avg_latency:.0f}ms",
                        "Messages Sent": str(old_metrics['message_sent']),
                        "Stale States": str(old_metrics['stale_states_detected']),
                        "Repairs": str(old_metrics['anti_entropy_repairs']),
                    },
                    node_id=self.node_id,
                ))

    def _record_gossip_compression(self, original_size: int, compressed_size: int):
        """Record gossip compression metrics.

        COMPRESSION METRICS: Track how effective compression is for gossip messages.
        Typical JSON gossip payloads compress 60-80% with gzip level 6.
        """
        if not hasattr(self, "_gossip_compression_stats"):
            self._gossip_compression_stats = {
                "total_original_bytes": 0,
                "total_compressed_bytes": 0,
                "messages_compressed": 0,
            }

        stats = self._gossip_compression_stats
        stats["total_original_bytes"] += original_size
        stats["total_compressed_bytes"] += compressed_size
        stats["messages_compressed"] += 1

    def _get_gossip_metrics_summary(self) -> dict:
        """Get summary of gossip metrics for /status endpoint."""
        metrics = getattr(self, "_gossip_metrics", {})
        delays = metrics.get("propagation_delay_ms", [])

        # Include compression stats
        compression = getattr(self, "_gossip_compression_stats", {})
        original = compression.get("total_original_bytes", 0)
        compressed = compression.get("total_compressed_bytes", 0)
        compression_ratio = 1.0 - (compressed / original) if original > 0 else 0

        return {
            "message_sent": metrics.get("message_sent", 0),
            "message_received": metrics.get("message_received", 0),
            "state_updates": metrics.get("state_updates", 0),
            "anti_entropy_repairs": metrics.get("anti_entropy_repairs", 0),
            "stale_states_detected": metrics.get("stale_states_detected", 0),
            "avg_latency_ms": sum(delays) / max(1, len(delays)) if delays else 0,
            "compression_ratio": round(compression_ratio, 3),
            "bytes_saved_kb": round((original - compressed) / 1024, 2),
        }

    async def _gossip_anti_entropy_repair(self):
        """DECENTRALIZED: Periodic full state reconciliation with random peer.

        ANTI-ENTROPY REPAIR: Gossip protocols can miss updates due to:
        - Network partitions
        - Message loss
        - Node restarts

        Solution: Periodically do full state exchange with a random peer to
        ensure eventual consistency. This catches any missed updates.

        Frequency: Every 5 minutes with a random healthy peer
        """
        now = time.time()

        # Rate limit: anti-entropy every 5 minutes
        last_repair = getattr(self, "_last_anti_entropy_repair", 0)
        if now - last_repair < 300:
            return
        self._last_anti_entropy_repair = now

        # Select a random healthy peer for full state exchange
        with self.peers_lock:
            alive_peers = [
                p for p in self.peers.values()
                if p.is_alive() and not getattr(p, "retired", False)
            ]

        if not alive_peers:
            return

        import random
        peer = random.choice(alive_peers)

        # Prepare full state dump (not just recent states)
        full_state = {
            "anti_entropy": True,  # Flag for full state exchange
            "sender": self.node_id,
            "timestamp": now,
            "all_known_states": {},
        }

        # Include all known peer states (not just recent)
        gossip_states = getattr(self, "_gossip_peer_states", {})
        for node_id, state in gossip_states.items():
            full_state["all_known_states"][node_id] = state

        # Include our own state
        self._update_self_info()
        full_state["all_known_states"][self.node_id] = {
            "node_id": self.node_id,
            "timestamp": now,
            "version": int(now * 1000),
            "role": self.role.value if hasattr(self.role, "value") else str(self.role),
            "leader_id": self.leader_id,
            "selfplay_jobs": getattr(self.self_info, "selfplay_jobs", 0),
            "training_jobs": getattr(self.self_info, "training_jobs", 0),
        }

        # Send anti-entropy request
        start_time = time.time()
        timeout = ClientTimeout(total=10)  # Longer timeout for full exchange
        try:
            async with get_client_session(timeout) as session:
                for url in self._urls_for_peer(peer, "/gossip/anti-entropy"):
                    try:
                        async with session.post(url, json=full_state, headers=self._auth_headers()) as resp:
                            if resp.status == 200:
                                response_data = await resp.json()
                                latency = (time.time() - start_time) * 1000
                                self._record_gossip_metrics("latency", peer.node_id, latency)

                                # Process peer's full state
                                peer_states = response_data.get("all_known_states", {})
                                updates = 0
                                for node_id, state in peer_states.items():
                                    if node_id == self.node_id:
                                        continue
                                    existing = self._gossip_peer_states.get(node_id, {})
                                    if state.get("version", 0) > existing.get("version", 0):
                                        self._gossip_peer_states[node_id] = state
                                        updates += 1
                                        self._record_gossip_metrics("update", node_id)

                                # Check for stale states we have that peer doesn't know
                                our_nodes = set(self._gossip_peer_states.keys())
                                peer_nodes = set(peer_states.keys())
                                stale_candidates = our_nodes - peer_nodes - {self.node_id}

                                for stale_node in stale_candidates:
                                    stale_state = self._gossip_peer_states.get(stale_node, {})
                                    # If state is older than 10 minutes and peer doesn't know it,
                                    # the node might be offline - mark as stale
                                    if stale_state.get("timestamp", 0) < now - 600:
                                        self._record_gossip_metrics("stale", stale_node)

                                if updates > 0:
                                    self._record_gossip_metrics("anti_entropy")
                                    logger.debug(f"[GOSSIP] Anti-entropy repair: {updates} state updates from {peer.node_id}")

                                return
                    except Exception:
                        continue
        except Exception:
            pass  # Silent failure, will retry next cycle

    # =========================================================================
    # DISTRIBUTED TRAINING COORDINATION
    # =========================================================================
    # These functions enable nodes to coordinate training decisions without
    # relying on a leader, using gossip to share training state cluster-wide.
    # =========================================================================

    def _get_local_active_training_configs(self) -> list[dict]:
        """Get list of training configs currently running on this node.

        DISTRIBUTED TRAINING: Share what training this node is doing so other
        nodes can avoid duplicate training for the same configuration.

        Returns list of dicts with:
        - config_key: e.g. "square8_2p"
        - job_type: "nnue", "cmaes", etc.
        - started_at: timestamp when training started
        """
        active_configs = []
        with self.jobs_lock:
            for _job_id, job in self.local_jobs.items():
                job_type = getattr(job, "job_type", "")
                # Only include training-type jobs
                if job_type in ("nnue", "nnue_training", "training", "cmaes"):
                    board_type = getattr(job, "board_type", "")
                    num_players = getattr(job, "num_players", 2)
                    if board_type:
                        config_key = f"{board_type}_{num_players}p"
                        started_at = getattr(job, "started_at", time.time())
                        active_configs.append({
                            "config_key": config_key,
                            "job_type": job_type,
                            "started_at": started_at,
                        })
        return active_configs

    def _get_cluster_active_training_configs(self) -> dict[str, list[str]]:
        """Get all active training configs across the cluster via gossip.

        DISTRIBUTED TRAINING COORDINATION: Query gossip state to see what
        training is running cluster-wide. This enables nodes to avoid
        duplicate training without leader coordination.

        Returns: { config_key -> [list of node_ids training that config] }
        """
        cluster_configs: dict[str, list[str]] = {}

        # Include our own training
        for config in self._get_local_active_training_configs():
            config_key = config["config_key"]
            if config_key not in cluster_configs:
                cluster_configs[config_key] = []
            cluster_configs[config_key].append(self.node_id)

        # Include training from gossip state
        gossip_states = getattr(self, "_gossip_peer_states", {})
        now = time.time()
        for node_id, state in gossip_states.items():
            # Skip stale states (older than 2 minutes)
            if state.get("timestamp", 0) < now - 120:
                continue
            # Skip our own state
            if node_id == self.node_id:
                continue

            active_training = state.get("active_training_configs", [])
            for config in active_training:
                config_key = config.get("config_key", "")
                if config_key:
                    if config_key not in cluster_configs:
                        cluster_configs[config_key] = []
                    if node_id not in cluster_configs[config_key]:
                        cluster_configs[config_key].append(node_id)

        return cluster_configs

    def _is_config_being_trained_cluster_wide(self, config_key: str) -> tuple[bool, list[str]]:
        """Check if a config is already being trained somewhere in the cluster.

        DISTRIBUTED TRAINING: Before starting training for a config, check if
        another node is already training it. This avoids wasted resources.

        Returns: (is_being_trained, list_of_nodes_training_it)
        """
        cluster_configs = self._get_cluster_active_training_configs()
        training_nodes = cluster_configs.get(config_key, [])
        return (len(training_nodes) > 0, training_nodes)

    def _should_claim_training_slot(self, config_key: str) -> bool:
        """Decide if this node should claim a training slot for a config.

        DISTRIBUTED TRAINING COORDINATION: Use a deterministic algorithm to
        decide which node gets to train a config when multiple nodes want to.

        Algorithm:
        - If no one is training this config, the node with lowest ID claims it
        - If already training, don't start a duplicate
        - Include jitter to handle race conditions
        """
        is_training, _training_nodes = self._is_config_being_trained_cluster_wide(config_key)

        if is_training:
            # Config is already being trained somewhere
            return False

        # Get all nodes that might want to train (GPU nodes with data)
        candidate_nodes = [self.node_id]
        gossip_states = getattr(self, "_gossip_peer_states", {})
        now = time.time()
        for node_id, state in gossip_states.items():
            if state.get("timestamp", 0) < now - 120:
                continue
            if state.get("has_gpu", False):
                training_jobs = state.get("training_jobs", 0)
                # Only consider nodes with capacity (< 3 training jobs)
                if training_jobs < 3:
                    candidate_nodes.append(node_id)

        # Sort deterministically
        candidate_nodes = sorted(set(candidate_nodes))

        # The node with lowest ID that has capacity claims the slot
        # Add position-based jitter: higher position = less likely to claim
        import random
        my_position = candidate_nodes.index(self.node_id) if self.node_id in candidate_nodes else len(candidate_nodes)

        # First candidate always claims, others have decreasing probability
        claim_probability = max(0.1, 1.0 - (my_position * 0.3))

        return random.random() < claim_probability

    def _get_distributed_training_summary(self) -> dict:
        """Get summary of distributed training state for /status endpoint."""
        cluster_configs = self._get_cluster_active_training_configs()
        return {
            "active_configs": list(cluster_configs.keys()),
            "total_training_jobs": sum(len(nodes) for nodes in cluster_configs.values()),
            "configs_by_node_count": {k: len(v) for k, v in cluster_configs.items()},
        }

    # =========================================================================
    # DISTRIBUTED ELO
    # =========================================================================
    # Share ELO ratings via gossip for cluster-wide visibility without
    # requiring every node to query the ELO database directly.
    # =========================================================================

    def _get_local_elo_summary(self) -> dict:
        """Get summary of local ELO ratings for gossip propagation.

        DISTRIBUTED ELO: Share top models and their ratings via gossip so all
        nodes have visibility into model performance without querying the DB.

        LAZY LOADING: Defers ELO query until after startup (60s) to avoid
        slowing node initialization. Uses 10-minute cache to reduce DB load.

        Returns dict with:
        - top_models: List of top 5 models with ratings
        - total_models: Total number of rated models
        - last_update: Timestamp of last ELO update
        """
        now = time.time()
        cache_key = "_elo_summary_cache"
        cache_time_key = "_elo_summary_cache_time"
        startup_key = "_elo_startup_time"
        cached = getattr(self, cache_key, None)
        cached_time = getattr(self, cache_time_key, 0)

        # Track startup time for lazy loading
        if not hasattr(self, startup_key):
            setattr(self, startup_key, now)

        startup_time = getattr(self, startup_key, now)

        # LAZY LOADING: Don't query ELO during first 60s of startup
        if now - startup_time < 60:
            return {"top_models": [], "total_models": 0, "last_update": 0, "deferred": True}

        # Use 10-minute cache (was 5 min) to reduce DB load
        if cached and now - cached_time < 600:
            return cached

        summary = {
            "top_models": [],
            "total_models": 0,
            "last_update": 0,
        }

        try:
            from app.tournament import get_elo_database
            db = get_elo_database()

            # Get top 5 models by ELO (single optimized query)
            with db._get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT model_name, elo_rating, games_played, last_updated,
                           (SELECT COUNT(*) FROM elo_ratings) as total,
                           (SELECT MAX(last_updated) FROM elo_ratings) as max_updated
                    FROM elo_ratings
                    ORDER BY elo_rating DESC
                    LIMIT 5
                """)
                rows = cursor.fetchall()

                if rows:
                    summary["total_models"] = rows[0][4] if rows[0][4] else 0
                    summary["last_update"] = rows[0][5] if rows[0][5] else 0

                for row in rows:
                    summary["top_models"].append({
                        "model": row[0],
                        "elo": round(row[1]),
                        "games": row[2],
                    })

        except Exception:
            # Silently fail - ELO summary is optional
            pass

        # Cache the result
        setattr(self, cache_key, summary)
        setattr(self, cache_time_key, now)

        return summary

    def _get_cluster_elo_summary(self) -> dict:
        """Get cluster-wide ELO summary from gossip state.

        DISTRIBUTED ELO: Aggregate ELO info from all nodes via gossip to get
        a cluster-wide view of model performance.
        """
        all_models = {}
        gossip_states = getattr(self, "_gossip_peer_states", {})
        now = time.time()

        # Include our own ELO summary
        local_summary = self._get_local_elo_summary()
        for model_info in local_summary.get("top_models", []):
            model_name = model_info.get("model", "")
            if model_name:
                all_models[model_name] = model_info

        # Include ELO summaries from gossip
        for _node_id, state in gossip_states.items():
            if state.get("timestamp", 0) < now - 300:  # Skip stale states
                continue

            elo_summary = state.get("elo_summary", {})
            for model_info in elo_summary.get("top_models", []):
                model_name = model_info.get("model", "")
                if model_name:
                    # Keep highest ELO seen for each model
                    existing = all_models.get(model_name, {})
                    if model_info.get("elo", 0) > existing.get("elo", 0):
                        all_models[model_name] = model_info

        # Sort by ELO and return top 10
        sorted_models = sorted(all_models.values(), key=lambda x: x.get("elo", 0), reverse=True)
        return {
            "top_models": sorted_models[:10],
            "total_unique_models": len(all_models),
        }

    # =========================================================================
    # AUTOMATIC NODE RECOVERY
    # =========================================================================
    # Detect stuck/unhealthy nodes via gossip and trigger automatic recovery
    # (service restart) to maintain cluster health without manual intervention.
    # =========================================================================

    async def _check_node_recovery(self):
        """DECENTRALIZED: Detect and recover stuck nodes via gossip.

        AUTOMATIC NODE RECOVERY: Uses gossip to detect nodes that are:
        - Unresponsive (stale gossip timestamp)
        - Stuck (high failure count, no job progress)
        - Resource-exhausted (high disk/memory)

        Recovery actions:
        - SSH to node and restart the ringrift-p2p service
        - Only leader attempts recovery to avoid duplicate restarts
        - Rate limit recovery attempts (one per node per 10 minutes)
        """
        # Only leader performs recovery to avoid duplicate restarts
        if self.role != NodeRole.LEADER:
            return

        now = time.time()

        # Rate limit: check every 2 minutes
        last_check = getattr(self, "_last_node_recovery_check", 0)
        if now - last_check < 120:
            return
        self._last_node_recovery_check = now

        # Initialize recovery tracking
        if not hasattr(self, "_node_recovery_attempts"):
            self._node_recovery_attempts = {}  # node_id -> last_attempt_time
        if not hasattr(self, "_node_recovery_metrics"):
            self._node_recovery_metrics = {"attempts": 0, "successes": 0, "failures": 0}

        # Check each peer for health issues
        gossip_states = getattr(self, "_gossip_peer_states", {})
        nodes_to_recover = []

        with self.peers_lock:
            for node_id, peer in self.peers.items():
                if node_id == self.node_id:
                    continue

                # Skip recently recovered nodes (10 minute cooldown)
                last_attempt = self._node_recovery_attempts.get(node_id, 0)
                if now - last_attempt < 600:
                    continue

                # Check for unhealthy indicators
                needs_recovery = False
                reason = ""

                # 1. Peer not alive (no recent heartbeat)
                if not peer.is_alive():
                    needs_recovery = True
                    reason = "not responding to heartbeat"

                # 2. Stale gossip state (no updates in 5 minutes)
                elif node_id in gossip_states:
                    state = gossip_states[node_id]
                    state_age = now - state.get("timestamp", 0)
                    if state_age > 300:
                        needs_recovery = True
                        reason = f"stale gossip ({int(state_age)}s old)"

                # 3. High consecutive failures
                elif getattr(peer, "consecutive_failures", 0) >= 5:
                    needs_recovery = True
                    reason = f"high failure count ({peer.consecutive_failures})"

                # 4. Disk nearly full (>95%)
                elif getattr(peer, "disk_percent", 0) > 95:
                    needs_recovery = True
                    reason = f"disk full ({peer.disk_percent}%)"

                if needs_recovery:
                    nodes_to_recover.append((node_id, peer, reason))

        # Attempt recovery for identified nodes (max 2 per cycle)
        for node_id, peer, reason in nodes_to_recover[:2]:
            logger.info(f"NODE RECOVERY: Attempting to recover {node_id} ({reason})")
            self._node_recovery_attempts[node_id] = now
            self._node_recovery_metrics["attempts"] += 1

            # ALERTING: Notify on node recovery attempt
            asyncio.create_task(self.notifier.send(
                title="Node Recovery Initiated",
                message=f"Attempting to recover node {node_id}: {reason}",
                level="warning",
                fields={
                    "Node": node_id,
                    "Reason": reason,
                    "Host": getattr(peer, "host", "unknown"),
                },
                node_id=self.node_id,
            ))

            success = await self._attempt_node_recovery(node_id, peer)
            if success:
                self._node_recovery_metrics["successes"] += 1
                logger.info(f"NODE RECOVERY: Successfully restarted {node_id}")
                # ALERTING: Notify on successful recovery
                asyncio.create_task(self.notifier.send(
                    title="Node Recovery Success",
                    message=f"Successfully recovered node {node_id}",
                    level="info",
                    fields={"Node": node_id, "Reason": reason},
                    node_id=self.node_id,
                ))
            else:
                self._node_recovery_metrics["failures"] += 1
                logger.info(f"NODE RECOVERY: Failed to restart {node_id}")
                # ALERTING: Notify on failed recovery
                asyncio.create_task(self.notifier.send(
                    title="Node Recovery Failed",
                    message=f"Failed to recover node {node_id} ({reason})",
                    level="error",
                    fields={
                        "Node": node_id,
                        "Reason": reason,
                        "Action": "Manual intervention may be required",
                    },
                    node_id=self.node_id,
                ))

    async def _attempt_node_recovery(self, node_id: str, peer) -> bool:
        """Attempt to recover a node by restarting its service via SSH.

        Returns True if recovery command succeeded, False otherwise.
        """
        host = getattr(peer, "host", None)
        if not host:
            return False

        try:
            pass

            # Try to restart the service via SSH
            cmd = f"timeout 30 ssh -o ConnectTimeout=10 -o StrictHostKeyChecking=no {host} 'sudo systemctl restart ringrift-p2p'"
            proc = await asyncio.create_subprocess_shell(
                cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            _stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=45)

            if proc.returncode == 0:
                return True
            else:
                logger.info(f"NODE RECOVERY: SSH failed for {node_id}: {stderr.decode()[:100]}")
                return False

        except asyncio.TimeoutError:
            logger.info(f"NODE RECOVERY: SSH timeout for {node_id}")
            return False
        except Exception as e:
            logger.info(f"NODE RECOVERY: Error recovering {node_id}: {e}")
            return False

    def _get_node_recovery_metrics(self) -> dict:
        """Get node recovery metrics for /status endpoint."""
        metrics = getattr(self, "_node_recovery_metrics", {"attempts": 0, "successes": 0, "failures": 0})
        attempts = getattr(self, "_node_recovery_attempts", {})
        now = time.time()

        # Count nodes in recovery cooldown
        in_cooldown = sum(1 for t in attempts.values() if now - t < 600)

        return {
            "total_attempts": metrics.get("attempts", 0),
            "successes": metrics.get("successes", 0),
            "failures": metrics.get("failures", 0),
            "nodes_in_cooldown": in_cooldown,
        }

    # =========================================================================
    # GOSSIP-BASED LEADER HINTS
    # =========================================================================
    # Share leader preferences via gossip to enable faster leader elections.
    # When current leader fails, nodes can quickly converge on a new leader
    # based on hints from peers rather than running full election.
    # =========================================================================

    def _get_leader_hint(self) -> dict:
        """Get this node's leader hint for gossip propagation.

        LEADER HINTS: Share information about preferred leader candidates to
        enable faster convergence during elections. Hints include:
        - Current known leader and lease expiry
        - Preferred successor (highest-priority eligible node)
        - This node's priority rank
        """
        hint = {
            "current_leader": self.leader_id,
            "lease_expires": getattr(self, "leader_lease_expires", 0),
            "preferred_successor": None,
            "my_priority": 0,
        }

        # Calculate this node's priority (lower is better for Bully algorithm)
        # But we want to express it as a score (higher is better)
        with self.peers_lock:
            all_nodes = [self.node_id] + [p.node_id for p in self.peers.values() if p.is_alive()]

        all_nodes_sorted = sorted(all_nodes, reverse=True)  # Bully: higher ID wins
        if self.node_id in all_nodes_sorted:
            hint["my_priority"] = len(all_nodes_sorted) - all_nodes_sorted.index(self.node_id)

        # Find preferred successor (highest priority eligible node that's not current leader)
        voter_ids = list(getattr(self, "voter_node_ids", []) or [])
        for node_id in all_nodes_sorted:
            if node_id == self.leader_id:
                continue
            if voter_ids and node_id not in voter_ids:
                continue
            hint["preferred_successor"] = node_id
            break

        return hint

    def _get_cluster_leader_consensus(self) -> dict:
        """Get cluster consensus on leader from gossip hints.

        LEADER CONSENSUS: Aggregate leader hints from all nodes to determine
        if there's agreement on who the leader is/should be.
        """
        leader_votes = {}
        successor_votes = {}
        gossip_states = getattr(self, "_gossip_peer_states", {})
        now = time.time()

        # Count our vote
        our_hint = self._get_leader_hint()
        if our_hint["current_leader"]:
            leader_votes[our_hint["current_leader"]] = leader_votes.get(our_hint["current_leader"], 0) + 1
        if our_hint["preferred_successor"]:
            successor_votes[our_hint["preferred_successor"]] = successor_votes.get(our_hint["preferred_successor"], 0) + 1

        # Count votes from gossip
        for _node_id, state in gossip_states.items():
            if state.get("timestamp", 0) < now - 120:  # Skip stale states
                continue

            hint = state.get("leader_hint", {})
            leader = hint.get("current_leader")
            successor = hint.get("preferred_successor")

            if leader:
                leader_votes[leader] = leader_votes.get(leader, 0) + 1
            if successor:
                successor_votes[successor] = successor_votes.get(successor, 0) + 1

        # Find consensus leader and successor
        consensus_leader = max(leader_votes.items(), key=lambda x: x[1])[0] if leader_votes else None
        consensus_successor = max(successor_votes.items(), key=lambda x: x[1])[0] if successor_votes else None

        result = {
            "consensus_leader": consensus_leader,
            "leader_agreement": leader_votes.get(consensus_leader, 0) if consensus_leader else 0,
            "consensus_successor": consensus_successor,
            "successor_agreement": successor_votes.get(consensus_successor, 0) if consensus_successor else 0,
            "total_voters": len(gossip_states) + 1,
        }

        # ALERTING: Check for low leader consensus (only leader alerts, rate limited)
        if self.role == NodeRole.LEADER and result["total_voters"] >= 3:
            agreement_ratio = result["leader_agreement"] / result["total_voters"]
            last_low_consensus_alert = getattr(self, "_last_low_consensus_alert", 0)
            if agreement_ratio < 0.5 and now - last_low_consensus_alert > 3600:  # Alert once per hour max
                self._last_low_consensus_alert = now
                asyncio.create_task(self.notifier.send(
                    title="Low Leader Consensus",
                    message=f"Only {result['leader_agreement']}/{result['total_voters']} nodes agree on leader",
                    level="warning",
                    fields={
                        "Agreement": f"{agreement_ratio*100:.0f}%",
                        "Consensus Leader": str(consensus_leader),
                        "Total Voters": str(result["total_voters"]),
                        "Action": "Check for network partitions or stale nodes",
                    },
                    node_id=self.node_id,
                ))

        return result

    # =========================================================================
    # PEER REPUTATION TRACKING
    # =========================================================================
    # Track peer reliability over time for better peer selection in P2P sync,
    # gossip, and other distributed operations.
    # =========================================================================

    def _record_peer_interaction(self, peer_id: str, success: bool, interaction_type: str = "general"):
        """Record a peer interaction for reputation tracking.

        PEER REPUTATION: Track success/failure rates for different interaction types:
        - sync: File sync operations
        - gossip: Gossip message exchanges
        - heartbeat: Heartbeat responses
        - command: Remote command executions
        """
        if not hasattr(self, "_peer_reputation"):
            self._peer_reputation = {}

        if peer_id not in self._peer_reputation:
            self._peer_reputation[peer_id] = {
                "total_success": 0,
                "total_failure": 0,
                "recent_success": 0,
                "recent_failure": 0,
                "last_success": 0,
                "last_failure": 0,
                "last_reset": time.time(),
                "by_type": {},
            }

        rep = self._peer_reputation[peer_id]
        now = time.time()

        # Reset recent counters every hour
        if now - rep["last_reset"] > 3600:
            rep["recent_success"] = 0
            rep["recent_failure"] = 0
            rep["last_reset"] = now

        if success:
            rep["total_success"] += 1
            rep["recent_success"] += 1
            rep["last_success"] = now
        else:
            rep["total_failure"] += 1
            rep["recent_failure"] += 1
            rep["last_failure"] = now

        # Track by type
        if interaction_type not in rep["by_type"]:
            rep["by_type"][interaction_type] = {"success": 0, "failure": 0}
        if success:
            rep["by_type"][interaction_type]["success"] += 1
        else:
            rep["by_type"][interaction_type]["failure"] += 1

    def _get_peer_reputation_score(self, peer_id: str) -> float:
        """Get reputation score for a peer (0-100, higher is better).

        PEER REPUTATION SCORE: Combines multiple factors:
        - Recent success rate (70% weight) - last hour
        - Historical success rate (20% weight) - all time
        - Recency bonus (10% weight) - recent activity
        """
        if not hasattr(self, "_peer_reputation"):
            return 50.0  # Default neutral score

        rep = self._peer_reputation.get(peer_id)
        if not rep:
            return 50.0

        now = time.time()

        # Recent success rate (last hour)
        recent_total = rep["recent_success"] + rep["recent_failure"]
        recent_rate = rep["recent_success"] / max(1, recent_total)

        # Historical success rate
        total = rep["total_success"] + rep["total_failure"]
        historical_rate = rep["total_success"] / max(1, total)

        # Recency bonus (active peers get a boost)
        last_interaction = max(rep["last_success"], rep["last_failure"])
        recency_hours = (now - last_interaction) / 3600 if last_interaction > 0 else 24
        recency_score = max(0, 1.0 - (recency_hours / 24))  # Decays over 24 hours

        # Weighted score
        score = (recent_rate * 70) + (historical_rate * 20) + (recency_score * 10)

        return min(100.0, max(0.0, score))

    def _get_peer_reputation_summary(self) -> dict:
        """Get summary of peer reputation for gossip propagation.

        Share top/bottom peers by reputation to help cluster converge on
        reliable peer selection.
        """
        if not hasattr(self, "_peer_reputation"):
            return {"reliable_peers": [], "unreliable_peers": []}

        scores = []
        for peer_id in self._peer_reputation:
            score = self._get_peer_reputation_score(peer_id)
            scores.append((peer_id, score))

        scores.sort(key=lambda x: x[1], reverse=True)

        return {
            "reliable_peers": [{"peer": p, "score": round(s)} for p, s in scores[:5] if s >= 70],
            "unreliable_peers": [{"peer": p, "score": round(s)} for p, s in scores[-3:] if s < 30],
        }

    def _get_cluster_peer_reputation(self) -> dict:
        """Aggregate peer reputation from gossip for cluster-wide view."""
        all_scores = {}
        gossip_states = getattr(self, "_gossip_peer_states", {})
        now = time.time()

        # Include our own reputation data
        local_summary = self._get_peer_reputation_summary()
        for peer_info in local_summary.get("reliable_peers", []):
            peer_id = peer_info["peer"]
            if peer_id not in all_scores:
                all_scores[peer_id] = []
            all_scores[peer_id].append(peer_info["score"])

        # Include reputation from gossip
        for _node_id, state in gossip_states.items():
            if state.get("timestamp", 0) < now - 300:
                continue

            rep_summary = state.get("peer_reputation", {})
            for peer_info in rep_summary.get("reliable_peers", []):
                peer_id = peer_info["peer"]
                if peer_id not in all_scores:
                    all_scores[peer_id] = []
                all_scores[peer_id].append(peer_info["score"])

        # Calculate average scores
        avg_scores = {peer: sum(scores) / len(scores) for peer, scores in all_scores.items() if scores}
        sorted_peers = sorted(avg_scores.items(), key=lambda x: x[1], reverse=True)

        return {
            "most_reliable": [{"peer": p, "avg_score": round(s)} for p, s in sorted_peers[:10]],
            "peers_tracked": len(all_scores),
        }

    # ============================================================================
    # ADAPTIVE SYNC INTERVAL MANAGEMENT
    # ============================================================================
    # Dynamically adjusts P2P sync intervals based on:
    # - Cluster activity (training happening = more frequent sync)
    # - Success rate (failures = back off, successes = speed up)
    # - Data freshness (new data in cluster = more frequent sync)
    # ============================================================================

    def _init_adaptive_sync_intervals(self):
        """Initialize adaptive sync interval tracking."""
        self._adaptive_intervals = {
            "data": P2P_DATA_SYNC_BASE,
            "model": P2P_MODEL_SYNC_BASE,
            "training_db": P2P_TRAINING_DB_SYNC_BASE,
        }
        self._sync_success_streak = {
            "data": 0,
            "model": 0,
            "training_db": 0,
        }
        self._sync_failure_streak = {
            "data": 0,
            "model": 0,
            "training_db": 0,
        }
        self._last_interval_adjustment = 0

    def _get_adaptive_sync_interval(self, sync_type: str) -> float:
        """Get the current adaptive interval for a sync type.

        ADAPTIVE SYNC INTERVALS: Intervals adjust based on:
        1. Cluster activity (training = faster sync for models)
        2. Success rate (failures = back off)
        3. Base/min/max bounds per sync type

        Args:
            sync_type: One of "data", "model", "training_db"

        Returns:
            Current interval in seconds
        """
        if not hasattr(self, "_adaptive_intervals"):
            self._init_adaptive_sync_intervals()

        # Get current interval
        current = self._adaptive_intervals.get(sync_type, P2P_DATA_SYNC_BASE)

        # Apply activity-based adjustment
        activity_factor = self._calculate_cluster_activity_factor()

        # Get bounds for this sync type
        if sync_type == "data":
            min_interval = P2P_DATA_SYNC_MIN
            max_interval = P2P_DATA_SYNC_MAX
        elif sync_type == "model":
            min_interval = P2P_MODEL_SYNC_MIN
            max_interval = P2P_MODEL_SYNC_MAX
        elif sync_type == "training_db":
            min_interval = P2P_TRAINING_DB_SYNC_MIN
            max_interval = P2P_TRAINING_DB_SYNC_MAX
        else:
            min_interval = 120
            max_interval = 600

        # Apply activity factor (0.5-1.0 = active cluster, 1.0-2.0 = idle cluster)
        adjusted = current * activity_factor

        # Clamp to bounds
        return max(min_interval, min(max_interval, adjusted))

    def _calculate_cluster_activity_factor(self) -> float:
        """Calculate cluster activity factor for sync interval adjustment.

        CLUSTER ACTIVITY FACTOR:
        - < 1.0: Active cluster (training, selfplay) = faster sync
        - 1.0: Normal activity
        - > 1.0: Idle cluster = slower sync

        Returns:
            Activity factor (0.5 to 2.0)
        """
        now = time.time()

        # Check training activity (with defensive checks)
        training_active = False
        training_lock = getattr(self, "training_lock", None)
        training_jobs = getattr(self, "training_jobs", {})
        if training_lock and training_jobs:
            try:
                with training_lock:
                    for job in training_jobs.values():
                        if getattr(job, "status", None) == "running":
                            training_active = True
                            break
            except Exception:
                pass

        # Check selfplay activity (count active jobs)
        selfplay_count = 0
        jobs_lock = getattr(self, "jobs_lock", None)
        selfplay_jobs = getattr(self, "selfplay_jobs", {})
        if jobs_lock and selfplay_jobs:
            try:
                with jobs_lock:
                    for job in selfplay_jobs.values():
                        if getattr(job, "status", None) == "running":
                            selfplay_count += 1
            except Exception:
                pass

        # Check recent data generation from gossip
        recent_data = False
        gossip_states = getattr(self, "_gossip_node_states", {}) or {}
        for _node_id, state in gossip_states.items():
            if not isinstance(state, dict):
                continue
            last_game = state.get("last_game_time", 0)
            if now - last_game < 300:  # Game in last 5 min
                recent_data = True
                break

        # Calculate factor
        factor = 1.0

        if training_active:
            factor *= 0.5  # Much faster sync during training
        elif selfplay_count >= 5:
            factor *= 0.7  # Faster sync with active selfplay
        elif selfplay_count > 0:
            factor *= 0.85  # Slightly faster with some activity

        if recent_data:
            factor *= 0.9  # Faster sync when new data available

        # If completely idle (no jobs, stale data), slow down
        if selfplay_count == 0 and not training_active and not recent_data:
            factor *= 1.5

        return max(0.5, min(2.0, factor))

    def _record_sync_result_for_adaptive(self, sync_type: str, success: bool):
        """Record sync result to adjust adaptive intervals.

        ADAPTIVE INTERVAL ADJUSTMENT:
        - On success: reduce interval (speed up) up to min
        - On failure: increase interval (back off) up to max

        Args:
            sync_type: One of "data", "model", "training_db"
            success: Whether sync succeeded
        """
        if not hasattr(self, "_adaptive_intervals"):
            self._init_adaptive_sync_intervals()

        if success:
            self._sync_success_streak[sync_type] = self._sync_success_streak.get(sync_type, 0) + 1
            self._sync_failure_streak[sync_type] = 0

            # After 3 consecutive successes, speed up
            if self._sync_success_streak[sync_type] >= 3:
                current = self._adaptive_intervals[sync_type]
                new_interval = current * P2P_SYNC_SPEEDUP_FACTOR

                # Get min bound
                if sync_type == "data":
                    min_interval = P2P_DATA_SYNC_MIN
                elif sync_type == "model":
                    min_interval = P2P_MODEL_SYNC_MIN
                else:
                    min_interval = P2P_TRAINING_DB_SYNC_MIN

                self._adaptive_intervals[sync_type] = max(min_interval, new_interval)
                self._sync_success_streak[sync_type] = 0  # Reset streak
        else:
            self._sync_failure_streak[sync_type] = self._sync_failure_streak.get(sync_type, 0) + 1
            self._sync_success_streak[sync_type] = 0

            # On any failure, back off
            current = self._adaptive_intervals[sync_type]
            new_interval = current * P2P_SYNC_BACKOFF_FACTOR

            # Get max bound
            if sync_type == "data":
                max_interval = P2P_DATA_SYNC_MAX
            elif sync_type == "model":
                max_interval = P2P_MODEL_SYNC_MAX
            else:
                max_interval = P2P_TRAINING_DB_SYNC_MAX

            self._adaptive_intervals[sync_type] = min(max_interval, new_interval)

    def _get_sync_interval_summary(self) -> dict:
        """Get summary of current adaptive sync intervals for monitoring."""
        if not hasattr(self, "_adaptive_intervals"):
            self._init_adaptive_sync_intervals()

        return {
            "data_interval": round(self._get_adaptive_sync_interval("data")),
            "model_interval": round(self._get_adaptive_sync_interval("model")),
            "training_db_interval": round(self._get_adaptive_sync_interval("training_db")),
            "activity_factor": round(self._calculate_cluster_activity_factor(), 2),
            "data_streak": {
                "success": self._sync_success_streak.get("data", 0),
                "failure": self._sync_failure_streak.get("data", 0),
            },
            "model_streak": {
                "success": self._sync_success_streak.get("model", 0),
                "failure": self._sync_failure_streak.get("model", 0),
            },
        }

    # ============================================================================
    # SELFPLAY DATA DEDUPLICATION
    # ============================================================================
    # Tracks synced files and game IDs to avoid redundant transfers during P2P sync.
    # Uses bloom filter for efficient game ID tracking and file hash caching.
    # ============================================================================

    def _init_data_deduplication(self):
        """Initialize data deduplication tracking."""
        self._synced_file_hashes: set[str] = set()  # Hash -> synced
        self._known_game_ids: set[str] = set()  # Game IDs we have
        self._dedup_stats = {
            "files_skipped": 0,
            "games_skipped": 0,
            "bytes_saved": 0,
            "last_cleanup": time.time(),
        }
        self._dedup_lock = threading.Lock()

    def _record_synced_file(self, file_hash: str, file_size: int):
        """Record a file as synced for deduplication.

        DATA DEDUPLICATION: Track file hashes we've synced to avoid
        re-syncing the same file from different peers.

        Args:
            file_hash: Hash of the synced file
            file_size: Size in bytes (for metrics)
        """
        if not hasattr(self, "_synced_file_hashes"):
            self._init_data_deduplication()

        with self._dedup_lock:
            self._synced_file_hashes.add(file_hash)

    def _is_file_already_synced(self, file_hash: str) -> bool:
        """Check if file was already synced based on hash.

        Args:
            file_hash: Hash to check

        Returns:
            True if file was already synced
        """
        if not hasattr(self, "_synced_file_hashes"):
            self._init_data_deduplication()

        if not file_hash:
            return False

        with self._dedup_lock:
            return file_hash in self._synced_file_hashes

    def _record_game_ids(self, game_ids: list[str]):
        """Record game IDs as known for deduplication.

        GAME ID TRACKING: Track game IDs we have to avoid syncing
        duplicate games from different DB files.

        Args:
            game_ids: List of game IDs to record
        """
        if not hasattr(self, "_known_game_ids"):
            self._init_data_deduplication()

        with self._dedup_lock:
            self._known_game_ids.update(game_ids)

    def _filter_unknown_games(self, game_ids: list[str]) -> list[str]:
        """Filter out game IDs we already have.

        Args:
            game_ids: List of game IDs to check

        Returns:
            List of game IDs we don't have yet
        """
        if not hasattr(self, "_known_game_ids"):
            self._init_data_deduplication()

        with self._dedup_lock:
            return [gid for gid in game_ids if gid not in self._known_game_ids]

    def _record_dedup_skip(self, file_count: int = 0, game_count: int = 0, bytes_saved: int = 0):
        """Record deduplication skip for metrics.

        Args:
            file_count: Number of files skipped
            game_count: Number of games skipped
            bytes_saved: Bytes saved by skipping
        """
        if not hasattr(self, "_dedup_stats"):
            self._init_data_deduplication()

        with self._dedup_lock:
            self._dedup_stats["files_skipped"] += file_count
            self._dedup_stats["games_skipped"] += game_count
            self._dedup_stats["bytes_saved"] += bytes_saved

    def _cleanup_dedup_cache(self, max_age: float = 86400):
        """Clean up old entries from deduplication cache.

        CACHE CLEANUP: Periodically remove old entries to prevent unbounded growth.
        Default: clean entries older than 24 hours.

        Args:
            max_age: Max age in seconds before cleanup
        """
        if not hasattr(self, "_dedup_stats"):
            return

        now = time.time()
        with self._dedup_lock:
            # Only clean once per hour
            if now - self._dedup_stats.get("last_cleanup", 0) < 3600:
                return

            # For now, just limit set sizes (more sophisticated cleanup later)
            if len(self._synced_file_hashes) > 10000:
                # Keep only half (LRU would be better but requires more tracking)
                hashes_list = list(self._synced_file_hashes)
                self._synced_file_hashes = set(hashes_list[-5000:])

            if len(self._known_game_ids) > 100000:
                # Keep only recent half
                ids_list = list(self._known_game_ids)
                self._known_game_ids = set(ids_list[-50000:])

            self._dedup_stats["last_cleanup"] = now

    def _get_dedup_summary(self) -> dict:
        """Get deduplication metrics summary."""
        if not hasattr(self, "_dedup_stats"):
            self._init_data_deduplication()

        with self._dedup_lock:
            return {
                "files_skipped": self._dedup_stats.get("files_skipped", 0),
                "games_skipped": self._dedup_stats.get("games_skipped", 0),
                "bytes_saved_mb": round(self._dedup_stats.get("bytes_saved", 0) / (1024 * 1024), 2),
                "known_file_hashes": len(self._synced_file_hashes),
                "known_game_ids": len(self._known_game_ids),
            }

    # ============================================================================
    # DISTRIBUTED TOURNAMENT SCHEDULING
    # ============================================================================
    # Allows tournaments to be scheduled and coordinated via gossip protocol
    # without requiring a leader. Uses consensus to elect tournament coordinator.
    # ============================================================================

    def _init_distributed_tournament_scheduling(self):
        """Initialize distributed tournament scheduling state."""
        self._tournament_proposals: dict[str, dict] = {}  # proposal_id -> proposal
        self._tournament_votes: dict[str, dict[str, str]] = {}  # proposal_id -> {node_id: vote}
        self._active_tournaments_gossip: dict[str, dict] = {}  # tournament_id -> state
        self._last_tournament_check = 0
        self._tournament_coordination_lock = threading.Lock()

    def _propose_tournament(self, board_type: str = "square8", num_players: int = 2,
                           agent_ids: list[str] | None = None, games_per_pairing: int = 2) -> dict:
        """Create a tournament proposal for gossip-based coordination.

        DISTRIBUTED TOURNAMENT: Instead of requiring leader, any node can propose
        a tournament. The proposal is shared via gossip, and nodes vote to elect
        a coordinator.

        Args:
            board_type: Board type for tournament
            num_players: Number of players per game
            agent_ids: List of agent IDs to include
            games_per_pairing: Games per agent pairing

        Returns:
            Proposal dict with unique ID
        """
        import uuid
        if not hasattr(self, "_tournament_proposals"):
            self._init_distributed_tournament_scheduling()

        proposal_id = f"tourney_prop_{uuid.uuid4().hex[:8]}"
        now = time.time()

        proposal = {
            "proposal_id": proposal_id,
            "proposer": self.node_id,
            "board_type": board_type,
            "num_players": num_players,
            "agent_ids": agent_ids or [],
            "games_per_pairing": games_per_pairing,
            "proposed_at": now,
            "status": "proposed",
            "coordinator": None,  # Elected via consensus
            "votes": {self.node_id: "approve"},  # Proposer auto-approves
        }

        with self._tournament_coordination_lock:
            self._tournament_proposals[proposal_id] = proposal

        logger.info(f"TOURNAMENT: Created proposal {proposal_id} for {len(agent_ids or [])} agents")
        return proposal

    def _vote_on_tournament_proposal(self, proposal_id: str, vote: str = "approve") -> bool:
        """Vote on a tournament proposal.

        DISTRIBUTED VOTING: Nodes vote on proposals. A proposal is approved
        when majority of alive peers approve. The coordinator is the highest-ID
        node among approvers.

        Args:
            proposal_id: ID of proposal to vote on
            vote: "approve" or "reject"

        Returns:
            True if vote was recorded
        """
        if not hasattr(self, "_tournament_proposals"):
            self._init_distributed_tournament_scheduling()

        with self._tournament_coordination_lock:
            if proposal_id not in self._tournament_proposals:
                return False

            proposal = self._tournament_proposals[proposal_id]
            if proposal["status"] != "proposed":
                return False

            proposal["votes"][self.node_id] = vote
            return True

    def _get_tournament_gossip_state(self) -> dict:
        """Get tournament state for gossip propagation.

        TOURNAMENT GOSSIP: Share active tournament info via gossip so nodes
        can coordinate without leader.

        Returns:
            Dict with proposals and active tournaments
        """
        if not hasattr(self, "_tournament_proposals"):
            self._init_distributed_tournament_scheduling()

        now = time.time()

        with self._tournament_coordination_lock:
            # Only share recent proposals (last 10 min)
            active_proposals = {
                pid: p for pid, p in self._tournament_proposals.items()
                if now - p.get("proposed_at", 0) < 600 and p.get("status") == "proposed"
            }

        # Get active distributed tournaments
        active_tournaments = {}
        for tid, state in getattr(self, "distributed_tournament_state", {}).items():
            if hasattr(state, "status") and state.status == "running":
                active_tournaments[tid] = {
                    "job_id": tid,
                    "coordinator": self.node_id,  # We're coordinating if we have it
                    "progress": state.completed_matches / max(1, state.total_matches),
                    "status": state.status,
                }

        return {
            "proposals": list(active_proposals.values()),
            "active": active_tournaments,
            "last_update": now,
        }

    def _process_tournament_gossip(self, node_id: str, tournament_state: dict):
        """Process tournament info received via gossip.

        GOSSIP PROCESSING: When receiving tournament state from peers,
        - Record their proposals and votes
        - Check if any proposals reached consensus
        - Start tournaments that we're elected to coordinate

        Args:
            node_id: ID of node that sent this state
            tournament_state: Tournament state from gossip
        """
        if not hasattr(self, "_tournament_proposals"):
            self._init_distributed_tournament_scheduling()

        if not tournament_state or not isinstance(tournament_state, dict):
            return

        time.time()

        # Process proposals from gossip
        for proposal in tournament_state.get("proposals", []):
            if not isinstance(proposal, dict):
                continue

            proposal_id = proposal.get("proposal_id")
            if not proposal_id:
                continue

            with self._tournament_coordination_lock:
                if proposal_id not in self._tournament_proposals:
                    # New proposal from peer - add it and auto-approve
                    self._tournament_proposals[proposal_id] = proposal.copy()
                    self._tournament_proposals[proposal_id]["votes"][self.node_id] = "approve"
                else:
                    # Merge votes
                    existing = self._tournament_proposals[proposal_id]
                    for voter, vote in proposal.get("votes", {}).items():
                        if voter not in existing["votes"]:
                            existing["votes"][voter] = vote

    def _check_tournament_consensus(self):
        """Check if any tournament proposals have reached consensus.

        CONSENSUS CHECK: A proposal is approved when majority of alive peers approve.
        The coordinator is elected as the highest-ID approving voter node.
        """
        if not hasattr(self, "_tournament_proposals"):
            return

        now = time.time()

        # Get alive peer count for quorum
        with self.peers_lock:
            alive_peers = [p for p in self.peers.values() if p.is_alive()]
        alive_count = len(alive_peers) + 1  # +1 for self

        quorum = (alive_count // 2) + 1

        with self._tournament_coordination_lock:
            for proposal_id, proposal in list(self._tournament_proposals.items()):
                if proposal.get("status") != "proposed":
                    continue

                # Count votes
                approve_votes = [
                    voter for voter, vote in proposal.get("votes", {}).items()
                    if vote == "approve"
                ]

                if len(approve_votes) >= quorum:
                    # Consensus reached! Elect coordinator (highest ID)
                    coordinator = max(approve_votes)
                    proposal["status"] = "approved"
                    proposal["coordinator"] = coordinator

                    logger.info(f"TOURNAMENT: Proposal {proposal_id} approved! "
                          f"Coordinator: {coordinator} ({len(approve_votes)}/{alive_count} votes)")

                    # If we're the coordinator, start the tournament
                    if coordinator == self.node_id:
                        asyncio.create_task(self._start_tournament_from_proposal(proposal))

                # Expire old proposals
                elif now - proposal.get("proposed_at", 0) > 600:
                    proposal["status"] = "expired"

    async def _start_tournament_from_proposal(self, proposal: dict):
        """Start a tournament from an approved proposal.

        COORDINATOR DUTIES: When elected as coordinator, start the tournament
        and manage match distribution to workers.

        Args:
            proposal: Approved proposal dict
        """
        import uuid

        job_id = f"tournament_{uuid.uuid4().hex[:8]}"
        agent_ids = proposal.get("agent_ids", [])

        if len(agent_ids) < 2:
            logger.info("TOURNAMENT: Cannot start - need at least 2 agents")
            return

        # Create round-robin pairings
        pairings = []
        for i, a1 in enumerate(agent_ids):
            for a2 in agent_ids[i+1:]:
                for game_num in range(proposal.get("games_per_pairing", 2)):
                    pairings.append({
                        "agent1": a1,
                        "agent2": a2,
                        "game_num": game_num,
                        "status": "pending",
                    })

        state = DistributedTournamentState(
            job_id=job_id,
            board_type=proposal.get("board_type", "square8"),
            num_players=proposal.get("num_players", 2),
            agent_ids=agent_ids,
            games_per_pairing=proposal.get("games_per_pairing", 2),
            total_matches=len(pairings),
            pending_matches=pairings,
            status="running",
            started_at=time.time(),
            last_update=time.time(),
        )

        # Find workers
        with self.peers_lock:
            workers = [p.node_id for p in self.peers.values() if p.is_healthy()]
        state.worker_nodes = workers

        if not state.worker_nodes:
            logger.info(f"TOURNAMENT: No workers available for {job_id}")
            return

        self.distributed_tournament_state[job_id] = state

        logger.info(f"TOURNAMENT: Started {job_id} from proposal {proposal.get('proposal_id')}: "
              f"{len(agent_ids)} agents, {len(pairings)} matches, {len(workers)} workers")

        # Launch coordinator task
        asyncio.create_task(self._run_distributed_tournament(job_id))

    def _get_distributed_tournament_summary(self) -> dict:
        """Get summary of distributed tournament scheduling for status endpoint."""
        if not hasattr(self, "_tournament_proposals"):
            self._init_distributed_tournament_scheduling()

        with self._tournament_coordination_lock:
            pending_proposals = sum(
                1 for p in self._tournament_proposals.values()
                if p.get("status") == "proposed"
            )
            approved_proposals = sum(
                1 for p in self._tournament_proposals.values()
                if p.get("status") == "approved"
            )

        active_tournaments = sum(
            1 for s in getattr(self, "distributed_tournament_state", {}).values()
            if hasattr(s, "status") and s.status == "running"
        )

        return {
            "pending_proposals": pending_proposals,
            "approved_proposals": approved_proposals,
            "active_tournaments": active_tournaments,
            "enabled": True,
        }

    async def _start_monitoring_if_leader(self):
        """Start Prometheus/Grafana when we become leader (P2P monitoring resilience)."""
        if not self.monitoring_manager:
            return
        if self.role != NodeRole.LEADER:
            return
        if self._monitoring_was_leader:
            return  # Already started

        try:
            # Update peer list for Prometheus config
            with self.peers_lock:
                peer_list = [
                    {"node_id": p.node_id, "host": p.host, "port": getattr(p, "metrics_port", 9091)}
                    for p in self.peers.values()
                    if p.node_id != self.node_id and p.is_healthy()
                ]
            self.monitoring_manager.update_peers(peer_list)

            # Start monitoring services
            success = await self.monitoring_manager.start_as_leader()
            if success:
                logger.info("Monitoring services started on leader node")
                self._monitoring_was_leader = True
            else:
                logger.error("Failed to start monitoring services")
        except Exception as e:
            logger.error(f"starting monitoring services: {e}")

    async def _stop_monitoring_if_not_leader(self):
        """Stop Prometheus/Grafana when we step down from leadership."""
        if not self.monitoring_manager:
            return
        if not self._monitoring_was_leader:
            return  # Never started

        if self.role != NodeRole.LEADER:
            try:
                await self.monitoring_manager.stop()
                logger.info("Monitoring services stopped (no longer leader)")
                self._monitoring_was_leader = False
            except Exception as e:
                logger.error(f"stopping monitoring services: {e}")

    async def _start_p2p_auto_deployer(self):
        """Start P2P auto-deployer when we become leader.

        The auto-deployer ensures P2P orchestrator is running on all cluster nodes.
        This solves the fundamental gap where P2P deployment was manual-only.
        """
        if self.role != NodeRole.LEADER:
            return
        if self._auto_deployer_task is not None:
            return  # Already running

        try:
            from app.coordination.p2p_auto_deployer import P2PAutoDeployer, P2PDeploymentConfig

            config = P2PDeploymentConfig(
                check_interval_seconds=300.0,  # Check every 5 minutes
                min_coverage_percent=90.0,
            )
            self.p2p_auto_deployer = P2PAutoDeployer(config=config)

            # Run as background task
            self._auto_deployer_task = asyncio.create_task(
                self.p2p_auto_deployer.run_daemon(),
                name="p2p_auto_deployer"
            )
            logger.info("P2P Auto-Deployer started (leader responsibility)")
        except Exception as e:
            logger.error(f"Failed to start P2P auto-deployer: {e}")

    async def _stop_p2p_auto_deployer(self):
        """Stop P2P auto-deployer when we step down from leadership."""
        if self.p2p_auto_deployer:
            self.p2p_auto_deployer.stop()
        if self._auto_deployer_task:
            self._auto_deployer_task.cancel()
            try:
                await self._auto_deployer_task
            except asyncio.CancelledError:
                pass
            self._auto_deployer_task = None
        self.p2p_auto_deployer = None
        logger.info("P2P Auto-Deployer stopped")

    async def _update_monitoring_peers(self):
        """Update Prometheus config with current peer list."""
        if not self.monitoring_manager or not self._monitoring_was_leader:
            return
        if self.role != NodeRole.LEADER:
            return

        try:
            with self.peers_lock:
                peer_list = [
                    {"node_id": p.node_id, "host": p.host, "port": getattr(p, "metrics_port", 9091)}
                    for p in self.peers.values()
                    if p.node_id != self.node_id and p.is_healthy()
                ]
            self.monitoring_manager.update_peers(peer_list)
            await self.monitoring_manager.reload_config()
        except Exception as e:
            logger.error(f"updating monitoring peers: {e}")

    async def _renew_leader_lease(self):
        """Renew our leadership lease and broadcast to peers."""
        if self.role != NodeRole.LEADER:
            return
        if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
            logger.info(f"Lost voter quorum; stepping down: {self.node_id}")
            self.role = NodeRole.FOLLOWER
            self.leader_id = None
            self.leader_lease_id = ""
            self.leader_lease_expires = 0.0
            self.last_lease_renewal = 0.0
            self._release_voter_grant_if_self()
            self._save_state()
            return

        now = time.time()
        if now - self.last_lease_renewal < LEADER_LEASE_RENEW_INTERVAL:
            return  # Too soon to renew

        lease_id = str(self.leader_lease_id or "")
        if not lease_id:
            lease_id = f"{self.node_id}_{int(time.time())}_{uuid.uuid4().hex[:8]}"
        lease_expires = await self._acquire_voter_lease_quorum(lease_id, int(LEADER_LEASE_DURATION))
        if getattr(self, "voter_node_ids", []) and not lease_expires:
            # Voter quorum failed - try arbiter fallback before stepping down
            logger.info("Voter lease quorum failed; checking arbiter...")
            arbiter_leader = await self._query_arbiter_for_leader()
            if arbiter_leader == self.node_id:
                # Arbiter still recognizes us as leader - extend lease provisionally
                logger.info("Arbiter confirms us as leader despite quorum failure; continuing with provisional lease")
                lease_expires = now + LEADER_LEASE_DURATION / 2  # Shorter lease until quorum recovers
            elif arbiter_leader:
                # Arbiter says someone else is leader - defer to arbiter
                logger.info(f"Arbiter reports different leader ({arbiter_leader}); stepping down")
                self.role = NodeRole.FOLLOWER
                self.leader_id = arbiter_leader
                self.leader_lease_id = ""
                self.leader_lease_expires = 0.0
                self.last_lease_renewal = 0.0
                self._release_voter_grant_if_self()
                self._save_state()
                return
            else:
                # Arbiter also unreachable - step down to be safe
                logger.error(f"Failed to renew voter lease quorum and arbiter unreachable; stepping down: {self.node_id}")
                self.role = NodeRole.FOLLOWER
                self.leader_id = None
                self.leader_lease_id = ""
                self.leader_lease_expires = 0.0
                self.last_lease_renewal = 0.0
                self._release_voter_grant_if_self()
                self._save_state()
                return

        self.leader_lease_id = lease_id
        self.leader_lease_expires = float(lease_expires or (now + LEADER_LEASE_DURATION))
        self.last_lease_renewal = now

        # Broadcast lease renewal to all peers
        with self.peers_lock:
            peers = list(self.peers.values())

        timeout = ClientTimeout(total=3)
        try:
            async with get_client_session(timeout) as session:
                for peer in peers:
                    if peer.node_id != self.node_id and peer.is_alive():
                        try:
                            url = self._url_for_peer(peer, "/coordinator")
                            await session.post(url, json={
                                "leader_id": self.node_id,
                                "lease_id": self.leader_lease_id,
                                "lease_expires": self.leader_lease_expires,
                                "lease_renewal": True,
                                "voter_node_ids": list(getattr(self, "voter_node_ids", []) or []),
                            }, headers=self._auth_headers())
                        except Exception:
                            pass  # Network errors expected during lease renewal
        except Exception as e:
            logger.info(f"Lease renewal error: {e}")

    def _is_leader_lease_valid(self) -> bool:
        """Check if the current leader's lease is still valid."""
        if not self.leader_id:
            return False
        if self.leader_id == self.node_id:
            # We are leader - check our own lease
            return time.time() < self.leader_lease_expires
        else:
            # Another node is leader - check if we've received recent lease renewal
            # Allow some grace period (2x lease duration) for network delays
            return time.time() < self.leader_lease_expires + LEADER_LEASE_DURATION

    async def _check_and_resolve_split_brain(self) -> bool:
        """Check for split-brain (multiple leaders) and resolve by stepping down if needed.

        LEARNED LESSONS - This addresses the cluster status showing multiple leaders.
        Uses Bully algorithm: highest node_id wins.

        Returns True if we stepped down (caller should skip leadership duties).
        """
        if self.role != NodeRole.LEADER:
            return False

        with self.peers_lock:
            peers_snapshot = [p for p in self.peers.values() if p.node_id != self.node_id]

        conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])

        # Gather all peers claiming to be leader.
        other_leaders = [peer for peer in peers_snapshot if peer.role == NodeRole.LEADER and peer.is_alive()]

        voter_ids = list(getattr(self, "voter_node_ids", []) or [])

        # PROACTIVE VOTER ACK VERIFICATION: Even when no other leaders are visible,
        # periodically verify that voters still acknowledge us as leader.
        # This catches split-brain where we can't see the other partition's leader.
        if not other_leaders and voter_ids:
            now = time.time()
            last_voter_check = float(getattr(self, "_last_voter_ack_check", 0) or 0)
            # Check every 30 seconds (more frequent than lease renewal)
            if now - last_voter_check >= 30:
                self._last_voter_ack_check = now
                leased_leader = await self._determine_leased_leader_from_voters()
                if leased_leader and leased_leader != self.node_id:
                    logger.info(f"VOTER ACK CHECK: Voters grant to {leased_leader}, not us; stepping down")
                    self.role = NodeRole.FOLLOWER
                    self.leader_id = leased_leader
                    self.leader_lease_id = ""
                    self.leader_lease_expires = 0.0
                    self._release_voter_grant_if_self()
                    self._save_state()
                    return True
            return False  # No split-brain detected

        if not other_leaders:
            return False  # No split-brain
        if voter_ids:
            # In quorum-gated clusters, only voters may safely lead.
            if self.node_id not in voter_ids:
                print(
                    f"[P2P] SPLIT-BRAIN detected, but {self.node_id} is not a voter; stepping down."
                )
                self.role = NodeRole.FOLLOWER
                self.leader_id = None
                self.leader_lease_id = ""
                self.leader_lease_expires = 0.0
                self._release_voter_grant_if_self()
                self._save_state()
                return True

            leased_leader = await self._determine_leased_leader_from_voters()
            if leased_leader and leased_leader != self.node_id:
                print(
                    f"[P2P] SPLIT-BRAIN resolved by voter quorum: stepping down for lease-holder {leased_leader}"
                )
                self.role = NodeRole.FOLLOWER
                self.leader_id = leased_leader
                self.leader_lease_id = ""
                self.leader_lease_expires = 0.0
                self._release_voter_grant_if_self()
                self._save_state()
                return True

        # Find the highest-priority *eligible* leader (including ourselves).
        considered_leaders = other_leaders
        if voter_ids:
            # Prefer voter leaders when resolving conflicts; non-voter leaders
            # are treated as noise from older configs/versions.
            voter_leaders = [p for p in other_leaders if p.node_id in voter_ids]
            if voter_leaders:
                considered_leaders = voter_leaders

        eligible_leaders = [p for p in considered_leaders if self._is_leader_eligible(p, conflict_keys)]
        if self._is_leader_eligible(self.self_info, conflict_keys):
            eligible_leaders.append(self.self_info)

        # If none are eligible, fall back to bully ordering (best-effort).
        candidates = eligible_leaders or ([*considered_leaders, self.self_info])
        highest_leader = max(candidates, key=lambda p: p.node_id)

        if highest_leader.node_id != self.node_id:
            # We're not the highest-priority leader - step down
            logger.info(f"SPLIT-BRAIN detected! Found leaders: {[p.node_id for p in other_leaders]}")
            logger.info(f"Stepping down in favor of higher-priority leader: {highest_leader.node_id}")
            self.role = NodeRole.FOLLOWER
            self.leader_id = highest_leader.node_id
            self.leader_lease_id = ""
            self.leader_lease_expires = 0.0
            self._release_voter_grant_if_self()
            self._save_state()
            return True

        # We are the highest - other leaders should step down
        # Send coordinator message to assert our leadership
        logger.info(f"SPLIT-BRAIN detected! Asserting leadership over: {[p.node_id for p in other_leaders]}")
        timeout = ClientTimeout(total=5)
        async with get_client_session(timeout) as session:
            for peer in other_leaders:
                try:
                    url = self._url_for_peer(peer, "/coordinator")
                    await session.post(
                        url,
                        json={
                            "leader_id": self.node_id,
                            "lease_id": self.leader_lease_id,
                            "lease_expires": self.leader_lease_expires,
                            "voter_node_ids": list(getattr(self, "voter_node_ids", []) or []),
                        },
                        headers=self._auth_headers(),
                    )
                except Exception:
                    pass  # Network errors expected during step-down notifications

        return False  # We remain leader

    async def _job_management_loop(self):
        """Manage jobs - leader coordinates cluster, all nodes handle local operations."""
        while self.running:
            try:
                # ==== DECENTRALIZED OPERATIONS (all nodes) ====
                # These run on every node to ensure cluster health even without a leader
                # Makes the cluster resilient to leader instability

                # Local data consolidation: merge siloed job DBs to main selfplay.db
                await self._consolidate_selfplay_data()

                # Local stuck job detection: each node monitors its own processes
                await self._check_local_stuck_jobs()

                # Local resource cleanup: handle disk/memory pressure independently
                await self._local_resource_cleanup()

                # Local job management: start/stop jobs based on node capacity
                await self._manage_local_jobs_decentralized()

                # Local GPU auto-scaling: optimize GPU utilization independently
                await self._local_gpu_auto_scale()

                # Leaderless training fallback: trigger local training if no leader for too long
                await self._check_local_training_fallback()

                # Emergency coordinator: if voter quorum unavailable for >5min, take leadership
                await self._check_emergency_coordinator_fallback()

                # P2P data sync: nodes can sync data directly without leader
                await self._p2p_data_sync()

                # P2P model sync: dedicated model distribution (more frequent)
                await self._p2p_model_sync()

                # P2P training DB sync: sync training databases for diversity
                await self._p2p_training_db_sync()

                # Gossip protocol: share state with random peers
                await self._gossip_state_to_peers()

                # Anti-entropy repair: periodic full state reconciliation
                await self._gossip_anti_entropy_repair()

                # ==== LEADER-ONLY OPERATIONS ====
                if self.role == NodeRole.LEADER:
                    # LEARNED LESSONS - Check for split-brain before acting as leader
                    if await self._check_and_resolve_split_brain():
                        # We stepped down, skip this cycle
                        await asyncio.sleep(JOB_CHECK_INTERVAL)
                        continue

                    await self._manage_cluster_jobs()
                    # Cluster rebalancing: migrate jobs from weak to powerful nodes
                    await self._check_cluster_balance()
                    # Phase 3: Check if training should be triggered automatically
                    await self._check_and_trigger_training()
                    # Phase 5: Check improvement cycles for automated training
                    await self._check_improvement_cycles()
                    # Cluster-wide stuck job detection (remote nodes)
                    await self._check_and_kill_stuck_jobs()
                    # Work queue rebalancing: assign queued work to idle nodes
                    await self._auto_rebalance_from_work_queue()
                    # Self-healing: auto-scale GPU utilization toward 60-80% target
                    await self._auto_scale_gpu_utilization()
                    # Self-healing: probe NAT-blocked peers to check if they've become reachable
                    await self._sweep_nat_recovery()
                    # Self-healing: detect and recover stuck nodes via SSH restart
                    await self._check_node_recovery()
            except Exception as e:
                logger.info(f"Job management error: {e}")

            await asyncio.sleep(JOB_CHECK_INTERVAL)

    async def _check_and_kill_stuck_jobs(self) -> int:
        """Detect and terminate stuck training/selfplay jobs.

        A job is considered stuck if:
        - Training: No log output for 10+ minutes while process still running
        - Selfplay: No new games generated for 15+ minutes

        Returns:
            Number of stuck jobs terminated
        """
        killed = 0
        now = time.time()
        TRAINING_STUCK_THRESHOLD = 600  # 10 minutes
        SELFPLAY_STUCK_THRESHOLD = 900  # 15 minutes

        # Check training jobs
        with self.training_lock:
            training_snapshot = list(self.training_jobs.values())

        for job in training_snapshot:
            if job.status != "running":
                continue
            started = getattr(job, "started_at", 0) or 0
            last_progress = getattr(job, "last_progress_time", started) or started
            if now - last_progress > TRAINING_STUCK_THRESHOLD and now - started > TRAINING_STUCK_THRESHOLD:
                logger.info(f"STUCK DETECTED: Training job {job.job_id} on {job.target_node} - no progress for {int((now - last_progress)/60)}min")
                # Try to kill the process on the target node
                target_node = job.target_node
                if target_node and target_node != self.node_id:
                    await self._remote_kill_stuck_job(target_node, job.job_id, "training")
                else:
                    # Local kill
                    try:
                        import subprocess
                        subprocess.run(["pkill", "-9", "-f", f"train.*{job.job_id}"], timeout=5, capture_output=True)
                    except Exception:
                        pass
                job.status = "failed"
                job.error_message = "Killed: no progress detected"
                job.completed_at = now
                killed += 1
                logger.info(f"Killed stuck training job {job.job_id}")
                # ALERTING: Notify when stuck job is killed
                asyncio.create_task(self.notifier.send(
                    title="Stuck Job Killed",
                    message=f"Training job {job.job_id} killed after no progress for {int((now - last_progress)/60)}min",
                    level="warning",
                    fields={
                        "Job ID": job.job_id,
                        "Type": job.job_type,
                        "Node": job.target_node or "local",
                        "Config": f"{job.board_type}_{job.num_players}p",
                        "Stuck For": f"{int((now - last_progress)/60)} minutes",
                    },
                    node_id=self.node_id,
                ))

        # Check for GPU nodes with 0% GPU but running GPU jobs
        with self.peers_lock:
            peers_snapshot = list(self.peers.values())

        for peer in peers_snapshot:
            if not peer.is_alive():
                continue
            gpu_percent = float(getattr(peer, "gpu_percent", 0) or 0)
            selfplay_jobs = int(getattr(peer, "selfplay_jobs", 0) or 0)
            has_gpu = bool(getattr(peer, "has_gpu", False))

            # Check for stuck GPU selfplay (has GPU, jobs running, but 0% GPU util)
            if has_gpu and selfplay_jobs > 0 and gpu_percent == 0:
                last_gpu_active = getattr(peer, "_last_gpu_active_time", 0)
                if last_gpu_active == 0:
                    peer._last_gpu_active_time = now
                elif now - last_gpu_active > SELFPLAY_STUCK_THRESHOLD:
                    logger.info(f"STUCK DETECTED: {peer.node_id} has {selfplay_jobs} jobs but 0% GPU for {int((now - last_gpu_active)/60)}min")
                    # Don't auto-kill selfplay, just log - might be CPU selfplay
            elif has_gpu and gpu_percent > 5:
                peer._last_gpu_active_time = now

        if killed > 0:
            logger.info(f"Self-healing: killed {killed} stuck job(s)")
        return killed

    async def _check_local_stuck_jobs(self) -> int:
        """DECENTRALIZED: Detect and kill stuck processes on THIS node only.

        Runs on ALL nodes (not just leader) to ensure each node can self-heal
        even when there's no functioning leader in the cluster.

        Detects:
        - GPU selfplay processes with 0% GPU utilization for too long
        - Training processes that haven't made progress
        - Orphaned processes that aren't tracked in local_jobs

        Returns:
            Number of stuck processes terminated
        """
        killed = 0
        now = time.time()
        STUCK_THRESHOLD = 900  # 15 minutes

        # Only check periodically to avoid excessive process scanning
        last_check = getattr(self, "_last_local_stuck_check", 0)
        if now - last_check < 300:  # Check every 5 minutes
            return 0
        self._last_local_stuck_check = now

        # Check if local GPU is at 0% but we have running GPU selfplay jobs
        gpu_percent = float(getattr(self.self_info, "gpu_percent", 0) or 0)
        selfplay_jobs = int(getattr(self.self_info, "selfplay_jobs", 0) or 0)
        has_gpu = bool(getattr(self.self_info, "has_gpu", False))

        if has_gpu and selfplay_jobs > 0 and gpu_percent < 5:
            last_gpu_active = getattr(self, "_local_last_gpu_active", 0)
            if last_gpu_active == 0:
                self._local_last_gpu_active = now
            elif now - last_gpu_active > STUCK_THRESHOLD:
                logger.info(f"LOCAL STUCK: {selfplay_jobs} selfplay jobs but {gpu_percent:.0f}% GPU for {int((now - last_gpu_active)/60)}min")
                # Kill all local GPU selfplay processes and let them restart
                try:
                    import subprocess
                    # Kill gpu_selfplay processes specifically
                    result = subprocess.run(
                        ["pkill", "-9", "-f", "gpu_selfplay"],
                        timeout=10, capture_output=True
                    )
                    if result.returncode == 0:
                        killed += 1
                        logger.info("LOCAL: Killed stuck GPU selfplay processes")
                        # Clear job tracking so they restart
                        with self.jobs_lock:
                            gpu_jobs = [jid for jid, job in self.local_jobs.items()
                                       if "gpu" in str(getattr(job, "job_type", "")).lower()]
                            for jid in gpu_jobs:
                                del self.local_jobs[jid]
                        self._local_last_gpu_active = now
                except Exception as e:
                    logger.info(f"LOCAL: Failed to kill stuck processes: {e}")
        elif has_gpu and gpu_percent >= 5:
            self._local_last_gpu_active = now

        # Check for orphaned selfplay processes (processes running but not in job tracking)
        try:
            import subprocess
            # Count actual selfplay processes
            result = subprocess.run(
                ["pgrep", "-fc", "selfplay|gpu_selfplay"],
                timeout=5, capture_output=True, text=True
            )
            actual_processes = int(result.stdout.strip() or "0") if result.returncode == 0 else 0

            with self.jobs_lock:
                tracked_jobs = len(self.local_jobs)

            # If we have way more processes than tracked jobs, we have orphans
            if actual_processes > tracked_jobs + 10:
                last_orphan_check = getattr(self, "_last_orphan_kill", 0)
                if now - last_orphan_check > 3600:  # Max once per hour
                    logger.info(f"LOCAL: Orphan detection: {actual_processes} processes vs {tracked_jobs} tracked")
                    # Don't auto-kill orphans yet, just warn
                    # Could add aggressive cleanup here if needed
                    self._last_orphan_kill = now
        except Exception:
            pass

        if killed > 0:
            logger.info(f"LOCAL self-healing: terminated {killed} stuck process(es)")
        return killed

    async def _remote_kill_stuck_job(self, target_node: str, job_id: str, job_type: str) -> bool:
        """Send kill command to remote node for stuck job."""
        with self.peers_lock:
            peer = self.peers.get(target_node)
        if not peer or not peer.is_alive():
            return False

        try:
            timeout = ClientTimeout(total=10)
            async with get_client_session(timeout) as session:
                url = self._url_for_peer(peer, "/job/kill")
                payload = {"job_id": job_id, "job_type": job_type, "reason": "stuck"}
                async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                    return resp.status == 200
        except Exception as e:
            logger.error(f"Failed to kill stuck job on {target_node}: {e}")
            return False

    async def _manage_local_jobs_decentralized(self) -> int:
        """DECENTRALIZED: Each node manages its own job count without leader.

        Runs on ALL nodes to ensure selfplay continues even during leader elections.
        Each node autonomously:
        1. Checks its own resource pressure (disk, memory, CPU)
        2. Calculates target job count based on hardware
        3. Starts or stops local jobs as needed

        This prevents the cluster from being idle during leader instability.

        Returns:
            Number of jobs started/stopped
        """
        changes = 0
        now = time.time()

        # Rate limit: check every 60 seconds
        last_check = getattr(self, "_last_local_job_manage", 0)
        if now - last_check < 60:
            return 0
        self._last_local_job_manage = now

        # Skip if leader is managing (avoid conflicts)
        # But continue if leaderless for > 60 seconds
        if self.role == NodeRole.LEADER:
            return 0  # Leader uses centralized management
        if self.leader_id:
            leaderless_duration = now - getattr(self, "last_leader_seen", now)
            if leaderless_duration < 60:
                return 0  # Have a leader, let them manage

        # Update self info
        self._update_self_info()
        node = self.self_info

        # Check resource pressure - don't start jobs if under pressure
        if node.disk_percent >= DISK_WARNING_THRESHOLD:
            logger.info(f"LOCAL: Disk at {node.disk_percent:.0f}% - skipping job starts")
            await self._cleanup_local_disk()
            return 0

        if node.memory_percent >= MEMORY_WARNING_THRESHOLD:
            logger.info(f"LOCAL: Memory at {node.memory_percent:.0f}% - skipping job starts")
            return 0

        # Calculate target jobs for this node
        target_selfplay = self._target_selfplay_jobs_for_node(node)
        current_jobs = int(getattr(node, "selfplay_jobs", 0) or 0)

        # Start jobs if below target
        if current_jobs < target_selfplay:
            needed = min(target_selfplay - current_jobs, 3)  # Max 3 per cycle
            logger.info(f"LOCAL: Starting {needed} selfplay job(s) ({current_jobs}/{target_selfplay})")

            for _ in range(needed):
                try:
                    # Pick a config weighted by priority (same as leader logic)
                    config = self._pick_weighted_selfplay_config(node)
                    if config:
                        job = await self._start_local_job(
                            JobType.HYBRID_SELFPLAY,
                            board_type=config["board_type"],
                            num_players=config["num_players"],
                            engine_mode=config.get("engine_mode", "gumbel-mcts"),  # GPU-accelerated
                        )
                        if job:
                            changes += 1
                except Exception as e:
                    logger.info(f"LOCAL: Failed to start selfplay: {e}")
                    break

        # Stop jobs if way over target (2x or more)
        elif current_jobs > target_selfplay * 2:
            excess = current_jobs - target_selfplay
            logger.info(f"LOCAL: Reducing selfplay jobs by {excess} ({current_jobs}/{target_selfplay})")
            await self._reduce_local_selfplay_jobs(target_selfplay, reason="over_target")
            changes += excess

        if changes > 0:
            logger.info(f"LOCAL job management: {changes} change(s)")
        return changes

    async def _local_gpu_auto_scale(self) -> int:
        """DECENTRALIZED: Each GPU node manages its own GPU utilization.

        Runs on ALL GPU nodes to ensure optimal GPU usage without leader.
        Targets 60-80% GPU utilization by starting/stopping GPU selfplay jobs.

        Returns:
            Number of GPU jobs started
        """
        started = 0
        now = time.time()

        # Rate limit: check every 2 minutes
        last_check = getattr(self, "_last_local_gpu_scale", 0)
        if now - last_check < 120:
            return 0
        self._last_local_gpu_scale = now

        # Skip if not a GPU node
        if not getattr(self.self_info, "has_gpu", False):
            return 0

        # Skip if training is running (training uses GPU)
        training_jobs = int(getattr(self.self_info, "training_jobs", 0) or 0)
        if training_jobs > 0:
            return 0

        # DECENTRALIZED: Always allow local GPU scaling
        # Leader manages cluster-wide coordination, but each node optimizes its own GPU
        # Use smaller batches when leader is present to avoid conflicts
        has_leader = bool(self.leader_id or self.role == NodeRole.LEADER)
        max_jobs_per_cycle = 1 if has_leader else 3

        TARGET_GPU_MIN = 60.0
        TARGET_GPU_MAX = 80.0
        MIN_IDLE_TIME = 120 if has_leader else 60  # Faster response when leaderless

        gpu_percent = float(getattr(self.self_info, "gpu_percent", 0) or 0)
        int(getattr(self.self_info, "selfplay_jobs", 0) or 0)
        gpu_name = (getattr(self.self_info, "gpu_name", "") or "").lower()

        # Track GPU idle time
        if gpu_percent < TARGET_GPU_MIN:
            idle_since = getattr(self, "_local_gpu_idle_since", 0)
            if idle_since == 0:
                self._local_gpu_idle_since = now
            elif now - idle_since > MIN_IDLE_TIME:
                # Calculate new jobs to add
                gpu_headroom = TARGET_GPU_MAX - gpu_percent
                if any(tag in gpu_name for tag in ("h100", "h200", "gh200", "5090")):
                    jobs_per_10_percent = 2
                elif any(tag in gpu_name for tag in ("a100", "4090", "3090")):
                    jobs_per_10_percent = 1.5
                else:
                    jobs_per_10_percent = 1

                new_jobs = max(1, int(gpu_headroom / 10 * jobs_per_10_percent))
                new_jobs = min(new_jobs, max_jobs_per_cycle)  # Cap based on leader presence

                logger.info(f"LOCAL: {gpu_percent:.0f}% GPU util, starting {new_jobs} diverse/hybrid selfplay job(s)")

                for _ in range(new_jobs):
                    try:
                        config = self._pick_weighted_selfplay_config(self.self_info)
                        if config:
                            # Use hybrid mode (CPU rules + GPU eval) for quality + speed
                            job = await self._start_local_job(
                                JobType.HYBRID_SELFPLAY,
                                board_type=config["board_type"],
                                num_players=config["num_players"],
                                engine_mode="mixed",  # Diverse AI matchups for better training data
                            )
                            if job:
                                started += 1
                    except Exception as e:
                        logger.info(f"LOCAL: Failed to start diverse selfplay: {e}")
                        break

                self._local_gpu_idle_since = now  # Reset after action
        else:
            self._local_gpu_idle_since = 0  # GPU is busy, reset

        if started > 0:
            logger.info(f"LOCAL GPU auto-scale: started {started} job(s)")
        return started

    async def _local_resource_cleanup(self):
        """DECENTRALIZED: Each node handles its own resource pressure.

        Runs on ALL nodes to ensure resource cleanup without leader coordination.
        Handles disk cleanup, memory pressure, and log rotation.
        """
        now = time.time()

        # Rate limit: check every 5 minutes
        last_check = getattr(self, "_last_local_resource_check", 0)
        if now - last_check < 300:
            return
        self._last_local_resource_check = now

        self._update_self_info()
        node = self.self_info

        # Disk cleanup
        if node.disk_percent >= DISK_CLEANUP_THRESHOLD:
            logger.info(f"LOCAL: Disk at {node.disk_percent:.0f}% - triggering cleanup")
            await self._cleanup_local_disk()

        # Memory pressure - reduce jobs
        if node.memory_percent >= MEMORY_CRITICAL_THRESHOLD:
            logger.info(f"LOCAL: Memory CRITICAL at {node.memory_percent:.0f}%")
            await self._reduce_local_selfplay_jobs(0, reason="memory_critical")
        elif node.memory_percent >= MEMORY_WARNING_THRESHOLD:
            current = int(getattr(node, "selfplay_jobs", 0) or 0)
            target = max(1, current // 2)
            logger.info(f"LOCAL: Memory warning at {node.memory_percent:.0f}% - reducing jobs to {target}")
            await self._reduce_local_selfplay_jobs(target, reason="memory_warning")

    def _get_elo_based_priority_boost(self, board_type: str, num_players: int) -> int:
        """Get priority boost based on ELO performance for this config.

        PRIORITY-BASED SCHEDULING: Configs with high-performing models get
        priority boost to allocate more resources to promising configurations.

        Returns:
            Priority boost (0-5) based on:
            - Top model ELO for this config
            - Recent improvement rate
            - Data coverage (inverse - underrepresented get boost)
        """
        boost = 0

        try:
            cluster_elo = self._get_cluster_elo_summary()
            top_models = cluster_elo.get("top_models", [])

            # Find best model for this board/player combo
            best_elo = 0
            for model in top_models:
                model_name = model.get("name", "")
                # Model names typically include board type and player count
                if board_type in model_name or str(num_players) in model_name:
                    best_elo = max(best_elo, model.get("elo", 0))

            # ELO-based boost (every 100 ELO above 1200 = +1 priority)
            if best_elo > 1200:
                boost += min(3, (best_elo - 1200) // 100)

            # Underrepresented config boost
            # (hex and square19 often have fewer games)
            if board_type in ("hexagonal", "square19"):
                boost += 1
            if num_players > 2:
                boost += 1

        except Exception:
            pass

        return min(5, boost)  # Cap at +5

    def _pick_weighted_selfplay_config(self, node) -> dict[str, Any] | None:
        """Pick a selfplay config weighted by priority and node capabilities.

        PRIORITY-BASED SCHEDULING: Combines static priority with dynamic
        ELO-based boosts to allocate more resources to high-performing configs.

        Returns config dict with board_type, num_players, engine_mode.
        """
        # Get the selfplay configs - DIVERSE mode prioritized for high-quality training data
        # Uses "mixed" engine mode for varied AI matchups (NNUE, MCTS, heuristic combinations)
        selfplay_configs = [
            # Priority 8: Underrepresented hex/sq19 combos with diverse AI (highest priority)
            {"board_type": "hexagonal", "num_players": 3, "engine_mode": "mixed", "priority": 8},
            {"board_type": "hexagonal", "num_players": 2, "engine_mode": "mixed", "priority": 8},
            {"board_type": "hexagonal", "num_players": 4, "engine_mode": "mixed", "priority": 8},
            {"board_type": "hex8", "num_players": 2, "engine_mode": "mixed", "priority": 8},
            {"board_type": "hex8", "num_players": 3, "engine_mode": "mixed", "priority": 8},
            {"board_type": "hex8", "num_players": 4, "engine_mode": "mixed", "priority": 8},
            {"board_type": "square19", "num_players": 3, "engine_mode": "mixed", "priority": 8},
            {"board_type": "square19", "num_players": 2, "engine_mode": "mixed", "priority": 8},
            {"board_type": "square19", "num_players": 4, "engine_mode": "mixed", "priority": 8},
            # Priority 7: Square8 multi-player with diverse AI
            {"board_type": "square8", "num_players": 3, "engine_mode": "mixed", "priority": 7},
            {"board_type": "square8", "num_players": 4, "engine_mode": "mixed", "priority": 7},
            # Priority 6: Cross-AI matches (specific matchup types)
            {"board_type": "square8", "num_players": 2, "engine_mode": "heuristic-vs-mcts", "priority": 6},
            {"board_type": "hexagonal", "num_players": 3, "engine_mode": "heuristic-vs-mcts", "priority": 6},
            {"board_type": "square19", "num_players": 2, "engine_mode": "heuristic-vs-mcts", "priority": 6},
            # Priority 5: Standard 2p square8 with diverse AI
            {"board_type": "square8", "num_players": 2, "engine_mode": "mixed", "priority": 5},
            # Priority 4: Tournament varied (for evaluation-style games)
            {"board_type": "square8", "num_players": 2, "engine_mode": "tournament-varied", "priority": 4},
            {"board_type": "hexagonal", "num_players": 2, "engine_mode": "tournament-varied", "priority": 4},
            # Priority 3: CPU-bound specialized modes
            {"board_type": "square8", "num_players": 2, "engine_mode": "mcts-only", "priority": 3},
            {"board_type": "hexagonal", "num_players": 2, "engine_mode": "descent-only", "priority": 3},
        ]

        # Filter by node memory (avoid large boards on small nodes)
        node_mem = int(getattr(node, "memory_gb", 0) or 0)
        if node_mem and node_mem < 48:
            selfplay_configs = [c for c in selfplay_configs if c.get("board_type") == "square8"]

        if not selfplay_configs:
            return None

        # PRIORITY-BASED SCHEDULING: Add ELO-based priority boosts
        # Phase 3.1: Also incorporate curriculum weights from unified AI loop
        curriculum_weights = {}
        if HAS_CURRICULUM_WEIGHTS and load_curriculum_weights is not None:
            try:
                curriculum_weights = load_curriculum_weights()
            except Exception:
                pass  # Use empty weights on error

        # Load board priority overrides from config (0=CRITICAL, 1=HIGH, 2=MEDIUM, 3=LOW)
        board_priority_overrides = get_board_priority_overrides()

        for cfg in selfplay_configs:
            elo_boost = self._get_elo_based_priority_boost(
                cfg.get("board_type", ""),
                cfg.get("num_players", 2),
            )

            # Phase 3.1: Apply curriculum weight boost
            # Config keys are formatted as "board_type_Np" (e.g., "square8_2p")
            config_key = f"{cfg.get('board_type', '')}_{cfg.get('num_players', 2)}p"
            curriculum_weight = curriculum_weights.get(config_key, 1.0)
            # Convert weight (0.7-1.5) to priority boost (0-3)
            # weight 0.7 = -1 boost, weight 1.0 = 0 boost, weight 1.5 = +2 boost
            curriculum_boost = int((curriculum_weight - 1.0) * 4)
            curriculum_boost = max(-2, min(3, curriculum_boost))  # Clamp to -2..+3

            # Apply board priority overrides from config
            # 0=CRITICAL adds +6, 1=HIGH adds +4, 2=MEDIUM adds +2, 3=LOW adds 0
            board_priority = board_priority_overrides.get(config_key, 3)  # Default to LOW (3)
            board_priority_boost = (3 - board_priority) * 2  # 0->6, 1->4, 2->2, 3->0

            cfg["effective_priority"] = cfg.get("priority", 1) + elo_boost + curriculum_boost + board_priority_boost

        # Build weighted list by effective priority
        weighted = []
        for cfg in selfplay_configs:
            # Ensure minimum priority of 1
            priority = max(1, cfg.get("effective_priority", 1))
            weighted.extend([cfg] * priority)

        import random
        return random.choice(weighted) if weighted else None

    async def _auto_scale_gpu_utilization(self) -> int:
        """Auto-scale diverse/hybrid selfplay jobs to reach 60-80% GPU utilization.

        Detects underutilized GPU nodes and starts HYBRID selfplay jobs to improve
        cluster throughput while maintaining game quality and rule fidelity.

        NOTE: GPU-only selfplay is DISABLED. All auto-scaled jobs use hybrid mode
        which provides 100% rule fidelity (CPU rules) + GPU-accelerated evaluation.
        This produces higher quality training data than pure GPU selfplay.

        Returns:
            Number of new diverse selfplay jobs started
        """
        TARGET_GPU_MIN = 60.0  # Target minimum GPU utilization
        TARGET_GPU_MAX = 80.0  # Target maximum GPU utilization
        MIN_IDLE_TIME = 120    # Seconds of low GPU before scaling up

        started = 0
        now = time.time()

        # Rate limit auto-scaling (once per 2 minutes)
        last_scale = getattr(self, "_last_gpu_auto_scale", 0)
        if now - last_scale < 120:
            return 0

        with self.peers_lock:
            peers_snapshot = list(self.peers.values())

        underutilized_gpu_nodes = []

        # Load policy manager for filtering
        policy_manager = None
        try:
            from app.coordination.node_policies import get_policy_manager
            policy_manager = get_policy_manager()
        except ImportError:
            pass

        for peer in peers_snapshot:
            if not peer.is_alive():
                continue
            has_gpu = bool(getattr(peer, "has_gpu", False))
            if not has_gpu:
                continue

            # Policy check: skip nodes that don't allow selfplay
            if policy_manager and not policy_manager.is_work_allowed(peer.node_id, "selfplay"):
                continue

            gpu_percent = float(getattr(peer, "gpu_percent", 0) or 0)
            gpu_name = (getattr(peer, "gpu_name", "") or "").lower()
            selfplay_jobs = int(getattr(peer, "selfplay_jobs", 0) or 0)
            training_jobs = int(getattr(peer, "training_jobs", 0) or 0)

            # Skip if already training
            if training_jobs > 0:
                continue

            # Check if underutilized
            if gpu_percent < TARGET_GPU_MIN:
                # Track how long it's been underutilized
                idle_key = f"_gpu_idle_since_{peer.node_id}"
                idle_since = getattr(self, idle_key, 0)
                if idle_since == 0:
                    setattr(self, idle_key, now)
                elif now - idle_since > MIN_IDLE_TIME:
                    # Calculate how many more jobs to add
                    gpu_headroom = TARGET_GPU_MAX - gpu_percent
                    # Estimate jobs based on GPU tier
                    if any(tag in gpu_name for tag in ("h100", "h200", "gh200", "5090")):
                        jobs_per_10_percent = 2
                    elif any(tag in gpu_name for tag in ("a100", "4090", "3090")):
                        jobs_per_10_percent = 1.5
                    else:
                        jobs_per_10_percent = 1

                    new_jobs = max(1, int(gpu_headroom / 10 * jobs_per_10_percent))
                    new_jobs = min(new_jobs, 4)  # Cap at 4 new jobs per cycle

                    underutilized_gpu_nodes.append({
                        "node_id": peer.node_id,
                        "gpu_percent": gpu_percent,
                        "gpu_name": gpu_name,
                        "current_jobs": selfplay_jobs,
                        "new_jobs": new_jobs,
                    })
            else:
                # GPU is utilized, reset idle timer
                idle_key = f"_gpu_idle_since_{peer.node_id}"
                setattr(self, idle_key, 0)

        # Start GPU selfplay on underutilized nodes
        for node_info in underutilized_gpu_nodes[:3]:  # Max 3 nodes per cycle
            node_id = node_info["node_id"]
            new_jobs = node_info["new_jobs"]

            print(
                f"[P2P] Auto-scale: {node_id} at {node_info['gpu_percent']:.0f}% GPU, "
                f"starting {new_jobs} diverse/hybrid selfplay job(s)"
            )

            for _ in range(new_jobs):
                try:
                    # Schedule diverse/hybrid selfplay job (GPU-only selfplay disabled)
                    job = await self._schedule_diverse_selfplay_on_node(node_id)
                    if job:
                        started += 1
                except Exception as e:
                    logger.error(f"Failed to start diverse selfplay on {node_id}: {e}")
                    break

        if started > 0:
            self._last_gpu_auto_scale = now
            logger.info(f"Auto-scale: started {started} new diverse/hybrid selfplay job(s)")

        return started

    async def _auto_rebalance_from_work_queue(self) -> int:
        """Auto-rebalance: assign queued work to idle GPU nodes.

        When idle GPU-heavy nodes are detected, check the work queue for pending
        high-priority work and dispatch it. This ensures queued work gets done
        before falling back to selfplay auto-scaling.

        Returns:
            Number of work items dispatched
        """
        GPU_IDLE_THRESHOLD = 10.0  # Node is idle if GPU < 10%
        MIN_IDLE_TIME = 60  # Seconds of idle before assigning work
        GPU_HEAVY_TAGS = ['gh200', 'h100', 'h200', 'a100', '4090', '5090']

        dispatched = 0
        now = time.time()

        # Rate limit rebalancing (once per minute)
        last_rebalance = getattr(self, "_last_work_queue_rebalance", 0)
        if now - last_rebalance < 60:
            return 0

        # Check if work queue is available
        wq = get_work_queue()
        if wq is None:
            return 0

        # Get queue status
        queue_status = wq.get_queue_status()
        pending_count = queue_status.get("by_status", {}).get("pending", 0)
        if pending_count == 0:
            return 0  # No work to dispatch

        # Find idle GPU-heavy nodes
        idle_nodes = []

        with self.peers_lock:
            peers_snapshot = list(self.peers.values())

        for peer in peers_snapshot:
            if not peer.is_alive() or peer.retired:
                continue

            has_gpu = bool(getattr(peer, "has_gpu", False))
            if not has_gpu:
                continue

            gpu_name = (getattr(peer, "gpu_name", "") or "").upper()
            is_gpu_heavy = any(tag.upper() in gpu_name for tag in GPU_HEAVY_TAGS)
            if not is_gpu_heavy:
                continue

            gpu_percent = float(getattr(peer, "gpu_percent", 0) or 0)
            training_jobs = int(getattr(peer, "training_jobs", 0) or 0)
            selfplay_jobs = int(getattr(peer, "selfplay_jobs", 0) or 0)

            # Skip if already busy
            if training_jobs > 0:
                continue

            # Check if truly idle
            if gpu_percent < GPU_IDLE_THRESHOLD:
                # Track how long it's been idle
                idle_key = f"_wq_idle_since_{peer.node_id}"
                idle_since = getattr(self, idle_key, 0)
                if idle_since == 0:
                    setattr(self, idle_key, now)
                elif now - idle_since > MIN_IDLE_TIME:
                    # Get allowed work types for this node
                    try:
                        from app.coordination.node_policies import get_policy_manager
                        pm = get_policy_manager()
                        allowed = list(pm.get_allowed_work_types(peer.node_id))
                    except ImportError:
                        allowed = ["training", "gpu_cmaes", "tournament", "selfplay"]

                    idle_nodes.append({
                        "node_id": peer.node_id,
                        "peer": peer,
                        "gpu_percent": gpu_percent,
                        "gpu_name": gpu_name,
                        "allowed": allowed,
                        "selfplay_jobs": selfplay_jobs,
                    })
            else:
                # Not idle, reset timer
                idle_key = f"_wq_idle_since_{peer.node_id}"
                setattr(self, idle_key, 0)

        # Dispatch work to idle nodes
        for node_info in idle_nodes[:5]:  # Max 5 nodes per cycle
            node_id = node_info["node_id"]
            allowed = node_info["allowed"]

            # Try to claim work for this node
            work_item = wq.claim_work(node_id, allowed)
            if work_item is None:
                continue

            print(
                f"[P2P] Work queue rebalance: {node_id} idle at {node_info['gpu_percent']:.0f}% GPU, "
                f"assigning {work_item.work_type.value} work ({work_item.work_id})"
            )

            # Dispatch work to the node
            success = await self._dispatch_queued_work(node_info["peer"], work_item)
            if success:
                wq.start_work(work_item.work_id)
                dispatched += 1
                # Reset idle timer since we assigned work
                idle_key = f"_wq_idle_since_{node_id}"
                setattr(self, idle_key, 0)
            else:
                # Failed to dispatch, reset work status for retry
                wq.fail_work(work_item.work_id, "dispatch_failed")

        if dispatched > 0:
            self._last_work_queue_rebalance = now
            logger.info(f"Work queue rebalance: dispatched {dispatched} work item(s) to idle nodes")

        return dispatched

    async def _dispatch_queued_work(self, peer: NodeInfo, work_item) -> bool:
        """Dispatch a work queue item to a specific node.

        Routes different work types to appropriate endpoints.
        """
        from app.coordination.work_queue import WorkType

        try:
            timeout = ClientTimeout(total=30)
            async with get_client_session(timeout) as session:
                work_type = work_item.work_type
                config = work_item.config

                if work_type == WorkType.TRAINING:
                    # Route to training endpoint
                    url = self._url_for_peer(peer, "/start_job")
                    payload = {
                        "job_type": "training",
                        "board_type": config.get("board_type", "square8"),
                        "num_players": config.get("num_players", 2),
                        "work_id": work_item.work_id,
                    }
                elif work_type == WorkType.GPU_CMAES:
                    # Route to CMA-ES endpoint
                    url = self._url_for_peer(peer, "/cmaes/start")
                    payload = {
                        "board_type": config.get("board_type", "square8"),
                        "num_players": config.get("num_players", 2),
                        "work_id": work_item.work_id,
                    }
                elif work_type == WorkType.TOURNAMENT:
                    # Route to tournament endpoint
                    url = self._url_for_peer(peer, "/tournament/start")
                    payload = {
                        "board_type": config.get("board_type", "square8"),
                        "num_players": config.get("num_players", 2),
                        "work_id": work_item.work_id,
                    }
                elif work_type == WorkType.SELFPLAY:
                    # Route to selfplay endpoint
                    url = self._url_for_peer(peer, "/selfplay/start")
                    payload = {
                        "board_type": config.get("board_type", "square8"),
                        "num_players": config.get("num_players", 2),
                        "num_games": config.get("num_games", 500),
                        "work_id": work_item.work_id,
                    }
                else:
                    logger.warning(f"Unknown work type: {work_type}")
                    return False

                async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                    if resp.status == 200:
                        logger.info(f"Dispatched {work_type.value} work to {peer.node_id}")
                        return True
                    else:
                        error = await resp.text()
                        logger.warning(f"Failed to dispatch work to {peer.node_id}: {error}")
                        return False

        except Exception as e:
            logger.error(f"Error dispatching work to {peer.node_id}: {e}")
            return False

    async def _schedule_diverse_selfplay_on_node(self, node_id: str) -> dict | None:
        """Schedule a diverse/hybrid selfplay job on a specific node.

        Uses HYBRID mode (CPU rules + GPU eval) for 100% rule fidelity.
        Rotates through all board/player configurations for diversity.
        """
        with self.peers_lock:
            peer = self.peers.get(node_id)
        if not peer or not peer.is_alive():
            return None

        # Policy check: ensure selfplay is allowed on this node
        try:
            from app.coordination.node_policies import is_work_allowed
            if not is_work_allowed(node_id, "selfplay"):
                logger.debug(f"Selfplay not allowed on {node_id} by policy")
                return None
        except ImportError:
            pass

        # Rotate through diverse configurations with priority-based weighting
        # Uses board_priority_overrides from config (0=CRITICAL, 1=HIGH, 2=MEDIUM, 3=LOW)
        board_priority_overrides = get_board_priority_overrides()

        diverse_configs = [
            ("hexagonal", 2), ("hexagonal", 3), ("hexagonal", 4),
            ("square19", 3), ("square19", 4), ("square19", 2),
            ("hex8", 2), ("hex8", 3), ("hex8", 4),
            ("square8", 3), ("square8", 4), ("square8", 2),
        ]

        # Build weighted list based on priority overrides
        # Lower priority value = more weight (0=CRITICAL gets 4x, 3=LOW gets 1x)
        weighted_configs = []
        for board_type, num_players in diverse_configs:
            config_key = f"{board_type}_{num_players}p"
            priority = board_priority_overrides.get(config_key, 3)  # Default LOW
            weight = 4 - priority  # 0->4, 1->3, 2->2, 3->1
            weighted_configs.extend([(board_type, num_players)] * weight)

        # Round-robin through weighted list based on node-specific counter
        counter_key = f"_diverse_config_counter_{node_id}"
        counter = getattr(self, counter_key, 0)
        setattr(self, counter_key, counter + 1)
        board_type, num_players = weighted_configs[counter % len(weighted_configs)]

        try:
            timeout = ClientTimeout(total=30)
            async with get_client_session(timeout) as session:
                url = self._url_for_peer(peer, "/selfplay/start")
                payload = {
                    "board_type": board_type,
                    "num_players": num_players,
                    "num_games": 200,  # Smaller batches for diversity
                    "engine_mode": "mixed",  # HYBRID mode: CPU rules + GPU eval
                    "auto_scaled": True,
                    "job_type": "hybrid_selfplay",  # Explicitly request hybrid
                }
                async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        logger.info(f"Started diverse selfplay on {node_id}: {board_type} {num_players}p")
                        return data
                    else:
                        error = await resp.text()
                        logger.info(f"Diverse selfplay start failed on {node_id}: {error}")
                        return None
        except Exception as e:
            logger.error(f"Failed to schedule diverse selfplay on {node_id}: {e}")
            return None

    # Backward compatibility alias (GPU selfplay now redirects to diverse/hybrid)
    _schedule_gpu_selfplay_on_node = _schedule_diverse_selfplay_on_node

    def _target_selfplay_jobs_for_node(self, node: NodeInfo) -> int:
        """Return the desired selfplay concurrency for a node.

        Uses unified resource targets for consistent 60-80% utilization:
        - Backpressure-aware: Reduces jobs when training queue is full
        - Adaptive scaling: Increases jobs when underutilized, decreases when overloaded
        - Host-tier aware: Adjusts targets based on hardware capability

        Target: 60-80% CPU/GPU utilization for optimal training throughput.
        """
        # Check safeguards first
        if HAS_SAFEGUARDS and _safeguards and _safeguards.is_emergency_active():
            return 0

        # Check backpressure - reduce production when training queue is full
        backpressure_factor = 1.0
        if HAS_NEW_COORDINATION:
            try:
                if should_stop_production(QueueType.TRAINING_DATA):
                    logger.info(f"Backpressure STOP: training queue full, halting selfplay on {node.node_id}")
                    return 0
                if should_throttle_production(QueueType.TRAINING_DATA):
                    backpressure_factor = get_throttle_factor(QueueType.TRAINING_DATA)
                    logger.info(f"Backpressure throttle: factor={backpressure_factor:.2f}")
            except Exception as e:
                logger.info(f"Backpressure check error: {e}")

        # Minimum memory requirement - skip low-memory machines to avoid OOM
        memory_gb = int(getattr(node, "memory_gb", 0) or 0)
        if memory_gb > 0 and memory_gb < MIN_MEMORY_GB_FOR_TASKS:
            return 0

        # Extract node metrics
        has_gpu = bool(getattr(node, "has_gpu", False))
        cpu_count = int(getattr(node, "cpu_count", 0) or 0)
        cpu_percent = float(getattr(node, "cpu_percent", 0.0) or 0.0)
        mem_percent = float(getattr(node, "memory_percent", 0.0) or 0.0)
        disk_percent = float(getattr(node, "disk_percent", 0.0) or 0.0)
        gpu_percent = float(getattr(node, "gpu_percent", 0.0) or 0.0)
        gpu_mem_percent = float(getattr(node, "gpu_memory_percent", 0.0) or 0.0)
        current_jobs = int(getattr(node, "selfplay_jobs", 0) or 0)

        # Record utilization for adaptive feedback
        if HAS_NEW_COORDINATION:
            with contextlib.suppress(Exception):
                record_utilization(node.node_id, cpu_percent, gpu_percent, mem_percent, current_jobs)

        # Use unified resource targets if available
        if HAS_NEW_COORDINATION:
            try:
                # Get host-specific targets adjusted for tier and backpressure
                host_targets = get_host_targets(node.node_id)

                # Use the unified target calculator
                target_selfplay = get_target_job_count(
                    node.node_id,
                    cpu_count if cpu_count > 0 else 8,
                    cpu_percent,
                    gpu_percent if has_gpu else 0.0,
                )

                # Check if we should scale up (underutilized)
                scale_up, reason = should_scale_up(
                    node.node_id, cpu_percent, gpu_percent, current_jobs
                )
                if scale_up and current_jobs < target_selfplay:
                    # Controlled scale-up: Add 2-4 jobs at a time, not all at once
                    scale_up_increment = min(4, target_selfplay - current_jobs)
                    target_selfplay = current_jobs + scale_up_increment
                    if self.verbose:
                        logger.info(f"Scale-up on {node.node_id}: {reason}, target={target_selfplay}")

                # Check if we should scale down (overloaded)
                scale_down, reduction, reason = should_scale_down(
                    node.node_id, cpu_percent, gpu_percent, mem_percent
                )
                if scale_down:
                    target_selfplay = max(1, current_jobs - reduction)
                    logger.info(f"Scale-down on {node.node_id}: {reason}, target={target_selfplay}")

                # Apply backpressure factor
                target_selfplay = int(target_selfplay * backpressure_factor)

                # Apply host-specific max
                target_selfplay = min(target_selfplay, host_targets.max_selfplay)

                return int(max(1, target_selfplay))

            except Exception as e:
                logger.info(f"Resource targets error, falling back to hardware-aware: {e}")

        # FALLBACK: Use unified hardware-aware limits from resource_optimizer
        # This ensures consistent limits across all orchestrators
        gpu_name = (getattr(node, "gpu_name", "") or "")
        gpu_count = int(getattr(node, "gpu_count", 1) or 1) if has_gpu else 0

        if HAS_HW_AWARE_LIMITS and get_max_selfplay_for_node is not None:
            # Use single source of truth from resource_optimizer
            max_selfplay = get_max_selfplay_for_node(
                node_id=node.node_id,
                gpu_count=gpu_count,
                gpu_name=gpu_name,
                cpu_count=cpu_count,
                memory_gb=memory_gb,
                has_gpu=has_gpu,
            )
        else:
            # Minimal fallback when resource_optimizer unavailable
            # Values calibrated from observed workloads (GH200: 48 jobs at 70% GPU)
            if has_gpu:
                gpu_upper = gpu_name.upper()
                if any(g in gpu_upper for g in ["GH200"]):
                    # GH200 with unified 480GB memory - CPU is bottleneck
                    max_selfplay = int(cpu_count * 0.8) if cpu_count > 0 else 48
                elif any(g in gpu_upper for g in ["H100", "H200"]):
                    max_selfplay = min(int(cpu_count * 0.5), 48) if cpu_count > 0 else 32
                elif any(g in gpu_upper for g in ["A100", "L40"]):
                    max_selfplay = min(int(cpu_count * 0.4), 32) if cpu_count > 0 else 24
                elif any(g in gpu_upper for g in ["5090"]):
                    # RTX 5090 (32GB) - very high capacity
                    max_selfplay = min(int(cpu_count * 0.3), gpu_count * 12, 64) if cpu_count > 0 else 48
                elif any(g in gpu_upper for g in ["A10", "4090", "3090"]):
                    max_selfplay = min(int(cpu_count * 0.3), 24) if cpu_count > 0 else 16
                elif any(g in gpu_upper for g in ["4080", "4070", "3080", "4060"]):
                    max_selfplay = min(int(cpu_count * 0.25), 12) if cpu_count > 0 else 8
                elif any(g in gpu_upper for g in ["3070", "3060", "2060", "2070", "2080"]):
                    max_selfplay = min(int(cpu_count * 0.2), 10) if cpu_count > 0 else 6
                else:
                    max_selfplay = min(int(cpu_count * 0.2), 8) if cpu_count > 0 else 6
            else:
                # CPU-only: ~0.3 jobs per core, capped at 32
                max_selfplay = min(int(cpu_count * 0.3), 32) if cpu_count > 0 else 8

        target_selfplay = max_selfplay

        # Utilization-aware adjustments (target 60-80%)
        gpu_overloaded = gpu_percent > 85 or gpu_mem_percent > 85
        cpu_overloaded = cpu_percent > 80
        gpu_has_headroom = gpu_percent < 60 and gpu_mem_percent < 75
        cpu_has_headroom = cpu_percent < 60

        # Scale DOWN if overloaded
        if gpu_overloaded:
            target_selfplay = max(2, target_selfplay - 2)
        if cpu_overloaded:
            target_selfplay = max(2, target_selfplay - 1)

        # Scale UP only if both resources have headroom (gradual)
        if (not gpu_overloaded and not cpu_overloaded and current_jobs > 0 and (has_gpu and gpu_has_headroom and cpu_has_headroom)) or ((not has_gpu and cpu_has_headroom) and current_jobs < target_selfplay):
            target_selfplay = min(target_selfplay, current_jobs + 2)

        # Resource pressure warnings
        if disk_percent >= DISK_WARNING_THRESHOLD:
            target_selfplay = min(target_selfplay, 4)
        if mem_percent >= MEMORY_WARNING_THRESHOLD:
            target_selfplay = min(target_selfplay, 2)

        # Apply backpressure factor
        target_selfplay = int(target_selfplay * backpressure_factor)

        return int(max(1, target_selfplay))

    def _get_hybrid_job_targets(self, node: NodeInfo) -> dict[str, int]:
        """Get separate GPU and CPU-only selfplay job targets for hybrid mode.

        For high-CPU nodes with limited GPU VRAM (like Vast hosts), this enables:
        - Running GPU jobs up to VRAM limit
        - Running additional CPU-only jobs to utilize excess CPU capacity

        Returns:
            Dict with 'gpu_jobs', 'cpu_only_jobs', 'total_jobs'
        """
        has_gpu = bool(getattr(node, "has_gpu", False))
        cpu_count = int(getattr(node, "cpu_count", 0) or 0)
        memory_gb = int(getattr(node, "memory_gb", 0) or 0)
        gpu_name = getattr(node, "gpu_name", "") or ""
        gpu_count = int(getattr(node, "gpu_count", 1) or 1) if has_gpu else 0

        # Use hybrid limits function if available
        if HAS_HW_AWARE_LIMITS and get_hybrid_selfplay_limits is not None:
            try:
                limits = get_hybrid_selfplay_limits(
                    node_id=node.node_id,
                    gpu_count=gpu_count,
                    gpu_name=gpu_name,
                    cpu_count=cpu_count,
                    memory_gb=memory_gb,
                    has_gpu=has_gpu,
                )
                return limits
            except Exception as e:
                logger.info(f"Hybrid limits error: {e}")

        # Fallback: No CPU-only jobs, use standard target
        gpu_jobs = self._target_selfplay_jobs_for_node(node)
        return {"gpu_jobs": gpu_jobs, "cpu_only_jobs": 0, "total_jobs": gpu_jobs}

    def _should_spawn_cpu_only_jobs(self, node: NodeInfo) -> bool:
        """Check if a node should spawn CPU-only jobs in addition to GPU jobs.

        CPU-only jobs are beneficial when:
        1. Node has many CPU cores (64+)
        2. Node has limited GPU VRAM (<=16GB per GPU)
        3. GPU jobs are already at capacity (VRAM-limited)
        """
        if not HAS_HW_AWARE_LIMITS:
            return False

        cpu_count = int(getattr(node, "cpu_count", 0) or 0)
        has_gpu = bool(getattr(node, "has_gpu", False))
        gpu_name = (getattr(node, "gpu_name", "") or "").upper()

        # Must have significant CPU resources (64+ cores)
        if cpu_count < 64:
            return False

        # For GPU nodes, only spawn CPU-only if GPU has limited VRAM
        if has_gpu:
            # High-end datacenter GPUs don't need CPU-only jobs (plenty of VRAM)
            if any(g in gpu_name for g in ["GH200", "H100", "H200", "A100", "L40"]):
                return False
            # Consumer GPUs with limited VRAM benefit from CPU-only supplement
            if any(g in gpu_name for g in ["3070", "3060", "2060", "2070", "2080", "4060", "4070"]):
                return True
            # 5090/4090 with 24-32GB might not need it unless very high CPU count
            if any(g in gpu_name for g in ["5090", "4090", "3090"]):
                return cpu_count >= 128

        # CPU-only nodes always benefit from full CPU utilization
        return True


    async def _check_cluster_balance(self) -> dict[str, Any]:
        """Check and rebalance jobs across the cluster.

        This method identifies:
        1. Powerful nodes that are underutilized (high capacity, low jobs)
        2. Weak nodes that are overloaded (low capacity, high jobs)

        When imbalance is detected, it reduces jobs on weak nodes so the
        scheduler can assign them to more powerful nodes.

        Returns dict with rebalancing actions taken.
        """
        try:
            with self.peers_lock:
                alive_peers = [p for p in self.peers.values() if p.is_alive()]

            all_nodes = [*alive_peers, self.self_info]
            healthy_nodes = [n for n in all_nodes if n.is_healthy()]

            if len(healthy_nodes) < 2:
                return {"action": "none", "reason": "insufficient_nodes"}

            # Calculate capacity and utilization for each node
            node_stats = []
            for node in healthy_nodes:
                target = self._target_selfplay_jobs_for_node(node)
                current = int(getattr(node, "selfplay_jobs", 0) or 0)
                utilization = current / max(1, target)  # How full is this node
                capacity_score = target  # Higher = more powerful

                node_stats.append({
                    "node": node,
                    "target": target,
                    "current": current,
                    "utilization": utilization,
                    "capacity": capacity_score,
                    "load_score": node.get_load_score(),
                })

            # Find underutilized powerful nodes (capacity > median, utilization < 50%)
            sorted_by_capacity = sorted(node_stats, key=lambda x: x["capacity"], reverse=True)
            median_capacity = sorted_by_capacity[len(sorted_by_capacity) // 2]["capacity"]

            underutilized_powerful = [
                n for n in node_stats
                if n["capacity"] > median_capacity and n["utilization"] < 0.5
            ]

            # Find overloaded weak nodes (capacity < median, utilization > 100%)
            overloaded_weak = [
                n for n in node_stats
                if n["capacity"] < median_capacity and n["utilization"] > 1.0
            ]

            if not underutilized_powerful or not overloaded_weak:
                return {"action": "none", "reason": "balanced"}

            # Calculate rebalancing opportunity
            spare_capacity = sum(
                max(0, n["target"] - n["current"]) for n in underutilized_powerful
            )
            excess_load = sum(
                max(0, n["current"] - n["target"]) for n in overloaded_weak
            )

            if spare_capacity < 2 or excess_load < 2:
                return {"action": "none", "reason": "minimal_imbalance"}

            # Migrate: reduce jobs on weak nodes
            rebalance_actions = []
            jobs_to_migrate = min(spare_capacity, excess_load)

            for weak_node in sorted(overloaded_weak, key=lambda x: x["utilization"], reverse=True):
                if jobs_to_migrate <= 0:
                    break

                node = weak_node["node"]
                reduce_by = min(
                    weak_node["current"] - weak_node["target"],
                    jobs_to_migrate
                )
                new_target = weak_node["current"] - reduce_by

                if reduce_by > 0:
                    print(
                        f"[P2P] Cluster rebalance: {node.node_id} overloaded "
                        f"({weak_node['current']}/{weak_node['target']} jobs, "
                        f"{weak_node['utilization']*100:.0f}% util) - reducing by {reduce_by}"
                    )

                    if node.node_id == self.node_id:
                        await self._reduce_local_selfplay_jobs(new_target, reason="cluster_rebalance")
                    else:
                        await self._request_reduce_selfplay(node, new_target, reason="cluster_rebalance")

                    rebalance_actions.append({
                        "node": node.node_id,
                        "reduced_by": reduce_by,
                        "new_target": new_target,
                    })
                    jobs_to_migrate -= reduce_by

            # Record rebalancing metric
            if rebalance_actions:
                self.record_metric(
                    "cluster_rebalance",
                    len(rebalance_actions),
                    metadata={
                        "spare_capacity": spare_capacity,
                        "excess_load": excess_load,
                        "actions": rebalance_actions,
                    },
                )

            return {
                "action": "rebalanced",
                "spare_capacity": spare_capacity,
                "excess_load": excess_load,
                "actions": rebalance_actions,
            }

        except Exception as e:
            logger.info(f"Cluster balance check error: {e}")
            return {"action": "error", "error": str(e)}

    async def _manage_cluster_jobs(self):
        """Manage jobs across the cluster (leader only).

        LEARNED LESSONS incorporated:
        - Check disk space BEFORE starting jobs (Vast.ai 91-93% disk issue)
        - Check memory to prevent OOM (AWS instance crashed at 31GB+)
        - Trigger cleanup when approaching limits
        - Use is_healthy() not just is_alive()
        """
        logger.info("Leader: Managing cluster jobs...")

        # Gather cluster state
        with self.peers_lock:
            alive_peers = [p for p in self.peers.values() if p.is_alive()]

        # Add self
        self._update_self_info()
        all_nodes = [*alive_peers, self.self_info]

        # Phase 1: Handle resource warnings and cleanup
        for node in all_nodes:
            # LEARNED LESSONS - Proactive disk cleanup before hitting critical
            if node.disk_percent >= DISK_CLEANUP_THRESHOLD:
                logger.info(f"{node.node_id}: Disk at {node.disk_percent:.0f}% - triggering cleanup")
                if node.node_id == self.node_id:
                    await self._cleanup_local_disk()
                else:
                    await self._request_remote_cleanup(node)
                continue  # Skip job creation this cycle

            # Load shedding: when a node is under memory/disk pressure, ask it to
            # stop excess selfplay jobs so it can recover (prevents OOM + disk-full).
            pressure_reasons: list[str] = []
            if node.memory_percent >= MEMORY_WARNING_THRESHOLD:
                pressure_reasons.append("memory")
            if node.disk_percent >= DISK_WARNING_THRESHOLD:
                pressure_reasons.append("disk")

            if pressure_reasons:
                desired = self._target_selfplay_jobs_for_node(node)
                if node.memory_percent >= MEMORY_CRITICAL_THRESHOLD or node.disk_percent >= DISK_CRITICAL_THRESHOLD:
                    desired = 0

                if node.selfplay_jobs > desired:
                    reason = "+".join(pressure_reasons)
                    print(
                        f"[P2P] {node.node_id}: Load shedding (reason={reason}) "
                        f"{node.selfplay_jobs}->{desired} selfplay jobs"
                    )
                    if node.node_id == self.node_id:
                        await self._reduce_local_selfplay_jobs(desired, reason=reason)
                    else:
                        await self._request_reduce_selfplay(node, desired, reason=reason)

        # Phase 1.5: LEARNED LESSONS - Detect stuck jobs (GPU idle with running processes)
        # This addresses the vast-5090-quad issue where 582 processes ran at 0% GPU
        for node in all_nodes:
            if not node.has_gpu or node.selfplay_jobs <= 0:
                # No GPU or no jobs running - not stuck
                if node.node_id in self.gpu_idle_since:
                    del self.gpu_idle_since[node.node_id]
                continue

            # Check if GPU is idle (< threshold) with jobs running
            gpu_name = (node.gpu_name or "").upper()
            is_cuda_gpu = "MPS" not in gpu_name and "APPLE" not in gpu_name
            if not is_cuda_gpu:
                continue  # Skip Apple Silicon, doesn't have nvidia-smi

            if node.gpu_percent < GPU_IDLE_THRESHOLD:
                # GPU idle with jobs running - track or take action
                if node.node_id not in self.gpu_idle_since:
                    self.gpu_idle_since[node.node_id] = time.time()
                    logger.info(f"{node.node_id}: GPU idle ({node.gpu_percent:.0f}%) with {node.selfplay_jobs} jobs - monitoring")
                else:
                    idle_duration = time.time() - self.gpu_idle_since[node.node_id]
                    if idle_duration >= GPU_IDLE_RESTART_TIMEOUT:
                        logger.info(f"{node.node_id}: STUCK! GPU idle for {idle_duration:.0f}s with {node.selfplay_jobs} jobs")
                        logger.info(f"{node.node_id}: Requesting job restart...")
                        if node.node_id == self.node_id:
                            await self._restart_local_stuck_jobs()
                        else:
                            await self._request_job_restart(node)
                        del self.gpu_idle_since[node.node_id]
            else:
                # GPU is working - clear idle tracking
                if node.node_id in self.gpu_idle_since:
                    del self.gpu_idle_since[node.node_id]

        # Phase 1.6: Detect runaway selfplay processes (lost tracking / manual runs).
        # If a node reports an absurd number of selfplay processes, request a
        # restart sweep to kill untracked jobs and recover capacity.
        for node in all_nodes:
            try:
                target_selfplay = self._target_selfplay_jobs_for_node(node)
                dynamic_threshold = max(16, target_selfplay * 3)
                runaway_threshold = (
                    int(RUNAWAY_SELFPLAY_PROCESS_THRESHOLD)
                    if int(RUNAWAY_SELFPLAY_PROCESS_THRESHOLD) > 0
                    else int(dynamic_threshold)
                )
                if int(getattr(node, "selfplay_jobs", 0) or 0) < runaway_threshold:
                    continue
            except Exception:
                continue

            print(
                f"[P2P] {node.node_id}: RUNAWAY selfplay count ({node.selfplay_jobs}) "
                f">= {runaway_threshold}  requesting restart sweep"
            )
            if node.node_id == self.node_id:
                await self._restart_local_stuck_jobs()
            else:
                await self._request_job_restart(node)

        # Phase 2: Calculate desired job distribution for healthy nodes
        # LEARNED LESSONS - Sort nodes by load score for load balancing
        # Least-loaded nodes get jobs first to ensure even distribution
        healthy_nodes = [n for n in all_nodes if n.is_healthy()]
        healthy_nodes.sort(key=lambda n: n.get_load_score())

        if healthy_nodes:
            load_summary = ", ".join(
                f"{n.node_id[:12]}={n.get_load_score():.0f}%"
                for n in healthy_nodes[:5]
            )
            logger.info(f"Load balancing: {load_summary}")

        for node in healthy_nodes:
            load_score = node.get_load_score()
            if load_score >= LOAD_MAX_FOR_NEW_JOBS:
                logger.info(f"{node.node_id}: Load {load_score:.0f}% - skipping new job starts")
                continue

            # LEARNED LESSONS - Reduce target when approaching limits
            # Base targets:
            # - GPU nodes: fixed concurrency tuned for GPU throughput.
            # - CPU-only nodes: scale with CPU cores (and cap by memory).
            # HYBRID MODE: Get separate GPU and CPU-only job targets
            hybrid_targets = self._get_hybrid_job_targets(node)
            gpu_job_target = hybrid_targets.get("gpu_jobs", 0)
            cpu_only_target = hybrid_targets.get("cpu_only_jobs", 0)
            total_target = hybrid_targets.get("total_jobs", gpu_job_target)

            # Backward compat: use total_target like the old target_selfplay
            target_selfplay = total_target

            # Check if node needs more jobs
            if node.selfplay_jobs < target_selfplay:
                needed = target_selfplay - node.selfplay_jobs
                logger.info(f"{node.node_id} needs {needed} more selfplay jobs")

                # Job configuration diversity - cycle through different AI methods
                # LEARNED LESSONS - Prioritize varied AI methods for better training:
                # - nn-only: Neural network evaluation (NNUE + Descent)
                # - best-vs-pool: Tournament-style asymmetric play (best model vs varied pool)
                # - nn-vs-mcts: NN player vs MCTS player (asymmetric tournament)
                # - nn-vs-minimax: NN player vs Minimax player (asymmetric tournament)
                # - nn-vs-descent: NN player vs heuristic Descent (asymmetric tournament)
                # - tournament-varied: Each player gets different AI type (max variety)
                # - mcts-only: Pure Monte Carlo Tree Search
                # - descent-only: Gradient descent based evaluation (no NN)
                # - minimax-only: Classic minimax with alpha-beta pruning
                # NOTE: Heuristic-only modes removed to ensure NN/strong AI in every game
                selfplay_configs = [
                    # ================================================================
                    # UNDERREPRESENTED COMBINATIONS - MIXED MODE (HIGHEST PRIORITY 8)
                    # "mixed" mode provides varied AI matchups (NNUE, MCTS, heuristic combos)
                    # for maximum training data diversity
                    # ================================================================
                    {"board_type": "hexagonal", "num_players": 3, "engine_mode": "mixed", "priority": 8},
                    {"board_type": "hexagonal", "num_players": 2, "engine_mode": "mixed", "priority": 8},
                    {"board_type": "hexagonal", "num_players": 4, "engine_mode": "mixed", "priority": 8},
                    {"board_type": "hex8", "num_players": 2, "engine_mode": "mixed", "priority": 8},
                    {"board_type": "hex8", "num_players": 3, "engine_mode": "mixed", "priority": 8},
                    {"board_type": "hex8", "num_players": 4, "engine_mode": "mixed", "priority": 8},
                    {"board_type": "square19", "num_players": 3, "engine_mode": "mixed", "priority": 8},
                    {"board_type": "square19", "num_players": 2, "engine_mode": "mixed", "priority": 8},
                    {"board_type": "square19", "num_players": 4, "engine_mode": "mixed", "priority": 8},
                    # ================================================================
                    # UNDERREPRESENTED COMBINATIONS (PRIORITY 7)
                    # Specific AI matchup modes for variety
                    # ================================================================
                    {"board_type": "square19", "num_players": 2, "engine_mode": "nn-only", "priority": 7},
                    {"board_type": "square19", "num_players": 2, "engine_mode": "nn-vs-mcts", "priority": 7},
                    {"board_type": "hexagonal", "num_players": 2, "engine_mode": "nn-only", "priority": 7},
                    {"board_type": "hexagonal", "num_players": 2, "engine_mode": "nn-vs-mcts", "priority": 7},
                    {"board_type": "hexagonal", "num_players": 3, "engine_mode": "nn-only", "priority": 7},
                    {"board_type": "hexagonal", "num_players": 3, "engine_mode": "nn-vs-mcts", "priority": 7},
                    # ================================================================
                    # SQUARE8 MULTI-PLAYER WITH MIXED MODE (PRIORITY 6.5)
                    # ================================================================
                    {"board_type": "square8", "num_players": 3, "engine_mode": "mixed", "priority": 7},
                    {"board_type": "square8", "num_players": 4, "engine_mode": "mixed", "priority": 7},
                    {"board_type": "square8", "num_players": 2, "engine_mode": "mixed", "priority": 6},
                    # ================================================================
                    # CROSS-AI MATCHES (PRIORITY 6) - Variety via asymmetric opponents
                    # heuristic/random vs strong AI (MCTS, Minimax, Descent, NN)
                    # ================================================================
                    {"board_type": "square8", "num_players": 2, "engine_mode": "heuristic-vs-nn", "priority": 6},
                    {"board_type": "square8", "num_players": 2, "engine_mode": "heuristic-vs-mcts", "priority": 6},
                    {"board_type": "square8", "num_players": 2, "engine_mode": "random-vs-mcts", "priority": 6},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "heuristic-vs-nn", "priority": 6},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "heuristic-vs-mcts", "priority": 6},
                    {"board_type": "square19", "num_players": 2, "engine_mode": "heuristic-vs-mcts", "priority": 6},
                    {"board_type": "hexagonal", "num_players": 2, "engine_mode": "heuristic-vs-mcts", "priority": 6},
                    {"board_type": "hexagonal", "num_players": 3, "engine_mode": "heuristic-vs-mcts", "priority": 6},
                    # ================================================================
                    # NEURAL NETWORK MODES (PRIORITY 5)
                    # ================================================================
                    {"board_type": "square8", "num_players": 2, "engine_mode": "nn-only", "priority": 5},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "nn-only", "priority": 5},
                    {"board_type": "square8", "num_players": 4, "engine_mode": "nn-only", "priority": 5},
                    {"board_type": "square8", "num_players": 2, "engine_mode": "best-vs-pool", "priority": 5},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "best-vs-pool", "priority": 5},
                    {"board_type": "square19", "num_players": 3, "engine_mode": "nn-only", "priority": 5},
                    {"board_type": "square19", "num_players": 4, "engine_mode": "nn-only", "priority": 5},
                    {"board_type": "hexagonal", "num_players": 4, "engine_mode": "nn-only", "priority": 5},
                    # ================================================================
                    # ASYMMETRIC TOURNAMENT MODES (PRIORITY 5) - NN vs other AI types
                    # ================================================================
                    {"board_type": "square8", "num_players": 2, "engine_mode": "nn-vs-mcts", "priority": 5},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "nn-vs-mcts", "priority": 5},
                    {"board_type": "square8", "num_players": 4, "engine_mode": "nn-vs-mcts", "priority": 5},
                    {"board_type": "square8", "num_players": 2, "engine_mode": "nn-vs-minimax", "priority": 5},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "nn-vs-minimax", "priority": 5},
                    {"board_type": "square8", "num_players": 2, "engine_mode": "nn-vs-descent", "priority": 5},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "nn-vs-descent", "priority": 5},
                    {"board_type": "square19", "num_players": 3, "engine_mode": "nn-vs-mcts", "priority": 5},
                    {"board_type": "square19", "num_players": 4, "engine_mode": "nn-vs-mcts", "priority": 5},
                    {"board_type": "hexagonal", "num_players": 4, "engine_mode": "nn-vs-mcts", "priority": 5},
                    # ================================================================
                    # TOURNAMENT-VARIED (PRIORITY 4) - Max diversity, always includes NN
                    # ================================================================
                    {"board_type": "square8", "num_players": 2, "engine_mode": "tournament-varied", "priority": 4},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "tournament-varied", "priority": 4},
                    {"board_type": "square8", "num_players": 4, "engine_mode": "tournament-varied", "priority": 4},
                    {"board_type": "square19", "num_players": 2, "engine_mode": "tournament-varied", "priority": 4},
                    {"board_type": "square19", "num_players": 3, "engine_mode": "tournament-varied", "priority": 4},
                    {"board_type": "hexagonal", "num_players": 2, "engine_mode": "tournament-varied", "priority": 4},
                    {"board_type": "hexagonal", "num_players": 3, "engine_mode": "tournament-varied", "priority": 4},
                    # ================================================================
                    # CPU-BOUND AI METHODS (PRIORITY 3) - MCTS, Descent, Minimax
                    # For CPU-only instances and variety
                    # ================================================================
                    {"board_type": "hexagonal", "num_players": 4, "engine_mode": "mcts-only", "priority": 3},
                    {"board_type": "hexagonal", "num_players": 3, "engine_mode": "descent-only", "priority": 3},
                    {"board_type": "square19", "num_players": 4, "engine_mode": "mcts-only", "priority": 3},
                    {"board_type": "square19", "num_players": 3, "engine_mode": "mcts-only", "priority": 3},
                    {"board_type": "square19", "num_players": 3, "engine_mode": "descent-only", "priority": 3},
                    {"board_type": "square8", "num_players": 2, "engine_mode": "mcts-only", "priority": 3},
                    {"board_type": "square8", "num_players": 2, "engine_mode": "descent-only", "priority": 3},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "descent-only", "priority": 3},
                    {"board_type": "square8", "num_players": 4, "engine_mode": "mcts-only", "priority": 3},
                    # ================================================================
                    # MINIMAX (PRIORITY 2) - Classical approach, good for variety
                    # ================================================================
                    {"board_type": "square8", "num_players": 2, "engine_mode": "minimax-only", "priority": 2},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "minimax-only", "priority": 2},
                    {"board_type": "square19", "num_players": 2, "engine_mode": "descent-only", "priority": 2},
                    {"board_type": "square19", "num_players": 2, "engine_mode": "minimax-only", "priority": 2},
                    {"board_type": "hexagonal", "num_players": 2, "engine_mode": "descent-only", "priority": 2},
                    {"board_type": "hexagonal", "num_players": 2, "engine_mode": "minimax-only", "priority": 2},
                    # NO PURE HEURISTIC-ONLY MODES - all modes include at least one
                    # strong AI (NN/MCTS/Descent/Minimax) for quality training data
                ]

                # LEARNED LESSONS - Weighted selection favoring high priority configs
                # Expand configs by priority for weighted random selection
                node_mem = int(getattr(node, "memory_gb", 0) or 0)
                filtered_configs = selfplay_configs
                if node_mem and node_mem < 48:
                    # Smaller CPU nodes should avoid square19/hexagonal to reduce
                    # OOM risk and thrash. Keep them productive with square8.
                    filtered_configs = [cfg for cfg in selfplay_configs if cfg.get("board_type") == "square8"]

                weighted_configs = []
                for cfg in filtered_configs:
                    weighted_configs.extend([cfg] * cfg.get("priority", 1))

                # FIXED: Start all needed jobs with fair config distribution
                # Instead of max 2, start up to 10 at a time to quickly fill all configs
                # Use round-robin across unique configs to ensure coverage
                unique_configs = list({(c["board_type"], c["num_players"]): c for c in filtered_configs}.values())
                jobs_to_start = min(needed, 10)  # Start up to 10 jobs per iteration

                # HYBRID MODE: Calculate how many GPU vs CPU-only jobs to spawn
                # If node already has gpu_job_target GPU jobs, spawn CPU-only jobs instead
                current_gpu_jobs = min(node.selfplay_jobs, gpu_job_target)
                remaining_gpu_slots = max(0, gpu_job_target - current_gpu_jobs)
                remaining_cpu_slots = max(0, cpu_only_target)  # Can always spawn CPU-only if capacity
                should_use_cpu_only = self._should_spawn_cpu_only_jobs(node) and cpu_only_target > 0

                for i in range(jobs_to_start):
                    # LEARNED LESSONS - Smart CPU/GPU task routing:
                    # - High-end GPUs (H100/H200/A100/5090/4090) get GPU_SELFPLAY for max throughput
                    #   with automatic CPU validation to ensure data quality
                    # - Mid-tier GPUs get HYBRID_SELFPLAY (CPU rules + GPU eval)
                    # - CPU-only nodes or GPU-saturated nodes get CPU_SELFPLAY
                    # This ensures expensive GPU resources are utilized properly
                    # while CPU instances handle CPU-bound tasks efficiently
                    gpu_name = (node.gpu_name or "").upper()
                    is_high_end_gpu = any(tag in gpu_name for tag in ("H100", "H200", "GH200", "A100", "5090", "4090"))
                    is_apple_gpu = "MPS" in gpu_name or "APPLE" in gpu_name

                    # GPU utilization check: if GPU is at 0% with jobs running, those jobs
                    # are probably CPU-only. This is an opportunity to START GPU work, not avoid it.
                    # Only consider GPU "unavailable" if there are existing GPU jobs AND utilization is 0%
                    # (which would indicate driver/container issues)
                    gpu_percent = getattr(node, "gpu_percent", 0) or 0
                    gpu_jobs_count = getattr(node, "gpu_selfplay_jobs", 0) or 0
                    gpu_seems_unavailable = (
                        node.has_gpu
                        and not is_apple_gpu
                        and gpu_jobs_count > 2  # Only flag if GPU JOBS (not CPU jobs) are running
                        and gpu_percent < 1
                    )
                    if gpu_seems_unavailable:
                        logger.info(f"WARNING: {node.node_id} has GPU but 0% utilization with {gpu_jobs_count} GPU jobs - possible driver issue")
                    elif node.has_gpu and gpu_percent < 10 and node.selfplay_jobs > 0:
                        # GPU idle but has CPU jobs - this is normal, will prioritize GPU work
                        logger.debug(f"Node {node.node_id} has {node.selfplay_jobs} CPU jobs but GPU idle ({gpu_percent:.0f}%) - will add GPU work")

                    # HYBRID MODE: Decide between GPU and CPU-only based on capacity
                    spawn_cpu_only = False
                    if remaining_gpu_slots > 0 and not gpu_seems_unavailable:
                        remaining_gpu_slots -= 1
                    elif should_use_cpu_only and remaining_cpu_slots > 0:
                        spawn_cpu_only = True
                        remaining_cpu_slots -= 1
                    elif gpu_seems_unavailable:
                        spawn_cpu_only = True

                    if spawn_cpu_only:
                        # Pure CPU selfplay to utilize excess CPU capacity
                        job_type = JobType.CPU_SELFPLAY
                        task_type_str = "CPU-only (hybrid mode)"
                    elif node.has_gpu and is_high_end_gpu and not is_apple_gpu and not gpu_seems_unavailable:
                        # High-end CUDA GPUs: Mix of GPU_SELFPLAY (volume) and GUMBEL_SELFPLAY (quality)
                        # GPU selfplay now has high parity with CPU rules (2025-12 upgrade)
                        # Use Gumbel MCTS ~50% of time for high-quality training data (self-improvement loop)
                        # Increased from 20% to close the training loop - AlphaZero needs NN+MCTS data
                        import random
                        if random.random() < 0.5:  # 50% chance for Gumbel MCTS (quality) - was 20%
                            job_type = JobType.GUMBEL_SELFPLAY
                            task_type_str = "GUMBEL (high-quality)"
                        else:  # 50% for GPU selfplay (volume)
                            job_type = JobType.GPU_SELFPLAY
                            task_type_str = "GPU (high-parity)"
                    elif node.has_gpu and not is_apple_gpu and not gpu_seems_unavailable:
                        # Mid-tier GPUs: Use hybrid (CPU rules + GPU eval)
                        job_type = JobType.HYBRID_SELFPLAY
                        task_type_str = "HYBRID (accel)"
                    else:
                        job_type = JobType.SELFPLAY
                        task_type_str = "CPU-only"

                    gpu_info = f"gpu={node.gpu_name or 'none'}, gpu%={getattr(node, 'gpu_percent', 0):.0f}" if node.has_gpu else "no-gpu"
                    logger.info(f"Assigning {task_type_str} task to {node.node_id} ({gpu_info}, load={node.get_load_score():.0f}%)")

                    # FIXED: Round-robin config selection to ensure all configs get coverage
                    # Use unique_configs list for fair distribution across all 9 board/player combos
                    if self.improvement_cycle_manager and hasattr(self.improvement_cycle_manager, 'get_next_selfplay_config_for_node'):
                        # Node-aware dynamic selection: routes hex/sq19/3p/4p to powerful nodes
                        node_gpu_power = node.gpu_power_score() if hasattr(node, 'gpu_power_score') else 0
                        node_memory = int(getattr(node, 'memory_gb', 0) or 0)
                        config = self.improvement_cycle_manager.get_next_selfplay_config_for_node(
                            node_gpu_power=node_gpu_power,
                            node_memory_gb=node_memory,
                            cluster_data=self.cluster_data_manifest
                        )
                    else:
                        # Round-robin across unique configs for fair coverage
                        # Each iteration picks the next config in the list
                        config_idx = i % len(unique_configs)
                        config = unique_configs[config_idx]

                    # Track diversity metrics for monitoring
                    self._track_selfplay_diversity(config)

                    if node.node_id == self.node_id:
                        await self._start_local_job(
                            job_type,
                            board_type=config["board_type"],
                            num_players=config["num_players"],
                            engine_mode=config["engine_mode"],
                        )
                    else:
                        await self._request_remote_job(
                            node, job_type,
                            board_type=config["board_type"],
                            num_players=config["num_players"],
                            engine_mode=config["engine_mode"],
                        )

    async def _cleanup_local_disk(self):
        """Clean up disk space on local node.

        LEARNED LESSONS - Automatically archive old data:
        - Remove deprecated selfplay databases
        - Compress and archive old logs
        - Clear /tmp files older than 24h
        """
        logger.info("Running local disk cleanup...")
        try:
            # Prefer the shared disk monitor (used by cron/resilience) for consistent cleanup policy.
            disk_monitor = Path(self.ringrift_path) / "ai-service" / "scripts" / "disk_monitor.py"
            if disk_monitor.exists():
                usage = self._get_resource_usage()
                disk_percent = float(usage.get("disk_percent", 0.0) or 0.0)
                cmd = [
                    sys.executable,  # Use venv Python
                    str(disk_monitor),
                    "--threshold",
                    str(DISK_CLEANUP_THRESHOLD),
                    "--ringrift-path",
                    str(self.ringrift_path),
                    "--aggressive",
                ]
                if disk_percent >= DISK_CRITICAL_THRESHOLD:
                    cmd.append("--force")

                out = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    timeout=300,
                    cwd=str(Path(self.ringrift_path) / "ai-service"),
                )
                if out.returncode == 0:
                    logger.info("Disk monitor cleanup completed")
                else:
                    logger.info(f"Disk monitor cleanup failed: {out.stderr[:200]}")
            else:
                # Minimal fallback: clear old logs if disk monitor isn't available.
                log_dir = Path(self.ringrift_path) / "ai-service" / "logs"
                if log_dir.exists():
                    for logfile in log_dir.rglob("*.log"):
                        if time.time() - logfile.stat().st_mtime > 7 * 86400:  # 7 days
                            logfile.unlink()
                            logger.info(f"Cleaned old log: {logfile}")

        except Exception as e:
            logger.info(f"Disk cleanup error: {e}")

    async def _request_remote_cleanup(self, node: NodeInfo):
        """Request a remote node to clean up disk space."""
        try:
            if getattr(node, "nat_blocked", False):
                cmd_id = await self._enqueue_relay_command_for_peer(node, "cleanup", {})
                if cmd_id:
                    logger.info(f"Enqueued relay cleanup for {node.node_id}")
                else:
                    logger.info(f"Relay queue full; skipping cleanup enqueue for {node.node_id}")
                return
            timeout = ClientTimeout(total=HTTP_TOTAL_TIMEOUT)
            async with get_client_session(timeout) as session:
                last_err: str | None = None
                for url in self._urls_for_peer(node, "/cleanup"):
                    try:
                        async with session.post(url, json={}, headers=self._auth_headers()) as resp:
                            if resp.status == 200:
                                logger.info(f"Cleanup requested on {node.node_id}")
                                return
                            last_err = f"http_{resp.status}"
                    except Exception as e:
                        last_err = str(e)
                        continue
                if last_err:
                    logger.info(f"Cleanup request failed on {node.node_id}: {last_err}")
        except Exception as e:
            logger.error(f"Failed to request cleanup from {node.node_id}: {e}")

    async def _reduce_local_selfplay_jobs(self, target_selfplay_jobs: int, *, reason: str) -> dict[str, Any]:
        """Best-effort: stop excess selfplay jobs on this node (load shedding).

        Used when disk/memory pressure is high: we want the node to recover and
        avoid OOM/disk-full scenarios, even if it means reducing throughput.
        """
        try:
            target = max(0, int(target_selfplay_jobs))
        except Exception:
            target = 0

        # First, get an overall count using the same mechanism used for cluster
        # reporting (includes untracked processes).
        try:
            selfplay_before, _training_before = self._count_local_jobs()
        except Exception:
            selfplay_before = 0

        # Hard shedding (target=0): reuse the existing restart sweep, which
        # kills both tracked and untracked selfplay processes.
        if target <= 0:
            await self._restart_local_stuck_jobs()
            try:
                selfplay_after, _training_after = self._count_local_jobs()
            except Exception:
                selfplay_after = 0
            return {
                "running_before": int(selfplay_before),
                "running_after": int(selfplay_after),
                "stopped": max(0, int(selfplay_before) - int(selfplay_after)),
                "target": 0,
                "reason": reason,
            }

        with self.jobs_lock:
            running: list[tuple[str, ClusterJob]] = [
                (job_id, job)
                for job_id, job in self.local_jobs.items()
                if job.status == "running"
                and job.job_type in (JobType.SELFPLAY, JobType.GPU_SELFPLAY, JobType.HYBRID_SELFPLAY, JobType.CPU_SELFPLAY, JobType.GUMBEL_SELFPLAY)
            ]

        if selfplay_before <= target and len(running) <= target:
            return {
                "running_before": int(selfplay_before),
                "running_after": int(selfplay_before),
                "stopped": 0,
                "target": target,
                "reason": reason,
            }

        # Stop newest-first to avoid killing long-running jobs near completion.
        running.sort(key=lambda pair: float(getattr(pair[1], "started_at", 0.0) or 0.0), reverse=True)
        to_stop = running[target:]

        stopped = 0
        with self.jobs_lock:
            for _job_id, job in to_stop:
                try:
                    if job.pid:
                        os.kill(int(job.pid), signal.SIGTERM)
                    job.status = "stopped"
                    stopped += 1
                except Exception:
                    continue

        # If job tracking was lost, we may still have a large number of
        # untracked selfplay processes. Best-effort kill enough to hit target.
        try:
            selfplay_mid, _training_mid = self._count_local_jobs()
        except Exception:
            selfplay_mid = max(0, int(selfplay_before) - stopped)

        if selfplay_mid > target:
            try:
                import shutil

                if shutil.which("pgrep"):
                    pids: list[int] = []
                    for pattern in (
                        "run_self_play_soak.py",
                        "run_gpu_selfplay.py",
                        "run_hybrid_selfplay.py",
                        "run_random_selfplay.py",
                    ):
                        out = subprocess.run(
                            ["pgrep", "-f", pattern],
                            capture_output=True,
                            text=True,
                            timeout=5,
                        )
                        if out.returncode == 0 and out.stdout.strip():
                            for token in out.stdout.strip().split():
                                try:
                                    pids.append(int(token))
                                except Exception:
                                    continue

                    # Kill newest-ish (highest PID) first.
                    pids = sorted(set(pids), reverse=True)
                    excess = int(selfplay_mid) - int(target)
                    killed = 0
                    for pid in pids:
                        if killed >= excess:
                            break
                        try:
                            os.kill(pid, signal.SIGTERM)
                            killed += 1
                        except Exception:
                            continue
                    stopped += killed
            except Exception:
                pass

        if stopped:
            self._save_state()

        try:
            selfplay_after, _training_after = self._count_local_jobs()
        except Exception:
            selfplay_after = max(0, int(selfplay_before) - stopped)

        return {
            "running_before": int(selfplay_before),
            "running_after": int(selfplay_after),
            "stopped": int(max(0, int(selfplay_before) - int(selfplay_after))),
            "target": target,
            "reason": reason,
        }

    async def _request_reduce_selfplay(self, node: NodeInfo, target_selfplay_jobs: int, *, reason: str) -> None:
        """Ask a node to shed excess selfplay (used for memory/disk pressure)."""
        try:
            target = max(0, int(target_selfplay_jobs))
        except Exception:
            target = 0

        if getattr(node, "nat_blocked", False):
            payload = {"target_selfplay_jobs": target, "reason": reason}
            cmd_id = await self._enqueue_relay_command_for_peer(node, "reduce_selfplay", payload)
            if cmd_id:
                logger.info(f"Enqueued relay reduce_selfplay for {node.node_id} (target={target}, reason={reason})")
            else:
                logger.info(f"Relay queue full for {node.node_id}; skipping reduce_selfplay enqueue")
            return

        timeout = ClientTimeout(total=HTTP_TOTAL_TIMEOUT)
        async with get_client_session(timeout) as session:
            last_err: str | None = None
            payload = {"target_selfplay_jobs": target, "reason": reason}
            for url in self._urls_for_peer(node, "/reduce_selfplay"):
                try:
                    async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                        if resp.status == 200:
                            logger.info(f"Requested load shedding on {node.node_id} (target={target}, reason={reason})")
                            return
                        last_err = f"http_{resp.status}"
                except Exception as e:
                    last_err = str(e)
                    continue
            if last_err:
                logger.info(f"reduce_selfplay request failed on {node.node_id}: {last_err}")

    async def _restart_local_stuck_jobs(self):
        """Kill stuck selfplay processes and let job management restart them.

        LEARNED LESSONS - Addresses the issue where processes accumulate but GPU stays at 0%.
        """
        logger.info("Restarting stuck local selfplay jobs...")
        try:
            # Kill tracked selfplay jobs (avoid broad pkill patterns).
            jobs_to_clear: list[str] = []
            pids_to_kill: set[int] = set()
            with self.jobs_lock:
                for job_id, job in self.local_jobs.items():
                    if job.job_type not in (JobType.SELFPLAY, JobType.GPU_SELFPLAY, JobType.HYBRID_SELFPLAY, JobType.CPU_SELFPLAY, JobType.GUMBEL_SELFPLAY):
                        continue
                    jobs_to_clear.append(job_id)
                    if job.pid:
                        try:
                            pids_to_kill.add(int(job.pid))
                        except Exception:
                            continue

            # Sweep for untracked selfplay processes (e.g. lost local_jobs state) and kill them too.
            try:
                import shutil

                if shutil.which("pgrep"):
                    for pattern in (
                        "run_self_play_soak.py",
                        "run_gpu_selfplay.py",
                        "run_hybrid_selfplay.py",
                        "run_random_selfplay.py",
                    ):
                        out = subprocess.run(
                            ["pgrep", "-f", pattern],
                            capture_output=True,
                            text=True,
                            timeout=5,
                        )
                        if out.returncode == 0 and out.stdout.strip():
                            for token in out.stdout.strip().split():
                                try:
                                    pids_to_kill.add(int(token))
                                except Exception:
                                    continue
            except Exception:
                pass

            pids_to_kill.discard(int(os.getpid()))

            killed = 0
            for pid in sorted(pids_to_kill):
                try:
                    os.kill(pid, signal.SIGKILL)
                    killed += 1
                except Exception:
                    continue

            # Clear our job tracking - they'll be restarted next cycle.
            with self.jobs_lock:
                for job_id in jobs_to_clear:
                    self.local_jobs.pop(job_id, None)

            logger.info(f"Killed {killed} processes, cleared {len(jobs_to_clear)} job records")
        except Exception as e:
            logger.error(f"killing stuck processes: {e}")

    async def _request_job_restart(self, node: NodeInfo):
        """Request a remote node to restart its stuck selfplay jobs."""
        try:
            if getattr(node, "nat_blocked", False):
                cmd_id = await self._enqueue_relay_command_for_peer(node, "restart_stuck_jobs", {})
                if cmd_id:
                    logger.info(f"Enqueued relay restart_stuck_jobs for {node.node_id}")
                else:
                    logger.info(f"Relay queue full for {node.node_id}; skipping restart enqueue")
                return
            timeout = ClientTimeout(total=HTTP_TOTAL_TIMEOUT)
            async with get_client_session(timeout) as session:
                last_err: str | None = None
                for url in self._urls_for_peer(node, "/restart_stuck_jobs"):
                    try:
                        async with session.post(url, json={}, headers=self._auth_headers()) as resp:
                            if resp.status != 200:
                                last_err = f"http_{resp.status}"
                                continue
                            data = await resp.json()
                            if data.get("success"):
                                logger.info(f"Job restart requested on {node.node_id}")
                                return
                            last_err = str(data.get("error") or "restart_failed")
                    except Exception as e:
                        last_err = str(e)
                        continue
                if last_err:
                    logger.info(f"Job restart request failed on {node.node_id}: {last_err}")
        except Exception as e:
            logger.error(f"Failed to request job restart from {node.node_id}: {e}")

    async def _start_local_job(
        self,
        job_type: JobType,
        board_type: str = "square8",
        num_players: int = 2,
        engine_mode: str = "gumbel-mcts",  # GPU-accelerated Gumbel MCTS
        job_id: str | None = None,
        cuda_visible_devices: str | None = None,
        export_params: dict[str, Any] | None = None,
    ) -> ClusterJob | None:
        """Start a job on the local node.

        SAFEGUARD: Checks coordination safeguards before spawning.
        """
        try:
            # SAFEGUARD: Check safeguards before spawning
            if HAS_SAFEGUARDS and _safeguards:
                task_type_str = job_type.value if hasattr(job_type, 'value') else str(job_type)
                allowed, reason = check_before_spawn(task_type_str, self.node_id)
                if not allowed:
                    logger.info(f"SAFEGUARD blocked {task_type_str} on {self.node_id}: {reason}")
                    return None

                # Apply backpressure delay
                delay = _safeguards.get_delay()
                if delay > 0:
                    logger.info(f"SAFEGUARD applying {delay:.1f}s backpressure delay")
                    await asyncio.sleep(delay)

            if job_id:
                job_id = str(job_id)
                with self.jobs_lock:
                    existing = self.local_jobs.get(job_id)
                if existing and existing.status == "running":
                    return existing
            else:
                job_id = str(uuid.uuid4())[:8]

            if job_type == JobType.SELFPLAY:
                # Normalize engine_mode to what run_self_play_soak.py supports.
                # LEARNED LESSONS - Variety of AI methods for better training:
                # - nn-only: Uses NNUE/neural network evaluation
                # - best-vs-pool: Tournament-style with varied opponents
                # - mcts-only/descent-only/minimax-only: Single AI method
                supported_engine_modes = {
                    "descent-only",
                    "mixed",
                    "random-only",
                    "heuristic-only",
                    "minimax-only",
                    "mcts-only",
                    "nn-only",
                    "best-vs-pool",
                    # Cross-AI asymmetric matches for variety
                    "nn-vs-mcts",
                    "nn-vs-minimax",
                    "nn-vs-descent",
                    "tournament-varied",
                    "heuristic-vs-nn",
                    "heuristic-vs-mcts",
                    "random-vs-mcts",
                }
                engine_mode_norm = engine_mode if engine_mode in supported_engine_modes else "nn-only"

                # Memory-safety defaults for large boards.
                num_games = 1000
                extra_args: list[str] = []
                if board_type in ("square19", "hexagonal"):
                    num_games = 200 if board_type == "square19" else 100
                    extra_args.extend(["--memory-constrained"])

                output_dir = Path(
                    self.ringrift_path,
                    "ai-service",
                    "data",
                    "selfplay",
                    "p2p",
                    f"{board_type}_{num_players}p",
                    job_id,
                )
                output_dir.mkdir(parents=True, exist_ok=True)

                # Use venv python if available, otherwise fall back to system python3
                venv_python = Path(self.ringrift_path, "ai-service", "venv", "bin", "python")
                python_exec = str(venv_python) if venv_python.exists() else "python3"

                cmd = [
                    python_exec,
                    f"{self.ringrift_path}/ai-service/scripts/run_self_play_soak.py",
                    "--num-games", str(num_games),
                    "--board-type", board_type,
                    "--num-players", str(num_players),
                    "--engine-mode", engine_mode_norm,
                    "--max-moves", "10000",  # LEARNED LESSONS - Avoid draws due to move limit
                    "--log-jsonl", str(output_dir / "games.jsonl"),
                    "--summary-json", str(output_dir / "summary.json"),
                    "--record-db", str(output_dir / "games.db"),
                    "--lean-db",
                    "--verbose", "0",
                    *extra_args,
                ]

                # Start process
                env = os.environ.copy()
                env["PYTHONPATH"] = f"{self.ringrift_path}/ai-service"
                env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"
                env["RINGRIFT_JOB_ORIGIN"] = "p2p_orchestrator"

                # SAFEGUARD: Final check before spawning (load + rate limit)
                can_spawn, spawn_reason = self._can_spawn_process(f"selfplay-{board_type}-{num_players}p")
                if not can_spawn:
                    logger.info(f"BLOCKED selfplay spawn: {spawn_reason}")
                    return None

                log_handle = open(output_dir / "run.log", "a")
                try:
                    proc = subprocess.Popen(
                        cmd,
                        stdout=log_handle,
                        stderr=subprocess.STDOUT,
                        env=env,
                        cwd=self.ringrift_path,
                    )
                    self._record_spawn()  # Track spawn for rate limiting
                finally:
                    log_handle.close()

                job = ClusterJob(
                    job_id=job_id,
                    job_type=job_type,
                    node_id=self.node_id,
                    board_type=board_type,
                    num_players=num_players,
                    engine_mode=engine_mode_norm,
                    pid=proc.pid,
                    started_at=time.time(),
                    status="running",
                )

                with self.jobs_lock:
                    self.local_jobs[job_id] = job

                logger.info(f"Started {job_type.value} job {job_id} (PID {proc.pid})")
                self._save_state()
                return job

            elif job_type == JobType.CPU_SELFPLAY:
                # Pure CPU selfplay for high-CPU nodes with limited GPU VRAM
                # Uses CPU-efficient engine modes
                # This enables utilizing excess CPU capacity on Vast.ai hosts etc.

                # CPU-friendly engine modes - include descent, mcts, and nn-only (work on CPU)
                # run_self_play_soak.py supports all these modes
                cpu_engine_modes = {
                    "descent-only", "minimax-only", "mcts-only", "heuristic-only",
                    "random-only", "mixed", "nn-only", "best-vs-pool",
                    # Cross-AI asymmetric matches
                    "nn-vs-mcts", "nn-vs-minimax", "nn-vs-descent", "tournament-varied",
                    "heuristic-vs-nn", "heuristic-vs-mcts", "random-vs-mcts",
                }
                engine_mode_norm = engine_mode if engine_mode in cpu_engine_modes else "nn-only"

                # CPU-only jobs can handle more games per batch
                num_games = 2000
                extra_args: list[str] = []
                if board_type in ("square19", "hexagonal"):
                    num_games = 400 if board_type == "square19" else 200
                    extra_args.extend(["--memory-constrained"])

                output_dir = Path(
                    self.ringrift_path,
                    "ai-service",
                    "data",
                    "selfplay",
                    "p2p",
                    f"{board_type}_{num_players}p_cpu",  # Separate subdir for CPU-only
                    job_id,
                )
                output_dir.mkdir(parents=True, exist_ok=True)

                venv_python = Path(self.ringrift_path, "ai-service", "venv", "bin", "python")
                python_exec = str(venv_python) if venv_python.exists() else "python3"

                cmd = [
                    python_exec,
                    f"{self.ringrift_path}/ai-service/scripts/run_self_play_soak.py",
                    "--num-games", str(num_games),
                    "--board-type", board_type,
                    "--num-players", str(num_players),
                    "--engine-mode", engine_mode_norm,
                    "--max-moves", "10000",
                    "--log-jsonl", str(output_dir / "games.jsonl"),
                    "--summary-json", str(output_dir / "summary.json"),
                    "--record-db", str(output_dir / "games.db"),
                    "--lean-db",
                    "--verbose", "0",
                    *extra_args,
                ]

                env = os.environ.copy()
                env["PYTHONPATH"] = f"{self.ringrift_path}/ai-service"
                env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"
                env["RINGRIFT_JOB_ORIGIN"] = "p2p_orchestrator"
                env["CUDA_VISIBLE_DEVICES"] = ""  # Disable GPU for CPU-only jobs

                can_spawn, spawn_reason = self._can_spawn_process(f"cpu-selfplay-{board_type}-{num_players}p")
                if not can_spawn:
                    logger.info(f"BLOCKED CPU selfplay spawn: {spawn_reason}")
                    return None

                log_handle = open(output_dir / "run.log", "a")
                try:
                    proc = subprocess.Popen(
                        cmd,
                        stdout=log_handle,
                        stderr=subprocess.STDOUT,
                        env=env,
                        cwd=self.ringrift_path,
                    )
                    self._record_spawn()
                finally:
                    log_handle.close()

                job = ClusterJob(
                    job_id=job_id,
                    job_type=job_type,
                    node_id=self.node_id,
                    board_type=board_type,
                    num_players=num_players,
                    engine_mode=engine_mode_norm,
                    pid=proc.pid,
                    started_at=time.time(),
                    status="running",
                )

                with self.jobs_lock:
                    self.local_jobs[job_id] = job

                logger.info(f"Started {job_type.value} job {job_id} (PID {proc.pid}) [CPU-only hybrid mode]")
                self._save_state()
                return job

            elif job_type == JobType.GPU_SELFPLAY:
                # DIVERSE selfplay using run_diverse_selfplay.py for high-quality training data
                # Uses varied AI matchups (NNUE, NN-MCTS, NN-Minimax, heuristic)
                # NOTE: Renamed from GPU_SELFPLAY but job type kept for backwards compatibility

                # run_diverse_selfplay expects --board (square8/square19/hexagonal).
                board_arg = {
                    "square8": "square8",
                    "square19": "square19",
                    "hexagonal": "hexagonal",
                    "hex": "hexagonal",
                }.get(board_type, "square8")

                # Games per matchup - diverse selfplay generates multiple matchup types
                games_per_matchup = 100
                if board_arg == "square19":
                    games_per_matchup = 50  # Square19 games are longer
                elif board_arg == "hexagonal":
                    games_per_matchup = 100

                output_dir = Path(
                    self.ringrift_path,
                    "ai-service",
                    "data",
                    "games",
                )
                output_dir.mkdir(parents=True, exist_ok=True)

                # Use venv python if available, otherwise fall back to system python3
                venv_python = Path(self.ringrift_path, "ai-service", "venv", "bin", "python")
                python_exec = str(venv_python) if venv_python.exists() else "python3"

                cmd = [
                    python_exec,
                    f"{self.ringrift_path}/ai-service/scripts/run_diverse_selfplay.py",
                    "--board", board_arg,
                    "--players", str(num_players),
                    "--games-per-matchup", str(games_per_matchup),
                    "--output-dir", str(output_dir),
                ]

                # Start process with GPU environment
                env = os.environ.copy()
                env["PYTHONPATH"] = f"{self.ringrift_path}/ai-service"
                env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"
                env["RINGRIFT_JOB_ORIGIN"] = "p2p_orchestrator"

                if cuda_visible_devices is not None and str(cuda_visible_devices).strip():
                    env["CUDA_VISIBLE_DEVICES"] = str(cuda_visible_devices).strip()
                # Choose a GPU automatically if not explicitly pinned.
                elif "CUDA_VISIBLE_DEVICES" not in env:
                    gpu_count = 0
                    try:
                        out = subprocess.run(
                            ["nvidia-smi", "--query-gpu=name", "--format=csv,noheader"],
                            capture_output=True, text=True, timeout=5
                        )
                        if out.returncode == 0 and out.stdout.strip():
                            gpu_count = len([line for line in out.stdout.splitlines() if line.strip()])
                    except Exception:
                        gpu_count = 0

                    if gpu_count > 0:
                        with self.jobs_lock:
                            running_gpu_jobs = sum(
                                1 for j in self.local_jobs.values()
                                if j.job_type == JobType.GPU_SELFPLAY and j.status == "running"
                            )
                        env["CUDA_VISIBLE_DEVICES"] = str(running_gpu_jobs % gpu_count)
                    else:
                        env["CUDA_VISIBLE_DEVICES"] = "0"

                # SAFEGUARD: Final check before spawning (load + rate limit)
                can_spawn, spawn_reason = self._can_spawn_process(f"gpu-selfplay-{board_type}-{num_players}p")
                if not can_spawn:
                    logger.info(f"BLOCKED GPU selfplay spawn: {spawn_reason}")
                    return None

                log_handle = open(output_dir / "gpu_run.log", "a")
                try:
                    proc = subprocess.Popen(
                        cmd,
                        stdout=log_handle,
                        stderr=subprocess.STDOUT,
                        env=env,
                        cwd=self.ringrift_path,
                    )
                    self._record_spawn()  # Track spawn for rate limiting
                finally:
                    log_handle.close()

                # Use gumbel-mcts for GPU selfplay (177x speedup with GPU tree)
                gpu_engine_mode = "gumbel-mcts"
                batch_size = games_per_matchup

                job = ClusterJob(
                    job_id=job_id,
                    job_type=job_type,
                    node_id=self.node_id,
                    board_type=board_type,
                    num_players=num_players,
                    engine_mode=gpu_engine_mode,
                    pid=proc.pid,
                    started_at=time.time(),
                    status="running",
                )

                with self.jobs_lock:
                    self.local_jobs[job_id] = job

                logger.info(f"Started GPU selfplay job {job_id} (PID {proc.pid}, batch={batch_size})")
                self._save_state()

                # Monitor GPU selfplay and trigger CPU validation when complete
                asyncio.create_task(self._monitor_gpu_selfplay_and_validate(
                    job_id, proc, output_dir, board_type, num_players
                ))

                return job

            elif job_type == JobType.HYBRID_SELFPLAY:
                # Hybrid CPU/GPU selfplay using run_hybrid_selfplay.py
                # Uses CPU for game rules (100% canonical) but GPU for heuristic evaluation
                # This is the recommended default for GPU nodes

                # Normalize engine_mode
                # run_hybrid_selfplay.py supports: random-only, heuristic-only, mixed, nnue-guided, mcts
                # Map NN-based modes to nnue-guided for neural network evaluation
                hybrid_engine_modes = {"random-only", "heuristic-only", "mixed", "nnue-guided", "mcts"}
                nn_modes = {"nn-only", "best-vs-pool", "nn-vs-mcts", "nn-vs-minimax", "nn-vs-descent", "tournament-varied"}
                if engine_mode in hybrid_engine_modes:
                    engine_mode_norm = engine_mode
                elif engine_mode in nn_modes:
                    engine_mode_norm = "nnue-guided"  # Use neural network
                elif engine_mode in ("mcts-only", "descent-only"):
                    engine_mode_norm = "mcts"  # Use MCTS
                elif engine_mode == "minimax-only":
                    engine_mode_norm = "mixed"  # Minimax included in mixed
                else:
                    engine_mode_norm = "heuristic-only"

                # Game counts based on board type
                num_games = 1000
                if board_type == "square19":
                    num_games = 500
                elif board_type in ("hex", "hexagonal"):
                    num_games = 300

                output_dir = Path(
                    self.ringrift_path,
                    "ai-service",
                    "data",
                    "selfplay",
                    "p2p_hybrid",
                    f"{board_type}_{num_players}p",
                    job_id,
                )
                output_dir.mkdir(parents=True, exist_ok=True)

                # Normalize board type for hybrid script (uses 'hex' not 'hexagonal')
                board_arg = "hex" if board_type == "hexagonal" else board_type

                # Use venv python if available, otherwise fall back to system python3
                venv_python = Path(self.ringrift_path, "ai-service", "venv", "bin", "python")
                python_exec = str(venv_python) if venv_python.exists() else "python3"

                cmd = [
                    python_exec,
                    f"{self.ringrift_path}/ai-service/scripts/run_hybrid_selfplay.py",
                    "--board-type", board_arg,
                    "--num-players", str(num_players),
                    "--num-games", str(num_games),
                    "--output-dir", str(output_dir),
                    "--record-db", str(output_dir / "games.db"),
                    "--lean-db",
                    "--engine-mode", engine_mode_norm,
                    "--seed", str(int(time.time() * 1000) % 2**31),
                ]

                # Start process with GPU environment
                env = os.environ.copy()
                env["PYTHONPATH"] = f"{self.ringrift_path}/ai-service"
                env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"
                env["RINGRIFT_JOB_ORIGIN"] = "p2p_orchestrator"

                if cuda_visible_devices is not None and str(cuda_visible_devices).strip():
                    env["CUDA_VISIBLE_DEVICES"] = str(cuda_visible_devices).strip()
                elif "CUDA_VISIBLE_DEVICES" not in env:
                    gpu_count = 0
                    try:
                        out = subprocess.run(
                            ["nvidia-smi", "--query-gpu=name", "--format=csv,noheader"],
                            capture_output=True,
                            text=True,
                            timeout=5,
                        )
                        if out.returncode == 0 and out.stdout.strip():
                            gpu_count = len([line for line in out.stdout.splitlines() if line.strip()])
                    except Exception:
                        gpu_count = 0

                    if gpu_count > 0:
                        with self.jobs_lock:
                            running_hybrid_jobs = sum(
                                1
                                for j in self.local_jobs.values()
                                if j.job_type == JobType.HYBRID_SELFPLAY and j.status == "running"
                            )
                        env["CUDA_VISIBLE_DEVICES"] = str(running_hybrid_jobs % gpu_count)
                    else:
                        env["CUDA_VISIBLE_DEVICES"] = "0"

                # SAFEGUARD: Final check before spawning (load + rate limit)
                can_spawn, spawn_reason = self._can_spawn_process(f"hybrid-selfplay-{board_type}-{num_players}p")
                if not can_spawn:
                    logger.info(f"BLOCKED hybrid selfplay spawn: {spawn_reason}")
                    return None

                log_handle = open(output_dir / "hybrid_run.log", "a")
                try:
                    proc = subprocess.Popen(
                        cmd,
                        stdout=log_handle,
                        stderr=subprocess.STDOUT,
                        env=env,
                        cwd=self.ringrift_path,
                    )
                    self._record_spawn()  # Track spawn for rate limiting
                finally:
                    log_handle.close()

                job = ClusterJob(
                    job_id=job_id,
                    job_type=job_type,
                    node_id=self.node_id,
                    board_type=board_type,
                    num_players=num_players,
                    engine_mode=engine_mode_norm,
                    pid=proc.pid,
                    started_at=time.time(),
                    status="running",
                )

                with self.jobs_lock:
                    self.local_jobs[job_id] = job

                logger.info(f"Started HYBRID selfplay job {job_id} (PID {proc.pid})")
                self._save_state()
                return job

            elif job_type == JobType.GUMBEL_SELFPLAY:
                # High-quality Gumbel MCTS selfplay with NN policy for self-improvement training
                # Uses generate_gumbel_selfplay.py with proper MCTS simulation budget
                # AlphaZero used 800 sims - we match this for maximum move quality

                # Games and simulation budget based on board type
                # 800 sims = AlphaZero standard for high-quality training data
                num_games = 20  # Reduced for 800-sim budget
                simulation_budget = 800  # AlphaZero standard
                if board_type == "square19":
                    num_games = 10  # Fewer games for large board
                    simulation_budget = 800
                elif board_type in ("hex", "hexagonal", "hex8"):
                    num_games = 20
                    simulation_budget = 800

                output_dir = Path(
                    self.ringrift_path,
                    "ai-service",
                    "data",
                    "selfplay",
                    "gumbel",
                    f"{board_type}_{num_players}p",
                    job_id,
                )
                output_dir.mkdir(parents=True, exist_ok=True)

                # Normalize board type for gumbel script
                board_arg = {
                    "hex": "hexagonal",
                    "hex8": "hex8",
                }.get(board_type, board_type)

                # Use venv python if available
                venv_python = Path(self.ringrift_path, "ai-service", "venv", "bin", "python")
                python_exec = str(venv_python) if venv_python.exists() else "python3"

                cmd = [
                    python_exec,
                    f"{self.ringrift_path}/ai-service/scripts/generate_gumbel_selfplay.py",
                    "--board", board_arg,
                    "--num-players", str(num_players),
                    "--num-games", str(num_games),
                    "--simulation-budget", str(simulation_budget),
                    "--output-dir", str(output_dir),
                    "--db", str(output_dir / "games.db"),
                    "--seed", str(int(time.time() * 1000) % 2**31),
                    "--allow-fresh-weights",  # Allow running even without trained model
                    "--use-gpu-tree",  # 170x speedup with GPU tensor tree MCTS (RR-GPU-TREE-001 fixed)
                ]

                # Start process with GPU environment
                env = os.environ.copy()
                env["PYTHONPATH"] = f"{self.ringrift_path}/ai-service"
                env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"
                env["RINGRIFT_JOB_ORIGIN"] = "p2p_orchestrator"

                if cuda_visible_devices is not None and str(cuda_visible_devices).strip():
                    env["CUDA_VISIBLE_DEVICES"] = str(cuda_visible_devices).strip()
                elif "CUDA_VISIBLE_DEVICES" not in env:
                    gpu_count = 0
                    try:
                        out = subprocess.run(
                            ["nvidia-smi", "--query-gpu=name", "--format=csv,noheader"],
                            capture_output=True,
                            text=True,
                            timeout=5,
                        )
                        if out.returncode == 0 and out.stdout.strip():
                            gpu_count = len([line for line in out.stdout.splitlines() if line.strip()])
                    except Exception:
                        gpu_count = 0

                    if gpu_count > 0:
                        with self.jobs_lock:
                            running_gumbel_jobs = sum(
                                1
                                for j in self.local_jobs.values()
                                if j.job_type == JobType.GUMBEL_SELFPLAY and j.status == "running"
                            )
                        env["CUDA_VISIBLE_DEVICES"] = str(running_gumbel_jobs % gpu_count)
                    else:
                        env["CUDA_VISIBLE_DEVICES"] = "0"

                # SAFEGUARD: Final check before spawning
                can_spawn, spawn_reason = self._can_spawn_process(f"gumbel-selfplay-{board_type}-{num_players}p")
                if not can_spawn:
                    logger.info(f"BLOCKED gumbel selfplay spawn: {spawn_reason}")
                    return None

                log_handle = open(output_dir / "gumbel_run.log", "a")
                try:
                    proc = subprocess.Popen(
                        cmd,
                        stdout=log_handle,
                        stderr=subprocess.STDOUT,
                        env=env,
                        cwd=self.ringrift_path,
                    )
                    self._record_spawn()
                finally:
                    log_handle.close()

                job = ClusterJob(
                    job_id=job_id,
                    job_type=job_type,
                    node_id=self.node_id,
                    board_type=board_type,
                    num_players=num_players,
                    engine_mode="gumbel-mcts",
                    pid=proc.pid,
                    started_at=time.time(),
                    status="running",
                )

                with self.jobs_lock:
                    self.local_jobs[job_id] = job

                logger.info(f"Started GUMBEL selfplay job {job_id} (PID {proc.pid}, sims={simulation_budget})")
                self._save_state()
                return job

            elif job_type == JobType.DATA_EXPORT:
                # CPU-intensive data export job (NPZ creation)
                # These jobs should be routed to high-CPU nodes (vast nodes preferred)
                if not export_params:
                    logger.info("DATA_EXPORT job requires export_params")
                    return None

                input_path = export_params.get("input_path")
                output_path = export_params.get("output_path")
                encoder_version = export_params.get("encoder_version", "v3")
                max_games = export_params.get("max_games", 5000)
                is_jsonl = export_params.get("is_jsonl", False)

                if not input_path or not output_path:
                    logger.info("DATA_EXPORT requires input_path and output_path")
                    return None

                # Ensure output directory exists
                output_dir = Path(output_path).parent
                output_dir.mkdir(parents=True, exist_ok=True)

                # Use venv python if available
                venv_python = Path(self.ringrift_path, "ai-service", "venv", "bin", "python")
                python_exec = str(venv_python) if venv_python.exists() else "python3"

                if is_jsonl:
                    # Use jsonl_to_npz.py for JSONL input (GPU selfplay data)
                    export_script = f"{self.ringrift_path}/ai-service/scripts/jsonl_to_npz.py"
                    cmd = [
                        python_exec,
                        export_script,
                        "--input", str(input_path),
                        "--output", str(output_path),
                        "--board-type", board_type,
                        "--num-players", str(num_players),
                        "--gpu-selfplay",
                        "--max-games", str(max_games),
                    ]
                    if encoder_version and encoder_version != "default":
                        cmd.extend(["--encoder-version", encoder_version])
                else:
                    # Use export_replay_dataset.py for DB input
                    export_script = f"{self.ringrift_path}/ai-service/scripts/export_replay_dataset.py"
                    cmd = [
                        python_exec,
                        export_script,
                        "--db", str(input_path),
                        "--output", str(output_path),
                        "--board-type", board_type,
                        "--num-players", str(num_players),
                        "--max-games", str(max_games),
                        "--require-completed",
                        "--min-moves", "10",
                    ]
                    if encoder_version and encoder_version != "default":
                        cmd.extend(["--encoder-version", encoder_version])

                # Start export process
                env = os.environ.copy()
                env["PYTHONPATH"] = f"{self.ringrift_path}/ai-service"
                env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

                log_path = output_dir / f"export_{job_id}.log"
                log_handle = open(log_path, "w")
                try:
                    proc = subprocess.Popen(
                        cmd,
                        stdout=log_handle,
                        stderr=subprocess.STDOUT,
                        env=env,
                        cwd=f"{self.ringrift_path}/ai-service",
                    )
                finally:
                    log_handle.close()

                job = ClusterJob(
                    job_id=job_id,
                    job_type=job_type,
                    node_id=self.node_id,
                    board_type=board_type,
                    num_players=num_players,
                    engine_mode="export",
                    pid=proc.pid,
                    started_at=time.time(),
                    status="running",
                )

                with self.jobs_lock:
                    self.local_jobs[job_id] = job

                logger.info(f"Started DATA_EXPORT job {job_id} (PID {proc.pid}): {input_path} -> {output_path}")
                self._save_state()
                return job

        except Exception as e:
            logger.error(f"Failed to start job: {e}")
        return None

    async def _dispatch_export_job(
        self,
        node: NodeInfo,
        input_path: str,
        output_path: str,
        board_type: str,
        num_players: int,
        encoder_version: str = "v3",
        max_games: int = 5000,
        is_jsonl: bool = False,
    ):
        """Dispatch a CPU-intensive export job to a high-CPU node.

        CPU-intensive jobs like NPZ export should run on vast nodes
        (256-512 CPUs) rather than lambda nodes (64 CPUs) to free
        GPU resources for training/selfplay.
        """
        try:
            job_id = f"export_{board_type}_{num_players}p_{int(time.time())}_{uuid.uuid4().hex[:6]}"

            payload = {
                "job_id": job_id,
                "job_type": JobType.DATA_EXPORT.value,
                "board_type": board_type,
                "num_players": num_players,
                "input_path": input_path,
                "output_path": output_path,
                "encoder_version": encoder_version,
                "max_games": max_games,
                "is_jsonl": is_jsonl,
            }

            # NAT-blocked nodes need relay command
            if getattr(node, "nat_blocked", False):
                cmd_id = await self._enqueue_relay_command_for_peer(node, "start_job", payload)
                if cmd_id:
                    logger.info(f"Enqueued relay export job for {node.node_id}: {job_id}")
                else:
                    logger.info(f"Relay queue full for {node.node_id}; export not dispatched")
                return

            timeout = ClientTimeout(total=30)
            async with get_client_session(timeout) as session:
                last_err: str | None = None
                for url in self._urls_for_peer(node, "/start_job"):
                    try:
                        async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                            if resp.status == 200:
                                result = await resp.json()
                                if result.get("success"):
                                    logger.info(f"Export job dispatched to {node.node_id}: {job_id}")
                                    return
                                last_err = result.get("error", "unknown")
                            else:
                                last_err = f"http_{resp.status}"
                    except Exception as e:
                        last_err = str(e)

                if last_err:
                    logger.info(f"Export job dispatch failed to {node.node_id}: {last_err}")

        except Exception as e:
            logger.error(f"Failed to dispatch export job to {node.node_id}: {e}")

    async def _request_remote_job(
        self,
        node: NodeInfo,
        job_type: JobType,
        board_type: str = "square8",
        num_players: int = 2,
        engine_mode: str = "hybrid",
    ):
        """Request a remote node to start a job with specific configuration.

        SAFEGUARD: Checks coordination safeguards before requesting remote spawn.
        """
        try:
            # SAFEGUARD: Check safeguards before requesting remote spawn
            if HAS_SAFEGUARDS and _safeguards:
                task_type_str = job_type.value if hasattr(job_type, 'value') else str(job_type)
                allowed, reason = check_before_spawn(task_type_str, node.node_id)
                if not allowed:
                    logger.info(f"SAFEGUARD blocked remote {task_type_str} on {node.node_id}: {reason}")
                    return

            job_id = f"{job_type.value}_{board_type}_{num_players}p_{int(time.time())}_{uuid.uuid4().hex[:6]}"

            # NAT-blocked nodes can't accept inbound /start_job; enqueue a relay command instead.
            if getattr(node, "nat_blocked", False):
                payload = {
                    "job_id": job_id,
                    "job_type": job_type.value,
                    "board_type": board_type,
                    "num_players": num_players,
                    "engine_mode": engine_mode,
                }
                cmd_id = await self._enqueue_relay_command_for_peer(node, "start_job", payload)
                if cmd_id:
                    print(
                        f"[P2P] Enqueued relay job for {node.node_id}: "
                        f"{job_type.value} {board_type} {num_players}p ({job_id})"
                    )
                else:
                    logger.info(f"Relay queue full for {node.node_id}; skipping enqueue")
                return

            timeout = ClientTimeout(total=10)
            async with get_client_session(timeout) as session:
                payload = {
                    "job_id": job_id,
                    "job_type": job_type.value,
                    "board_type": board_type,
                    "num_players": num_players,
                    "engine_mode": engine_mode,
                }
                last_err: str | None = None
                for url in self._urls_for_peer(node, "/start_job"):
                    try:
                        async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                            if resp.status != 200:
                                last_err = f"http_{resp.status}"
                                continue
                            data = await resp.json()
                            if data.get("success"):
                                logger.info(f"Started remote {board_type} {num_players}p job on {node.node_id}")
                                return
                            last_err = str(data.get("error") or "start_failed")
                    except Exception as e:
                        last_err = str(e)
                        continue
                if last_err:
                    logger.error(f"Failed to start remote job on {node.node_id}: {last_err}")
        except Exception as e:
            logger.error(f"Failed to request remote job from {node.node_id}: {e}")

    def _enqueue_relay_command(self, node_id: str, cmd_type: str, payload: dict[str, Any]) -> str | None:
        """Leader-side: enqueue a command for a NAT-blocked node to pull."""
        now = time.time()
        cmd_type = str(cmd_type)
        payload = dict(payload or {})

        with self.relay_lock:
            queue = list(self.relay_command_queue.get(node_id, []))
            queue = [
                cmd for cmd in queue
                if float(cmd.get("expires_at", 0.0) or 0.0) > now
            ]

            if cmd_type == "start_job":
                pending = sum(1 for c in queue if str(c.get("type") or "") == "start_job")
                if pending >= RELAY_MAX_PENDING_START_JOBS:
                    self.relay_command_queue[node_id] = queue
                    return None

                job_id = str(payload.get("job_id") or "")
                if job_id:
                    for c in queue:
                        if str(c.get("payload", {}).get("job_id") or "") == job_id:
                            self.relay_command_queue[node_id] = queue
                            return str(c.get("id") or "")

            cmd_id = uuid.uuid4().hex
            queue.append(
                {
                    "id": cmd_id,
                    "type": cmd_type,
                    "payload": payload,
                    "created_at": now,
                    "expires_at": now + RELAY_COMMAND_TTL_SECONDS,
                }
            )
            self.relay_command_queue[node_id] = queue
            return cmd_id

    async def _enqueue_relay_command_for_peer(
        self,
        peer: NodeInfo,
        cmd_type: str,
        payload: dict[str, Any],
    ) -> str | None:
        """Enqueue a relay command for `peer`, forwarding via its relay hub when needed.

        Default behavior: NAT-blocked nodes poll the leader's `/relay/heartbeat`
        endpoint and the leader stores commands in-memory.

        Some nodes (notably certain containerized GPU providers) may be unable to
        reach the leader over the mesh network (e.g. TUN-less Tailscale) and also
        cannot accept inbound connections. Those nodes will instead send relay
        heartbeats to an internet-reachable hub (e.g. `aws-staging`). When
        `peer.relay_via` points to such a hub, the leader must enqueue the relay
        command on that hub so the node can pull and execute it.
        """
        if not peer or not getattr(peer, "node_id", ""):
            return None

        peer_id = str(getattr(peer, "node_id", "") or "").strip()
        if not peer_id:
            return None

        relay_node_id = str(getattr(peer, "relay_via", "") or "").strip()
        if relay_node_id and relay_node_id != self.node_id:
            with self.peers_lock:
                relay_peer = self.peers.get(relay_node_id)
            if relay_peer:
                timeout = ClientTimeout(total=10)
                async with get_client_session(timeout) as session:
                    last_err: str | None = None
                    for url in self._urls_for_peer(relay_peer, "/relay/enqueue"):
                        try:
                            async with session.post(
                                url,
                                json={
                                    "target_node_id": peer_id,
                                    "type": cmd_type,
                                    "payload": payload or {},
                                },
                                headers=self._auth_headers(),
                            ) as resp:
                                if resp.status != 200:
                                    last_err = f"http_{resp.status}"
                                    continue
                                data = await resp.json()
                                if data.get("success"):
                                    return str(data.get("id") or "")
                                last_err = str(data.get("error") or "enqueue_failed")
                        except Exception as e:
                            last_err = str(e)
                            continue
                    if last_err:
                        logger.info(f"Relay enqueue via {relay_node_id} failed for {peer_id}: {last_err}")

        # Fallback: enqueue locally (works when peer polls the leader directly).
        return self._enqueue_relay_command(peer_id, cmd_type, payload)

    async def _discovery_loop(self):
        """Broadcast UDP discovery messages to find peers on local network."""
        while self.running:
            try:
                # Create UDP socket
                sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
                sock.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)
                sock.settimeout(1.0)

                # Broadcast our presence
                message = json.dumps({
                    "type": "p2p_discovery",
                    "node_id": self.node_id,
                    "host": self.self_info.host,
                    "port": self.port,
                }).encode()

                try:
                    sock.sendto(message, ('<broadcast>', DISCOVERY_PORT))
                except OSError:
                    pass  # Broadcast may fail on some networks

                # Listen for responses
                try:
                    while True:
                        data, _addr = sock.recvfrom(1024)
                        msg = json.loads(data.decode())
                        if msg.get("type") == "p2p_discovery" and msg.get("node_id") != self.node_id:
                            # Found a peer!
                            peer_addr = f"{msg.get('host')}:{msg.get('port')}"
                            if peer_addr not in self.known_peers:
                                self.known_peers.append(peer_addr)
                                logger.info(f"Discovered peer: {msg.get('node_id')} at {peer_addr}")
                except TimeoutError:
                    pass

                sock.close()

            except Exception:
                pass

            await asyncio.sleep(DISCOVERY_INTERVAL)

    async def run(self):
        """Main entry point - start the orchestrator."""
        if not HAS_AIOHTTP:
            logger.error("aiohttp is required. Install with: pip install aiohttp")
            return

        # Set up HTTP server
        @web.middleware
        async def auth_middleware(request: web.Request, handler):
            if self.auth_token and request.method not in ("GET", "HEAD", "OPTIONS") and not self._is_request_authorized(request):
                return web.json_response({"error": "unauthorized"}, status=401)
            return await handler(request)

        app = web.Application(middlewares=[auth_middleware])
        app.router.add_post('/heartbeat', self.handle_heartbeat)
        app.router.add_get('/status', self.handle_status)
        app.router.add_get('/external_work', self.handle_external_work)

        # Work queue routes (centralized work distribution)
        app.router.add_post('/work/add', self.handle_work_add)
        app.router.add_post('/work/add_batch', self.handle_work_add_batch)
        app.router.add_get('/work/claim', self.handle_work_claim)
        app.router.add_post('/work/start', self.handle_work_start)
        app.router.add_post('/work/complete', self.handle_work_complete)
        app.router.add_post('/work/fail', self.handle_work_fail)
        app.router.add_get('/work/status', self.handle_work_status)
        app.router.add_get('/work/populator', self.handle_populator_status)
        app.router.add_get('/work/node/{node_id}', self.handle_work_for_node)
        app.router.add_post('/work/cancel', self.handle_work_cancel)
        app.router.add_get('/work/history', self.handle_work_history)

        app.router.add_post('/election', self.handle_election)
        app.router.add_post('/election/lease', self.handle_lease_request)
        app.router.add_get('/election/grant', self.handle_voter_grant_status)
        app.router.add_post('/coordinator', self.handle_coordinator)
        app.router.add_post('/start_job', self.handle_start_job)
        app.router.add_post('/stop_job', self.handle_stop_job)
        app.router.add_post('/job/kill', self.handle_job_kill)
        app.router.add_post('/cleanup', self.handle_cleanup)
        app.router.add_post('/restart_stuck_jobs', self.handle_restart_stuck_jobs)
        app.router.add_post('/reduce_selfplay', self.handle_reduce_selfplay)
        app.router.add_post('/selfplay/start', self.handle_selfplay_start)  # GPU selfplay dispatch endpoint
        app.router.add_get('/health', self.handle_health)
        app.router.add_get('/cluster/health', self.handle_cluster_health)
        app.router.add_get('/git/status', self.handle_git_status)
        app.router.add_post('/git/update', self.handle_git_update)

        # Dynamic host registry routes (for IP auto-updates)
        app.router.add_post('/register', self.handle_register)
        app.router.add_get('/registry/status', self.handle_registry_status)
        app.router.add_post('/registry/update_vast', self.handle_registry_update_vast)
        app.router.add_post('/registry/update_aws', self.handle_registry_update_aws)
        app.router.add_post('/registry/update_tailscale', self.handle_registry_update_tailscale)
        app.router.add_post('/registry/save_yaml', self.handle_registry_save_yaml)

        # Connectivity diagnosis routes (SSH/HTTP fallback)
        app.router.add_get('/connectivity/diagnose/{node_id}', self.handle_connectivity_diagnose)
        app.router.add_get('/connectivity/transport_stats', self.handle_transport_stats)
        app.router.add_post('/connectivity/probe_vast', self.handle_probe_vast_nodes)

        # Gauntlet evaluation routes
        app.router.add_post('/gauntlet/execute', self.handle_gauntlet_execute)
        app.router.add_get('/gauntlet/status', self.handle_gauntlet_status)
        app.router.add_post('/gauntlet/quick-eval', self.handle_gauntlet_quick_eval)

        # Relay/Hub routes for NAT-blocked nodes
        app.router.add_post('/relay/heartbeat', self.handle_relay_heartbeat)
        app.router.add_get('/relay/peers', self.handle_relay_peers)
        app.router.add_get('/relay/status', self.handle_relay_status)
        app.router.add_post('/relay/enqueue', self.handle_relay_enqueue)

        # Gossip protocol for decentralized state sharing
        app.router.add_post('/gossip', self.handle_gossip)
        app.router.add_post('/gossip/anti-entropy', self.handle_gossip_anti_entropy)

        # Phase 2: Distributed data manifest routes
        app.router.add_get('/data_manifest', self.handle_data_manifest)
        app.router.add_get('/cluster_data_manifest', self.handle_cluster_data_manifest)
        app.router.add_post('/refresh_manifest', self.handle_refresh_manifest)

        # Distributed CMA-ES routes
        app.router.add_post('/cmaes/start', self.handle_cmaes_start)
        app.router.add_post('/cmaes/evaluate', self.handle_cmaes_evaluate)
        app.router.add_get('/cmaes/status', self.handle_cmaes_status)
        app.router.add_post('/cmaes/result', self.handle_cmaes_result)

        # Distributed tournament routes
        app.router.add_post('/tournament/start', self.handle_tournament_start)
        app.router.add_post('/tournament/match', self.handle_tournament_match)
        app.router.add_post('/tournament/play_elo_match', self.handle_play_elo_match)
        app.router.add_get('/tournament/status', self.handle_tournament_status)
        app.router.add_post('/tournament/result', self.handle_tournament_result)
        app.router.add_post('/tournament/ssh_start', self.handle_ssh_tournament_start)
        app.router.add_get('/tournament/ssh_status', self.handle_ssh_tournament_status)
        app.router.add_post('/tournament/ssh_cancel', self.handle_ssh_tournament_cancel)

        # Improvement loop routes
        app.router.add_post('/improvement/start', self.handle_improvement_start)
        app.router.add_get('/improvement/status', self.handle_improvement_status)
        app.router.add_post('/improvement/phase_complete', self.handle_improvement_phase_complete)

        # Phase 2: P2P data sync routes
        app.router.add_post('/sync/start', self.handle_sync_start)
        app.router.add_get('/sync/status', self.handle_sync_status)
        app.router.add_post('/sync/pull', self.handle_sync_pull)
        app.router.add_get('/sync/file', self.handle_sync_file)
        app.router.add_post('/sync/job_update', self.handle_sync_job_update)
        app.router.add_post('/sync/training', self.handle_training_sync)  # Training node priority sync
        app.router.add_get('/gpu/rankings', self.handle_gpu_rankings)      # GPU power rankings
        app.router.add_post('/cleanup/files', self.handle_cleanup_files)   # File-specific cleanup
        app.router.add_get('/admin/purge_retired', self.handle_purge_retired_peers)  # Purge retired peers (GET for auth bypass)
        app.router.add_get('/admin/purge_stale', self.handle_purge_stale_peers)      # Purge stale peers by heartbeat age
        app.router.add_post('/admin/unretire', self.handle_admin_unretire)           # Unretire specific node
        app.router.add_post('/admin/restart', self.handle_admin_restart)             # Force restart orchestrator

        # Phase 5: Event subscription visibility (December 2025)
        app.router.add_get('/subscriptions', self.handle_subscriptions)              # Show event subscriptions

        # Phase 3: Training pipeline routes
        app.router.add_post('/training/start', self.handle_training_start)
        app.router.add_get('/training/status', self.handle_training_status)
        app.router.add_post('/training/update', self.handle_training_update)
        app.router.add_post('/training/nnue/start', self.handle_nnue_start)
        app.router.add_post('/training/cmaes/start', self.handle_cmaes_start_auto)

        # Phase 5: Improvement cycle routes
        app.router.add_get('/improvement_cycles/status', self.handle_improvement_cycles_status)
        app.router.add_get('/improvement_cycles/leaderboard', self.handle_improvement_cycles_leaderboard)
        app.router.add_post('/improvement_cycles/training_complete', self.handle_improvement_training_complete)
        app.router.add_post('/improvement_cycles/evaluation_complete', self.handle_improvement_evaluation_complete)

        # Metrics observability routes
        app.router.add_get('/metrics', self.handle_metrics)
        app.router.add_get('/metrics/prometheus', self.handle_metrics_prometheus)

        # Canonical pipeline routes (for pipeline_orchestrator.py integration)
        app.router.add_post('/pipeline/start', self.handle_pipeline_start)
        app.router.add_get('/pipeline/status', self.handle_pipeline_status)
        app.router.add_post('/pipeline/selfplay_worker', self.handle_pipeline_selfplay_worker)

        # Phase 4: REST API and Dashboard routes
        app.router.add_get('/', self.handle_root)
        app.router.add_get('/api/cluster/status', self.handle_api_cluster_status)
        app.router.add_post('/api/cluster/git/update', self.handle_api_cluster_git_update)
        app.router.add_get('/api/selfplay/stats', self.handle_api_selfplay_stats)
        app.router.add_get('/api/elo/leaderboard', self.handle_api_elo_leaderboard)
        app.router.add_get('/elo/table', self.handle_elo_table)
        app.router.add_get('/elo/history', self.handle_elo_history)

        # Elo Database Sync routes (cluster-wide Elo consistency)
        app.router.add_get('/elo/sync/status', self.handle_elo_sync_status)
        app.router.add_post('/elo/sync/trigger', self.handle_elo_sync_trigger)
        app.router.add_get('/elo/sync/db', self.handle_elo_sync_download)
        app.router.add_post('/elo/sync/upload', self.handle_elo_sync_upload)
        app.router.add_get('/nodes/table', self.handle_nodes_table)
        app.router.add_get('/victory/table', self.handle_victory_table)
        app.router.add_get('/games/analytics', self.handle_games_analytics)
        app.router.add_get('/training/metrics', self.handle_training_metrics)
        app.router.add_get('/holdout/metrics', self.handle_holdout_metrics)
        app.router.add_get('/holdout/table', self.handle_holdout_table)
        app.router.add_get('/mcts/stats', self.handle_mcts_stats)
        app.router.add_get('/mcts/table', self.handle_mcts_table)
        # Feature endpoints
        app.router.add_get('/matchups/matrix', self.handle_matchup_matrix)
        app.router.add_get('/matchups/table', self.handle_matchup_table)
        app.router.add_get('/models/lineage', self.handle_model_lineage)
        app.router.add_get('/models/lineage/table', self.handle_model_lineage_table)
        app.router.add_get('/data/quality', self.handle_data_quality)
        app.router.add_get('/data/quality/table', self.handle_data_quality_table)
        app.router.add_get('/data/quality/issues', self.handle_data_quality_issues)
        app.router.add_get('/training/efficiency', self.handle_training_efficiency)
        app.router.add_get('/training/efficiency/table', self.handle_training_efficiency_table)
        app.router.add_get('/rollback/status', self.handle_rollback_status)
        app.router.add_get('/rollback/candidates', self.handle_rollback_candidates)
        app.router.add_post('/rollback/execute', self.handle_rollback_execute)
        app.router.add_post('/rollback/auto', self.handle_rollback_auto)
        app.router.add_get('/autoscale/metrics', self.handle_autoscale_metrics)
        app.router.add_get('/autoscale/recommendations', self.handle_autoscale_recommendations)
        app.router.add_get('/resource/optimizer', self.handle_resource_optimizer)
        app.router.add_get('/resource/history', self.handle_resource_utilization_history)
        app.router.add_post('/webhook/test', self.handle_webhook_test)
        app.router.add_get('/trends/summary', self.handle_trends_summary)
        app.router.add_get('/trends/history', self.handle_trends_history)
        app.router.add_get('/trends/table', self.handle_trends_table)

        # A/B Testing endpoints
        app.router.add_post('/abtest/create', self.handle_abtest_create)
        app.router.add_post('/abtest/result', self.handle_abtest_result)
        app.router.add_get('/abtest/status', self.handle_abtest_status)
        app.router.add_get('/abtest/list', self.handle_abtest_list)
        app.router.add_post('/abtest/cancel', self.handle_abtest_cancel)
        app.router.add_get('/abtest/table', self.handle_abtest_table)
        app.router.add_post('/abtest/run', self.handle_abtest_run)

        app.router.add_get('/api/training/status', self.handle_api_training_status)
        app.router.add_get('/api/canonical/health', self.handle_api_canonical_health)
        app.router.add_get('/api/canonical/jobs', self.handle_api_canonical_jobs_list)
        app.router.add_get('/api/canonical/jobs/{job_id}', self.handle_api_canonical_job_get)
        app.router.add_get('/api/canonical/jobs/{job_id}/log', self.handle_api_canonical_job_log)
        app.router.add_get('/api/canonical/logs', self.handle_api_canonical_logs_list)
        app.router.add_get('/api/canonical/logs/{log_name}/tail', self.handle_api_canonical_log_tail)
        app.router.add_post('/api/canonical/generate', self.handle_api_canonical_generate)
        app.router.add_post('/api/canonical/jobs/{job_id}/cancel', self.handle_api_canonical_job_cancel)
        app.router.add_get('/api/jobs', self.handle_api_jobs_list)
        app.router.add_post('/api/jobs/submit', self.handle_api_jobs_submit)
        app.router.add_get('/api/jobs/{job_id}', self.handle_api_job_get)
        app.router.add_post('/api/jobs/{job_id}/cancel', self.handle_api_job_cancel)
        app.router.add_get('/dashboard', self.handle_dashboard)
        app.router.add_get('/work_queue', self.handle_work_queue_dashboard)

        runner = web.AppRunner(app)
        await runner.setup()

        # Verify NFS sync before starting (prevents import errors from stale code)
        try:
            from scripts.verify_nfs_sync import verify_before_startup
            if not verify_before_startup():
                logger.warning("NFS sync verification found mismatches - check logs for details")
        except ImportError:
            logger.debug("NFS sync verification not available")
        except Exception as e:
            logger.warning(f"NFS sync verification failed: {e}")

        # Increase backlog to handle burst of connections from many nodes
        # Default is ~128, which can overflow when many vast nodes heartbeat simultaneously
        site = web.TCPSite(runner, self.host, self.port, reuse_address=True, backlog=1024)
        await site.start()

        logger.info(f"HTTP server started on {self.host}:{self.port} (backlog=1024)")

        # Notify systemd that we're ready to serve
        systemd_notify_ready()

        # Start background tasks
        tasks = [
            asyncio.create_task(self._heartbeat_loop()),
            asyncio.create_task(self._manifest_collection_loop()),
            asyncio.create_task(self._job_management_loop()),
            asyncio.create_task(self._discovery_loop()),
            # IMPROVED: Dedicated voter heartbeat loop for reliable leader election
            asyncio.create_task(self._voter_heartbeat_loop()),
            # IMPROVED: Advanced NAT management for better connectivity
            asyncio.create_task(self._nat_management_loop()),
        ]

        # Add git update loop if enabled
        if AUTO_UPDATE_ENABLED:
            tasks.append(asyncio.create_task(self._git_update_loop()))

        # Add training node priority sync loop (leader-only sync to high-GPU nodes)
        tasks.append(asyncio.create_task(self._training_sync_loop()))

        # Add cloud IP refresh loops (best-effort; no-op if not configured).
        if HAS_DYNAMIC_REGISTRY:
            tasks.append(asyncio.create_task(self._vast_ip_update_loop()))
            tasks.append(asyncio.create_task(self._aws_ip_update_loop()))
            tasks.append(asyncio.create_task(self._tailscale_ip_update_loop()))

        # Peer recovery loop - ensures all Tailscale nodes stay in P2P network
        tasks.append(asyncio.create_task(self._tailscale_peer_recovery_loop()))

        # Phase 26: Continuous bootstrap loop - ensures isolated nodes can rejoin
        tasks.append(asyncio.create_task(self._continuous_bootstrap_loop()))

        # Phase 30: Follower discovery loop - non-leaders actively discover peers
        tasks.append(asyncio.create_task(self._follower_discovery_loop()))

        # Add automatic data management loop (export triggers, training triggers, data sync)
        tasks.append(asyncio.create_task(self._data_management_loop()))

        # Add model sync loop (syncs NN/NNUE models across cluster)
        if HAS_MODEL_SYNC:
            tasks.append(asyncio.create_task(self._model_sync_loop()))

        # Add Elo database sync loop (cluster-wide Elo consistency)
        if HAS_ELO_SYNC and self.elo_sync_manager:
            tasks.append(asyncio.create_task(self._elo_sync_loop()))

        # Add worker pull loop (workers poll leader for work)
        tasks.append(asyncio.create_task(self._worker_pull_loop()))

        # Add work queue maintenance loop (leader cleans up timeouts and old items)
        tasks.append(asyncio.create_task(self._work_queue_maintenance_loop()))

        # Add idle detection loop (leader auto-assigns work to idle nodes)
        tasks.append(asyncio.create_task(self._idle_detection_loop()))

        # === AUTOMATION LOOPS (2024-12) ===
        # These loops enable hands-free cluster operation

        # Auto-scaling loop: provision/deprovision Vast.ai instances based on queue depth
        tasks.append(asyncio.create_task(self._auto_scaling_loop()))

        # Predictive monitoring loop: alert before problems occur
        tasks.append(asyncio.create_task(self._predictive_monitoring_loop()))

        # Self-healing loop: recover stuck jobs and unhealthy nodes
        tasks.append(asyncio.create_task(self._self_healing_loop()))

        # Job reaper loop: enforce job timeouts with SSH kill and work reassignment (leader-only)
        tasks.append(asyncio.create_task(self._job_reaper_loop()))

        # Validation loop: auto-queue validation for newly trained models
        tasks.append(asyncio.create_task(self._validation_loop()))

        # Queue populator loop: maintain 50+ work items until 2000 Elo target met
        tasks.append(asyncio.create_task(self._queue_populator_loop()))

        # Best-effort bootstrap from seed peers before running elections. This
        # helps newly started cloud nodes quickly learn about the full cluster.
        with contextlib.suppress(Exception):
            await self._bootstrap_from_known_peers()

        # If no leader known, start election after short delay
        await asyncio.sleep(5)
        if not self.leader_id and not self._maybe_adopt_leader_from_peers():
            await self._start_election()

        # Run forever
        try:
            await asyncio.gather(*tasks)
        except asyncio.CancelledError:
            pass
        finally:
            self.running = False
            await runner.cleanup()


def main():
    parser = argparse.ArgumentParser(description="P2P Orchestrator for RingRift cluster")
    parser.add_argument("--node-id", required=True, help="Unique identifier for this node")
    parser.add_argument("--host", default="0.0.0.0", help="Host to bind to")
    parser.add_argument("--port", type=int, default=DEFAULT_PORT, help="Port to listen on")
    parser.add_argument(
        "--advertise-host",
        default=None,
        help=f"Host to advertise to peers (or set {ADVERTISE_HOST_ENV})",
    )
    parser.add_argument(
        "--advertise-port",
        type=int,
        default=None,
        help=f"Port to advertise to peers (or set {ADVERTISE_PORT_ENV})",
    )
    parser.add_argument("--peers", help="Comma-separated list of known peers (host[:port] or http(s)://host[:port])")
    parser.add_argument("--relay-peers", help="Comma-separated list of peers to use relay heartbeats with (for NAT-blocked nodes)")
    parser.add_argument("--ringrift-path", help="Path to RingRift installation")
    parser.add_argument("--auth-token", help=f"Shared auth token (or set {AUTH_TOKEN_ENV})")
    parser.add_argument("--require-auth", action="store_true", help="Require auth token to be set")
    parser.add_argument("--storage-type", choices=["disk", "ramdrive", "auto"], default="auto",
                        help="Storage type: 'disk', 'ramdrive' (/dev/shm), or 'auto' (detect based on RAM/disk)")
    parser.add_argument("--sync-to-disk-interval", type=int, default=300,
                        help="When using ramdrive, sync to disk every N seconds (0 = no sync, default: 300)")
    parser.add_argument("--supervised", action="store_true",
                        help="Running under cluster_supervisor.py - disable self-restart logic")

    args = parser.parse_args()

    known_peers = []
    if args.peers:
        known_peers = [p.strip() for p in args.peers.split(',')]

    relay_peers = []
    if args.relay_peers:
        relay_peers = [p.strip() for p in args.relay_peers.split(',')]

    # Wrap orchestrator creation and run in try/except to ensure crashes are logged
    orchestrator = None
    try:
        logger.info(f"Initializing P2P orchestrator: node_id={args.node_id}")
        orchestrator = P2POrchestrator(
            node_id=args.node_id,
            host=args.host,
            port=args.port,
            known_peers=known_peers,
            relay_peers=relay_peers,
            ringrift_path=args.ringrift_path,
            advertise_host=args.advertise_host,
            advertise_port=args.advertise_port,
            auth_token=args.auth_token,
            require_auth=args.require_auth,
            storage_type=args.storage_type,
            sync_to_disk_interval=args.sync_to_disk_interval,
        )
        logger.info(f"P2P orchestrator initialized successfully: {args.node_id}")
    except Exception as e:
        logger.exception(f"Failed to initialize P2P orchestrator: {e}")
        sys.exit(1)

    # Handle shutdown
    def signal_handler(sig, frame):
        logger.info("Shutting down...")
        if orchestrator:
            orchestrator.running = False
            # Stop ramdrive syncer with final sync
            orchestrator.stop_ramdrive_syncer(final_sync=True)
        sys.exit(0)

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    # Run with exception logging
    try:
        logger.info(f"Starting P2P orchestrator main loop: {args.node_id}")
        asyncio.run(orchestrator.run())
    except Exception as e:
        logger.exception(f"P2P orchestrator crashed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
