#!/usr/bin/env python3
"""Distributed P2P Orchestrator - Self-healing compute cluster for RingRift AI training.

This orchestrator runs on each node in the cluster and:
1. Discovers other nodes via broadcast UDP or known peer list
2. Participates in leader election for coordination tasks
3. Monitors local resources and shares status with peers
4. Auto-starts selfplay/training jobs based on cluster needs
5. Self-heals when nodes go offline or IPs change

Architecture:
- Each node runs this script as a daemon
- Nodes communicate via HTTP REST API (port 8770)
- Leader election uses Bully algorithm (highest node_id wins)
- Heartbeats every 30 seconds detect failures
- Nodes maintain local SQLite state for crash recovery

Usage:
    # On each node:
    python scripts/p2p_orchestrator.py --node-id mac-studio
    python scripts/p2p_orchestrator.py --node-id vast-5090-quad --port 8770

    # With known peers (for cloud nodes without broadcast):
    python scripts/p2p_orchestrator.py --node-id vast-3090 --peers <peer-ip>:8770,<peer-ip>:8770
"""
from __future__ import annotations

# Load .env.local BEFORE app.p2p.constants imports (for SWIM/Raft feature flags)
# This must happen before any app.* imports that read environment variables
def _load_env_local():
    """Load .env.local from script directory or ai-service root."""
    import os as _os
    from pathlib import Path as _Path
    for base in [_Path(__file__).parent.parent, _Path.cwd()]:
        env_file = base / ".env.local"
        if env_file.exists():
            try:
                with open(env_file) as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith("#") and "=" in line:
                            key, _, value = line.partition("=")
                            key = key.strip()
                            value = value.strip().strip('"').strip("'")
                            if key not in _os.environ:  # Don't override existing
                                _os.environ[key] = value
                break
            except (OSError, IOError, UnicodeDecodeError):
                pass  # Skip if .env.local can't be read

_load_env_local()

import argparse
import asyncio
import contextlib
import gzip
import importlib
import ipaddress
import json
import os
import secrets
import shutil
import signal
import socket
import sqlite3
import subprocess
import sys

# Safe database connection context manager (December 2025)
try:
    from app.distributed.db_utils import safe_db_connection
except ImportError:
    # Fallback for when db_utils isn't available
    from contextlib import contextmanager as _cm
    @_cm
    def safe_db_connection(db_path, timeout=30):
        conn = sqlite3.connect(str(db_path), timeout=timeout)
        try:
            yield conn
            conn.commit()
        except sqlite3.Error:
            conn.rollback()
            raise
        finally:
            conn.close()
import threading
import time
import uuid
from collections.abc import Generator
from dataclasses import asdict
from datetime import datetime
from pathlib import Path
from typing import TYPE_CHECKING, Any, Optional
from urllib.parse import urlparse

if TYPE_CHECKING:
    from app.coordination.unified_queue_populator import UnifiedQueuePopulator as QueuePopulator
    from app.coordination.p2p_auto_deployer import P2PAutoDeployer
    from scripts.p2p.loops import LoopManager

# Work queue for centralized work distribution (lazy import to avoid circular deps)
_work_queue = None
def get_work_queue():
    """Get the work queue singleton (lazy load)."""
    global _work_queue
    if _work_queue is None:
        try:
            from app.coordination.work_queue import get_work_queue as _get_wq
            _work_queue = _get_wq()
        except ImportError:
            _work_queue = None
    return _work_queue

# Automation managers (lazy imports to avoid circular deps)
_health_manager = None  # December 2025: Consolidated from recovery_manager
_predictive_alerts = None
# Dec 2025: Removed unused _tier_calibrator global (never used)
# Dec 28, 2025: Removed unused get_auto_scaler() - never called

def get_health_manager():
    """Get the health manager singleton (lazy load).

    December 2025: Consolidated from get_recovery_manager().
    Uses UnifiedHealthManager which combines recovery + error coordination.
    """
    global _health_manager
    if _health_manager is None:
        try:
            from app.coordination.unified_health_manager import (
                get_health_manager as _get_uhm,
            )
            _health_manager = _get_uhm()
        except ImportError:
            _health_manager = None
    return _health_manager


# Backward compatibility alias (deprecated)
def get_recovery_manager():
    """DEPRECATED: Use get_health_manager() instead."""
    return get_health_manager()

# Job Reaper Daemon (leader-only, kills stuck jobs and reassigns work)
_job_reaper = None
def get_job_reaper(work_queue=None, ssh_config=None):
    """Get the job reaper singleton (lazy load).

    The JobReaperDaemon enforces job timeouts by:
    1. Detecting jobs past their timeout
    2. Killing stuck processes via SSH
    3. Marking jobs as TIMEOUT
    4. Reassigning failed work to other nodes
    5. Blacklisting nodes that repeatedly fail
    """
    global _job_reaper
    if _job_reaper is None and work_queue is not None:
        try:
            from app.coordination.job_reaper import JobReaperDaemon
            _job_reaper = JobReaperDaemon(
                work_queue=work_queue,
                ssh_config=ssh_config,
            )
        except ImportError as e:
            logger.warning(f"JobReaperDaemon not available: {e}")
            _job_reaper = None
    return _job_reaper

def get_predictive_alerts():
    """Get the predictive alerts manager (lazy load)."""
    global _predictive_alerts
    if _predictive_alerts is None:
        try:
            from app.monitoring.predictive_alerts import PredictiveAlertManager
            _predictive_alerts = PredictiveAlertManager()
        except ImportError:
            _predictive_alerts = None
    return _predictive_alerts

# Dec 2025: Removed unused get_tier_calibrator() function


# SWIM membership manager for leaderless gossip-based membership
_swim_manager = None
SWIM_AVAILABLE = False


def get_swim_manager(node_id: str | None = None, bind_port: int = 7947):
    """Get the SWIM membership manager singleton (lazy load).

    SWIM (Scalable Weakly-consistent Infection-style Membership) provides:
    - O(1) message complexity per node (constant bandwidth)
    - Failure detection in <5 seconds (vs 60+ seconds with heartbeat-based)
    - No single leader required - truly distributed
    - Suspicion mechanism to reduce false positives

    Args:
        node_id: Node identifier (required for first initialization)
        bind_port: UDP port for SWIM protocol (default 7947)

    Returns:
        SwimMembershipManager instance or None if swim-p2p not installed
    """
    global _swim_manager, SWIM_AVAILABLE
    if _swim_manager is None and node_id is not None:
        try:
            from app.p2p.swim_adapter import SwimMembershipManager, SWIM_AVAILABLE as _swim_avail
            SWIM_AVAILABLE = _swim_avail
            if SWIM_AVAILABLE:
                _swim_manager = SwimMembershipManager.from_distributed_hosts(
                    node_id=node_id,
                    bind_port=bind_port,
                )
                logger.info(f"SWIM membership manager initialized for {node_id}")
            else:
                logger.warning("swim-p2p not installed - using HTTP heartbeats only")
        except ImportError as e:
            logger.warning(f"SWIM adapter not available: {e}")
            _swim_manager = None
    return _swim_manager


# ============================================
# Phase 4: Extracted Background Loops (Dec 2025)
# ============================================
# These loops are extracted from the monolithic orchestrator for modularity.
# They use dependency injection via callbacks for testability.

# Feature flag for gradual rollout
EXTRACTED_LOOPS_ENABLED = os.environ.get("RINGRIFT_EXTRACTED_LOOPS", "true").lower() in ("true", "1", "yes")
JOB_REAPER_FALLBACK_ENABLED = os.environ.get("RINGRIFT_JOB_REAPER_FALLBACK_ENABLED", "true").lower() in ("true", "1", "yes")

# Lazy import to avoid circular dependencies
_loop_manager_instance = None
_loop_classes_loaded = False


def _load_loop_classes():
    """Lazy-load loop classes to avoid import-time dependencies."""
    global _loop_classes_loaded
    if _loop_classes_loaded:
        return True
    try:
        from scripts.p2p.loops import (
            LoopManager,
            QueuePopulatorLoop,
            EloSyncLoop,
            ModelSyncLoop,
            DataAggregationLoop,
            IpDiscoveryLoop,
            TailscaleRecoveryLoop,
            TailscalePeerDiscoveryLoop,
            FollowerDiscoveryLoop,
            AutoScalingLoop,
            HealthAggregationLoop,
            JobReaperLoop,
            IdleDetectionLoop,
            UdpDiscoveryLoop,
            SplitBrainDetectionLoop,
        )
        _loop_classes_loaded = True
        return True
    except ImportError as e:
        logger.error(f"[LoopManager] CRITICAL: Extracted loops import failed: {e}")
        logger.error("[LoopManager] WorkerPullLoop will NOT start - workers won't claim work!")
        return False


def get_loop_manager() -> "LoopManager | None":
    """Get or create the global LoopManager singleton.

    Returns None if extracted loops are disabled or unavailable.
    """
    global _loop_manager_instance
    if not EXTRACTED_LOOPS_ENABLED:
        return None
    if _loop_manager_instance is None:
        if not _load_loop_classes():
            return None
        try:
            from scripts.p2p.loops import LoopManager
            _loop_manager_instance = LoopManager(name="p2p_loops")
            logger.info("LoopManager: initialized for extracted background loops")
        except (ImportError, TypeError, ValueError, AttributeError) as e:
            # ImportError: loops module not available
            # TypeError: wrong constructor signature
            # ValueError: invalid argument
            # AttributeError: LoopManager not found in module
            logger.error(f"LoopManager: failed to initialize: {e}")
            return None
    return _loop_manager_instance


# Board priority overrides from unified_loop.yaml
# 0=CRITICAL, 1=HIGH, 2=MEDIUM, 3=LOW (lower value = higher priority)
_board_priority_cache: dict[str, int] | None = None
_board_priority_cache_time: float = 0


def get_board_priority_overrides() -> dict[str, int]:
    """Load board priority overrides from config, cached for 60 seconds.

    Returns dict mapping config keys (e.g., 'hexagonal_2p') to priority levels.
    Priority levels: 0=CRITICAL, 1=HIGH, 2=MEDIUM, 3=LOW
    """
    global _board_priority_cache, _board_priority_cache_time
    now = time.time()

    # Return cached value if fresh (60 second TTL)
    if _board_priority_cache is not None and now - _board_priority_cache_time < 60:
        return _board_priority_cache

    try:
        import yaml
        config_path = Path(__file__).parent.parent / "config" / "unified_loop.yaml"
        if config_path.exists():
            with open(config_path) as f:
                yaml_config = yaml.safe_load(f)
            selfplay_config = yaml_config.get("selfplay", {})
            overrides = selfplay_config.get("board_priority_overrides", {})
            # Convert config keys like "hexagonal_2p" -> priority int
            _board_priority_cache = {k: int(v) for k, v in overrides.items()}
            _board_priority_cache_time = now
            return _board_priority_cache
    except (OSError, ValueError, AttributeError, ImportError):
        pass

    # Default: empty (no overrides)
    return {}


# =============================================================================
# P2P Event Emission Helpers (December 2025 - CRITICAL gap fix)
# =============================================================================
# These helpers safely emit events for P2P lifecycle changes. Events enable:
# - LeadershipCoordinator to track leader changes
# - UnifiedHealthManager to respond to node failures
# - Cluster-wide coordination on membership changes

_p2p_event_emitters_available: bool | None = None
_p2p_event_emitters_last_check: float = 0.0
_P2P_EMITTER_CACHE_TTL: float = 30.0  # Retry every 30 seconds if failed


def _check_event_emitters() -> bool:
    """Check if event emitters are available (cached with TTL for retries).

    December 27, 2025: Fixed bug where negative result was cached permanently.
    Now retries every 30 seconds if event system becomes available later.
    """
    global _p2p_event_emitters_available, _p2p_event_emitters_last_check
    import time

    now = time.time()

    # Use cached positive result indefinitely
    if _p2p_event_emitters_available is True:
        return True

    # For negative results, retry after TTL expires
    if _p2p_event_emitters_available is False:
        if now - _p2p_event_emitters_last_check < _P2P_EMITTER_CACHE_TTL:
            return False
        # TTL expired, retry below

    try:
        from app.coordination.event_router import (
            emit_host_online,
            emit_host_offline,
            emit_leader_elected,
        )
        _p2p_event_emitters_available = True
        _p2p_event_emitters_last_check = now
        return True
    except ImportError:
        _p2p_event_emitters_available = False
        _p2p_event_emitters_last_check = now
        return False


# December 28, 2025: Module-level emit functions (27 methods, ~911 LOC) were moved to
# EventEmissionMixin in scripts/p2p/event_emission_mixin.py.
# P2POrchestrator now inherits from EventEmissionMixin and uses self._emit_* methods.
# See scripts/p2p/__init__.py for the mixin export.


# Add project root to path for scripts.lib imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scripts.lib.file_formats import open_jsonl_file
from scripts.lib.logging_config import setup_script_logging
from scripts.lib.process import (
    SingletonLock,
    find_processes_by_pattern,
    kill_process,
    is_process_running,
)

logger = setup_script_logging("p2p_orchestrator")

# Singleton lock for duplicate process prevention (December 2025)
_P2P_LOCK: SingletonLock | None = None


def _validate_p2p_dependencies() -> None:
    """Pre-flight check for required modules. Exits with code 2 if missing.

    This catches import errors early with a clear message, rather than
    failing deep in the call stack with confusing tracebacks.
    """
    required_modules = [
        ("aiohttp", "pip install aiohttp"),
        ("psutil", "pip install psutil"),
        ("yaml", "pip install pyyaml"),
    ]
    missing = []
    for module_name, install_hint in required_modules:
        try:
            __import__(module_name)
        except ImportError:
            missing.append(f"{module_name} ({install_hint})")

    if missing:
        # Use print since logger may not be fully initialized
        print(f"CRITICAL: Missing required dependencies: {', '.join(missing)}", file=sys.stderr)
        print("Run: pip install -r requirements.txt", file=sys.stderr)
        sys.exit(2)  # Exit code 2 = missing dependencies


# Validate dependencies before any heavy imports
_validate_p2p_dependencies()


@contextlib.contextmanager
def db_connection(db_path: str | Path, timeout: float = 30.0) -> Generator[sqlite3.Connection]:
    """Context manager for SQLite connections to prevent leaks.

    Usage:
        with db_connection(db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(...)
    """
    conn = None
    try:
        conn = sqlite3.connect(str(db_path), timeout=timeout)
        yield conn
    finally:
        if conn:
            with contextlib.suppress(Exception):
                conn.close()


# Centralized ramdrive utilities for auto-detection
# Shared database integrity utilities
from app.db.integrity import (
    check_and_repair_databases,
)

# Circuit breaker for fault-tolerant network operations
from app.distributed.circuit_breaker import (
    CircuitState,
    get_circuit_registry,
)
from app.utils.ramdrive import (
    RamdriveSyncer,
    get_system_resources,
    log_storage_recommendation,
    should_use_ramdrive,
)
from scripts.p2p.cluster_config import (
    get_cluster_config,
    get_webhook_urls,
)
from scripts.p2p.handlers import (
    ABTestHandlersMixin,
    AdminHandlersMixin,
    CanonicalGateHandlersMixin,
    CMAESHandlersMixin,
    DeliveryHandlersMixin,
    ElectionHandlersMixin,
    EloSyncHandlersMixin,
    GauntletHandlersMixin,
    GossipHandlersMixin,
    ImprovementHandlersMixin,
    JobsApiHandlersMixin,
    RegistryHandlersMixin,
    ManifestHandlersMixin,
    RelayHandlersMixin,
    SSHTournamentHandlersMixin,
    SyncHandlersMixin,
    TableHandlersMixin,
    TournamentHandlersMixin,
    WorkQueueHandlersMixin,
)
from scripts.p2p.network_utils import NetworkUtilsMixin
from scripts.p2p.peer_manager import PeerManagerMixin
from scripts.p2p.leader_election import LeaderElectionMixin
from scripts.p2p.gossip_protocol import GossipProtocolMixin  # Contains merged GossipMetricsMixin (Dec 28, 2025)

# Phase 5: SWIM + Raft integration mixins (Dec 26, 2025)
from scripts.p2p.membership_mixin import MembershipMixin
from scripts.p2p.consensus_mixin import ConsensusMixin
from scripts.p2p.handlers.swim import SwimHandlersMixin
from scripts.p2p.handlers.raft import RaftHandlersMixin
from scripts.p2p.handlers.network_health import NetworkHealthMixin, setup_network_health_routes

# Import constants from the refactored module (Phase 2 refactoring - consolidated)
from scripts.p2p.constants import (
    ADVERTISE_HOST_ENV,
    ADVERTISE_PORT_ENV,
    AGENT_MODE_ENABLED,
    ARBITER_URL,
    # Auth and build info
    AUTH_TOKEN_ENV,
    AUTH_TOKEN_FILE_ENV,
    AUTO_ASSIGN_ENABLED,
    AUTO_TRAINING_THRESHOLD_MB,
    AUTO_UPDATE_ENABLED,
    AUTO_WORK_BATCH_SIZE,
    BUILD_VERSION_ENV,
    COORDINATOR_URL,
    DATA_MANAGEMENT_INTERVAL,
    DB_EXPORT_THRESHOLD_MB,
    # Network configuration
    DEFAULT_PORT,
    DISCOVERY_INTERVAL,
    DISCOVERY_PORT,
    DISK_CLEANUP_THRESHOLD,
    # Resource thresholds
    DISK_CRITICAL_THRESHOLD,
    DISK_WARNING_THRESHOLD,
    # Dynamic voter management
    DYNAMIC_VOTER_ENABLED,
    DYNAMIC_VOTER_MAX_QUORUM,
    DYNAMIC_VOTER_MIN,
    DYNAMIC_VOTER_TARGET,
    ELECTION_TIMEOUT,
    ELO_K_FACTOR,
    GH200_MAX_SELFPLAY,
    GH200_MIN_SELFPLAY,
    GIT_BRANCH_NAME,
    GIT_REMOTE_NAME,
    # Auto-update settings
    GIT_UPDATE_CHECK_INTERVAL,
    # Safeguards
    GPU_IDLE_RESTART_TIMEOUT,
    GPU_IDLE_THRESHOLD,
    GPU_POWER_RANKINGS,
    GRACEFUL_SHUTDOWN_BEFORE_UPDATE,
    HEARTBEAT_INTERVAL,
    # Connection robustness
    HTTP_CONNECT_TIMEOUT,
    HTTP_TOTAL_TIMEOUT,
    IDLE_CHECK_INTERVAL,
    IDLE_GPU_THRESHOLD,
    IDLE_GRACE_PERIOD,
    # Elo constants (from app.config.thresholds)
    BASELINE_ELO_RANDOM,  # Random AI pinned at 400 Elo
    INITIAL_ELO_RATING,
    JOB_CHECK_INTERVAL,
    LEADER_DEGRADED_STEPDOWN_DELAY,
    LEADER_HEALTH_CHECK_INTERVAL,
    LEADER_LEASE_DURATION,
    LEADER_LEASE_RENEW_INTERVAL,
    LEADER_MIN_RESPONSE_RATE,
    LEADERLESS_TRAINING_TIMEOUT,
    LOAD_AVERAGE_MAX_MULTIPLIER,
    LOAD_MAX_FOR_NEW_JOBS,
    MANIFEST_JSONL_LINECOUNT_CHUNK_BYTES,
    # Data management
    MANIFEST_JSONL_LINECOUNT_MAX_BYTES,
    MANIFEST_JSONL_SAMPLE_BYTES,
    MAX_CONCURRENT_EXPORTS,
    MAX_CONSECUTIVE_FAILURES,
    MAX_DISK_USAGE_PERCENT,
    MAX_GAUNTLET_RUNTIME,
    # Stale process cleanup
    MAX_SELFPLAY_RUNTIME,
    MAX_TOURNAMENT_RUNTIME,
    MAX_TRAINING_RUNTIME,
    MEMORY_CRITICAL_THRESHOLD,
    MEMORY_WARNING_THRESHOLD,
    MIN_GAMES_FOR_SYNC,
    MIN_MEMORY_GB_FOR_TASKS,
    MODEL_SYNC_INTERVAL,
    NAT_BLOCKED_PROBE_INTERVAL,
    NAT_BLOCKED_PROBE_TIMEOUT,
    NAT_BLOCKED_RECOVERY_TIMEOUT,
    NAT_EXTERNAL_IP_CACHE_TTL,
    NAT_HOLE_PUNCH_RETRY_COUNT,
    # NAT/Relay settings
    NAT_INBOUND_HEARTBEAT_STALE_SECONDS,
    NAT_RELAY_PREFERENCE_THRESHOLD,
    NAT_STUN_LIKE_PROBE_INTERVAL,
    NAT_SYMMETRIC_DETECTION_ENABLED,
    P2P_DATA_SYNC_BASE,
    P2P_DATA_SYNC_MAX,
    P2P_DATA_SYNC_MIN,
    P2P_MODEL_SYNC_BASE,
    P2P_MODEL_SYNC_MAX,
    P2P_MODEL_SYNC_MIN,
    P2P_SYNC_BACKOFF_FACTOR,
    P2P_SYNC_SPEEDUP_FACTOR,
    P2P_TRAINING_DB_SYNC_BASE,
    P2P_TRAINING_DB_SYNC_MAX,
    P2P_TRAINING_DB_SYNC_MIN,
    PEER_BOOTSTRAP_INTERVAL,
    PEER_BOOTSTRAP_MIN_PEERS,
    PEER_PURGE_AFTER_SECONDS,
    PEER_RECOVERY_RETRY_INTERVAL,
    PEER_RETIRE_AFTER_SECONDS,
    PEER_TIMEOUT,
    RELAY_COMMAND_MAX_ATTEMPTS,
    RELAY_COMMAND_MAX_BATCH,
    RELAY_COMMAND_TTL_SECONDS,
    RELAY_HEARTBEAT_INTERVAL,
    RELAY_MAX_PENDING_START_JOBS,
    RETRY_DEAD_NODE_INTERVAL,
    RETRY_RETIRED_NODE_INTERVAL,
    RUNAWAY_SELFPLAY_PROCESS_THRESHOLD,
    SPAWN_RATE_LIMIT_PER_MINUTE,
    STALE_PROCESS_CHECK_INTERVAL,
    STARTUP_GRACE_PERIOD,
    STALE_PROCESS_PATTERNS,
    STARTUP_JSONL_GRACE_PERIOD_SECONDS,
    # State directory
    STATE_DIR,
    TAILSCALE_CGNAT_NETWORK,
    TARGET_GPU_UTIL_MAX,
    # GPU configuration
    TARGET_GPU_UTIL_MIN,
    TRAINING_DATA_SYNC_THRESHOLD_MB,
    # Training node sync
    TRAINING_NODE_COUNT,
    TRAINING_SYNC_INTERVAL,
    # Unified inventory / Idle detection
    UNIFIED_DISCOVERY_INTERVAL,
    VOTER_DEMOTION_FAILURES,
    VOTER_HEALTH_THRESHOLD,
    VOTER_HEARTBEAT_INTERVAL,
    VOTER_HEARTBEAT_TIMEOUT,
    VOTER_MESH_REFRESH_INTERVAL,
    VOTER_MIN_QUORUM,
    VOTER_NAT_RECOVERY_AGGRESSIVE,
    VOTER_PROMOTION_UPTIME,
    # Phase 26: Multi-seed bootstrap and mesh resilience
    BOOTSTRAP_SEEDS,
    MIN_BOOTSTRAP_ATTEMPTS,
    ISOLATED_BOOTSTRAP_INTERVAL,
    MIN_CONNECTED_PEERS,
    # Phase 28: Gossip protocol
    GOSSIP_FANOUT,
    GOSSIP_INTERVAL,
    GOSSIP_MAX_PEER_ENDPOINTS,
    # Phase 27: Peer cache
    PEER_CACHE_TTL_SECONDS,
    PEER_CACHE_MAX_ENTRIES,
    PEER_REPUTATION_ALPHA,
    # Phase 29: Cluster epochs
    INITIAL_CLUSTER_EPOCH,
)
from scripts.p2p.models import (
    ClusterDataManifest,
    ClusterJob,
    ClusterSyncPlan,
    DataFileInfo,
    DataSyncJob,
    DistributedCMAESState,
    DistributedTournamentState,
    ImprovementLoopState,
    NodeDataManifest,
    NodeInfo,
    SSHTournamentRun,
    TrainingJob,
    TrainingThresholds,
)
from scripts.p2p.p2p_mixin_base import SubscriptionRetryConfig
from scripts.p2p.network import (
    AsyncLockWrapper,
    TimeoutAsyncLockWrapper,
    get_client_session,
)

# Import refactored utilities (Phase 2 refactoring)
from scripts.p2p.resource import (
    check_disk_has_capacity,
)

# Import refactored P2P types and models
# These were extracted from this file for modularity (Phase 1 refactoring)
from scripts.p2p.types import JobType, NodeRole
from scripts.p2p.utils import (
    safe_json_response,
    systemd_notify_ready,
    systemd_notify_watchdog,
)
from scripts.p2p.managers import (
    JobManager,
    NodeSelector,
    SelfplayScheduler,
    StateManager,
    SyncPlanner,
    SyncPlannerConfig,
    TrainingCoordinator,
)
from scripts.p2p.managers.state_manager import PersistedLeaderState
from scripts.p2p.metrics_manager import MetricsManager
from scripts.p2p.resource_detector import ResourceDetector, ResourceDetectorMixin
from scripts.p2p.event_emission_mixin import EventEmissionMixin

# Unified resource checking utilities (80% max utilization)
# Includes graceful degradation for dynamic workload management
try:
    from app.utils.resource_guard import (
        LIMITS as RESOURCE_LIMITS,
        OperationPriority,
        check_cpu as unified_check_cpu,
        check_disk_space as unified_check_disk,
        check_memory as unified_check_memory,
        get_degradation_level,
        should_proceed_with_priority,
    )
    HAS_RESOURCE_GUARD = True
except ImportError:
    HAS_RESOURCE_GUARD = False
    unified_check_disk = None
    unified_check_memory = None
    unified_check_cpu = None
    RESOURCE_LIMITS = None
    should_proceed_with_priority = None
    OperationPriority = None
    get_degradation_level = None

# ELO database sync manager for cluster-wide consistency
try:
    from app.tournament.elo_sync_manager import (
        EloSyncManager,
        ensure_elo_synced,
        get_elo_sync_manager,
        sync_elo_after_games,
    )
    HAS_ELO_SYNC = True
except ImportError:
    HAS_ELO_SYNC = False
    EloSyncManager = None
    get_elo_sync_manager = None
    sync_elo_after_games = None
    ensure_elo_synced = None

# Distributed data sync manager for model/data distribution
# Prefer new sync_coordinator, fallback to deprecated data_sync
try:
    from app.distributed.sync_coordinator import SyncCoordinator, full_cluster_sync
    HAS_SYNC_COORDINATOR = True

    def get_sync_coordinator():
        return SyncCoordinator.get_instance()
except ImportError:
    HAS_SYNC_COORDINATOR = False
    SyncCoordinator = None
    full_cluster_sync = None

# SyncRouter: Intelligent data routing with quality-based priority (December 2025)
try:
    from app.coordination.sync_router import get_sync_router, SyncRouter
    HAS_SYNC_ROUTER = True
except ImportError:
    HAS_SYNC_ROUTER = False
    get_sync_router = None
    SyncRouter = None

# Phase 3.1: Curriculum weights integration for selfplay prioritization
try:
    from scripts.unified_loop.curriculum import load_curriculum_weights
    HAS_CURRICULUM_WEIGHTS = True
except ImportError:
    HAS_CURRICULUM_WEIGHTS = False
    load_curriculum_weights = None

# Unified node inventory for multi-CLI discovery (Vast, Tailscale, Lambda, Hetzner)
try:
    from app.coordination.unified_inventory import UnifiedInventory, get_inventory
    HAS_UNIFIED_INVENTORY = True
except ImportError:
    HAS_UNIFIED_INVENTORY = False
    UnifiedInventory = None
    get_inventory = None

# HTTP server imports
try:
    import aiohttp
    from aiohttp import ClientSession, ClientTimeout, web
    HAS_AIOHTTP = True
except ImportError:
    HAS_AIOHTTP = False
    aiohttp = None
    logger.warning("aiohttp not installed. Install with: pip install aiohttp")

# SOCKS proxy support for userspace Tailscale networking
try:
    from aiohttp_socks import ProxyConnector
    HAS_SOCKS = True
except ImportError:
    HAS_SOCKS = False
    ProxyConnector = None

# Get SOCKS proxy from environment (e.g., socks5://localhost:1055)
SOCKS_PROXY = os.environ.get("RINGRIFT_SOCKS_PROXY", "")


# =============================================================================
# HTTP Handler Timeout Decorator (December 30, 2025)
# =============================================================================
# Added to fix P2P cluster connectivity issues where HTTP handlers blocked
# indefinitely on slow operations (lock acquisition, daemon status collection).

def with_request_timeout(timeout_seconds: float = 10.0):
    """Decorator to add timeout protection to HTTP handlers.

    December 30, 2025: Added to prevent HTTP endpoints from blocking indefinitely.

    Usage:
        @with_request_timeout(5.0)
        async def handle_health(self, request):
            ...

    Args:
        timeout_seconds: Maximum time in seconds for handler to complete.

    Returns:
        Decorated handler that returns 504 Gateway Timeout on timeout.
    """
    import functools

    def decorator(handler):
        @functools.wraps(handler)
        async def wrapper(self_or_request, *args, **kwargs):
            # Handle both bound methods (self, request) and plain functions (request)
            try:
                return await asyncio.wait_for(
                    handler(self_or_request, *args, **kwargs),
                    timeout=timeout_seconds
                )
            except asyncio.TimeoutError:
                # Return 504 Gateway Timeout with details
                return web.json_response(
                    {
                        "error": "Request timed out",
                        "timeout_seconds": timeout_seconds,
                        "timestamp": time.time(),
                    },
                    status=504
                )
        return wrapper
    return decorator


# Systemd watchdog support for service health monitoring
# When running under systemd with WatchdogSec set, we need to periodically
# notify systemd that the service is healthy. If we miss the deadline,
# systemd will restart the service.
try:
    import sdnotify
    SYSTEMD_NOTIFIER = sdnotify.SystemdNotifier()
    HAS_SYSTEMD = True
except ImportError:
    SYSTEMD_NOTIFIER = None
    HAS_SYSTEMD = False


# ============================================
# Utilities (Refactored - Phase 2)
# ============================================
# The following utilities have been moved to scripts/p2p/ for modularity:
# - systemd_notify_watchdog, systemd_notify_ready (scripts/p2p/utils.py)
# - AsyncLockWrapper, get_client_session (scripts/p2p/network.py)
# - check_peer_circuit, record_peer_success, record_peer_failure (scripts/p2p/network.py)
# - peer_request (scripts/p2p/network.py)
# - get_disk_usage_percent, check_disk_has_capacity, check_all_resources (scripts/p2p/resource.py)
#
# They are imported at the top of this file for backward compatibility.
# ============================================

# Dynamic host registry for IP auto-update
try:
    from app.distributed.dynamic_registry import (
        NodeState,
        get_registry,
    )
    HAS_DYNAMIC_REGISTRY = True
except ImportError:
    HAS_DYNAMIC_REGISTRY = False
    get_registry = None
    NodeState = None

# Hybrid transport layer for HTTP/SSH fallback (self-healing Vast connectivity)
try:
    from app.distributed.hybrid_transport import (
        HybridTransport,
        diagnose_node_connectivity,
        get_hybrid_transport,
    )
    from app.distributed.ssh_transport import (
        SSHTransport,
        get_ssh_transport,
        probe_vast_nodes_via_ssh,
    )
    HAS_HYBRID_TRANSPORT = True
except ImportError:
    HAS_HYBRID_TRANSPORT = False
    HybridTransport = None
    get_hybrid_transport = None
    diagnose_node_connectivity = None
    SSHTransport = None
    get_ssh_transport = None
    probe_vast_nodes_via_ssh = None

# Improvement cycle manager for automated training
# Note: ImprovementCycleManager is deprecated - unified_ai_loop.py is the new approach
# Kept for backwards compatibility with older scripts
try:
    from scripts.improvement_cycle_manager import ImprovementCycleManager
    HAS_IMPROVEMENT_MANAGER = True
except ImportError:
    # Fallback - deprecated archive location removed in 2025-12
    HAS_IMPROVEMENT_MANAGER = False
    ImprovementCycleManager = None

# Task coordination safeguards - prevents runaway spawning
try:
    from app.coordination.safeguards import Safeguards, check_before_spawn
    HAS_SAFEGUARDS = True
    _safeguards = Safeguards.get_instance()
except ImportError:
    HAS_SAFEGUARDS = False
    _safeguards = None
    def check_before_spawn(task_type, node_id):
        return True, ""

# New coordination features: OrchestratorRole, backpressure, sync_lock, bandwidth
try:
    from app.coordination import (
        NodeResources,
        # Orchestrator role management (SQLite-backed with heartbeat)
        OrchestratorRole,
        # Queue backpressure
        QueueType,
        # Resource optimizer for cluster-wide PID-controlled optimization
        ResourceOptimizer,
        TransferPriority,
        acquire_orchestrator_role,
        get_cluster_utilization,
        get_host_targets,
        get_optimal_concurrency,
        get_resource_optimizer,
        # Resource targets for unified utilization management
        get_resource_targets,
        get_target_job_count,
        get_throttle_factor,
        record_utilization,
        release_bandwidth,
        release_orchestrator_role,
        # Bandwidth management
        request_bandwidth,
        should_scale_down,
        should_scale_up,
        should_stop_production,
        should_throttle_production,
        # Sync mutex for data transfer coordination
        sync_lock,
    )

    # Import rate negotiation functions for cooperative utilization (60-80% target)
    from app.coordination.resource_optimizer import (
        apply_feedback_adjustment,
        get_config_weights,
        get_current_selfplay_rate,
        get_hybrid_selfplay_limits,
        get_max_cpu_only_selfplay,
        # Hardware-aware selfplay limits (single source of truth)
        get_max_selfplay_for_node,
        get_utilization_status,
        negotiate_selfplay_rate,
        update_config_weights,
    )
    HAS_RATE_NEGOTIATION = True
    HAS_NEW_COORDINATION = True
    HAS_HW_AWARE_LIMITS = True
    # Get targets from unified source
    _unified_targets = get_resource_targets()
except ImportError:
    HAS_NEW_COORDINATION = False
    HAS_RATE_NEGOTIATION = False
    HAS_HW_AWARE_LIMITS = False
    OrchestratorRole = None
    _unified_targets = None
    negotiate_selfplay_rate = None
    get_current_selfplay_rate = None
    apply_feedback_adjustment = None
    get_utilization_status = None
    update_config_weights = None
    get_config_weights = None
    get_max_selfplay_for_node = None
    get_hybrid_selfplay_limits = None
    get_max_cpu_only_selfplay = None

# P2P-integrated monitoring management
try:
    from app.monitoring.p2p_monitoring import MonitoringManager
    HAS_P2P_MONITORING = True
except ImportError:
    HAS_P2P_MONITORING = False
    MonitoringManager = None

# Model sync across cluster
try:
    from scripts.sync_models import (
        HOSTS_MODULE_AVAILABLE as HAS_HOSTS_FOR_SYNC,
        ClusterModelState,
        scan_cluster as scan_cluster_models,
        sync_missing_models,
    )
    # Also import load_remote_hosts for scanning
    if HAS_HOSTS_FOR_SYNC:
        from app.distributed.hosts import filter_ready_hosts, load_remote_hosts
    HAS_MODEL_SYNC = True
except ImportError:
    HAS_MODEL_SYNC = False
    scan_cluster_models = None
    sync_missing_models = None
    ClusterModelState = None
    HAS_HOSTS_FOR_SYNC = False
    load_remote_hosts = None
    filter_ready_hosts = None

# PFSP (Prioritized Fictitious Self-Play) opponent pool
try:
    from app.training.advanced_training import (
        CMAESAutoTuner,
        OpponentStats,
        PFSPOpponentPool,
        PlateauConfig,
    )
    HAS_PFSP = True
except ImportError:
    HAS_PFSP = False
    PFSPOpponentPool = None
    OpponentStats = None
    CMAESAutoTuner = None
    PlateauConfig = None

# ============================================
# Configuration
# ============================================
# NOTE: All constants have been consolidated into scripts/p2p/constants.py
# and are imported at the top of this file (Phase 2 refactoring).
# See scripts/p2p/constants.py for configuration values and documentation.
# ============================================


# ============================================
# Types and Models (Refactored)
# ============================================
# The following types have been moved to scripts/p2p/ for modularity:
# - NodeRole, JobType (scripts/p2p/types.py)
# - NodeInfo, ClusterJob, DistributedCMAESState, DistributedTournamentState,
#   SSHTournamentRun, ImprovementLoopState, TrainingJob, TrainingThresholds,
#   DataFileInfo, NodeDataManifest, ClusterDataManifest, DataSyncJob,
#   ClusterSyncPlan (scripts/p2p/models.py)
#
# They are imported at the top of this file for backward compatibility.
# ============================================


class WebhookNotifier:
    """Sends alerts to Slack/Discord webhooks for important events.

    Configure via environment variables:
    - RINGRIFT_SLACK_WEBHOOK: Slack incoming webhook URL
    - RINGRIFT_DISCORD_WEBHOOK: Discord webhook URL
    - RINGRIFT_ALERT_LEVEL: Minimum level to alert (debug/info/warning/error) default: warning
    """

    LEVELS = {"debug": 0, "info": 1, "warning": 2, "error": 3}

    def __init__(self):
        # Try environment variables first, then fall back to cluster.yaml
        self.slack_webhook = os.environ.get("RINGRIFT_SLACK_WEBHOOK", "")
        self.discord_webhook = os.environ.get("RINGRIFT_DISCORD_WEBHOOK", "")

        # Fall back to cluster.yaml config if env vars not set
        if not self.slack_webhook or not self.discord_webhook:
            try:
                yaml_webhooks = get_webhook_urls()
                if not self.slack_webhook and "slack" in yaml_webhooks:
                    self.slack_webhook = yaml_webhooks["slack"]
                if not self.discord_webhook and "discord" in yaml_webhooks:
                    self.discord_webhook = yaml_webhooks["discord"]
            except (KeyError, IndexError, AttributeError):
                pass  # Ignore config loading errors

        self.min_level = self.LEVELS.get(
            os.environ.get("RINGRIFT_ALERT_LEVEL", "warning").lower(), 2
        )
        self._session: ClientSession | None = None
        self._last_alert: dict[str, float] = {}  # Throttle repeated alerts
        self._throttle_seconds = 300  # 5 minutes between duplicate alerts

    async def _get_session(self) -> ClientSession:
        if self._session is None or self._session.closed:
            self._session = ClientSession(timeout=ClientTimeout(total=10))
        return self._session

    async def close(self) -> None:
        """Close the HTTP session to prevent memory leaks.

        December 2025: Added to fix memory leak from unclosed sessions.
        """
        if self._session is not None and not self._session.closed:
            await self._session.close()
            self._session = None

    def close_sync(self) -> None:
        """Synchronously close the HTTP session (for finally blocks)."""
        if self._session is not None and not self._session.closed:
            try:
                loop = asyncio.new_event_loop()
                loop.run_until_complete(self._session.close())
                loop.close()
            except (RuntimeError, OSError, asyncio.CancelledError) as e:
                # Dec 2025: Narrowed from bare Exception; best effort cleanup
                logger.debug(f"HTTP session close failed (best effort): {e}")
            self._session = None

    def _should_throttle(self, alert_key: str) -> bool:
        """Check if this alert should be throttled (duplicate within window)."""
        now = time.time()
        if alert_key in self._last_alert and now - self._last_alert[alert_key] < self._throttle_seconds:
            return True
        self._last_alert[alert_key] = now
        return False

    async def send(
        self,
        title: str,
        message: str = "",
        level: str = "warning",
        fields: dict[str, str] | None = None,
        node_id: str = "",
        # Aliases for backward compatibility (December 28, 2025)
        severity: str | None = None,
        context: dict[str, str] | None = None,
    ):
        """Send an alert to configured webhooks.

        Args:
            title: Alert title/subject (or message if message not provided)
            message: Alert body text
            level: debug/info/warning/error
            fields: Additional key-value pairs to include
            node_id: Node ID for deduplication
            severity: Alias for level (backward compatibility)
            context: Alias for fields (backward compatibility)

        December 28, 2025: Added severity and context aliases to fix API mismatch
        with callers using the alternative parameter names.
        """
        # Handle aliases - severity takes precedence if provided
        if severity is not None:
            level = severity
        if context is not None:
            fields = context
        # If message is empty, use title as message (for single-arg callers)
        if not message:
            message = title
            title = "RingRift Alert"

        if self.LEVELS.get(level, 2) < self.min_level:
            return

        if not self.slack_webhook and not self.discord_webhook:
            return

        # Throttle duplicate alerts
        alert_key = f"{title}:{node_id}"
        if self._should_throttle(alert_key):
            return

        try:
            session = await self._get_session()

            # Color based on level
            colors = {"debug": "#808080", "info": "#36a64f", "warning": "#ff9800", "error": "#ff0000"}
            color = colors.get(level, "#808080")

            # Send to Slack
            if self.slack_webhook:
                slack_fields = []
                if fields:
                    for k, v in fields.items():
                        slack_fields.append({"title": k, "value": str(v), "short": True})

                slack_payload = {
                    "attachments": [{
                        "color": color,
                        "title": f"[{level.upper()}] {title}",
                        "text": message,
                        "fields": slack_fields,
                        "footer": f"RingRift AI | {node_id}" if node_id else "RingRift AI",
                        "ts": int(time.time()),
                    }]
                }
                try:
                    async with session.post(self.slack_webhook, json=slack_payload) as resp:
                        if resp.status != 200:
                            logger.warning(f"[Webhook] Slack alert failed: {resp.status}")
                except Exception as e:  # noqa: BLE001
                    logger.error(f"[Webhook] Slack error: {e}")

            # Send to Discord
            if self.discord_webhook:
                discord_fields = []
                if fields:
                    for k, v in fields.items():
                        discord_fields.append({"name": k, "value": str(v), "inline": True})

                discord_payload = {
                    "embeds": [{
                        "title": f"[{level.upper()}] {title}",
                        "description": message,
                        "color": int(color.lstrip("#"), 16),
                        "fields": discord_fields,
                        "footer": {"text": f"RingRift AI | {node_id}" if node_id else "RingRift AI"},
                        "timestamp": datetime.utcnow().isoformat(),
                    }]
                }
                try:
                    async with session.post(self.discord_webhook, json=discord_payload) as resp:
                        if resp.status not in (200, 204):
                            logger.warning(f"[Webhook] Discord alert failed: {resp.status}")
                except Exception as e:  # noqa: BLE001
                    logger.error(f"[Webhook] Discord error: {e}")

        except Exception as e:  # noqa: BLE001
            logger.error(f"[Webhook] Alert send error: {e}")

    # Dec 28, 2025: Removed duplicate close() method (was lines 2031-2033)
    # The proper close() is defined at line 1898 with docstring and session=None cleanup


class P2POrchestrator(
    WorkQueueHandlersMixin,
    ElectionHandlersMixin,
    RelayHandlersMixin,
    GauntletHandlersMixin,
    GossipHandlersMixin,
    AdminHandlersMixin,
    EloSyncHandlersMixin,
    TournamentHandlersMixin,
    CMAESHandlersMixin,
    SSHTournamentHandlersMixin,
    DeliveryHandlersMixin,  # Phase 3: Delivery verification (Dec 27, 2025)
    SyncHandlersMixin,      # Phase 8: Sync handlers extraction (Dec 28, 2025)
    TableHandlersMixin,     # Phase 8: Table/dashboard handlers extraction (Dec 28, 2025)
    RegistryHandlersMixin,  # Phase 8: Registry handlers extraction (Dec 28, 2025)
    ManifestHandlersMixin,  # Phase 8: Manifest handlers extraction (Dec 28, 2025)
    ABTestHandlersMixin,    # Phase 8: A/B test handlers extraction (Dec 28, 2025)
    ImprovementHandlersMixin,  # Phase 8: Improvement loop handlers extraction (Dec 28, 2025)
    CanonicalGateHandlersMixin,  # Phase 8: Canonical gate handlers extraction (Dec 28, 2025)
    JobsApiHandlersMixin,        # Phase 8: Jobs API handlers extraction (Dec 28, 2025)
    NetworkHealthMixin,          # Network health endpoints (Dec 30, 2025)
    NetworkUtilsMixin,
    PeerManagerMixin,
    LeaderElectionMixin,
    GossipProtocolMixin,  # Provides gossip protocol + metrics (merged Dec 28, 2025)
    # Phase 5: SWIM + Raft integration (Dec 26, 2025)
    MembershipMixin,      # SWIM gossip-based membership
    ConsensusMixin,       # PySyncObj Raft consensus
    SwimHandlersMixin,    # /swim/* HTTP handlers
    RaftHandlersMixin,    # /raft/* HTTP handlers
    ResourceDetectorMixin,  # Resource detection delegation (Dec 28, 2025)
    EventEmissionMixin,     # Event emission consolidation (Dec 28, 2025 - Phase 8)
):
    """Main P2P orchestrator class that runs on each node.

    Inherits from:
    - WorkQueueHandlersMixin: Work queue HTTP handlers (handle_work_*)
    - ElectionHandlersMixin: Leader election handlers (handle_election*, handle_lease*, handle_voter*)
    - RelayHandlersMixin: NAT relay handlers (handle_relay_*)
    - GauntletHandlersMixin: Gauntlet evaluation handlers (handle_gauntlet_*)
    - GossipHandlersMixin: Gossip protocol handlers (handle_gossip*)
    - AdminHandlersMixin: Admin and git handlers (handle_git_*, handle_admin_*)
    - EloSyncHandlersMixin: Elo sync handlers (handle_elo_sync_*)
    - TournamentHandlersMixin: Tournament handlers (handle_tournament_*)
    - CMAESHandlersMixin: CMA-ES optimization handlers (handle_cmaes_*)
    - SSHTournamentHandlersMixin: SSH tournament handlers (handle_ssh_tournament_*)
    - NetworkUtilsMixin: Peer address parsing, URL building, Tailscale detection
    - PeerManagerMixin: Peer discovery, reputation tracking, cache management
    """

    def __init__(
        self,
        node_id: str,
        host: str = "0.0.0.0",
        port: int = DEFAULT_PORT,
        known_peers: list[str] | None = None,
        relay_peers: list[str] | None = None,
        ringrift_path: str | None = None,
        advertise_host: str | None = None,
        advertise_port: int | None = None,
        auth_token: str | None = None,
        require_auth: bool = False,
        storage_type: str = "auto",  # "disk", "ramdrive", or "auto"
        sync_to_disk_interval: int = 300,  # Sync ramdrive to disk every N seconds
    ):
        self.node_id = node_id
        self.host = host
        self.port = port

        # Phase 26: Multi-seed bootstrap - merge CLI peers with hardcoded seeds
        # Priority: CLI peers first, then hardcoded seeds
        # Shuffle seeds to distribute load across bootstrap attempts
        import random
        cli_peers = known_peers or []
        merged_seeds = list(cli_peers)  # CLI peers have highest priority
        for seed in BOOTSTRAP_SEEDS:
            if seed not in merged_seeds:
                merged_seeds.append(seed)
        # Shuffle only the hardcoded portion to avoid overloading any single seed
        if len(merged_seeds) > len(cli_peers):
            hardcoded_portion = merged_seeds[len(cli_peers):]
            random.shuffle(hardcoded_portion)
            merged_seeds = merged_seeds[:len(cli_peers)] + hardcoded_portion
        self.known_peers = merged_seeds
        self.bootstrap_seeds = list(BOOTSTRAP_SEEDS)  # Store original for reference
        logger.info(f"Bootstrap seeds: {len(cli_peers)} CLI + {len(BOOTSTRAP_SEEDS)} hardcoded = {len(self.known_peers)} total")

        # Peers that should always receive relay heartbeats (for NAT-blocked nodes)
        self.relay_peers: set[str] = set(relay_peers or [])
        self.ringrift_path = ringrift_path or self._detect_ringrift_path()

        # Phase 29: Cluster epoch tracking for split-brain resolution
        self._cluster_epoch: int = INITIAL_CLUSTER_EPOCH
        # P2P Health state tracking (Dec 2025)
        self._cluster_health_degraded: bool = False
        # Gossip-learned peer endpoints (Phase 28)
        self._gossip_learned_endpoints: dict[str, dict[str, Any]] = {}

        # Phase 2.4 (Dec 29, 2025): Partition read-only mode
        # When in minority partition, pause job dispatch to prevent data divergence
        self._partition_readonly_mode: bool = False
        self._partition_readonly_since: float = 0.0
        self._last_partition_check: float = 0.0
        self._partition_check_interval: float = 30.0  # Check every 30 seconds

        # Storage configuration: "disk", "ramdrive", or "auto" (detected)
        self.sync_to_disk_interval = sync_to_disk_interval
        self.ramdrive_path = "/dev/shm/ringrift/data"  # Standard ramdrive location
        self.ramdrive_syncer: RamdriveSyncer | None = None

        # Resolve "auto" storage type based on system resources
        if storage_type == "auto":
            resources = get_system_resources()
            if should_use_ramdrive():
                self.storage_type = "ramdrive"
                logger.info(f"Auto-detected storage: RAMDRIVE "
                      f"(RAM: {resources.total_ram_gb:.0f}GB, "
                      f"Disk: {resources.free_disk_gb:.0f}GB free / {resources.disk_usage_percent:.0f}% used)")
            else:
                self.storage_type = "disk"
                logger.info(f"Auto-detected storage: DISK "
                      f"(RAM: {resources.total_ram_gb:.0f}GB, "
                      f"Disk: {resources.free_disk_gb:.0f}GB free / {resources.disk_usage_percent:.0f}% used)")
            log_storage_recommendation()
        else:
            self.storage_type = storage_type
        # Git 2.35+ enforces safe.directory for repos with different ownership.
        # Many nodes run the orchestrator as root against a checkout owned by
        # another user (e.g. ubuntu), so always provide a safe.directory override
        # for all git operations.
        self._git_safe_directory = os.path.abspath(self.ringrift_path)
        self.build_version = self._detect_build_version()
        self.start_time = time.time()
        self.last_peer_bootstrap = 0.0

        # Resource detection delegation (Dec 28, 2025)
        self._resource_detector = ResourceDetector(
            ringrift_path=self.ringrift_path,
            start_time=self.start_time,
            startup_grace_period=STARTUP_JSONL_GRACE_PERIOD_SECONDS,
        )

        # Public endpoint peers should use to reach us. Peers learn our host from
        # the heartbeat socket address, but the port must be self-reported. This
        # matters for port-mapped environments like Vast.ai.
        self.advertise_host = (advertise_host or os.environ.get(ADVERTISE_HOST_ENV, "")).strip()
        if not self.advertise_host:
            # Prefer a stable mesh address (Tailscale) when available so nodes
            # behind NAT remain reachable and the cluster converges on a single
            # view of peer endpoints.
            ts_ip = self._get_tailscale_ip()
            self.advertise_host = ts_ip or self._get_local_ip()

        # Dec 30, 2025: Validate advertise_host to prevent private IP issues
        # that cause P2P quorum loss when nodes can't reach each other
        self._validate_and_fix_advertise_host()

        self.advertise_port = advertise_port if advertise_port is not None else self._infer_advertise_port()

        # Optional auth token used to protect mutating endpoints and cluster control.
        # Default is allow-all unless a token is configured.
        env_token = (os.environ.get(AUTH_TOKEN_ENV, "")).strip()
        token_from_arg = (auth_token or "").strip()
        token = token_from_arg or env_token

        if not token:
            token_file = (os.environ.get(AUTH_TOKEN_FILE_ENV, "")).strip()
            if token_file:
                try:
                    token = Path(token_file).read_text().strip()
                except Exception as e:  # noqa: BLE001
                    logger.info(f"Auth: failed to read {AUTH_TOKEN_FILE_ENV}={token_file}: {e}")

        self.auth_token = token.strip()
        self.require_auth = bool(require_auth)
        if self.require_auth and not self.auth_token:
            raise ValueError(
                f"--require-auth set but {AUTH_TOKEN_ENV}/{AUTH_TOKEN_FILE_ENV}/--auth-token is empty"
            )

        # Optional split-brain mitigation: require a majority of "voter" nodes
        # to be visible before assuming or renewing leadership.
        #
        # Voters can be configured via:
        # - env: RINGRIFT_P2P_VOTERS="node-a,node-b,..."
        # - ai-service/config/distributed_hosts.yaml: per-host `p2p_voter: true`
        self.voter_config_source: str = "none"  # env|config|state|learned|none
        self.voter_node_ids: list[str] = self._load_voter_node_ids()
        # SIMPLIFIED QUORUM: Fixed at 3 voters (or less if fewer voters exist)
        self.voter_quorum_size: int = min(VOTER_MIN_QUORUM, len(self.voter_node_ids)) if self.voter_node_ids else 0
        if self.voter_node_ids:
            print(
                f"[P2P] Voter quorum enabled: voters={len(self.voter_node_ids)}, "
                f"quorum={self.voter_quorum_size} ({', '.join(self.voter_node_ids)})"
            )

        # Node state
        self.role = NodeRole.FOLLOWER
        self.leader_id: str | None = None
        self.verbose = bool(os.environ.get("RINGRIFT_P2P_VERBOSE", "").strip())
        self.peers: dict[str, NodeInfo] = {}
        self.local_jobs: dict[str, ClusterJob] = {}
        self.active_jobs: dict[str, dict[str, Any]] = {}  # Track running jobs by type (selfplay, training, etc.)

        # Network health tracking (December 30, 2025)
        # Reference to TailscalePeerDiscoveryLoop for stats reporting in /network/health
        self._tailscale_discovery_loop: Any = None

        # Distributed job state tracking (leader-only)
        self.distributed_cmaes_state: dict[str, DistributedCMAESState] = {}
        self.distributed_tournament_state: dict[str, DistributedTournamentState] = {}
        self.ssh_tournament_runs: dict[str, SSHTournamentRun] = {}
        self.improvement_loop_state: dict[str, ImprovementLoopState] = {}
        # Limit CPU-heavy CMA-ES local evaluations to avoid runaway process
        # explosions that can starve the orchestrator (especially on relay hubs).
        try:
            raw = (os.environ.get("RINGRIFT_P2P_MAX_CONCURRENT_CMAES_EVALS", "") or "").strip()
            self.max_concurrent_cmaes_evals = max(1, int(raw)) if raw else 2
        except (ValueError, AttributeError):
            self.max_concurrent_cmaes_evals = 2
        self._cmaes_eval_semaphore = asyncio.Semaphore(int(self.max_concurrent_cmaes_evals))

        # Tournament match semaphore - limit concurrent Elo calibration matches to prevent OOM
        # Each match can potentially load neural networks which use significant memory
        # NOTE: Set to None here, created lazily in async context to avoid event loop issues
        self._tournament_match_semaphore: asyncio.Semaphore | None = None

        # Phase 2: Distributed data sync state
        self.local_data_manifest: NodeDataManifest | None = None
        self.cluster_data_manifest: ClusterDataManifest | None = None  # Leader-only
        self.manifest_collection_interval = 300.0  # Collect manifests every 5 minutes
        self.last_manifest_collection = 0.0

        # Dashboard/selfplay stats history (leader-only). Stored in-memory to
        # enable lightweight throughput charts without adding DB migrations.
        self.selfplay_stats_history: list[dict[str, Any]] = []
        self.selfplay_stats_history_max_samples: int = 288  # ~24h @ 5-min cadence

        # Canonical gate jobs (leader-only): dashboard-triggered runs of
        # scripts/generate_canonical_selfplay.py.
        self.canonical_gate_jobs: dict[str, dict[str, Any]] = {}
        self.canonical_gate_jobs_lock = threading.RLock()

        # Phase 2: P2P rsync coordination state
        self.active_sync_jobs: dict[str, DataSyncJob] = {}
        self.current_sync_plan: ClusterSyncPlan | None = None  # Leader-only
        self.pending_sync_requests: list[dict[str, Any]] = []  # Requests from non-leader nodes
        self.sync_in_progress = False
        self.last_sync_time = 0.0
        self.auto_sync_interval = 600.0  # Auto-sync every 10 minutes when data is missing

        # Training node priority sync state (leader-only)
        self.training_sync_interval = TRAINING_SYNC_INTERVAL
        self.last_training_sync_time = 0.0
        self.training_nodes_cache: list[str] = []  # Cached list of top GPU nodes
        self.training_nodes_cache_time = 0.0
        self.games_synced_to_training: dict[str, int] = {}  # node_id -> last synced game count

        # Circuit breaker for fault-tolerant peer communication
        self._circuit_registry = get_circuit_registry()

        # Phase 3: Training pipeline state (leader-only)
        self.training_jobs: dict[str, TrainingJob] = {}
        self.training_thresholds: TrainingThresholds = TrainingThresholds()
        self.last_training_check: float = 0.0
        self.training_check_interval: float = 300.0  # Check every 5 minutes
        self.games_at_last_nnue_train: dict[str, int] = {}  # board_type -> game_count
        self.games_at_last_cmaes_train: dict[str, int] = {}

        # Phase 5: Automated improvement cycle manager (leader-only)
        self.improvement_cycle_manager: ImprovementCycleManager | None = None
        if HAS_IMPROVEMENT_MANAGER:
            try:
                self.improvement_cycle_manager = ImprovementCycleManager(
                    db_path=STATE_DIR / f"{node_id}_improvement.db",
                    ringrift_path=self.ringrift_path,
                )
                logger.info("ImprovementCycleManager initialized")
            except Exception as e:  # noqa: BLE001
                logger.error(f"Failed to initialize ImprovementCycleManager: {e}")
        self.last_improvement_cycle_check: float = 0.0

        # P2P-integrated monitoring (leader starts Prometheus/Grafana)
        self.monitoring_manager: MonitoringManager | None = None
        if HAS_P2P_MONITORING:
            try:
                self.monitoring_manager = MonitoringManager(
                    node_id=node_id,
                    prometheus_port=9090,
                    grafana_port=3000,
                    config_dir=Path(self.ringrift_path) / "monitoring",
                )
                logger.info("MonitoringManager initialized")
            except Exception as e:  # noqa: BLE001
                logger.error(f"Failed to initialize MonitoringManager: {e}")
        self._monitoring_was_leader = False  # Track leadership changes
        self.improvement_cycle_check_interval: float = 600.0  # Check every 10 minutes

        # P2P Auto-Deployer (leader-only): ensures P2P runs on all cluster nodes
        self.p2p_auto_deployer: P2PAutoDeployer | None = None
        self._auto_deployer_task: asyncio.Task | None = None

        # Webhook notifications for alerts
        self.notifier = WebhookNotifier()

        # Diversity tracking metrics
        self.diversity_metrics = {
            "games_by_engine_mode": {},      # engine_mode -> count
            "games_by_board_config": {},     # "board_players" -> count
            "games_by_difficulty": {},       # difficulty -> count
            "asymmetric_games": 0,           # count of asymmetric games scheduled
            "symmetric_games": 0,            # count of symmetric games scheduled
            "training_triggers": 0,          # count of training triggers
            "cmaes_triggers": 0,             # count of CMA-ES triggers
            "promotions": 0,                 # count of model promotions
            "rollbacks": 0,                  # count of rollbacks
            "last_reset": time.time(),       # when metrics were last reset
        }

        # === CRITICAL SELF-IMPROVEMENT LOOP METRICS ===
        # Training progress tracking (populated by training callbacks)
        self.training_metrics: dict[str, dict[str, float]] = {}  # config -> {loss, val_loss, epoch}

        # Selfplay throughput tracking
        self.selfplay_throughput: dict[str, float] = {}  # config -> games/hour

        # Cost efficiency metrics
        self.cost_metrics: dict[str, float] = {
            "gpu_hours_total": 0.0,
            "estimated_cost_usd": 0.0,
            "elo_per_gpu_hour": 0.0,
        }

        # Promotion quality metrics
        self.promotion_metrics: dict[str, Any] = {
            "success_rate": 0.0,
            "avg_elo_gain": 0.0,
            "rejections": {},  # reason -> count
            "total_attempts": 0,
            "successful": 0,
        }

        # LEARNED LESSONS - Stuck job detection (leader-only)
        # Track when each node's GPU first went idle with running jobs
        self.gpu_idle_since: dict[str, float] = {}  # node_id -> timestamp when GPU went idle

        # A/B Testing Framework - Compare models head-to-head with statistical significance
        # Key: test_id (UUID), Value: ABTestState dict
        self.ab_tests: dict[str, dict[str, Any]] = {}
        self.ab_test_lock = threading.RLock()

        # Elo Sync Manager - Keeps unified_elo.db consistent across cluster
        self.elo_sync_manager: EloSyncManager | None = None
        if HAS_ELO_SYNC:
            try:
                db_path = Path(self.ringrift_path) / "ai-service" / "data" / "unified_elo.db"
                # Use env var for coordinator, fallback to nebius-backbone-1 (stable backbone node)
                elo_coordinator = os.environ.get("RINGRIFT_ELO_COORDINATOR", "nebius-backbone-1")
                self.elo_sync_manager = EloSyncManager(
                    db_path=db_path,
                    coordinator_host=elo_coordinator,
                    sync_interval=300,  # Sync every 5 minutes
                )
                logger.info(f"EloSyncManager initialized (db: {db_path})")
            except Exception as e:  # noqa: BLE001
                logger.error(f"Failed to initialize EloSyncManager: {e}")

        # Queue Populator - Maintains 50+ work items until 2000 Elo target met
        self._queue_populator: QueuePopulator | None = None

        # PFSP (Prioritized Fictitious Self-Play) opponent pool (leader-only)
        # Maintains a pool of historical models weighted by difficulty for diverse training
        self.pfsp_pools: dict[str, Any] = {}  # config_key -> PFSPOpponentPool
        if HAS_PFSP:
            try:
                for config_key in ["square8_2p", "square8_4p", "hex8_2p", "hexagonal_2p"]:
                    self.pfsp_pools[config_key] = PFSPOpponentPool(
                        max_pool_size=30,
                        hard_opponent_weight=0.6,
                        diversity_weight=0.25,
                        recency_weight=0.15,
                    )
                logger.info(f"PFSP opponent pools initialized for {len(self.pfsp_pools)} configs")
            except Exception as e:  # noqa: BLE001
                logger.error(f"Failed to initialize PFSP pools: {e}")

        # CMA-ES Auto-Tuner (leader-only)
        # Automatically triggers hyperparameter optimization when Elo plateaus
        self.cmaes_auto_tuners: dict[str, Any] = {}  # config_key -> CMAESAutoTuner
        self.last_cmaes_elo: dict[str, float] = {}  # config_key -> last recorded Elo
        if HAS_PFSP and CMAESAutoTuner:
            try:
                for config_key in ["square8_2p", "square8_4p", "hex8_2p", "hexagonal_2p"]:
                    parts = config_key.rsplit("_", 1)
                    board_type = parts[0]
                    num_players = int(parts[1].replace("p", ""))
                    plateau_cfg = PlateauConfig(patience=10)
                    self.cmaes_auto_tuners[config_key] = CMAESAutoTuner(
                        board_type=board_type,
                        num_players=num_players,
                        plateau_config=plateau_cfg,
                        min_epochs_between_tuning=50,
                        max_auto_tunes=3,
                    )
                logger.info(f"CMA-ES auto-tuners initialized for {len(self.cmaes_auto_tuners)} configs")
            except Exception as e:  # noqa: BLE001
                logger.error(f"Failed to initialize CMA-ES auto-tuners: {e}")

        # Locks for thread safety
        # Use RLock (reentrant lock) to allow nested acquisitions from same thread
        # This prevents deadlocks when helper methods like _select_best_relay are
        # called while already holding the lock
        self.peers_lock = threading.RLock()
        self.jobs_lock = threading.RLock()
        self.manifest_lock = threading.RLock()
        self.sync_lock = threading.RLock()
        self.training_lock = threading.RLock()
        self.ssh_tournament_lock = threading.RLock()
        self.relay_lock = threading.RLock()

        # ============================================
        # Phase 5: SWIM + Raft Integration (Dec 26, 2025)
        # ============================================
        # SWIM provides leaderless gossip-based membership with 5s failure detection
        # Raft provides replicated work queue with sub-second failover
        # Both are initialized here but started asynchronously in run()

        # Feature flag validation - warn if flags enabled but dependencies missing
        from scripts.p2p.constants import (
            SWIM_ENABLED, RAFT_ENABLED, MEMBERSHIP_MODE, CONSENSUS_MODE
        )
        try:
            from app.p2p.swim_adapter import SWIM_AVAILABLE
        except ImportError:
            SWIM_AVAILABLE = False
        try:
            from scripts.p2p.consensus_mixin import PYSYNCOBJ_AVAILABLE
        except ImportError:
            PYSYNCOBJ_AVAILABLE = False

        if SWIM_ENABLED and not SWIM_AVAILABLE:
            logger.warning(
                "RINGRIFT_SWIM_ENABLED=true but swim-p2p not installed or not compatible. "
                "SWIM features disabled. Install with: pip install swim-p2p>=1.2.0"
            )
        if RAFT_ENABLED and not PYSYNCOBJ_AVAILABLE:
            logger.warning(
                "RINGRIFT_RAFT_ENABLED=true but pysyncobj not installed. "
                "Raft features disabled. Install with: pip install pysyncobj>=0.3.14"
            )
        if MEMBERSHIP_MODE in ("swim", "hybrid") and not SWIM_AVAILABLE:
            logger.warning(
                f"RINGRIFT_MEMBERSHIP_MODE={MEMBERSHIP_MODE} but SWIM unavailable. "
                "Falling back to HTTP heartbeats."
            )
        if CONSENSUS_MODE in ("raft", "hybrid") and not PYSYNCOBJ_AVAILABLE:
            logger.warning(
                f"RINGRIFT_CONSENSUS_MODE={CONSENSUS_MODE} but PySyncObj unavailable. "
                "Falling back to Bully algorithm."
            )

        # Log active P2P protocol modes
        logger.info(
            f"P2P protocols: MEMBERSHIP_MODE={MEMBERSHIP_MODE} (SWIM={'available' if SWIM_AVAILABLE else 'unavailable'}), "
            f"CONSENSUS_MODE={CONSENSUS_MODE} (Raft={'available' if PYSYNCOBJ_AVAILABLE else 'unavailable'})"
        )

        # Initialize SWIM membership (from MembershipMixin)
        self._swim_initialized = self._init_swim_membership()
        if self._swim_initialized:
            logger.info("SWIM membership initialized (will start in run())")

        # Initialize Raft consensus (from ConsensusMixin)
        # Note: Raft requires advertise_host which is set above
        self._raft_init_attempted = False
        # Raft initialization deferred to after peers are discovered
        # to ensure we have partner addresses available

        # Try early Raft initialization if we have voter nodes
        if RAFT_ENABLED and PYSYNCOBJ_AVAILABLE and self.voter_node_ids:
            try:
                self._raft_init_attempted = True
                raft_ok = self._init_raft_consensus()
                if raft_ok:
                    logger.info("Raft consensus initialized (will sync with peers in run())")
            except Exception as e:  # noqa: BLE001
                logger.warning(f"Early Raft initialization failed (will retry later): {e}")

        # State persistence (Phase 1 refactoring: delegated to StateManager)
        self.db_path = STATE_DIR / f"{node_id}_state.db"
        self.state_manager = StateManager(self.db_path, verbose=self.verbose)
        self.state_manager.init_database()
        self._cluster_epoch = self.state_manager.load_cluster_epoch()

        # Metrics recording (Phase 1 refactoring: delegated to MetricsManager)
        self.metrics_manager = MetricsManager(self.db_path)

        # Event flags
        self.running = True
        self.election_in_progress = False
        self.last_election_attempt: float = 0.0

        # LEARNED LESSONS - Lease-based leadership to prevent split-brain
        # Leader must continuously renew lease; if lease expires, leadership is void
        self.leader_lease_expires: float = 0.0  # timestamp when current leader's lease expires
        self.last_lease_renewal: float = 0.0  # when we last renewed our lease (if leader)
        self.leader_lease_id: str = ""  # unique ID for current leadership term
        # LEADERLESS FALLBACK: Track when we last had a functioning leader.
        # If leaderless for too long, nodes can trigger local training independently.
        self.last_leader_seen: float = time.time()  # When we last saw a functioning leader
        self.last_local_training_fallback: float = 0.0  # When we last triggered local training fallback

        # Voter-backed lease grants (split-brain resistance).
        #
        # When quorum gating is enabled, voters act as a lightweight consensus
        # group by granting an exclusive leader lease to a single node at a time.
        # A leader must renew its lease with a quorum of voters; otherwise it
        # steps down. This prevents split-brain even if multiple nodes think
        # they are eligible leaders.
        self.voter_grant_leader_id: str = ""
        self.voter_grant_lease_id: str = ""
        self.voter_grant_expires: float = 0.0

        # Phase 15.1.1: Fenced lease tokens with monotonic epoch
        # These prevent split-brain during network partitions by ensuring
        # only one leader per epoch can issue commands.
        self._lease_epoch: int = 0  # Monotonic, never decreases
        self._fence_token: str = ""  # Unique per lease grant: node_id:epoch:timestamp
        self._last_seen_epoch: int = 0  # Highest epoch seen from any leader

        # Job completion tracking for auto-restart
        self.completed_jobs: dict[str, float] = {}  # node_id -> last job completion time
        self.jobs_started_at: dict[str, dict[str, float]] = {}  # node_id -> {job_id: start_time}

        # NAT/relay support (for nodes without inbound connectivity).
        # NAT-blocked nodes poll a relay endpoint for commands; the leader enqueues
        # commands keyed by node_id.
        self.last_inbound_heartbeat: float = 0.0
        self.last_relay_heartbeat: float = 0.0
        self.relay_command_queue: dict[str, list[dict[str, Any]]] = {}
        self.pending_relay_acks: set[str] = set()
        self.pending_relay_results: list[dict[str, Any]] = []
        self.relay_command_attempts: dict[str, int] = {}
        # Background tasks list for graceful shutdown (Dec 2025)
        self._background_tasks: list[asyncio.Task] = []

        # SAFEGUARDS - Rate limiting and coordinator integration (added 2025-12-15)
        self.spawn_timestamps: list[float] = []  # Timestamps of recent process spawns
        self.agent_mode = AGENT_MODE_ENABLED
        self.coordinator_url = COORDINATOR_URL
        self.last_coordinator_check: float = 0.0
        self.coordinator_available: bool = False
        logger.info(f"Safeguards: rate_limit={SPAWN_RATE_LIMIT_PER_MINUTE}/min, "
              f"load_max={LOAD_AVERAGE_MAX_MULTIPLIER}x, agent_mode={self.agent_mode}")

        # Load persisted state
        self._load_state()
        if self.leader_id == self.node_id:
            self.role = NodeRole.LEADER

        # Self info
        self.self_info = self._create_self_info()

        # Phase 1 Refactoring: NodeSelector for node ranking/selection
        self.node_selector = NodeSelector(
            get_peers=lambda: self.peers,
            get_self_info=lambda: self.self_info,
            peers_lock=self.peers_lock,
            get_training_jobs=lambda: self.training_jobs,
        )
        # December 2025: Subscribe to health events (HOST_OFFLINE, NODE_RECOVERED)
        # to track unhealthy nodes for filtering during selection
        # Note: NodeSelector uses its own subscribe_to_events (not mixin)
        self.node_selector.subscribe_to_events()

        # Phase 2A Refactoring: SyncPlanner for data synchronization
        # NOTE: request_peer_manifest is wired AFTER SyncPlanner creation
        # because _request_peer_manifest is a method on this class
        self.sync_planner = SyncPlanner(
            node_id=self.node_id,
            data_directory=self.get_data_directory(),
            get_peers=lambda: self.peers,
            get_self_info=lambda: self.self_info,
            peers_lock=self.peers_lock,
            is_leader=lambda: self._is_leader(),
            request_peer_manifest=lambda peer_id: self._request_peer_manifest_sync(peer_id),
            check_disk_capacity=lambda: check_disk_has_capacity(),
            config=SyncPlannerConfig(),
        )
        # December 2025: Subscribe to cluster events (LEADER_ELECTED, NODE_RECOVERED)
        # to invalidate cached manifests and trigger re-collection
        # Wave 7 Phase 1.1: Use retry mechanism for reliable subscription
        self.sync_planner.subscribe_to_events_with_retry()

        # Phase 2B Refactoring: SelfplayScheduler for priority-based selfplay allocation
        # All callbacks wired for full delegation (Dec 2025)
        self.selfplay_scheduler = SelfplayScheduler(
            get_cluster_elo_fn=lambda: self._get_cluster_elo_summary(),
            load_curriculum_weights_fn=lambda: self._load_curriculum_weights(),
            get_board_priority_overrides_fn=lambda: getattr(self, "board_priority_overrides", {}),
            # Backpressure callbacks (wired Dec 2025)
            should_stop_production_fn=should_stop_production if HAS_NEW_COORDINATION else None,
            should_throttle_production_fn=should_throttle_production if HAS_NEW_COORDINATION else None,
            get_throttle_factor_fn=get_throttle_factor if HAS_NEW_COORDINATION else None,
            # Resource targeting callbacks (wired Dec 2025)
            record_utilization_fn=record_utilization if HAS_NEW_COORDINATION else None,
            get_host_targets_fn=get_host_targets if HAS_NEW_COORDINATION else None,
            get_target_job_count_fn=get_target_job_count if HAS_NEW_COORDINATION else None,
            should_scale_up_fn=should_scale_up if HAS_NEW_COORDINATION else None,
            should_scale_down_fn=should_scale_down if HAS_NEW_COORDINATION else None,
            # Hardware-aware limits (wired Dec 2025)
            get_max_selfplay_for_node_fn=get_max_selfplay_for_node if HAS_HW_AWARE_LIMITS else None,
            get_hybrid_selfplay_limits_fn=get_hybrid_selfplay_limits if HAS_HW_AWARE_LIMITS else None,
            # Safeguards callback (wired Dec 2025) - halt selfplay during emergency
            is_emergency_active_fn=_safeguards.is_emergency_active if HAS_SAFEGUARDS and _safeguards else None,
            verbose=self.verbose,
        )
        # Subscribe to feedback loop events (December 2025)
        # Wave 7 Phase 1.1: Use retry mechanism for reliable subscription
        self.selfplay_scheduler.subscribe_to_events_with_retry()

        # Phase 2B Refactoring: JobManager for job spawning and lifecycle
        self.job_manager = JobManager(
            ringrift_path=self.ringrift_path,
            node_id=self.node_id,
            peers=self.peers,
            peers_lock=self.peers_lock,
            active_jobs=self.active_jobs,
            jobs_lock=self.jobs_lock,
            improvement_loop_state=self.improvement_loop_state,
            distributed_tournament_state=self.distributed_tournament_state,
        )
        # December 2025: Subscribe to job-relevant events (HOST_OFFLINE, HOST_ONLINE)
        # Wave 7 Phase 1.1: Use retry mechanism for reliable subscription
        self.job_manager.subscribe_to_events_with_retry()

        # Phase 2B Refactoring: TrainingCoordinator for training dispatch and completion
        self.training_coordinator = TrainingCoordinator(
            ringrift_path=Path(self.ringrift_path),
            get_cluster_data_manifest=lambda: self.cluster_data_manifest,
            get_training_jobs=lambda: self.training_jobs,
            get_training_lock=lambda: self.training_lock,
            get_peers=lambda: self.peers,
            get_peers_lock=lambda: self.peers_lock,
            get_self_info=lambda: self.self_info,
            training_thresholds=self.training_thresholds,
            games_at_last_nnue_train=getattr(self, "games_at_last_nnue_train", None),
            games_at_last_cmaes_train=getattr(self, "games_at_last_cmaes_train", None),
            improvement_cycle_manager=getattr(self, "improvement_cycle_manager", None),
            auth_headers=lambda: self._auth_headers(),
            urls_for_peer=lambda node_id, endpoint: self._urls_for_peer(node_id, endpoint),
            save_state_callback=lambda: self._save_state(),
            has_voter_quorum=lambda: self._has_voter_quorum(),
        )
        # December 2025: Subscribe to training-relevant events
        # (SELFPLAY_COMPLETE, DATA_SYNC_COMPLETED, EVALUATION_COMPLETED, REGRESSION_DETECTED)
        # Wave 7 Phase 1.1: Use retry mechanism for reliable subscription
        self.training_coordinator.subscribe_to_events_with_retry()

        # December 2025: Wire feedback loops for self-improvement
        # This connects curriculum adjustments to Elo changes, evaluation results, etc.
        self._wire_feedback_loops()

        # December 2025: Subscribe to daemon status events for observability
        daemon_events_ok = self._subscribe_to_daemon_events()

        # December 2025: Subscribe to training feedback signals for dynamic orchestration
        feedback_signals_ok = self._subscribe_to_feedback_signals()

        # December 2025: Subscribe to manager lifecycle events for coordination
        manager_events_ok = self._subscribe_to_manager_events()

        # Dec 2025: Store subscription status for health checks via /status endpoint
        self._event_subscription_status = {
            "daemon_events": daemon_events_ok,
            "feedback_signals": feedback_signals_ok,
            "manager_events": manager_events_ok,
            "all_healthy": daemon_events_ok and feedback_signals_ok and manager_events_ok,
            "timestamp": time.time(),
        }

        # Log subscription status for debugging integration issues
        if self._event_subscription_status["all_healthy"]:
            logger.info("[P2P] Event subscriptions: daemon=, feedback=, manager=")
        else:
            logger.warning(
                f"[P2P] Event subscriptions incomplete: "
                f"daemon={'' if daemon_events_ok else ''}, "
                f"feedback={'' if feedback_signals_ok else ''}, "
                f"manager={'' if manager_events_ok else ''}"
            )

        # December 2025 (Wave 7 Phase 1.2): Critical subscription failure mode
        # These subscriptions are required for the training pipeline to function.
        # Without them, events like DATA_SYNC_COMPLETED and TRAINING_COMPLETED
        # won't trigger downstream actions, causing the pipeline to stall silently.
        CRITICAL_SUBSCRIPTION_GROUPS = ["manager_events"]  # Contains DATA_SYNC_COMPLETED, TRAINING_COMPLETED
        self._event_subscription_status["critical_failed"] = []

        for group in CRITICAL_SUBSCRIPTION_GROUPS:
            if not self._event_subscription_status.get(group, False):
                self._event_subscription_status["critical_failed"].append(group)

        if self._event_subscription_status["critical_failed"]:
            failed_groups = self._event_subscription_status["critical_failed"]
            msg = f"[P2P] CRITICAL: Event subscription groups failed: {failed_groups}"
            logger.critical(msg)

            # Optional: fail startup on critical subscription failure
            # Enable via environment variable for strict production deployments
            if os.environ.get("RINGRIFT_FAIL_ON_SUBSCRIPTION_FAILURE", "").lower() == "true":
                raise RuntimeError(
                    f"Critical event subscriptions failed: {failed_groups}. "
                    "Set RINGRIFT_FAIL_ON_SUBSCRIPTION_FAILURE=false to allow startup anyway."
                )

        # NOTE: _manager_health_status validation is deferred to after _loop_manager
        # initialization below (line ~2730). This avoids AttributeError on _loop_manager.

        print(
            f"[P2P] Initialized node {node_id} on {host}:{port} "
            f"(advertise {self.advertise_host}:{self.advertise_port})"
        )
        logger.info(f"RingRift path: {self.ringrift_path}")
        logger.info(f"Version: {self.build_version}")
        logger.info(f"Known peers: {self.known_peers}")
        if self.relay_peers:
            logger.info(f"Relay peers (forced relay mode): {list(self.relay_peers)}")
        if self.auth_token:
            logger.info(f"Auth: enabled via {AUTH_TOKEN_ENV}")
        else:
            logger.info(f"Auth: disabled (set {AUTH_TOKEN_ENV} to enable)")

        # Hybrid transport for HTTP/SSH fallback (self-healing Vast connectivity)
        self.hybrid_transport: HybridTransport | None = None
        if HAS_HYBRID_TRANSPORT:
            try:
                self.hybrid_transport = get_hybrid_transport()
                logger.info("HybridTransport: enabled (HTTP with SSH fallback for Vast)")
            except Exception as e:  # noqa: BLE001
                logger.info(f"HybridTransport: failed to initialize: {e}")

        # SWIM-based leaderless membership (gossip protocol)
        # This provides faster failure detection (<5s vs 60s+) and O(1) bandwidth
        self._swim_manager = get_swim_manager(node_id=node_id, bind_port=7947)
        self._swim_started = False

        # SyncRouter: Intelligent data routing with quality-based priority (December 2025)
        # Lazy-loaded to avoid import overhead on startup
        self._sync_router: SyncRouter | None = None
        self._sync_router_wired = False

        # Phase 4: LoopManager for extracted loops (Dec 2025)
        self._loop_manager: LoopManager | None = None
        self._loops_registered = False

        # December 27, 2025: Validate manager health at startup
        # This catches initialization issues early rather than at first use
        # NOTE: Must be called AFTER _loop_manager is initialized (was causing AttributeError)
        self._manager_health_status = self._validate_manager_health()

    def _get_loop_manager(self) -> "LoopManager | None":
        """Get the LoopManager, initializing if needed."""
        if self._loop_manager is None:
            self._loop_manager = get_loop_manager()
        return self._loop_manager

    def _register_extracted_loops(self) -> bool:
        """Register extracted loops with the LoopManager.

        December 2025: Extracted loops run alongside inline loops during
        the gradual migration. Once verified, inline loops will be deprecated.
        """
        logger.info(f"[LoopManager] _register_extracted_loops called, already_registered={self._loops_registered}")
        if self._loops_registered:
            return True

        manager = self._get_loop_manager()
        logger.info(f"[LoopManager] Got manager: {manager}")
        if manager is None:
            logger.info("LoopManager: not available, using inline loops only")
            return False

        try:
            from scripts.p2p.loops import (
                QueuePopulatorLoop,
                EloSyncLoop,
                JobReaperLoop,
                IdleDetectionLoop,
                AutoScalingLoop,
                WorkQueueMaintenanceLoop,
                NATManagementLoop,
                ManifestCollectionLoop,
                ValidationLoop,
                ModelSyncLoop,
                DataManagementLoop,
            )

            # QueuePopulatorLoop - maintains 50+ work items until 2000 Elo
            queue_populator = QueuePopulatorLoop(
                get_role=lambda: self.role,
                get_selfplay_scheduler=lambda: self.selfplay_scheduler,
                notifier=self.notifier,
            )
            manager.register(queue_populator)

            # EloSyncLoop - keeps unified_elo.db consistent across cluster
            if hasattr(self, 'elo_sync_manager') and self.elo_sync_manager is not None:
                elo_sync = EloSyncLoop(
                    get_elo_sync_manager=lambda: self.elo_sync_manager,
                    get_sync_in_progress=lambda: self.sync_in_progress,
                )
                manager.register(elo_sync)

            # JobReaperLoop - enforces job timeouts and reassigns work
            # December 27, 2025: Fixed constructor to match JobReaperLoop signature
            # Previous version passed wrong parameters causing reaper to never run
            job_reaper = JobReaperLoop(
                get_active_jobs=self._get_all_active_jobs_for_reaper,
                cancel_job=self._cancel_job_for_reaper,
                get_job_heartbeats=self._get_job_heartbeats_for_reaper,
            )
            manager.register(job_reaper)

            # IdleDetectionLoop - auto-assigns work to idle nodes
            idle_detection = IdleDetectionLoop(
                get_role=lambda: self.role,
                get_peers=lambda: self.peers,
                get_work_queue=get_work_queue,
                on_idle_detected=self._auto_start_selfplay,
            )
            manager.register(idle_detection)

            # AutoScalingLoop - provision/deprovision cloud instances
            # December 28, 2025 (Wave 7 Phase 2.2): Enabled with CompositeScaleAdapter
            # Uses RELUCTANT TERMINATION - nodes only terminated after confirmed unusability
            try:
                from scripts.p2p.adapters.scale_adapters import (
                    CompositeScaleAdapter,
                    AutoScalingConfig,
                    create_scale_adapter,
                )

                # Create scale adapter with conservative (reluctant) configuration
                scale_adapter = create_scale_adapter(
                    work_queue=get_work_queue(),
                    peers_getter=lambda: self.peers,
                    config=AutoScalingConfig.conservative(),  # Reluctant termination
                )

                # Store adapter for job activity tracking
                self._scale_adapter = scale_adapter

                auto_scaling = AutoScalingLoop(
                    get_pending_work=scale_adapter.get_pending_work,
                    get_active_nodes=scale_adapter.get_active_nodes,
                    get_idle_nodes=scale_adapter.get_idle_nodes,
                    scale_up=scale_adapter.scale_up,
                    scale_down=scale_adapter.scale_down,
                )
                manager.register(auto_scaling)
                logger.info("[P2P] AutoScalingLoop enabled with reluctant termination")
            except ImportError as e:
                logger.debug(f"[P2P] AutoScalingLoop disabled: {e}")
                self._scale_adapter = None
            except Exception as e:  # noqa: BLE001
                logger.warning(f"[P2P] AutoScalingLoop initialization failed: {e}")
                self._scale_adapter = None

            # WorkQueueMaintenanceLoop - leader cleans up timeouts and old items
            # December 27, 2025: Migrated from inline _work_queue_maintenance_loop
            work_queue_maint = WorkQueueMaintenanceLoop(
                is_leader=lambda: self.role == NodeRole.LEADER,
                get_work_queue=get_work_queue,
            )
            manager.register(work_queue_maint)

            # NATManagementLoop - STUN-like probing, symmetric NAT detection, relay selection
            # December 27, 2025: Migrated from inline _nat_management_loop
            nat_management = NATManagementLoop(
                detect_nat_type=self._detect_nat_type,
                probe_nat_blocked_peers=self._probe_nat_blocked_peers,
                update_relay_preferences=self._update_relay_preferences,
            )
            manager.register(nat_management)

            # ManifestCollectionLoop - periodic manifest collection for dashboard/training/sync
            # December 27, 2025: Migrated from inline _manifest_collection_loop
            manifest_collection = ManifestCollectionLoop(
                get_role=lambda: self.role,
                collect_cluster_manifest=self._collect_cluster_manifest,
                collect_local_manifest=self._collect_local_data_manifest,
                update_manifest=self._update_manifest_from_loop,
                update_improvement_cycle=self._update_improvement_cycle_from_loop,
                record_stats_sample=self._record_selfplay_stats_sample,
            )
            manager.register(manifest_collection)

            # DataManagementLoop - automatic data pipeline management
            # December 27, 2025: Migrated from inline _data_management_loop
            def _check_disk_capacity_wrapper(threshold: float) -> tuple[bool, float]:
                """Wrapper for check_disk_has_capacity."""
                try:
                    return check_disk_has_capacity(threshold)
                except Exception as e:
                    logger.debug(f"Disk capacity check error: {e}")
                    return True, 0.0  # Assume OK on error

            async def _check_db_integrity_wrapper(data_dir: Path) -> dict[str, int]:
                """Wrapper for check_and_repair_databases."""
                try:
                    return check_and_repair_databases(
                        data_dir=data_dir,
                        auto_repair=False,
                        log_prefix="[P2P]"
                    )
                except Exception as e:
                    logger.debug(f"DB integrity check error: {e}")
                    return {"checked": 0, "corrupted": 0, "failed": 0}

            data_management = DataManagementLoop(
                is_leader=self._is_leader,
                check_disk_capacity=_check_disk_capacity_wrapper,
                cleanup_disk=self._cleanup_local_disk,
                convert_jsonl_to_db=self._convert_jsonl_to_db,
                convert_jsonl_to_npz=self._convert_jsonl_to_npz_for_training,
                check_db_integrity=_check_db_integrity_wrapper,
                trigger_export=self._trigger_export_for_loop,
                start_training=self._start_auto_training,
                get_data_dir=self.get_data_directory,
                get_games_dir=lambda: self.get_data_directory() / "games",
                get_training_dir=lambda: self.get_data_directory() / "training",
                is_gpu_node=lambda: self.self_info.is_gpu_node() if self.self_info else False,
                has_training_jobs=lambda: (self.self_info.training_jobs > 0) if self.self_info else False,
            )
            manager.register(data_management)

            # ModelSyncLoop - syncs NN/NNUE models across cluster
            # December 27, 2025: Migrated from inline _model_sync_loop
            # Note: The inline version is more complex (thread pool, batch sync, event emission)
            # This registration uses the simpler ModelSyncLoop pattern with wrapper callbacks
            if HAS_MODEL_SYNC and HAS_HOSTS_FOR_SYNC:
                async def _get_node_models_for_sync(node_id: str) -> dict[str, str]:
                    """Get model versions on a specific node."""
                    # Use manifest data if available
                    if hasattr(self, 'cluster_data_manifest') and self.cluster_data_manifest:
                        node_data = self.cluster_data_manifest.get(node_id, {})
                        models = node_data.get("models", {})
                        return {k: v.get("version", "") for k, v in models.items()} if isinstance(models, dict) else {}
                    return {}

                async def _sync_model_to_node(node_id: str, config_key: str, source_path: str) -> bool:
                    """Sync a model to a specific node."""
                    try:
                        # Use rsync for model distribution
                        peer = self.peers.get(node_id)
                        if not peer:
                            return False
                        host = peer.get("ip") or peer.get("host", "")
                        if not host:
                            return False
                        # Use sync_models infrastructure
                        cmd = [
                            "rsync", "-az", "--timeout=60",
                            source_path,
                            f"{host}:{source_path}",
                        ]
                        proc = await asyncio.create_subprocess_exec(
                            *cmd,
                            stdout=asyncio.subprocess.DEVNULL,
                            stderr=asyncio.subprocess.DEVNULL,
                        )
                        await proc.wait()
                        return proc.returncode == 0
                    except (asyncio.TimeoutError, OSError, subprocess.SubprocessError, ValueError):
                        return False

                model_sync = ModelSyncLoop(
                    get_model_versions=lambda: getattr(self, '_model_versions', {}),
                    get_node_models=_get_node_models_for_sync,
                    sync_model=_sync_model_to_node,
                    get_active_nodes=lambda: list(self.peers.keys()) if self.peers else [],
                )
                manager.register(model_sync)
            else:
                logger.debug("ModelSyncLoop: skipped (model sync not available)")

            # ModelFetchLoop - fetch trained models FROM training nodes TO coordinator
            # December 2025: Critical fix for model distribution gap
            # Training nodes (nebius-h100-*, lambda-gh200-*) produce models that need
            # to be fetched to the coordinator for evaluation/promotion/distribution.
            try:
                from scripts.p2p.loops.data_loops import ModelFetchLoop

                # Track which jobs have had models fetched
                _fetched_model_jobs: set[str] = set()

                def _get_completed_training_jobs() -> list:
                    """Get completed training jobs that need model fetch."""
                    with self.training_jobs_lock:
                        return [
                            job for job in self.training_jobs.values()
                            if getattr(job, "status", "") == "completed"
                        ]

                async def _fetch_model_for_job(job) -> bool:
                    """Fetch model from training node."""
                    try:
                        return await self.training_coordinator._fetch_model_from_training_node(job)
                    except (AttributeError, TypeError) as e:
                        logger.debug(f"ModelFetchLoop: fetch error: {e}")
                        return False

                def _mark_model_fetched(job_id: str) -> None:
                    """Mark a job's model as fetched."""
                    _fetched_model_jobs.add(job_id)

                def _is_model_fetched(job_id: str) -> bool:
                    """Check if model was already fetched."""
                    return job_id in _fetched_model_jobs

                model_fetch = ModelFetchLoop(
                    is_leader=self._is_leader,
                    get_completed_training_jobs=_get_completed_training_jobs,
                    fetch_model=_fetch_model_for_job,
                    mark_model_fetched=_mark_model_fetched,
                    is_model_fetched=_is_model_fetched,
                )
                manager.register(model_fetch)
                logger.info("ModelFetchLoop: registered for training node model fetching")
            except (ImportError, TypeError, ValueError, AttributeError) as e:
                logger.debug(f"ModelFetchLoop: skipped ({e})")

            # ValidationLoop - automatic model validation scheduling
            # December 27, 2025: Migrated from inline _validation_loop
            def _get_model_registry():
                try:
                    from app.training.model_registry import ModelRegistry
                    return ModelRegistry()
                except (ImportError, TypeError, ValueError) as e:
                    logger.debug(f"ValidationLoop: registry not available: {e}")
                    return None

            async def _send_validation_notification(msg: str, severity: str, context: dict) -> None:
                await self.notifier.send(msg, severity=severity, context=context)

            validation = ValidationLoop(
                is_leader=lambda: self.role == NodeRole.LEADER,
                get_model_registry=_get_model_registry,
                get_work_queue=get_work_queue,
                send_notification=_send_validation_notification,
            )
            manager.register(validation)

            # TailscalePeerDiscoveryLoop - December 27, 2025
            # Proactively discovers and connects to Tailscale nodes not in P2P network
            # Extracted from _tailscale_peer_recovery_loop (note: different from TailscaleRecoveryLoop
            # which handles service recovery; this handles peer discovery)
            def _get_current_peer_ids() -> set[str]:
                """Get node IDs of current P2P peers."""
                with self.peers_lock:
                    return {p.node_id for p in self.peers.values()}

            def _get_alive_peer_count() -> int:
                """Count peers that are currently alive."""
                with self.peers_lock:
                    return sum(1 for p in self.peers.values() if p.is_alive())

            async def _probe_and_connect_peer(ip: str, hostname: str) -> bool:
                """Probe peer health endpoint and send heartbeat to connect."""
                try:
                    url = f"http://{ip}:{DEFAULT_PORT}/health"
                    timeout = ClientTimeout(total=10)
                    async with get_client_session(timeout) as session:
                        async with session.get(url) as resp:
                            data, error = await safe_json_response(resp, default={}, log_errors=False)
                            if error:
                                logger.debug(f"TailscalePeerDiscovery: {hostname} response error: {error}")
                                return False
                            node_id = data.get("node_id", hostname)
                            logger.debug(f"TailscalePeerDiscovery: connected to {node_id}, sending heartbeat")
                            await self._send_heartbeat_to_peer(ip, DEFAULT_PORT)
                            return True
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"TailscalePeerDiscovery: failed to connect to {hostname}: {e}")
                return False

            try:
                from scripts.p2p.loops import TailscalePeerDiscoveryLoop
                ts_peer_discovery = TailscalePeerDiscoveryLoop(
                    is_leader=lambda: self.role == NodeRole.LEADER,
                    get_current_peers=_get_current_peer_ids,
                    get_alive_peer_count=_get_alive_peer_count,
                    probe_and_connect=_probe_and_connect_peer,
                )
                manager.register(ts_peer_discovery)
                logger.info("[LoopManager] TailscalePeerDiscoveryLoop registered")
            except (ImportError, TypeError) as e:
                logger.warning(f"[LoopManager] TailscalePeerDiscoveryLoop: not available: {e}")

            # PeerCleanupLoop - December 30, 2025
            # Automatically purges stale peers to maintain accurate health ratio
            # Prevents false "isolated" partition status from accumulated stale peers
            try:
                from scripts.p2p.loops import PeerCleanupLoop, PeerCleanupConfig

                async def _purge_single_peer_async(node_id: str) -> bool:
                    """Purge a single peer from the peers dict."""
                    with self.peers_lock:
                        if node_id in self.peers:
                            del self.peers[node_id]
                            logger.debug(f"[PeerCleanup] Removed peer from dict: {node_id}")
                            return True
                    return False

                peer_cleanup = PeerCleanupLoop(
                    get_all_peers=lambda: dict(self.peers),
                    purge_peer=_purge_single_peer_async,
                    # emit_event omitted - optional, uses default None
                    config=PeerCleanupConfig(
                        cleanup_interval_seconds=float(
                            os.environ.get("RINGRIFT_PEER_CLEANUP_INTERVAL", "300")
                        ),
                        enabled=os.environ.get(
                            "RINGRIFT_PEER_CLEANUP_ENABLED", "1"
                        ).lower() in ("1", "true", "yes", "on"),
                    ),
                )
                manager.register(peer_cleanup)
                logger.info("[LoopManager] PeerCleanupLoop registered")
            except (ImportError, TypeError, AttributeError) as e:
                logger.warning(f"[LoopManager] PeerCleanupLoop: not available: {e}")

            # WorkerPullLoop - December 27, 2025
            # Workers poll leader for work (pull model instead of push)
            # Extracted from _worker_pull_loop
            def _get_self_metrics_for_pull() -> dict[str, Any]:
                """Get self node metrics for idle detection."""
                return {
                    "gpu_percent": getattr(self.self_info, "gpu_percent", 0),
                    "cpu_percent": getattr(self.self_info, "cpu_percent", 0),
                    "training_jobs": getattr(self.self_info, "training_jobs", 0),
                    "has_gpu": getattr(self.self_info, "has_gpu", False),
                }

            try:
                from scripts.p2p.loops import WorkerPullLoop
                worker_pull = WorkerPullLoop(
                    is_leader=lambda: self.role == NodeRole.LEADER,
                    get_leader_id=lambda: self.leader_id,
                    get_self_metrics=_get_self_metrics_for_pull,
                    claim_work_from_leader=self._claim_work_from_leader,
                    execute_work=self._execute_claimed_work,
                    report_work_result=self._report_work_result,
                )
                manager.register(worker_pull)
                logger.info("[LoopManager] WorkerPullLoop registered successfully")
            except (ImportError, TypeError) as e:
                logger.error(f"[LoopManager] WorkerPullLoop registration FAILED: {e}")
                logger.error("[LoopManager] This node will NOT claim work from leader!")

            # FollowerDiscoveryLoop - December 27, 2025
            # Simple peer list discovery for followers
            def _get_known_peer_addresses() -> list[str]:
                """Get list of known peer addresses for discovery.

                Returns alive peers from runtime list, falling back to bootstrap
                seeds when no peers are known yet.
                """
                with self.peers_lock:
                    addresses = [
                        f"{p.host}:{p.port}"
                        for p in self.peers.values()
                        if p.is_alive() and p.host and p.port
                    ]

                # Fall back to bootstrap seeds if no alive peers known
                # This enables discovery when starting fresh
                if not addresses and self.known_peers:
                    addresses = list(self.known_peers)
                    logger.debug(f"No alive peers, using {len(addresses)} bootstrap seeds for discovery")

                return addresses

            async def _query_peer_list(peer_addr: str) -> list[str] | None:
                """Query a peer for its peer list."""
                try:
                    host, port_str = peer_addr.rsplit(":", 1)
                    port = int(port_str)
                    async with aiohttp.ClientSession() as session:
                        async with session.get(
                            f"http://{host}:{port}/peers",
                            timeout=aiohttp.ClientTimeout(total=5.0),
                        ) as resp:
                            if resp.status == 200:
                                data = await resp.json()
                                return data.get("peers", [])
                except (aiohttp.ClientError, ValueError, asyncio.TimeoutError):
                    pass
                return None

            def _add_discovered_peer(peer_addr: str) -> None:
                """Add a newly discovered peer address."""
                try:
                    host, port_str = peer_addr.rsplit(":", 1)
                    port = int(port_str)
                    # Queue for background probing
                    asyncio.create_task(
                        self._send_heartbeat_to_peer(host, port),
                        name=f"discover_peer_{peer_addr}",
                    )
                except ValueError:
                    logger.debug(f"Invalid peer address format: {peer_addr}")

            try:
                from scripts.p2p.loops import FollowerDiscoveryLoop
                follower_discovery = FollowerDiscoveryLoop(
                    get_known_peers=_get_known_peer_addresses,
                    query_peer_list=_query_peer_list,
                    add_peer=_add_discovered_peer,
                    is_leader=lambda: self.role == NodeRole.LEADER,
                )
                manager.register(follower_discovery)
            except (ImportError, TypeError) as e:
                logger.warning(f"[LoopManager] FollowerDiscoveryLoop: not available: {e}")

            # SelfHealingLoop - December 28, 2025
            # Migrated from inline _self_healing_loop (~71 LOC removed)
            # Recovers stuck jobs (leader) and cleans stale processes (all nodes)
            try:
                from scripts.p2p.loops import SelfHealingLoop
                self_healing = SelfHealingLoop(
                    is_leader=lambda: self.role == NodeRole.LEADER,
                    get_health_manager=get_health_manager,
                    get_work_queue=get_work_queue,
                    cleanup_stale_processes=self._cleanup_stale_processes,
                )
                manager.register(self_healing)
            except (ImportError, TypeError) as e:
                logger.debug(f"SelfHealingLoop: not available: {e}")

            # PredictiveMonitoringLoop - December 28, 2025
            # Migrated from inline _predictive_monitoring_loop (~98 LOC removed)
            # Proactive monitoring and alerting for cluster health
            def _get_peers_for_monitoring() -> list[Any]:
                """Get list of alive peers for monitoring."""
                with self.peers_lock:
                    return [p for p in self.peers.values() if p.is_alive()]

            def _get_production_models() -> tuple[list[str], float]:
                """Get production model info for alert checks."""
                model_ids = []
                last_training = time.time() - 3600  # Default to 1 hour ago
                try:
                    from app.training.model_registry import ModelRegistry, ModelStage
                    registry = ModelRegistry()
                    production_models = registry.get_versions_by_stage(ModelStage.PRODUCTION)
                    model_ids = [f"{m['model_id']}_v{m['version']}" for m in production_models]
                    if production_models:
                        from datetime import datetime
                        latest_update = max(
                            datetime.fromisoformat(m['updated_at'].replace('Z', '+00:00'))
                            for m in production_models
                            if m.get('updated_at')
                        )
                        last_training = latest_update.timestamp()
                except Exception as e:
                    logger.debug(f"Model registry lookup failed: {e}")
                return model_ids, last_training

            try:
                from scripts.p2p.loops import PredictiveMonitoringLoop
                predictive_monitoring = PredictiveMonitoringLoop(
                    is_leader=lambda: self.role == NodeRole.LEADER,
                    get_alert_manager=get_predictive_alerts,
                    get_work_queue=get_work_queue,
                    get_peers=_get_peers_for_monitoring,
                    get_notifier=lambda: self.notifier,
                    get_production_models=_get_production_models,
                )
                manager.register(predictive_monitoring)
            except (ImportError, TypeError) as e:
                logger.debug(f"PredictiveMonitoringLoop: not available: {e}")

            # TrainingSyncLoop - December 28, 2025
            # CRITICAL for training: Syncs selfplay data to training nodes
            # Only runs when this node is the cluster leader
            # Emits DATA_SYNC_STARTED/COMPLETED/FAILED events for pipeline coordination
            try:
                from scripts.p2p.loops import TrainingSyncLoop

                training_sync = TrainingSyncLoop(
                    is_leader=self._is_leader,
                    sync_to_training_nodes=self._sync_selfplay_to_training_nodes,
                    get_last_sync_time=lambda: getattr(self, 'last_training_sync_time', 0.0),
                    check_disk_capacity=lambda: check_disk_has_capacity(70.0),
                )
                manager.register(training_sync)
                logger.info("[P2P] TrainingSyncLoop registered (critical for training pipeline)")
            except (ImportError, TypeError) as e:
                logger.warning(f"TrainingSyncLoop: failed to register (CRITICAL): {e}")

            # DataAggregationLoop - December 28, 2025
            # IMPORTANT for training: Collects selfplay game databases from distributed nodes
            # Consolidates data to central storage for training pipeline
            try:
                from scripts.p2p.loops import DataAggregationLoop

                def _get_node_game_counts_for_aggregation() -> dict[str, int]:
                    """Get game counts per node from manifest."""
                    result: dict[str, int] = {}
                    if hasattr(self, 'cluster_data_manifest') and self.cluster_data_manifest:
                        for node_id, node_data in self.cluster_data_manifest.items():
                            games = node_data.get("games", {})
                            total_games = sum(
                                g.get("count", 0) for g in games.values()
                            ) if isinstance(games, dict) else 0
                            result[node_id] = total_games
                    return result

                async def _aggregate_from_node_for_loop(node_id: str) -> dict[str, Any]:
                    """Aggregate selfplay data from a specific node."""
                    try:
                        peer = self.peers.get(node_id)
                        if not peer:
                            return {"success": False, "error": "peer_not_found"}
                        # Use SyncPlanner if available
                        if hasattr(self, 'sync_planner') and self.sync_planner:
                            result = await self.sync_planner.sync_from_node(node_id)
                            return {"success": result, "node_id": node_id}
                        return {"success": False, "error": "sync_planner_unavailable"}
                    except Exception as e:
                        return {"success": False, "error": str(e)}

                data_aggregation = DataAggregationLoop(
                    get_node_game_counts=_get_node_game_counts_for_aggregation,
                    aggregate_from_node=_aggregate_from_node_for_loop,
                )
                manager.register(data_aggregation)
                logger.info("[P2P] DataAggregationLoop registered (important for training data)")
            except (ImportError, TypeError) as e:
                logger.warning(f"[LoopManager] DataAggregationLoop: not available (training sync affected): {e}")

            # HealthAggregationLoop - December 28, 2025
            # IMPORTANT for scheduling: Aggregates health metrics from all nodes
            # Provides cluster-wide health view for SelfplayScheduler decisions
            try:
                from scripts.p2p.loops import HealthAggregationLoop

                def _get_node_ids_for_health() -> list[str]:
                    """Get list of alive peer node IDs."""
                    with self.peers_lock:
                        return [p.node_id for p in self.peers.values() if p.is_alive()]

                async def _fetch_node_health_for_loop(node_id: str) -> dict[str, Any]:
                    """Fetch health metrics from a specific node."""
                    try:
                        peer = self.peers.get(node_id)
                        if not peer:
                            return {"healthy": False, "error": "peer_not_found"}
                        host = peer.host or peer.ip
                        port = peer.port or DEFAULT_PORT
                        url = f"http://{host}:{port}/health"
                        timeout = ClientTimeout(total=10)
                        async with get_client_session(timeout) as session:
                            async with session.get(url) as resp:
                                if resp.status == 200:
                                    data = await resp.json()
                                    return {
                                        "healthy": True,
                                        "cpu_percent": data.get("cpu_percent", 0),
                                        "memory_percent": data.get("memory_percent", 0),
                                        "gpu_percent": data.get("gpu_percent", 0),
                                        "disk_percent": data.get("disk_percent", 0),
                                    }
                                return {"healthy": False, "error": f"status_{resp.status}"}
                    except Exception as e:
                        return {"healthy": False, "error": str(e)}

                def _on_health_updated_callback(health_data: dict[str, dict[str, Any]]) -> None:
                    """Store aggregated health data for scheduling decisions."""
                    self._aggregated_node_health = health_data

                health_aggregation = HealthAggregationLoop(
                    get_node_ids=_get_node_ids_for_health,
                    fetch_node_health=_fetch_node_health_for_loop,
                    on_health_updated=_on_health_updated_callback,
                )
                manager.register(health_aggregation)
                logger.info("[P2P] HealthAggregationLoop registered (important for scheduling)")
            except (ImportError, TypeError) as e:
                logger.warning(f"[LoopManager] HealthAggregationLoop: not available: {e}")

            # IpDiscoveryLoop - December 28, 2025
            # Discovers and updates node IP addresses for dynamic cloud instances
            try:
                from scripts.p2p.loops import IpDiscoveryLoop

                def _get_nodes_for_ip_discovery() -> dict[str, dict[str, Any]]:
                    """Get node info dict for IP discovery."""
                    with self.peers_lock:
                        return {
                            p.node_id: {
                                "ip": p.host or p.ip,
                                "tailscale_ip": getattr(p, "tailscale_ip", None),
                                "public_ip": getattr(p, "public_ip", None),
                                "hostname": getattr(p, "hostname", None),
                            }
                            for p in self.peers.values()
                        }

                async def _update_node_ip_for_discovery(node_id: str, new_ip: str) -> None:
                    """Update a node's IP address."""
                    with self.peers_lock:
                        if node_id in self.peers:
                            peer = self.peers[node_id]
                            peer.host = new_ip
                            peer.ip = new_ip
                            logger.info(f"[IpDiscovery] Updated {node_id} IP to {new_ip}")

                ip_discovery = IpDiscoveryLoop(
                    get_nodes=_get_nodes_for_ip_discovery,
                    update_node_ip=_update_node_ip_for_discovery,
                )
                manager.register(ip_discovery)
                logger.info("[P2P] IpDiscoveryLoop registered (handles dynamic IPs)")
            except (ImportError, TypeError) as e:
                logger.debug(f"IpDiscoveryLoop: not available: {e}")

            # TailscaleRecoveryLoop - December 28, 2025
            # Monitors and recovers Tailscale connections
            try:
                from scripts.p2p.loops import TailscaleRecoveryLoop

                def _get_tailscale_status_for_recovery() -> dict[str, Any]:
                    """Get Tailscale status for all nodes."""
                    result = {}
                    with self.peers_lock:
                        for p in self.peers.values():
                            result[p.node_id] = {
                                "tailscale_state": getattr(p, "tailscale_state", "unknown"),
                                "tailscale_online": getattr(p, "tailscale_online", True),
                            }
                    return result

                async def _run_ssh_for_tailscale_recovery(node_id: str, cmd: str) -> Any:
                    """Run SSH command for Tailscale recovery."""
                    try:
                        peer = self.peers.get(node_id)
                        if not peer:
                            return type("Result", (), {"returncode": 1, "stdout": "", "stderr": "peer not found"})()
                        host = peer.host or peer.ip
                        proc = await asyncio.create_subprocess_exec(
                            "ssh", "-o", "ConnectTimeout=10", "-o", "StrictHostKeyChecking=no",
                            f"root@{host}", cmd,
                            stdout=asyncio.subprocess.PIPE,
                            stderr=asyncio.subprocess.PIPE,
                        )
                        stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=30)
                        return type("Result", (), {"returncode": proc.returncode, "stdout": stdout.decode(), "stderr": stderr.decode()})()
                    except (asyncio.TimeoutError, OSError) as e:
                        return type("Result", (), {"returncode": 1, "stdout": "", "stderr": str(e)})()

                tailscale_recovery = TailscaleRecoveryLoop(
                    get_tailscale_status=_get_tailscale_status_for_recovery,
                    run_ssh_command=_run_ssh_for_tailscale_recovery,
                )
                manager.register(tailscale_recovery)
                logger.info("[P2P] TailscaleRecoveryLoop registered (handles Tailscale failures)")
            except (ImportError, TypeError) as e:
                logger.debug(f"TailscaleRecoveryLoop: not available: {e}")

            # UdpDiscoveryLoop - December 28, 2025
            # LAN peer discovery via UDP broadcast (useful for network partition recovery)
            try:
                from scripts.p2p.loops import UdpDiscoveryLoop

                def _get_known_peer_addrs_for_udp() -> list[str]:
                    """Get list of known peer addresses."""
                    with self.peers_lock:
                        return [
                            f"{p.host or p.ip}:{p.port or DEFAULT_PORT}"
                            for p in self.peers.values()
                            if p.host or p.ip
                        ]

                def _add_peer_from_udp_discovery(peer_addr: str) -> None:
                    """Add a peer discovered via UDP."""
                    try:
                        host, port_str = peer_addr.rsplit(":", 1)
                        port = int(port_str)
                        asyncio.create_task(
                            self._send_heartbeat_to_peer(host, port),
                            name=f"udp_discover_{peer_addr}",
                        )
                    except ValueError:
                        logger.debug(f"[UdpDiscovery] Invalid peer address: {peer_addr}")

                udp_discovery = UdpDiscoveryLoop(
                    get_node_id=lambda: self.node_id,
                    get_host=lambda: self.host or "0.0.0.0",
                    get_port=lambda: self.port,
                    get_known_peers=_get_known_peer_addrs_for_udp,
                    add_peer=_add_peer_from_udp_discovery,
                )
                manager.register(udp_discovery)
                logger.info("[P2P] UdpDiscoveryLoop registered (LAN peer discovery)")
            except (ImportError, TypeError) as e:
                logger.debug(f"UdpDiscoveryLoop: not available: {e}")

            # SplitBrainDetectionLoop - December 28, 2025
            # Detects multiple leaders (network partition) and emits alerts
            try:
                from scripts.p2p.loops import SplitBrainDetectionLoop

                def _get_peer_endpoint_for_split_brain(peer_id: str) -> str | None:
                    """Get HTTP endpoint for a peer."""
                    peer = self.peers.get(peer_id)
                    if not peer:
                        return None
                    host = peer.host or peer.ip
                    port = peer.port or DEFAULT_PORT
                    return f"http://{host}:{port}"

                async def _on_split_brain_detected_callback(leaders: list[str], epoch: int) -> None:
                    """Handle split-brain detection."""
                    logger.critical(
                        f"[SplitBrain] DETECTED: {len(leaders)} leaders in cluster: {leaders} "
                        f"(epoch={epoch})"
                    )
                    await self._emit_split_brain_detected(
                        detected_leaders=leaders,
                        resolution_action="election",
                    )

                split_brain_detection = SplitBrainDetectionLoop(
                    get_peers=lambda: dict(self.peers) if self.peers else {},
                    get_peer_endpoint=_get_peer_endpoint_for_split_brain,
                    get_own_leader_id=lambda: self.leader_id,
                    get_cluster_epoch=lambda: getattr(self, "cluster_epoch", 0),
                    on_split_brain_detected=_on_split_brain_detected_callback,
                )
                manager.register(split_brain_detection)
                logger.info("[P2P] SplitBrainDetectionLoop registered (partition detection)")
            except (ImportError, TypeError) as e:
                logger.debug(f"SplitBrainDetectionLoop: not available: {e}")

            self._loops_registered = True
            logger.info(f"LoopManager: registered {len(manager.loop_names)} loops")
            return True

        except Exception as e:  # noqa: BLE001
            logger.error(f"LoopManager: failed to register loops: {e}")
            return False

    # =========================================================================
    # JobReaperLoop callbacks - December 27, 2025
    # =========================================================================

    def _get_all_active_jobs_for_reaper(self) -> dict[str, Any]:
        """Get all active jobs across all job types for the job reaper.

        Returns a flat dict of job_id -> job_info, where job_info includes:
        - started_at: timestamp when job started
        - claimed_at: timestamp when job was claimed (if applicable)
        - status: current job status
        - pid: process ID (for killing stuck processes)
        - node_id: which node is running the job
        """
        result: dict[str, Any] = {}
        with self.jobs_lock:
            for job_type, jobs in self.active_jobs.items():
                for job_id, job_info in jobs.items():
                    if isinstance(job_info, dict):
                        result[job_id] = {
                            **job_info,
                            "job_type": job_type,
                        }
                    else:
                        # Handle non-dict job objects (legacy)
                        result[job_id] = {
                            "job_id": job_id,
                            "job_type": job_type,
                            "status": getattr(job_info, "status", "unknown"),
                            "started_at": getattr(job_info, "started_at", 0),
                            "pid": getattr(job_info, "pid", None),
                        }
        return result

    async def _cancel_job_for_reaper(self, job_id: str) -> bool:
        """Cancel a job by ID for the job reaper.

        Attempts to:
        1. Kill the process if PID is known
        2. Update job status to 'cancelled'
        3. Remove from active jobs dict
        4. Emit TASK_ABANDONED event

        Returns True if job was successfully cancelled.
        """
        import os
        import signal

        with self.jobs_lock:
            # Find the job across all job types
            for job_type, jobs in self.active_jobs.items():
                if job_id in jobs:
                    job_info = jobs[job_id]
                    pid = job_info.get("pid") if isinstance(job_info, dict) else getattr(job_info, "pid", None)

                    # Kill the process if we have a PID
                    if pid:
                        try:
                            os.kill(pid, signal.SIGTERM)
                            logger.info(f"[JobReaper] Sent SIGTERM to pid {pid} for job {job_id}")
                        except ProcessLookupError:
                            logger.debug(f"[JobReaper] Process {pid} already dead for job {job_id}")
                        except OSError as e:
                            logger.warning(f"[JobReaper] Failed to kill pid {pid}: {e}")

                    # Update status and remove from active jobs
                    if isinstance(job_info, dict):
                        job_info["status"] = "reaped"
                    del jobs[job_id]

                    # Emit event for coordination (fire-and-forget async task)
                    try:
                        asyncio.create_task(self._emit_task_abandoned(
                            task_id=job_id,
                            task_type=job_type,
                            reason="reaped_by_job_reaper",
                            node_id=job_info.get("node_id", "") if isinstance(job_info, dict) else "",
                        ))
                    except RuntimeError:
                        pass  # No event loop running

                    logger.info(f"[JobReaper] Cancelled job {job_id} (type: {job_type})")
                    return True

        logger.debug(f"[JobReaper] Job {job_id} not found in active jobs")
        return False

    def _get_job_heartbeats_for_reaper(self) -> dict[str, float]:
        """Get job heartbeat timestamps for the job reaper.

        Returns dict of job_id -> last_heartbeat_time.
        Jobs without recent heartbeats may be considered abandoned.

        Phase 15.1.9 (Dec 29, 2025): Updated to use JobManager.get_job_heartbeats()
        for actual heartbeat tracking instead of just job start times.
        """
        result: dict[str, float] = {}

        # Phase 15.1.9: Get actual heartbeats from JobManager
        if hasattr(self, "job_manager") and self.job_manager is not None:
            try:
                job_heartbeats = self.job_manager.get_job_heartbeats()
                result.update(job_heartbeats)
            except Exception as e:  # noqa: BLE001
                logger.debug(f"Failed to get heartbeats from JobManager: {e}")

        # Fallback: Also include jobs_started_at for jobs without heartbeat tracking
        # This ensures older jobs (started before heartbeat tracking) are still monitored
        if hasattr(self, "jobs_started_at"):
            for _node_id, jobs in self.jobs_started_at.items():
                for job_id, start_time in jobs.items():
                    # Only add if not already in result from heartbeat tracking
                    if job_id not in result:
                        result[job_id] = start_time

        return result

    # =========================================================================
    # ManifestCollectionLoop callbacks - December 27, 2025
    # =========================================================================

    def _update_manifest_from_loop(self, manifest: Any, is_cluster: bool) -> None:
        """Update stored manifest from ManifestCollectionLoop.

        Args:
            manifest: The collected manifest (cluster or local)
            is_cluster: True if this is a cluster-wide manifest, False for local
        """
        import time
        with self.manifest_lock:
            if is_cluster:
                self.cluster_data_manifest = manifest
            else:
                self.local_data_manifest = manifest
            self.last_manifest_collection = time.time()

    def _update_improvement_cycle_from_loop(self, by_board_type: dict[str, Any]) -> None:
        """Update ImprovementCycleManager from ManifestCollectionLoop.

        Args:
            by_board_type: Dict of board_type -> game counts from manifest
        """
        if self.improvement_cycle_manager:
            try:
                self.improvement_cycle_manager.update_from_cluster_totals(by_board_type)
            except Exception as e:  # noqa: BLE001
                logger.debug(f"ImprovementCycleManager update error: {e}")

    # =========================================================================
    # DataManagementLoop callbacks - December 27, 2025
    # =========================================================================

    async def _trigger_export_for_loop(
        self,
        db_path: Path,
        output_path: Path,
        board_type: str,
    ) -> bool:
        """Trigger export job for DataManagementLoop.

        Args:
            db_path: Path to database file to export
            output_path: Path for output NPZ file
            board_type: Board type (square8, hex8, etc.)

        Returns:
            True if export started successfully
        """
        import subprocess

        try:
            cmd = [
                sys.executable,
                f"{self.ringrift_path}/ai-service/scripts/export_replay_dataset.py",
                "--db", str(db_path),
                "--board-type", board_type,
                "--num-players", "2",
                "--board-aware-encoding",
                "--require-completed",
                "--min-moves", "10",
                "--output", str(output_path),
            ]

            env = os.environ.copy()
            env["PYTHONPATH"] = f"{self.ringrift_path}/ai-service"

            log_file = Path(f"/tmp/auto_export_{db_path.stem}.log")
            subprocess.Popen(
                cmd,
                stdout=open(log_file, "w"),
                stderr=subprocess.STDOUT,
                env=env,
                cwd=f"{self.ringrift_path}/ai-service",
            )
            logger.info(f"[DataManagement] Started export job for {db_path.name}")
            return True

        except Exception as e:
            logger.error(f"[DataManagement] Failed to start export for {db_path.name}: {e}")
            return False

    async def _inline_job_reaper_fallback_loop(self) -> None:
        """Inline job reaper fallback loop.

        December 27, 2025: Fallback implementation that runs if the extracted
        JobReaperLoop fails to start or hits persistent errors. Uses the same
        callbacks and thresholds as the extracted loop.

        This is NOT a replacement for JobReaperLoop - it's a safety net that
        ensures job cleanup continues even if the modular loop system fails.

        Thresholds:
        - STALE: Jobs older than 1 hour without heartbeat
        - STUCK: Jobs older than 2 hours regardless of heartbeat
        - INTERVAL: Checks every 5 minutes

        Environment:
        - RINGRIFT_JOB_REAPER_FALLBACK_ENABLED: Enable/disable (default: true)
        """
        STALE_THRESHOLD_SECONDS = 3600.0   # 1 hour
        STUCK_THRESHOLD_SECONDS = 7200.0   # 2 hours
        CHECK_INTERVAL_SECONDS = 300.0      # 5 minutes
        MAX_JOBS_PER_CYCLE = 10             # Limit to avoid overload

        logger.info("[JobReaper Fallback] Started inline fallback loop")
        stats = {"checks": 0, "reaped": 0, "errors": 0}

        while self.running:
            try:
                await asyncio.sleep(CHECK_INTERVAL_SECONDS)
                if not self.running:
                    break

                stats["checks"] += 1
                now = time.time()
                reaped_this_cycle = 0

                # Get all active jobs
                try:
                    active_jobs = self._get_all_active_jobs_for_reaper()
                except Exception as e:
                    logger.warning(f"[JobReaper Fallback] Failed to get active jobs: {e}")
                    stats["errors"] += 1
                    continue

                if not active_jobs:
                    continue

                # Get heartbeat info
                try:
                    heartbeats = self._get_job_heartbeats_for_reaper()
                except Exception as e:
                    logger.debug(f"[JobReaper Fallback] Failed to get heartbeats: {e}")
                    heartbeats = {}

                # Identify stale and stuck jobs
                jobs_to_reap: list[tuple[str, str]] = []  # [(job_id, reason), ...]

                for job_id, job_info in active_jobs.items():
                    if reaped_this_cycle >= MAX_JOBS_PER_CYCLE:
                        break

                    started_at = job_info.get("started_at", 0)
                    if not started_at:
                        continue

                    job_age = now - started_at
                    last_heartbeat = heartbeats.get(job_id, started_at)
                    heartbeat_age = now - last_heartbeat

                    # Check for stuck jobs (absolute age threshold)
                    if job_age > STUCK_THRESHOLD_SECONDS:
                        jobs_to_reap.append((job_id, "stuck"))
                        reaped_this_cycle += 1
                        continue

                    # Check for stale jobs (no recent heartbeat)
                    if heartbeat_age > STALE_THRESHOLD_SECONDS:
                        jobs_to_reap.append((job_id, "stale"))
                        reaped_this_cycle += 1

                # Reap identified jobs
                for job_id, reason in jobs_to_reap:
                    try:
                        success = await self._cancel_job_for_reaper(job_id)
                        if success:
                            stats["reaped"] += 1
                            logger.info(
                                f"[JobReaper Fallback] Reaped {reason} job {job_id} "
                                f"(total: {stats['reaped']})"
                            )
                    except Exception as e:
                        logger.warning(f"[JobReaper Fallback] Failed to reap {job_id}: {e}")
                        stats["errors"] += 1

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"[JobReaper Fallback] Unexpected error: {e}")
                stats["errors"] += 1
                await asyncio.sleep(60)  # Back off on error

        logger.info(
            f"[JobReaper Fallback] Stopped after {stats['checks']} checks, "
            f"{stats['reaped']} reaped, {stats['errors']} errors"
        )

    def _get_sync_router(self) -> SyncRouter | None:
        """Lazy-load SyncRouter singleton for intelligent sync routing."""
        if not HAS_SYNC_ROUTER:
            return None
        if self._sync_router is None:
            try:
                self._sync_router = get_sync_router()
                logger.info("SyncRouter: initialized for intelligent data routing")
            except Exception as e:  # noqa: BLE001
                logger.warning(f"SyncRouter: failed to initialize: {e}")
                return None
        return self._sync_router

    def _wire_sync_router_events(self) -> bool:
        """Wire SyncRouter to event system for real-time sync triggers."""
        if self._sync_router_wired:
            return True
        router = self._get_sync_router()
        if router is None:
            return False
        try:
            if hasattr(router, 'wire_to_event_router'):
                router.wire_to_event_router()
                self._sync_router_wired = True
                logger.info("SyncRouter: wired to event system")
                return True
        except Exception as e:  # noqa: BLE001
            logger.warning(f"SyncRouter: failed to wire events: {e}")
        return False

    def _wire_feedback_loops(self) -> bool:
        """Wire curriculum feedback loops for self-improvement.

        December 2025: Connects P2P orchestrator to the training feedback system:
        - Curriculum weights adjust based on Elo velocity
        - Weak configs get boosted/penalized based on evaluation results
        - Quality scores influence exploration temperature
        - Failed promotions reduce config priority

        Returns True if wiring succeeded, False otherwise.
        """
        try:
            from app.coordination.curriculum_integration import wire_all_feedback_loops

            status = wire_all_feedback_loops()
            if status.get("success", False):
                wired_count = status.get("wired_count", 0)
                logger.info(f"Feedback loops: wired {wired_count} bridges successfully")
                return True
            else:
                error = status.get("error", "Unknown error")
                logger.warning(f"Feedback loops: partial wiring - {error}")
                return False
        except ImportError as e:
            logger.debug(f"Feedback loops: curriculum_integration not available: {e}")
            return False
        except Exception as e:  # noqa: BLE001
            logger.warning(f"Feedback loops: failed to wire: {e}")
            return False

    def _validate_manager_health(self) -> dict[str, Any]:
        """Validate health of all P2P managers at startup.

        December 28, 2025: Checks that all 7 managers initialized correctly and
        are healthy. This catches initialization issues early rather than
        at first use, improving debuggability.

        Managers validated:
        - state_manager: SQLite persistence and cluster epoch tracking
        - node_selector: Node ranking for job dispatch
        - sync_planner: Manifest collection and sync planning
        - selfplay_scheduler: Priority-based config selection
        - job_manager: Job spawning and lifecycle management
        - training_coordinator: Training dispatch and model promotion
        - loop_manager: Background loop orchestration (Dec 2025)

        Returns:
            dict with manager health status and overall healthy flag
        """
        managers = [
            ("state_manager", self.state_manager),
            ("node_selector", self.node_selector),
            ("sync_planner", self.sync_planner),
            ("selfplay_scheduler", self.selfplay_scheduler),
            ("job_manager", self.job_manager),
            ("training_coordinator", self.training_coordinator),
            # December 2025: Include LoopManager in health validation
            ("loop_manager", self._get_loop_manager()),
        ]

        status = {
            "managers": {},
            "all_healthy": True,
            "unhealthy_count": 0,
            "timestamp": time.time(),
        }

        for name, manager in managers:
            try:
                if manager is None:
                    status["managers"][name] = {"status": "not_initialized", "error": "Manager is None"}
                    status["all_healthy"] = False
                    status["unhealthy_count"] += 1
                elif hasattr(manager, "health_check"):
                    health = manager.health_check()
                    # Handle both dict and HealthCheckResult return types
                    if hasattr(health, "status"):
                        is_healthy = str(health.status).lower() in ("healthy", "ready")
                        health_status = str(health.status)
                    else:
                        is_healthy = health.get("status") == "healthy"
                        health_status = health.get("status", "unknown")
                    status["managers"][name] = {
                        "status": health_status,
                        "operations": health.get("operations_count", 0) if isinstance(health, dict) else 0,
                        "errors": health.get("errors_count", 0) if isinstance(health, dict) else 0,
                    }
                    if not is_healthy:
                        status["all_healthy"] = False
                        status["unhealthy_count"] += 1
                else:
                    # Manager initialized but no health_check method
                    status["managers"][name] = {"status": "initialized", "health_check": "not_available"}
            except Exception as e:  # noqa: BLE001
                status["managers"][name] = {"status": "error", "error": str(e)}
                status["all_healthy"] = False
                status["unhealthy_count"] += 1

        # Log results
        manager_count = len(managers)
        if status["all_healthy"]:
            logger.info(f"[P2P] Manager health: all {manager_count} managers healthy ")
        else:
            unhealthy = [n for n, s in status["managers"].items() if s.get("status") not in ("healthy", "initialized", "ready")]
            logger.warning(f"[P2P] Manager health: {len(unhealthy)}/{manager_count} unhealthy: {unhealthy}")

        return status

    def health_check(self) -> "HealthCheckResult":
        """Return health check result for daemon protocol compliance.

        December 27, 2025: Added for DaemonManager integration. Returns a
        HealthCheckResult that can be used by the daemon infrastructure for
        health monitoring, auto-restart decisions, and liveness probes.

        Returns:
            HealthCheckResult with overall orchestrator health status
        """
        # Import from contracts (zero-dependency module)
        from app.coordination.contracts import CoordinatorStatus, HealthCheckResult

        # Get manager health status
        manager_health = self._validate_manager_health()

        # Calculate cluster metrics
        uptime_seconds = time.time() - getattr(self, "start_time", time.time())
        active_peers = sum(
            1 for p in self.peers.values()
            if time.time() - p.last_heartbeat < 120
        )

        details = {
            "node_id": self.node_id,
            "role": self.role.value if hasattr(self.role, "value") else str(self.role),
            "leader_id": self.leader_id,
            "active_peers": active_peers,
            "total_peers": len(self.peers),
            "uptime_seconds": uptime_seconds,
            "managers_healthy": manager_health.get("all_healthy", False),
            "unhealthy_managers": manager_health.get("unhealthy_count", 0),
            "selfplay_jobs": self.self_info.selfplay_jobs if hasattr(self, "self_info") else 0,
            "training_jobs": self.self_info.training_jobs if hasattr(self, "self_info") else 0,
        }

        # Determine overall health
        is_healthy = manager_health.get("all_healthy", False)

        # Additional health checks
        if uptime_seconds < 10:
            # Grace period for startup
            is_healthy = True
            message = "P2P Orchestrator starting up"
            status = CoordinatorStatus.RUNNING
        elif not is_healthy:
            message = f"P2P Orchestrator unhealthy: {manager_health.get('unhealthy_count', 0)} unhealthy managers"
            status = CoordinatorStatus.ERROR
        else:
            message = f"P2P Orchestrator healthy, {active_peers} peers active"
            status = CoordinatorStatus.RUNNING

        return HealthCheckResult(
            healthy=is_healthy,
            status=status,
            message=message,
            details=details,
        )

    async def _subscribe_with_retry(
        self,
        event_name: str,
        handler: Any,
        max_attempts: int = 3,
        is_critical: bool = False,
    ) -> bool:
        """Subscribe to event with exponential backoff retry.

        December 2025 (Wave 7 Phase 1.1): Implements reliable event subscription
        with automatic retry on failure. This prevents pipeline stalls caused by
        transient subscription failures at startup.

        Args:
            event_name: Name of the event to subscribe to
            handler: Handler function to call when event fires
            max_attempts: Maximum subscription attempts (default: 3)
            is_critical: If True, failure is logged at ERROR level

        Returns:
            True if subscription succeeded, False otherwise
        """
        from app.coordination.event_router import subscribe

        for attempt in range(max_attempts):
            try:
                subscribe(event_name, handler)
                logger.info(f"[P2P] Subscribed to {event_name}")
                return True
            except Exception as e:  # noqa: BLE001
                delay = 2 ** attempt  # 1s, 2s, 4s exponential backoff
                level = logger.error if is_critical else logger.warning
                level(
                    f"[P2P] Subscription to {event_name} failed "
                    f"(attempt {attempt + 1}/{max_attempts}): {e}"
                )
                if attempt < max_attempts - 1:
                    await asyncio.sleep(delay)

        if is_critical:
            logger.critical(
                f"[P2P] CRITICAL: Failed to subscribe to {event_name} "
                f"after {max_attempts} attempts"
            )
        return False

    def _subscribe_single(
        self,
        event_name: str,
        handler: Any,
        is_critical: bool = False,
    ) -> bool:
        """Synchronous single subscription attempt.

        Used for initial subscription setup. Returns True on success.
        For retry logic, use _subscribe_with_retry in async context.
        """
        try:
            from app.coordination.event_router import subscribe
            subscribe(event_name, handler)
            return True
        except Exception as e:  # noqa: BLE001
            level = logger.error if is_critical else logger.debug
            level(f"[P2P] Failed to subscribe to {event_name}: {e}")
            return False

    def _subscribe_to_daemon_events(self) -> bool:
        """Subscribe to daemon status events for observability.

        December 2025: Receives DAEMON_STATUS_CHANGED events from daemon_manager
        to track daemon health across the cluster. This enables:
        - Tracking which daemons are running/crashed on each node
        - Auto-recovery of critical daemons
        - Cluster-wide daemon health reporting via /status endpoint

        Returns True if subscription succeeded, False otherwise.
        """
        try:
            from app.coordination.event_router import subscribe

            def handle_daemon_status(event) -> None:
                """Handle daemon status change events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    daemon_name = payload.get("daemon_name", "unknown")
                    new_status = payload.get("new_status", "unknown")
                    hostname = payload.get("hostname", "unknown")
                    error = payload.get("error")

                    # Track daemon states for cluster health reporting
                    if not hasattr(self, "_daemon_states"):
                        self._daemon_states = {}
                    self._daemon_states[f"{hostname}:{daemon_name}"] = {
                        "status": new_status,
                        "last_update": time.time(),
                        "error": error,
                    }

                    # Log critical daemon failures
                    if new_status in ("crashed", "failed") and error:
                        logger.warning(
                            f"Daemon {daemon_name} on {hostname} {new_status}: {error}"
                        )
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling daemon status event: {e}")

            subscribe("DAEMON_STATUS_CHANGED", handle_daemon_status)
            logger.info("Subscribed to daemon status events")
            return True
        except ImportError as e:
            logger.debug(f"Daemon events: event_router not available: {e}")
            return False
        except Exception as e:  # noqa: BLE001
            logger.warning(f"Daemon events: failed to subscribe: {e}")
            return False

    def _subscribe_to_feedback_signals(self) -> bool:
        """Subscribe to training feedback signals for dynamic orchestration.

        December 2025: Subscribes to key feedback events that should influence
        cluster orchestration decisions:
        - ELO_VELOCITY_CHANGED: Adjust selfplay allocation based on training velocity
        - QUALITY_DEGRADED: Pause/slow selfplay when data quality drops
        - EVALUATION_COMPLETED: Trigger model promotion decisions
        - PROMOTION_FAILED: Revert curriculum weights if promotion fails
        - PLATEAU_DETECTED: Trigger hyperparameter search or curriculum changes

        Returns True if subscription succeeded, False otherwise.
        """
        try:
            from app.coordination.event_router import subscribe

            def handle_quality_degraded(event) -> None:
                """Handle quality degradation events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    config_key = payload.get("config_key", "unknown")
                    quality_score = payload.get("quality_score", 0)
                    threshold = payload.get("threshold", 0)

                    logger.warning(
                        f"Quality degraded for {config_key}: {quality_score:.2f} < {threshold:.2f}"
                    )
                    # Could pause selfplay for this config or trigger data cleanup
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling quality degraded event: {e}")

            def handle_elo_velocity_changed(event) -> None:
                """Handle Elo velocity change events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    config_key = payload.get("config_key", "unknown")
                    velocity = payload.get("velocity", 0)

                    if velocity < -50:  # Significant regression
                        logger.warning(f"Elo regression for {config_key}: velocity={velocity}")
                    elif velocity > 50:  # Good progress
                        logger.info(f"Elo progress for {config_key}: velocity={velocity}")
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling Elo velocity event: {e}")

            def handle_evaluation_completed(event) -> None:
                """Handle evaluation completion events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    config_key = payload.get("config_key", "unknown")
                    win_rate = payload.get("win_rate", 0)
                    opponent = payload.get("opponent", "unknown")

                    logger.info(
                        f"Evaluation completed for {config_key}: {win_rate:.1%} vs {opponent}"
                    )
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling evaluation completed event: {e}")

            def handle_plateau_detected(event) -> None:
                """Handle training plateau detection events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    config_key = payload.get("config_key", "unknown")
                    epochs_stalled = payload.get("epochs_stalled", 0)

                    logger.warning(
                        f"Training plateau for {config_key}: stalled {epochs_stalled} epochs"
                    )
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling plateau detected event: {e}")

            def handle_exploration_boost(event) -> None:
                """Handle exploration boost events from training feedback.

                P0.1 (Dec 2025): Added missing handler for EXPLORATION_BOOST events.
                When training anomalies (loss spikes, stalls) are detected, this
                signals that we should boost exploration in selfplay to generate
                more diverse training data.
                """
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    config_key = payload.get("config_key", payload.get("config", "unknown"))
                    boost_factor = payload.get("boost_factor", payload.get("boost", 1.0))
                    reason = payload.get("reason", "training_anomaly")
                    duration = payload.get("duration_seconds", 900)

                    logger.info(
                        f"Exploration boost for {config_key}: {boost_factor:.2f}x "
                        f"(reason={reason}, duration={duration}s)"
                    )

                    # Forward to selfplay scheduler if available
                    if hasattr(self, "selfplay_scheduler") and self.selfplay_scheduler:
                        self.selfplay_scheduler.set_exploration_boost(
                            config_key, boost_factor, duration
                        )
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling exploration boost event: {e}")

            def handle_promotion_failed(event) -> None:
                """Handle model promotion failure events.

                December 27, 2025: Added missing handler for PROMOTION_FAILED events.
                When model promotion fails (gauntlet failure, threshold not met),
                we should revert curriculum weights and potentially pause training
                for that config until issues are resolved.
                """
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    config_key = payload.get("config_key", "unknown")
                    model_path = payload.get("model_path", "unknown")
                    reason = payload.get("reason", "unknown")
                    win_rate = payload.get("win_rate", 0.0)

                    logger.warning(
                        f"[P2P] Promotion FAILED for {config_key}: {reason} "
                        f"(model={model_path}, win_rate={win_rate:.1%})"
                    )

                    # Revert curriculum weights if selfplay scheduler available
                    if hasattr(self, "selfplay_scheduler") and self.selfplay_scheduler:
                        # Reduce priority for this config temporarily
                        self.selfplay_scheduler.record_promotion_failure(config_key)
                        logger.info(f"[P2P] Reduced selfplay priority for {config_key} after promotion failure")

                    # Track failed promotions for monitoring
                    if not hasattr(self, "_promotion_failures"):
                        self._promotion_failures = {}
                    if config_key not in self._promotion_failures:
                        self._promotion_failures[config_key] = []
                    self._promotion_failures[config_key].append({
                        "timestamp": time.time(),
                        "reason": reason,
                        "win_rate": win_rate,
                    })
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling promotion failed event: {e}")

            def handle_handler_failed(event) -> None:
                """Handle event handler failure events.

                December 27, 2025: Added missing handler for HANDLER_FAILED events.
                When a coordination event handler throws an exception, this event
                is emitted. We need to track these for monitoring and potentially
                trigger alerts for critical handler failures.
                """
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    handler_name = payload.get("handler_name", "unknown")
                    event_type = payload.get("event_type", "unknown")
                    error = payload.get("error", "unknown")
                    coordinator = payload.get("coordinator", "unknown")

                    logger.error(
                        f"[P2P] Handler FAILED: {handler_name} for {event_type} "
                        f"in {coordinator}: {error}"
                    )

                    # Track handler failures for health monitoring
                    if not hasattr(self, "_handler_failures"):
                        self._handler_failures = {}
                    failure_key = f"{coordinator}.{handler_name}"
                    if failure_key not in self._handler_failures:
                        self._handler_failures[failure_key] = []
                    self._handler_failures[failure_key].append({
                        "timestamp": time.time(),
                        "event_type": event_type,
                        "error": str(error)[:200],  # Truncate long errors
                    })

                    # Keep only last 10 failures per handler
                    if len(self._handler_failures[failure_key]) > 10:
                        self._handler_failures[failure_key] = self._handler_failures[failure_key][-10:]
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling handler failed event: {e}")

            # Subscribe to all feedback signals
            subscribe("QUALITY_DEGRADED", handle_quality_degraded)
            subscribe("ELO_VELOCITY_CHANGED", handle_elo_velocity_changed)
            subscribe("EVALUATION_COMPLETED", handle_evaluation_completed)
            subscribe("PLATEAU_DETECTED", handle_plateau_detected)
            subscribe("EXPLORATION_BOOST", handle_exploration_boost)
            subscribe("PROMOTION_FAILED", handle_promotion_failed)
            subscribe("HANDLER_FAILED", handle_handler_failed)

            logger.info("Subscribed to training feedback signals")
            return True
        except ImportError as e:
            logger.debug(f"Feedback signals: event_router not available: {e}")
            return False
        except Exception as e:  # noqa: BLE001
            logger.warning(f"Feedback signals: failed to subscribe: {e}")
            return False

    def _subscribe_to_manager_events(self) -> bool:
        """Subscribe to manager lifecycle events for coordination.

        December 2025: Subscribes to critical manager events that were previously
        missing from P2P orchestrator integration:
        - TRAINING_STARTED/COMPLETED: Coordinate training transitions
        - TASK_SPAWNED/COMPLETED/FAILED: Track job lifecycle
        - DATA_SYNC_STARTED/COMPLETED: Coordinate data freshness

        Returns True if subscription succeeded, False otherwise.
        """
        try:
            from app.coordination.event_router import subscribe

            def handle_training_started(event) -> None:
                """Handle training start events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    config_key = payload.get("config_key", "unknown")
                    node_id = payload.get("node_id", "unknown")
                    logger.info(f"[P2P] Training started: {config_key} on {node_id}")
                    # Track active training in cluster state
                    if not hasattr(self, "_active_training"):
                        self._active_training = {}
                    self._active_training[config_key] = {
                        "node_id": node_id,
                        "started_at": time.time(),
                    }
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling training started event: {e}")

            def handle_training_completed(event) -> None:
                """Handle training completion events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    config_key = payload.get("config_key", "unknown")
                    model_path = payload.get("model_path", "")
                    final_loss = payload.get("final_loss", 0)
                    logger.info(
                        f"[P2P] Training completed: {config_key} "
                        f"(loss={final_loss:.4f}, model={model_path})"
                    )
                    # Clear from active training
                    if hasattr(self, "_active_training"):
                        self._active_training.pop(config_key, None)
                    # Trigger selfplay allocation refresh
                    if hasattr(self, "selfplay_scheduler"):
                        self.selfplay_scheduler.on_training_complete(config_key)
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling training completed event: {e}")

            def handle_task_spawned(event) -> None:
                """Handle task spawn events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    job_id = payload.get("job_id", "unknown")
                    job_type = payload.get("job_type", "unknown")
                    node_id = payload.get("node_id", "unknown")
                    logger.debug(f"[P2P] Task spawned: {job_type} {job_id} on {node_id}")
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling task spawned event: {e}")

            def handle_task_completed(event) -> None:
                """Handle task completion events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    job_id = payload.get("job_id", "unknown")
                    job_type = payload.get("job_type", "unknown")
                    duration = payload.get("duration", 0)
                    logger.debug(f"[P2P] Task completed: {job_type} {job_id} ({duration:.1f}s)")
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling task completed event: {e}")

            def handle_task_failed(event) -> None:
                """Handle task failure events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    job_id = payload.get("job_id", "unknown")
                    job_type = payload.get("job_type", "unknown")
                    error = payload.get("error", "unknown error")
                    logger.warning(f"[P2P] Task failed: {job_type} {job_id} - {error}")
                    # Could trigger recovery or rebalancing here
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling task failed event: {e}")

            def handle_data_sync_started(event) -> None:
                """Handle data sync start events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    sync_type = payload.get("sync_type", "unknown")
                    target_count = payload.get("target_nodes", 0)
                    logger.info(f"[P2P] Data sync started: {sync_type} to {target_count} nodes")
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling data sync started event: {e}")

            def handle_data_sync_completed(event) -> None:
                """Handle data sync completion events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    sync_type = payload.get("sync_type", "unknown")
                    duration = payload.get("duration", 0)
                    files_synced = payload.get("files_synced", 0)
                    logger.info(
                        f"[P2P] Data sync completed: {sync_type} "
                        f"({files_synced} files in {duration:.1f}s)"
                    )
                    # Could trigger training readiness check here
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling data sync completed event: {e}")

            # P2P Health event handlers (Dec 2025)
            def handle_node_unhealthy(event) -> None:
                """Handle NODE_UNHEALTHY events - pause jobs on unhealthy nodes."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    node_id = payload.get("node_id", "unknown")
                    reason = payload.get("reason", "")
                    logger.warning(f"[P2P] Node {node_id} unhealthy: {reason}")
                    # Mark node as unhealthy for job routing
                    if hasattr(self, "node_selector") and self.node_selector:
                        self.node_selector.mark_node_unhealthy(node_id, reason)
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling node unhealthy event: {e}")

            def handle_node_recovered(event) -> None:
                """Handle NODE_RECOVERED events - resume jobs on recovered nodes."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    node_id = payload.get("node_id", "unknown")
                    logger.info(f"[P2P] Node {node_id} recovered")
                    # Mark node as healthy for job routing
                    if hasattr(self, "node_selector") and self.node_selector:
                        self.node_selector.mark_node_healthy(node_id)
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling node recovered event: {e}")

            def handle_cluster_healthy(event) -> None:
                """Handle P2P_CLUSTER_HEALTHY events."""
                try:
                    logger.info("[P2P] Cluster is healthy - resuming normal operations")
                    self._cluster_health_degraded = False
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling cluster healthy event: {e}")

            def handle_cluster_unhealthy(event) -> None:
                """Handle P2P_CLUSTER_UNHEALTHY events - pause non-critical operations."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    reason = payload.get("reason", "")
                    alive_nodes = payload.get("alive_nodes", 0)
                    logger.warning(
                        f"[P2P] Cluster unhealthy: {reason} (alive_nodes={alive_nodes})"
                    )
                    self._cluster_health_degraded = True
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling cluster unhealthy event: {e}")

            # Subscribe to all manager events
            subscribe("TRAINING_STARTED", handle_training_started)
            subscribe("TRAINING_COMPLETED", handle_training_completed)
            subscribe("TASK_SPAWNED", handle_task_spawned)
            subscribe("TASK_COMPLETED", handle_task_completed)
            subscribe("TASK_FAILED", handle_task_failed)
            subscribe("DATA_SYNC_STARTED", handle_data_sync_started)
            subscribe("DATA_SYNC_COMPLETED", handle_data_sync_completed)

            # Subscribe to health events (Dec 2025)
            subscribe("NODE_UNHEALTHY", handle_node_unhealthy)
            subscribe("NODE_RECOVERED", handle_node_recovered)
            subscribe("P2P_CLUSTER_HEALTHY", handle_cluster_healthy)
            subscribe("P2P_CLUSTER_UNHEALTHY", handle_cluster_unhealthy)

            logger.info("Subscribed to manager lifecycle and health events")
            return True
        except ImportError as e:
            logger.debug(f"Manager events: event_router not available: {e}")
            return False
        except Exception as e:  # noqa: BLE001
            logger.warning(f"Manager events: failed to subscribe: {e}")
            return False

    def _is_leader(self) -> bool:
        """Check if this node is the current cluster leader with valid lease."""
        if self.leader_id != self.node_id:
            # Consistency: we should never claim role=leader while leader_id points elsewhere (or is None).
            if self.role == NodeRole.LEADER:
                logger.info("Inconsistent leadership state (role=leader but leader_id!=self); stepping down")
                self.role = NodeRole.FOLLOWER
                self.last_lease_renewal = 0.0
                if not self.leader_id:
                    self.leader_lease_id = ""
                    self.leader_lease_expires = 0.0
                self._release_voter_grant_if_self()
                self._save_state()
                # Only force an election when we have no known leader; otherwise we
                # may already be following a healthy leader and shouldn't flap.
                if not self.leader_id:
                    # CRITICAL: Check quorum before starting election to prevent quorum bypass
                    if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                        logger.warning("Skipping election: no voter quorum available")
                    else:
                        with contextlib.suppress(RuntimeError):
                            asyncio.get_running_loop().create_task(self._start_election())
            return False
        # Consistency: we should never claim leader_id=self while being a follower/candidate.
        if self.role != NodeRole.LEADER:
            logger.info("Inconsistent leadership state (leader_id=self but role!=leader); clearing leader_id")
            self.role = NodeRole.FOLLOWER
            self.leader_id = None
            self.leader_lease_id = ""
            self.leader_lease_expires = 0.0
            self.last_lease_renewal = 0.0
            self._release_voter_grant_if_self()
            self._save_state()
            # CRITICAL: Check quorum before starting election to prevent quorum bypass
            if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                logger.warning("Skipping election after inconsistent state: no voter quorum available")
            else:
                with contextlib.suppress(RuntimeError):
                    asyncio.get_running_loop().create_task(self._start_election())
            return False

        # LEARNED LESSONS - Lease-based leadership prevents split-brain
        # Must have valid lease to act as leader
        if self.leader_lease_expires > 0 and time.time() >= self.leader_lease_expires:
            logger.info("Leadership lease expired, stepping down")
            # Dec 2025: Emit LEADER_LOST before clearing leader_id
            old_leader_id = self.leader_id
            self.role = NodeRole.FOLLOWER
            self.leader_id = None
            self.leader_lease_id = ""
            self.leader_lease_expires = 0.0
            self.last_lease_renewal = 0.0
            self._release_voter_grant_if_self()
            self._save_state()
            self._emit_leader_lost_sync(old_leader_id, "lease_expired")
            # CRITICAL: Check quorum before starting election to prevent quorum bypass
            if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                logger.warning("Skipping election after lease expired: no voter quorum available")
            else:
                with contextlib.suppress(RuntimeError):
                    asyncio.get_running_loop().create_task(self._start_election())
            return False
        if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
            logger.info("Leadership without voter quorum, stepping down")
            # Dec 2025: Emit LEADER_LOST before clearing leader_id
            old_leader_id = self.leader_id
            self.role = NodeRole.FOLLOWER
            self.leader_id = None
            self.leader_lease_id = ""
            self.leader_lease_expires = 0.0
            self.last_lease_renewal = 0.0
            self._release_voter_grant_if_self()
            self._save_state()
            self._emit_leader_lost_sync(old_leader_id, "quorum_lost")
            # NOTE: Don't start election here - we just lost quorum, so election would fail anyway
            # Wait for quorum to be restored before attempting election
            logger.warning("Skipping election after quorum loss: no voter quorum available")
            return False
        return True

    @property
    def is_leader(self) -> bool:
        """Property alias for _is_leader() - required by WorkQueueHandlersMixin."""
        return self._is_leader()

    # =========================================================================
    # TASK ISOLATION - Prevent single task failure from crashing all tasks
    # =========================================================================

    # Task factory registry for restart support
    _task_factories: dict[str, "Callable[[], Coroutine]"] = {}

    async def _safe_task_wrapper(
        self,
        coro,
        task_name: str,
        factory: "Callable[[], Coroutine] | None" = None,
    ) -> None:
        """Wrap a coroutine to catch exceptions and prevent cascade failures.

        This is a CRITICAL stability fix: without isolation, a single exception
        in any of 18+ background tasks will crash the entire P2P orchestrator
        via asyncio.gather() propagating the exception.

        Args:
            coro: The coroutine to wrap
            task_name: Human-readable task name for logging
            factory: Optional callable that returns a new coroutine for restarts

        Returns:
            None - exceptions are logged but not raised
        """
        # Register factory for potential restarts
        if factory is not None:
            self._task_factories[task_name] = factory

        restart_count = 0
        max_restarts = 5

        while True:
            try:
                await coro
                return  # Normal completion
            except asyncio.CancelledError:
                logger.debug(f"Task '{task_name}' cancelled (shutdown)")
                raise  # Re-raise CancelledError for graceful shutdown
            except SystemExit:
                # SystemExit from main loop exit - ignore in background tasks
                # This prevents "Task exception was never retrieved" log pollution
                logger.debug(f"Task '{task_name}' received SystemExit (orchestrator shutdown)")
                return
            except Exception as e:  # noqa: BLE001
                # Log but don't propagate - other tasks continue running
                logger.error(f"Task '{task_name}' crashed: {e}", exc_info=True)

                # Check if we can restart
                restart_factory = factory or self._task_factories.get(task_name)
                if not self.running or restart_factory is None:
                    logger.warning(f"Task '{task_name}' cannot restart (no factory or shutdown)")
                    return

                restart_count += 1
                if restart_count > max_restarts:
                    logger.error(
                        f"Task '{task_name}' exceeded max restarts ({max_restarts}), giving up"
                    )
                    return

                # Exponential backoff: 30s, 60s, 120s, 240s, 480s
                delay = min(30 * (2 ** (restart_count - 1)), 480)
                logger.info(
                    f"Restarting task '{task_name}' in {delay}s "
                    f"(attempt {restart_count}/{max_restarts})..."
                )
                await asyncio.sleep(delay)

                if not self.running:
                    return

                # Create new coroutine from factory
                try:
                    coro = restart_factory()
                    logger.info(f"Restarted task '{task_name}'")
                except Exception as restart_error:
                    logger.error(f"Failed to restart task '{task_name}': {restart_error}")
                    return

    def _create_safe_task(
        self,
        coro,
        name: str,
        factory: "Callable[[], Coroutine] | None" = None,
    ) -> asyncio.Task:
        """Create a task wrapped with exception isolation and restart support.

        Args:
            coro: The coroutine to run
            name: Task name for logging
            factory: Optional callable that returns a new coroutine for restarts.
                     If not provided, task cannot be automatically restarted.

        Returns:
            asyncio.Task wrapped with safe error handling
        """
        return asyncio.create_task(
            self._safe_task_wrapper(coro, name, factory),
            name=name,
        )

    # =========================================================================
    # BOUNDED COLLECTIONS - Prevent unbounded memory growth
    # =========================================================================

    # Maximum pending relay items before cleanup
    MAX_PENDING_RELAY_ACKS = 10000
    MAX_PENDING_RELAY_RESULTS = 10000

    def _add_pending_relay_ack(self, cmd_id: str) -> None:
        """Add a relay ack with bounds checking."""
        if len(self.pending_relay_acks) >= self.MAX_PENDING_RELAY_ACKS:
            # Evict oldest entries (set doesn't have order, so clear half)
            half = len(self.pending_relay_acks) // 2
            to_remove = list(self.pending_relay_acks)[:half]
            for item in to_remove:
                self.pending_relay_acks.discard(item)
            logger.warning(f"Evicted {half} pending_relay_acks (max {self.MAX_PENDING_RELAY_ACKS})")
        self.pending_relay_acks.add(cmd_id)

    def _add_pending_relay_result(self, result: dict) -> None:
        """Add a relay result with bounds checking."""
        if len(self.pending_relay_results) >= self.MAX_PENDING_RELAY_RESULTS:
            # Evict oldest entries (keep most recent half)
            half = len(self.pending_relay_results) // 2
            self.pending_relay_results = self.pending_relay_results[half:]
            logger.warning(f"Evicted {half} pending_relay_results (max {self.MAX_PENDING_RELAY_RESULTS})")
        self.pending_relay_results.append(result)

    # =========================================================================
    # SAFEGUARDS - Load, rate limiting, and coordinator integration
    # =========================================================================

    def _check_spawn_rate_limit(self) -> tuple[bool, str]:
        """Check if we're within the spawn rate limit.

        SAFEGUARD: Prevents runaway process spawning by limiting spawns per minute.

        Returns:
            (can_spawn, reason) - True if within rate limit
        """
        now = time.time()
        # Clean old timestamps (older than 60 seconds)
        self.spawn_timestamps = [t for t in self.spawn_timestamps if now - t < 60]

        if len(self.spawn_timestamps) >= SPAWN_RATE_LIMIT_PER_MINUTE:
            return False, f"Rate limit: {len(self.spawn_timestamps)}/{SPAWN_RATE_LIMIT_PER_MINUTE} spawns in last minute"

        return True, f"Rate OK: {len(self.spawn_timestamps)}/{SPAWN_RATE_LIMIT_PER_MINUTE}"

    def _record_spawn(self) -> None:
        """Record a process spawn for rate limiting."""
        self.spawn_timestamps.append(time.time())

    async def _check_coordinator_available(self) -> bool:
        """Check if the unified coordinator is available.

        SAFEGUARD: In agent mode, defer job decisions to coordinator.

        Returns:
            True if coordinator is reachable
        """
        if not self.coordinator_url:
            return False

        # Cache check for 30 seconds
        now = time.time()
        if now - self.last_coordinator_check < 30:
            return self.coordinator_available

        self.last_coordinator_check = now

        try:
            async with get_client_session(timeout=ClientTimeout(total=5)) as session:
                async with session.get(f"{self.coordinator_url}/api/health") as resp:
                    self.coordinator_available = resp.status == 200
                    if self.coordinator_available:
                        logger.info(f"Coordinator available at {self.coordinator_url}")
                    return self.coordinator_available
        except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
            self.coordinator_available = False
            return False

    def _can_spawn_process(self, reason: str = "job") -> tuple[bool, str]:
        """Combined safeguard check before spawning any process.

        SAFEGUARD: Checks load average, rate limit, and agent mode.

        Args:
            reason: Description of why we want to spawn (for logging)

        Returns:
            (can_spawn, explanation) - True if all checks pass
        """
        # Check 1: Load average
        load_ok, load_reason = self.self_info.check_load_average_safe()
        if not load_ok:
            logger.info(f"BLOCKED spawn ({reason}): {load_reason}")
            return False, load_reason

        # Check 2: Rate limit
        rate_ok, rate_reason = self._check_spawn_rate_limit()
        if not rate_ok:
            logger.info(f"BLOCKED spawn ({reason}): {rate_reason}")
            return False, rate_reason

        # Check 3: Agent mode - if coordinator is available and we're in agent mode,
        # we should not autonomously spawn jobs (let coordinator decide)
        if self.agent_mode and self.coordinator_available:
            msg = "Agent mode: deferring to coordinator"
            logger.info(f"BLOCKED spawn ({reason}): {msg}")
            return False, msg

        # Check 4: Backpressure (new coordination) - if training queue is saturated,
        # don't spawn more selfplay jobs that would produce more data
        if HAS_NEW_COORDINATION and "selfplay" in reason.lower():
            if should_stop_production(QueueType.TRAINING_DATA):
                msg = "Backpressure: training queue at STOP level"
                logger.info(f"BLOCKED spawn ({reason}): {msg}")
                return False, msg
            if should_throttle_production(QueueType.TRAINING_DATA):
                throttle = get_throttle_factor(QueueType.TRAINING_DATA)
                import random
                if random.random() > throttle:
                    msg = f"Backpressure: throttled (factor={throttle:.2f})"
                    logger.info(f"BLOCKED spawn ({reason}): {msg}")
                    return False, msg

        # Check 5: Graceful degradation - don't spawn under heavy resource pressure
        if HAS_RESOURCE_GUARD and get_degradation_level is not None:
            degradation = get_degradation_level()
            if degradation >= 4:  # CRITICAL - resources at/above limits
                msg = f"Graceful degradation: critical resource pressure (level {degradation})"
                logger.info(f"BLOCKED spawn ({reason}): {msg}")
                return False, msg
            elif degradation >= 3:  # HEAVY - only critical ops proceed
                # Selfplay is NORMAL priority, blocked under heavy pressure
                if should_proceed_with_priority is not None and not should_proceed_with_priority(OperationPriority.NORMAL):
                    msg = f"Graceful degradation: heavy resource pressure (level {degradation})"
                    logger.info(f"BLOCKED spawn ({reason}): {msg}")
                    return False, msg

        return True, "All safeguards passed"

    def _detect_build_version(self) -> str:
        env_version = (os.environ.get(BUILD_VERSION_ENV, "") or "").strip()
        if env_version:
            return env_version

        commit = ""
        branch = ""
        try:
            result = subprocess.run(
                self._git_cmd("rev-parse", "--short", "HEAD"),
                cwd=self.ringrift_path,
                capture_output=True,
                text=True,
                timeout=3,
            )
            if result.returncode == 0:
                commit = result.stdout.strip()
        except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError, AttributeError):
            commit = ""

        try:
            result = subprocess.run(
                self._git_cmd("rev-parse", "--abbrev-ref", "HEAD"),
                cwd=self.ringrift_path,
                capture_output=True,
                text=True,
                timeout=3,
            )
            if result.returncode == 0:
                branch = result.stdout.strip()
        except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError, AttributeError):
            branch = ""

        if commit and branch:
            return f"{branch}@{commit}"
        return commit or "unknown"

    def _git_cmd(self, *args: str) -> list[str]:
        safe_dir = getattr(self, "_git_safe_directory", "") or os.path.abspath(self.ringrift_path)
        return ["git", "-c", f"safe.directory={safe_dir}", *args]

    def _detect_ringrift_path(self) -> str:
        """Detect the RingRift installation path."""
        # Try common locations
        candidates = [
            Path.home() / "Development" / "RingRift",
            Path.home() / "ringrift",
            Path("/home/ubuntu/ringrift"),
            Path("/root/ringrift"),
        ]
        for path in candidates:
            if (path / "ai-service").exists():
                return str(path)
        return str(Path(__file__).parent.parent.parent)

    def get_data_directory(self) -> Path:
        """Get the data directory path based on storage configuration.

        Returns:
            Path to data directory:
            - ramdrive: /dev/shm/ringrift/data (for disk-constrained Vast instances)
            - disk: {ringrift_path}/ai-service/data (default)

        The ramdrive option uses tmpfs for high-speed I/O and to work around
        limited disk space on some cloud instances. Data stored in ramdrive
        is volatile and should be synced to permanent storage periodically.
        """
        if self.storage_type == "ramdrive":
            ramdrive = Path(self.ramdrive_path)
            try:
                ramdrive.mkdir(parents=True, exist_ok=True)
            except (PermissionError, OSError) as e:
                # /dev/shm doesn't exist on macOS or may be inaccessible
                logger.warning(f"Cannot create ramdrive at {ramdrive}: {e}. Falling back to disk storage.")
                self.storage_type = "disk"
                return Path(self.ringrift_path) / "ai-service" / "data"

            # Set up automatic sync to persistent storage
            if self.ramdrive_syncer is None and self.sync_to_disk_interval > 0:
                persistent_path = Path(self.ringrift_path) / "ai-service" / "data"
                persistent_path.mkdir(parents=True, exist_ok=True)
                self.ramdrive_syncer = RamdriveSyncer(
                    source_dir=ramdrive,
                    target_dir=persistent_path,
                    interval=self.sync_to_disk_interval,
                    patterns=["*.db", "*.jsonl", "*.json", "*.npz"],
                )
                self.ramdrive_syncer.start()
                logger.info(f"Started ramdrive -> disk sync: {ramdrive} -> {persistent_path} "
                           f"every {self.sync_to_disk_interval}s")

            return ramdrive
        return Path(self.ringrift_path) / "ai-service" / "data"

    def stop_ramdrive_syncer(self, final_sync: bool = True) -> None:
        """Stop the ramdrive syncer and optionally perform final sync."""
        if self.ramdrive_syncer:
            logger.info("Stopping ramdrive syncer...")
            self.ramdrive_syncer.stop(final_sync=final_sync)
            logger.info(f"Ramdrive sync stats: {self.ramdrive_syncer.stats}")
            self.ramdrive_syncer = None

    def _infer_advertise_port(self) -> int:
        """Infer the externally reachable port for this node.

        - Explicit `RINGRIFT_ADVERTISE_PORT` always wins.
        - Vast.ai exposes container ports via `VAST_TCP_PORT_<PORT>`; when set,
          use that public port so peers can reach us.
        - Default to the listening port.
        """
        explicit = (os.environ.get(ADVERTISE_PORT_ENV, "")).strip()
        if explicit:
            try:
                return int(explicit)
            except ValueError:
                pass

        vast_key = f"VAST_TCP_PORT_{self.port}"
        mapped = (os.environ.get(vast_key, "")).strip()
        if mapped:
            try:
                return int(mapped)
            except ValueError:
                pass

        return int(self.port)

    def _validate_and_fix_advertise_host(self) -> None:
        """Validate advertise_host and fix private IP issues.

        December 30, 2025: Added to prevent P2P quorum loss caused by nodes
        advertising private LAN IPs (10.x, 192.168.x, 172.16-31.x) that other
        nodes in the mesh cannot reach.

        This method:
        1. Detects if advertise_host is a private IP
        2. If private and Tailscale is available, switches to Tailscale IP
        3. Emits warnings/errors for operator awareness
        """
        import ipaddress

        if not self.advertise_host:
            return

        try:
            ip = ipaddress.ip_address(self.advertise_host)
        except ValueError:
            # Not an IP address (maybe hostname), skip validation
            return

        is_private = ip.is_private and not ip.is_loopback

        if not is_private:
            # Public IP or loopback - no issue
            return

        # Private IP detected - try to get Tailscale IP
        ts_ip = self._get_tailscale_ip()

        if ts_ip and ts_ip != self.advertise_host:
            old_host = self.advertise_host
            self.advertise_host = ts_ip
            print(
                f"[P2P] WARNING: advertise_host was private IP {old_host}, "
                f"auto-switched to Tailscale IP {ts_ip} for mesh reachability"
            )
            logger.warning(
                f"P2P advertise_host auto-fixed: {old_host} -> {ts_ip} (private IP unreachable by peers)"
            )
        else:
            # No Tailscale available - emit warning
            print(
                f"[P2P] ERROR: advertise_host {self.advertise_host} is a private IP! "
                f"Other nodes in the mesh cannot reach this address. "
                f"Set RINGRIFT_P2P_ADVERTISE_HOST to your public/Tailscale IP."
            )
            logger.error(
                f"P2P advertise_host {self.advertise_host} is private - mesh connectivity will fail!"
            )

    def _load_voter_node_ids(self) -> list[str]:
        """Load the set of P2P voter node_ids (for quorum-based leadership).

        If no voters are configured, returns an empty list and quorum checks are
        disabled (backwards compatible).

        December 2025 (Phase 7.1.2): Consolidated to use cluster_config.get_p2p_voters()
        for YAML loading, while preserving env var priority for overrides.
        """
        # Priority 1: Environment variable override (highest priority)
        env = (os.environ.get("RINGRIFT_P2P_VOTERS") or "").strip()
        if env:
            self.voter_config_source = "env"
            voters = [t.strip() for t in env.split(",") if t.strip()]
            return sorted(set(voters))

        # Priority 2: Use cluster_config for YAML-based voter loading
        # This handles both p2p_voters list and legacy per-host p2p_voter: true
        try:
            from app.config.cluster_config import get_p2p_voters
            voters = get_p2p_voters()
            if voters:
                self.voter_config_source = "config"
                return voters
        except ImportError:
            logger.debug("[P2P] cluster_config not available, falling back to direct YAML load")
        except Exception as e:  # noqa: BLE001
            logger.warning(f"[P2P] Failed to load voters via cluster_config: {e}")

        # Priority 3: Fallback - direct YAML load for legacy compatibility
        cfg_path = Path(self.ringrift_path) / "ai-service" / "config" / "distributed_hosts.yaml"
        if not cfg_path.exists():
            self.voter_config_source = "none"
            return []

        try:
            import yaml
            data = yaml.safe_load(cfg_path.read_text()) or {}
            p2p_voters_list = data.get("p2p_voters", []) or []
            if p2p_voters_list and isinstance(p2p_voters_list, list):
                voters = sorted({str(v).strip() for v in p2p_voters_list if str(v).strip()})
                if voters:
                    self.voter_config_source = "config"
                    return voters
        except (OSError, yaml.YAMLError, ValueError, KeyError) as e:
            # Dec 2025: Narrowed from bare Exception; config file may not exist
            logger.debug(f"Failed to load voter config from file: {e}")

        self.voter_config_source = "none"
        return []

    def _maybe_adopt_voter_node_ids(self, voter_node_ids: list[str], *, source: str) -> bool:
        """Adopt/override the voter set when it's not explicitly configured.

        This is a convergence mechanism: some nodes may boot without local
        config (or with stale config), which would disable quorum gating and
        allow non-voter nodes to become leaders. Leaders propagate the stable
        voter set via `/coordinator` so the cluster converges.

        Dec 2025: Don't override if config source is 'env' or 'config' (YAML).
        Dec 2025: Added strict validation to prevent voter set flapping.
        """
        # If explicitly configured via env var, never override
        if (os.environ.get("RINGRIFT_P2P_VOTERS") or "").strip():
            return False

        # If explicitly configured via YAML, never override from gossip
        if getattr(self, "voter_config_source", "") == "config":
            return False

        normalized = sorted({str(v).strip() for v in (voter_node_ids or []) if str(v).strip()})
        if not normalized:
            return False

        # Dec 2025: Strict validation to prevent voter set flapping
        # Reject voter sets that are too small (need at least 3 for meaningful quorum)
        if len(normalized) < 3:
            return False

        # Dec 2025: Canonical voters that MUST be in any valid voter set
        # These are stable, always-on nodes that should always be voters
        canonical_voters = {"nebius-backbone-1", "vultr-a100-20gb"}
        has_canonical = bool(canonical_voters & set(normalized))
        if not has_canonical:
            # Reject voter sets that don't include any canonical voter
            return False

        current = sorted(set(getattr(self, "voter_node_ids", []) or []))
        if current == normalized:
            return False

        # Dec 2025: Once we have a learned voter set with 5+ voters, don't downgrade
        if len(current) >= 5 and len(normalized) < len(current):
            return False

        self.voter_node_ids = normalized
        # SIMPLIFIED QUORUM: Fixed at 3 voters (or less if fewer voters exist)
        self.voter_quorum_size = min(VOTER_MIN_QUORUM, len(normalized))
        self.voter_config_source = source or "learned"
        print(
            f"[P2P] Updated voter set ({self.voter_config_source}): voters={len(normalized)}, "
            f"quorum={self.voter_quorum_size} ({', '.join(normalized)})"
        )
        return True

    # _has_voter_quorum: Provided by LeaderElectionMixin
    # _release_voter_grant_if_self: Provided by LeaderElectionMixin

    def _enable_partition_local_election(self) -> bool:
        """Enable local leader election for partitioned nodes.

        When a partition is detected and no voters are reachable, this method
        temporarily adds reachable nodes to the voter set so they can elect a
        local leader and continue operating autonomously.

        This is a self-healing mechanism for network splits. When connectivity
        is restored, the partition will merge back with the main cluster.

        Returns:
            True if local election was enabled
        """
        # Don't override env-configured voters
        if (os.environ.get("RINGRIFT_P2P_VOTERS") or "").strip():
            return False

        # Check if we have any voters configured
        voters = list(getattr(self, "voter_node_ids", []) or [])

        # Count how many voters are reachable
        with self.peers_lock:
            peers_by_id = dict(self.peers)
        reachable_voters = 0
        for voter_id in voters:
            if voter_id == self.node_id:
                reachable_voters += 1
                continue
            peer = peers_by_id.get(voter_id)
            if peer and peer.is_alive():
                reachable_voters += 1

        # If we have quorum (simplified: 3 voters), no need for partition election
        quorum = min(VOTER_MIN_QUORUM, len(voters)) if voters else 1
        if reachable_voters >= quorum:
            return False

        # Build local partition voter set from reachable nodes
        local_voters = [self.node_id]  # Always include self
        for node_id, peer in peers_by_id.items():
            if peer.is_alive() and node_id not in local_voters:
                local_voters.append(node_id)

        if len(local_voters) < 2:
            # Need at least 2 nodes for meaningful election
            return False

        # Store original voters for restoration
        if not hasattr(self, "_original_voters"):
            self._original_voters = voters.copy()
            self._partition_election_started = time.time()

        # Enable partition-local election
        self.voter_node_ids = sorted(local_voters)
        self.voter_quorum_size = min(VOTER_MIN_QUORUM, len(local_voters))
        self.voter_config_source = "partition-local"
        print(
            f"[P2P] PARTITION: Enabling local election with {len(local_voters)} nodes: "
            f"{', '.join(local_voters)} (quorum={self.voter_quorum_size})"
        )
        return True

    def _restore_original_voters(self) -> bool:
        """Restore original voter configuration after partition heals.

        Called when connectivity to the main cluster is restored.

        Returns:
            True if voters were restored
        """
        if not hasattr(self, "_original_voters"):
            return False

        original = getattr(self, "_original_voters", [])
        if not original:
            return False

        # Check if we can reach any original voters
        with self.peers_lock:
            peers_by_id = dict(self.peers)
        for voter_id in original:
            if voter_id == self.node_id:
                continue
            peer = peers_by_id.get(voter_id)
            if peer and peer.is_alive():
                # We can reach at least one original voter, restore config
                self.voter_node_ids = original.copy()
                # SIMPLIFIED QUORUM: Fixed at 3 voters (or less if fewer voters exist)
                self.voter_quorum_size = min(VOTER_MIN_QUORUM, len(original))
                self.voter_config_source = "restored"
                delattr(self, "_original_voters")
                if hasattr(self, "_partition_election_started"):
                    delattr(self, "_partition_election_started")
                logger.info(f"Partition healed: restored original voters {', '.join(original)}")
                return True
        return False

    def _get_eligible_voters(self) -> list[str]:
        """Get list of nodes eligible to be voters (GPU nodes with good health)."""
        with self.peers_lock:
            peers = dict(self.peers)

        eligible = []
        now = time.time()

        for node_id, peer in peers.items():
            # Skip retired or NAT-blocked without relay
            if getattr(peer, "retired", False):
                continue

            # Must be alive
            if not peer.is_alive():
                continue

            # Must have GPU (CUDA or MPS)
            has_gpu = getattr(peer, "has_gpu", False)
            gpu_name = str(getattr(peer, "gpu_name", "") or "")
            if not has_gpu and "GH200" not in gpu_name and "H100" not in gpu_name and "A10" not in gpu_name and "aws" not in node_id.lower():
                continue

            # Must have been up for minimum time
            first_seen = getattr(peer, "first_seen", 0) or peer.last_heartbeat
            if now - first_seen < VOTER_PROMOTION_UPTIME:
                continue

            # Check health score (response rate)
            failures = getattr(peer, "consecutive_failures", 0)
            if failures >= VOTER_DEMOTION_FAILURES:
                continue

            eligible.append(node_id)

        # Always include self if we have GPU
        if self.node_id not in eligible:
            self_gpu = getattr(self, "has_gpu", False)
            if self_gpu or "aws" in self.node_id.lower() or "lambda" in self.node_id.lower():
                eligible.append(self.node_id)

        return sorted(eligible)

    def _manage_dynamic_voters(self) -> bool:
        """Manage dynamic voter pool - promote/demote voters as needed.

        Returns True if voter set was changed.
        """
        if not DYNAMIC_VOTER_ENABLED:
            return False

        # Don't override env-configured voters
        if (os.environ.get("RINGRIFT_P2P_VOTERS") or "").strip():
            return False

        current_voters = list(getattr(self, "voter_node_ids", []) or [])
        eligible = self._get_eligible_voters()

        # Count how many current voters are healthy
        with self.peers_lock:
            peers = dict(self.peers)

        healthy_voters = []
        unhealthy_voters = []

        for voter_id in current_voters:
            if voter_id == self.node_id:
                healthy_voters.append(voter_id)
                continue
            peer = peers.get(voter_id)
            if peer and peer.is_alive():
                failures = getattr(peer, "consecutive_failures", 0)
                if failures < VOTER_DEMOTION_FAILURES:
                    healthy_voters.append(voter_id)
                else:
                    unhealthy_voters.append(voter_id)
            else:
                unhealthy_voters.append(voter_id)

        changed = False
        new_voters = healthy_voters.copy()

        # Demote unhealthy voters
        if unhealthy_voters:
            logger.info(f"Dynamic voters: demoting unhealthy voters: {unhealthy_voters}")
            changed = True

        # Promote new voters if below target
        if len(new_voters) < DYNAMIC_VOTER_TARGET:
            candidates = [n for n in eligible if n not in new_voters]
            # Sort by reliability (fewer failures = better)
            candidates.sort(key=lambda n: getattr(peers.get(n), "consecutive_failures", 0) if peers.get(n) else 999)

            needed = DYNAMIC_VOTER_TARGET - len(new_voters)
            for candidate in candidates[:needed]:
                new_voters.append(candidate)
                logger.info(f"Dynamic voters: promoting {candidate} to voter")
                changed = True

        if changed and new_voters:
            new_voters = sorted(set(new_voters))
            self.voter_node_ids = new_voters
            # SIMPLIFIED QUORUM: Fixed at 3 voters (or less if fewer voters exist)
            self.voter_quorum_size = min(VOTER_MIN_QUORUM, len(new_voters))
            self.voter_config_source = "dynamic"
            print(
                f"[P2P] Dynamic voter set updated: {len(new_voters)} voters, "
                f"quorum={self.voter_quorum_size} ({', '.join(new_voters)})"
            )
            return True

        return False

    def _check_leader_health(self) -> bool:
        """Check leader health based on peer response rates.

        Returns True if health is good, False if degraded.
        """
        if self.role != NodeRole.LEADER:
            return True

        with self.peers_lock:
            peers = list(self.peers.values())

        if not peers:
            return True

        # Calculate response rate (peers that responded recently)
        now = time.time()
        alive_count = sum(1 for p in peers if p.is_alive() and not getattr(p, "retired", False))
        total_count = sum(1 for p in peers if not getattr(p, "retired", False))

        if total_count == 0:
            return True

        response_rate = alive_count / total_count

        # Track degraded state
        if not hasattr(self, "_leader_degraded_since"):
            self._leader_degraded_since = 0.0

        if response_rate < LEADER_MIN_RESPONSE_RATE:
            if self._leader_degraded_since == 0.0:
                self._leader_degraded_since = now
                logger.info(f"Leader health degraded: {response_rate:.1%} response rate (min: {LEADER_MIN_RESPONSE_RATE:.0%})")
            elif now - self._leader_degraded_since > LEADER_DEGRADED_STEPDOWN_DELAY:
                logger.info(f"Leader health critically degraded for {LEADER_DEGRADED_STEPDOWN_DELAY}s, stepping down")
                self._leader_degraded_since = 0.0
                return False
        else:
            if self._leader_degraded_since > 0:
                logger.info(f"Leader health recovered: {response_rate:.1%} response rate")
            self._leader_degraded_since = 0.0

        return True

    async def _acquire_voter_lease_quorum(self, lease_id: str, duration: int) -> float | None:
        """Acquire/renew an exclusive leader lease from a quorum of voters.

        December 29, 2025: Added retry with exponential backoff when initial
        quorum acquisition fails. This handles transient network issues.

        Returns the effective lease expiry timestamp if a quorum granted the
        lease; otherwise returns None.
        """
        voter_ids = list(getattr(self, "voter_node_ids", []) or [])
        if not voter_ids:
            return time.time() + float(duration)

        quorum = int(getattr(self, "voter_quorum_size", 0) or 0)
        if quorum <= 0:
            # SIMPLIFIED QUORUM: Fixed at 3 voters (or less if fewer voters exist)
            quorum = min(VOTER_MIN_QUORUM, len(voter_ids))

        duration = max(10, min(int(duration), int(LEADER_LEASE_DURATION * 2)))

        # December 29, 2025: Retry with exponential backoff
        max_retries = 3
        retry_delays = [0, 2, 5]  # Immediate, then 2s, then 5s

        for attempt in range(max_retries):
            if attempt > 0:
                await asyncio.sleep(retry_delays[attempt])
                logger.info(f"Voter lease acquisition retry {attempt + 1}/{max_retries}")

            now = time.time()
            acks = 0
            lease_ttls: list[float] = []

            # Self-grant (as a voter).
            if self.node_id in voter_ids:
                self.voter_grant_leader_id = self.node_id
                self.voter_grant_lease_id = lease_id
                self.voter_grant_expires = now + float(duration)
                lease_ttls.append(float(duration))
                acks += 1

            with self.peers_lock:
                peers_by_id = dict(self.peers)

            # STABILITY FIX: Use 15s timeout for voter lease operations (was 5s).
            # Cross-geographic Tailscale connections can have latency spikes.
            timeout = ClientTimeout(total=15)

            # Dec 29, 2025: Parallel lease acquisition for faster leadership transitions
            # Instead of sequential requests, we fire all lease requests in parallel
            async def _request_lease_from_voter(
                session: aiohttp.ClientSession,
                voter_id: str,
                voter: NodeInfo,
            ) -> tuple[bool, float | None]:
                """Request lease from a single voter. Returns (success, ttl)."""
                payload = {
                    "leader_id": self.node_id,
                    "lease_id": lease_id,
                    "lease_duration": duration,
                    "lease_epoch": self._lease_epoch + 1,
                }
                for url in self._tailscale_urls_for_voter(voter, "/election/lease"):
                    try:
                        async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                            if resp.status != 200:
                                continue
                            data, json_error = await safe_json_response(resp, default={}, log_errors=False)
                            if json_error or not data.get("granted"):
                                return False, None
                            ttl_raw = data.get("lease_ttl_seconds") or data.get("ttl_seconds")
                            if ttl_raw is not None:
                                try:
                                    return True, float(ttl_raw)
                                except (ValueError, TypeError):
                                    pass
                            return True, float(duration)
                    except (aiohttp.ClientError, asyncio.TimeoutError, ValueError, AttributeError, OSError):
                        continue
                return False, None

            async with get_client_session(timeout) as session:
                # Build list of voters to request from (excluding self and dead peers)
                voter_tasks = []
                for voter_id in voter_ids:
                    if voter_id == self.node_id:
                        continue
                    voter = peers_by_id.get(voter_id)
                    if not voter or not voter.is_alive():
                        continue
                    voter_tasks.append(_request_lease_from_voter(session, voter_id, voter))

                # Fire all requests in parallel
                if voter_tasks:
                    results = await asyncio.gather(*voter_tasks, return_exceptions=True)
                    for result in results:
                        if isinstance(result, Exception):
                            continue
                        success, ttl = result
                        if success:
                            acks += 1
                            if ttl is not None and ttl > 0:
                                lease_ttls.append(ttl)
                            else:
                                lease_ttls.append(float(duration))

            if acks >= quorum:
                # Use a relative TTL (computed by each voter on its own clock) to avoid
                # leader lease flapping under clock skew. Convert back to a local expiry.
                effective_ttl = min(lease_ttls) if lease_ttls else float(duration)
                effective_ttl = max(10.0, min(float(duration), float(effective_ttl)))
                if attempt > 0:
                    logger.info(f"Voter lease acquired on retry {attempt + 1}")
                return now + float(effective_ttl)

            # Log retry info
            if attempt < max_retries - 1:
                logger.warning(
                    f"Voter lease quorum not reached: {acks}/{quorum} acks, "
                    f"retrying in {retry_delays[attempt + 1]}s..."
                )

        # All retries exhausted
        logger.error(f"Failed to acquire voter lease quorum after {max_retries} attempts")
        return None

    # =========================================================================
    # Phase 15.1.1: Fence Token Helpers (December 29, 2025)
    # =========================================================================

    def get_fence_token(self) -> str:
        """Get the current fence token for including in leader operations.

        Phase 15.1.1: Fence tokens provide split-brain protection by ensuring
        workers can reject commands from stale leaders.

        Returns:
            Current fence token or empty string if not leader
        """
        if self.role != NodeRole.LEADER:
            return ""
        return self._fence_token

    def get_lease_epoch(self) -> int:
        """Get the current lease epoch.

        Phase 15.1.1: The epoch is monotonically increasing and helps
        resolve split-brain by allowing workers to compare epochs.

        Returns:
            Current lease epoch (0 if never been leader)
        """
        return self._lease_epoch

    def validate_fence_token(self, token: str) -> tuple[bool, str]:
        """Validate an incoming fence token from a claimed leader.

        Phase 15.1.1: Workers use this to reject commands from stale leaders.
        A token is valid if:
        1. It's from the current known leader
        2. Its epoch is >= our known epoch

        Args:
            token: Fence token to validate (format: node_id:epoch:timestamp)

        Returns:
            (valid, reason) tuple
        """
        if not token:
            return False, "empty_fence_token"

        try:
            parts = token.split(":")
            if len(parts) != 3:
                return False, "malformed_token"

            token_node_id = parts[0]
            token_epoch = int(parts[1])

            # Check if token is from known leader
            if self.leader_id and token_node_id != self.leader_id:
                return False, f"token_from_unknown_leader:{token_node_id}"

            # Check epoch - reject if lower than what we've seen
            if hasattr(self, "_last_seen_epoch"):
                if token_epoch < self._last_seen_epoch:
                    return False, f"stale_epoch:{token_epoch}<{self._last_seen_epoch}"
                self._last_seen_epoch = max(self._last_seen_epoch, token_epoch)
            else:
                self._last_seen_epoch = token_epoch

            return True, "valid"

        except (ValueError, IndexError) as e:
            return False, f"parse_error:{e}"

    def update_fence_token_from_leader(self, token: str, leader_id: str) -> bool:
        """Update internal fence token state when receiving leader announcement.

        Phase 15.1.1: Called when a follower receives a coordinator announcement
        to update the last seen epoch for fence token validation.

        Args:
            token: Fence token from leader
            leader_id: Leader node ID

        Returns:
            True if update was successful
        """
        if not token:
            return False

        try:
            parts = token.split(":")
            if len(parts) != 3:
                return False

            token_epoch = int(parts[1])

            # Update last seen epoch (but never decrease)
            if hasattr(self, "_last_seen_epoch"):
                if token_epoch >= self._last_seen_epoch:
                    self._last_seen_epoch = token_epoch
                    return True
                else:
                    # Reject older epoch
                    logger.warning(
                        f"Rejecting fence token with stale epoch {token_epoch} "
                        f"(current: {self._last_seen_epoch}) from {leader_id}"
                    )
                    return False
            else:
                self._last_seen_epoch = token_epoch
                return True

        except (ValueError, IndexError):
            return False

    async def _determine_leased_leader_from_voters(self) -> str | None:
        """Return the current lease-holder as reported by a quorum of voters.

        This is a read-only reconciliation step used to resolve split-brain once
        partitions heal. It queries the current voter grant state via
        `/election/grant` and selects the leader_id that has >= quorum votes with
        non-expired grants.
        """
        voter_ids = list(getattr(self, "voter_node_ids", []) or [])
        if not voter_ids:
            return None

        quorum = int(getattr(self, "voter_quorum_size", 0) or 0)
        if quorum <= 0:
            # SIMPLIFIED QUORUM: Fixed at 3 voters (or less if fewer voters exist)
            quorum = min(VOTER_MIN_QUORUM, len(voter_ids))

        now = time.time()
        counts: dict[str, int] = {}

        # Include local voter state.
        if self.node_id in voter_ids:
            leader_id = str(getattr(self, "voter_grant_leader_id", "") or "")
            expires = float(getattr(self, "voter_grant_expires", 0.0) or 0.0)
            if leader_id and expires > now:
                counts[leader_id] = counts.get(leader_id, 0) + 1

        with self.peers_lock:
            peers_by_id = dict(self.peers)

        # STABILITY FIX: Use 15s timeout for voter operations (was 5s).
        timeout = ClientTimeout(total=15)
        async with get_client_session(timeout) as session:
            for voter_id in voter_ids:
                if voter_id == self.node_id:
                    continue
                voter = peers_by_id.get(voter_id)
                if not voter or not voter.is_alive():
                    continue

                # Use Tailscale-exclusive URLs for voter communication to avoid NAT issues
                for url in self._tailscale_urls_for_voter(voter, "/election/grant"):
                    try:
                        async with session.get(url, headers=self._auth_headers()) as resp:
                            if resp.status != 200:
                                continue
                            data = await resp.json()
                        leader_id = str((data or {}).get("leader_id") or "")
                        if not leader_id:
                            break
                        ttl_raw = (data or {}).get("lease_ttl_seconds")
                        if ttl_raw is None:
                            ttl_raw = (data or {}).get("ttl_seconds")
                        ttl_val: float | None = None
                        if ttl_raw is not None:
                            try:
                                ttl_val = float(ttl_raw)
                            except (ValueError):
                                ttl_val = None

                        if ttl_val is not None:
                            if ttl_val <= 0:
                                break
                        else:
                            # Back-compat: use absolute expiry as best-effort, with
                            # a generous skew tolerance (1 lease duration).
                            expires = float((data or {}).get("lease_expires") or 0.0)
                            if expires <= 0:
                                break
                            if expires + float(LEADER_LEASE_DURATION) < now:
                                break
                        counts[leader_id] = counts.get(leader_id, 0) + 1
                        break
                    except (ValueError, AttributeError):
                        continue

        winners = [leader_id for leader_id, count in counts.items() if count >= quorum]
        if not winners:
            return None
        # Deterministic: if multiple satisfy quorum (shouldn't), pick highest node_id.
        return sorted(winners)[-1]

    async def _query_arbiter_for_leader(self) -> str | None:
        """Query the arbiter for the authoritative leader when voter quorum fails.

        The arbiter is a reliably-reachable node that maintains its view of
        who the leader should be. Used as a fallback when split-brain causes
        voter quorum to be unreachable.

        Returns:
            The leader_id from the arbiter, or None if arbiter is unreachable
        """
        arbiter_url = ARBITER_URL
        if not arbiter_url:
            return None

        # Try the configured arbiter URL
        urls_to_try = [arbiter_url]

        # Also try known peers as arbiters if main arbiter fails
        for peer_addr in (self.known_peers or []):
            if peer_addr not in urls_to_try:
                urls_to_try.append(peer_addr)

        timeout = ClientTimeout(total=5)
        try:
            async with get_client_session(timeout) as session:
                for url in urls_to_try:
                    try:
                        base_url = url.rstrip("/")
                        # Query the arbiter's election/grant endpoint to see who they think is leader
                        async with session.get(
                            f"{base_url}/election/grant",
                            headers=self._auth_headers()
                        ) as resp:
                            if resp.status == 200:
                                data = await resp.json()
                                leader_id = str((data or {}).get("leader_id") or "")
                                if leader_id:
                                    logger.info(f"Arbiter {base_url} reports leader: {leader_id}")
                                    return leader_id
                    except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
                        # Try next arbiter
                        continue
        except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
            pass

        return None

    # _parse_peer_address, _url_for_peer, _urls_for_peer provided by NetworkUtilsMixin

    def _auth_headers(self) -> dict[str, str]:
        if not self.auth_token:
            return {}
        return {"Authorization": f"Bearer {self.auth_token}"}

    def _get_leader_peer(self) -> NodeInfo | None:
        if self._is_leader():
            return self.self_info

        with self.peers_lock:
            peers_snapshot = list(self.peers.values())

        conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])

        leader_id = self.leader_id
        if leader_id and self._is_leader_lease_valid():
            for peer in peers_snapshot:
                if (
                    peer.node_id == leader_id
                    and peer.role == NodeRole.LEADER
                    and peer.is_alive()
                    and self._is_leader_eligible(peer, conflict_keys)
                ):
                    return peer

        eligible_leaders = [
            peer for peer in peers_snapshot
            if peer.role == NodeRole.LEADER and self._is_leader_eligible(peer, conflict_keys)
        ]
        if eligible_leaders:
            return sorted(eligible_leaders, key=lambda p: p.node_id)[-1]

        return None

    async def _proxy_to_leader(self, request: web.Request) -> web.StreamResponse:
        """Best-effort proxy for leader-only APIs when the dashboard hits a follower."""
        leader = self._get_leader_peer()
        if not leader:
            return web.json_response(
                {"success": False, "error": "leader_unknown", "leader_id": self.leader_id},
                status=503,
            )

        candidate_urls = self._urls_for_peer(leader, request.raw_path)
        if not candidate_urls:
            candidate_urls = [self._url_for_peer(leader, request.raw_path)]
        forward_headers: dict[str, str] = {}
        for h in ("Authorization", "X-RingRift-Auth", "Content-Type"):
            if h in request.headers:
                forward_headers[h] = request.headers[h]

        body: bytes | None = None
        if request.method not in ("GET", "HEAD", "OPTIONS"):
            body = await request.read()

        # Keep leader-proxy responsive: unreachable "leaders" (often NAT/firewall)
        # should fail fast so the dashboard doesn't hang for a full minute.
        timeout = ClientTimeout(total=10)
        last_exc: Exception | None = None
        async with get_client_session(timeout) as session:
            for target_url in candidate_urls:
                try:
                    async with session.request(
                        request.method,
                        target_url,
                        data=body,
                        headers=forward_headers,
                    ) as resp:
                        payload = await resp.read()
                        content_type = resp.headers.get("Content-Type")
                        headers: dict[str, str] = {}
                        if content_type:
                            headers["Content-Type"] = content_type
                        headers["X-RingRift-Proxied-By"] = self.node_id
                        headers["X-RingRift-Proxied-To"] = target_url
                        return web.Response(body=payload, status=resp.status, headers=headers)
                except Exception as exc:
                    last_exc = exc
                    continue

        return web.json_response(
            {
                "success": False,
                "error": "leader_proxy_failed",
                "message": str(last_exc) if last_exc else "unknown_error",
                "leader_id": self.leader_id,
                "attempted_urls": candidate_urls,
            },
            status=502,
        )

    def _is_request_authorized(self, request: web.Request) -> bool:
        if not self.auth_token:
            return True

        auth_header = request.headers.get("Authorization", "")
        token = ""
        if auth_header.lower().startswith("bearer "):
            token = auth_header[7:].strip()
        if not token:
            token = request.headers.get("X-RingRift-Auth", "").strip()
        if not token:
            return False

        return secrets.compare_digest(token, self.auth_token)

    def _init_database(self):
        """Initialize SQLite database for state persistence.

        Phase 1 Refactoring: Delegated to StateManager.
        Kept for backward compatibility with any external callers.
        """
        self.state_manager.init_database()

    def _db_connect(self) -> sqlite3.Connection:
        """Create a database connection with proper settings.

        Phase 1 Refactoring: Delegated to StateManager.
        Kept for backward compatibility with other internal methods.
        """
        return self.state_manager._db_connect()

    def _load_state(self):
        """Load persisted state from database.

        Phase 1 Refactoring: Delegated to StateManager.
        The StateManager returns a PersistedState object which is then
        applied to the orchestrator's instance variables.
        """
        try:
            state = self.state_manager.load_state(self.node_id)

            # P2P Hardening Phase 2 (Dec 2025): Validate and clean stale state
            is_valid, issues = self.state_manager.validate_loaded_state(state)
            if issues:
                # Clean up stale entries before applying state
                jobs_removed, peers_removed = self.state_manager.clean_stale_state(state)
                if self.verbose:
                    logger.info(
                        f"[P2POrchestrator] Startup cleanup: removed "
                        f"{jobs_removed} stale jobs, {peers_removed} stale peers"
                    )

            # Apply loaded peers
            for node_id, info_dict in state.peers.items():
                try:
                    info = NodeInfo.from_dict(info_dict)
                    self.peers[node_id] = info
                except Exception as e:  # noqa: BLE001
                    logger.error(f"Failed to load peer {node_id}: {e}")

            # Apply loaded jobs
            for job_dict in state.jobs:
                try:
                    job = ClusterJob(
                        job_id=job_dict["job_id"],
                        job_type=JobType(job_dict["job_type"]),
                        node_id=job_dict["node_id"],
                        board_type=job_dict.get("board_type", "square8"),
                        num_players=job_dict.get("num_players", 2),
                        engine_mode=job_dict.get("engine_mode", "descent-only"),
                        pid=job_dict.get("pid", 0),
                        started_at=job_dict.get("started_at", 0.0),
                        status=job_dict.get("status", "running"),
                    )
                    self.local_jobs[job.job_id] = job
                except Exception as e:  # noqa: BLE001
                    logger.error(f"Failed to load job: {e}")

            # Apply leader state
            ls = state.leader_state
            if ls.leader_id:
                self.leader_id = ls.leader_id
            if ls.leader_lease_id:
                self.leader_lease_id = ls.leader_lease_id
            if ls.leader_lease_expires:
                self.leader_lease_expires = ls.leader_lease_expires
            if ls.last_lease_renewal:
                self.last_lease_renewal = ls.last_lease_renewal
            if ls.role:
                with contextlib.suppress(Exception):
                    self.role = NodeRole(ls.role)

            # Voter grant state
            if ls.voter_grant_leader_id:
                self.voter_grant_leader_id = ls.voter_grant_leader_id
            if ls.voter_grant_lease_id:
                self.voter_grant_lease_id = ls.voter_grant_lease_id
            if ls.voter_grant_expires:
                self.voter_grant_expires = ls.voter_grant_expires

            # Phase 15.1.1: Restore fenced lease token state
            # These fields may not exist in older state files, so use getattr with defaults
            persisted_epoch = getattr(ls, "lease_epoch", 0) or 0
            persisted_fence = getattr(ls, "fence_token", "") or ""
            persisted_last_seen = getattr(ls, "last_seen_epoch", 0) or 0
            # Only restore if higher than current (monotonic guarantee)
            if persisted_epoch > self._lease_epoch:
                self._lease_epoch = persisted_epoch
            if persisted_fence and not self._fence_token:
                self._fence_token = persisted_fence
            if persisted_last_seen > self._last_seen_epoch:
                self._last_seen_epoch = persisted_last_seen
            if persisted_epoch > 0:
                logger.info(
                    f"[P2POrchestrator] Restored lease fencing: epoch={self._lease_epoch}, "
                    f"last_seen={self._last_seen_epoch}"
                )

            # Optional persisted voter configuration (convergence helper). Only
            # apply when voters are not explicitly configured via env/config.
            if (
                ls.voter_node_ids
                and not (getattr(self, "voter_node_ids", []) or [])
                and str(getattr(self, "voter_config_source", "none") or "none") == "none"
            ):
                self._maybe_adopt_voter_node_ids(ls.voter_node_ids, source="state")

            # Self-heal inconsistent persisted leader state (can happen after
            # abrupt shutdowns or partial writes): never keep role=leader without
            # a matching leader_id.
            if self.role == NodeRole.LEADER and not self.leader_id:
                logger.info("Loaded role=leader but leader_id is empty; stepping down to follower")
                self.role = NodeRole.FOLLOWER
                self.leader_lease_id = ""
                self.leader_lease_expires = 0.0
                self.last_lease_renewal = 0.0

            logger.info(f"Loaded state: {len(self.peers)} peers, {len(self.local_jobs)} jobs")

            # December 2025 P2P Hardening: Validate loaded state on startup
            # This detects stale jobs, stale peers, and expired leases
            is_valid, issues = self.state_manager.validate_loaded_state(state)
            if not is_valid:
                logger.warning(f"[P2P] Startup state validation found {len(issues)} issues:")
                for issue in issues:
                    logger.warning(f"  - {issue}")
                # Clean up stale entries
                stale_jobs_cleared = self.state_manager.clear_stale_jobs_by_age(max_age_hours=24.0)
                stale_peers_cleared = self.state_manager.clear_stale_peers(max_stale_seconds=300.0)
                if stale_jobs_cleared or stale_peers_cleared:
                    logger.info(f"[P2P] Cleared {stale_jobs_cleared} stale jobs, {stale_peers_cleared} stale peers")
            else:
                logger.info("[P2P] Startup state validation passed")

            # Dec 28, 2025 (Phase 7): Load persisted peer health state
            try:
                peer_health_states = self.state_manager.load_all_peer_health(max_age_seconds=3600.0)
                if peer_health_states:
                    self._apply_loaded_peer_health(peer_health_states)
                    logger.info(f"[P2P] Loaded {len(peer_health_states)} peer health records")
            except Exception as e:  # noqa: BLE001
                logger.warning(f"[P2P] Failed to load peer health state: {e}")

        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to load state: {e}")

    def _apply_loaded_peer_health(self, peer_health_states: dict) -> None:
        """Apply loaded peer health state to circuit breakers and gossip tracker.

        Dec 28, 2025 (Phase 7): Restores peer health history after restart.
        """
        try:
            # Try to get node circuit breaker for restoring circuit states
            from app.coordination.node_circuit_breaker import get_node_circuit_breaker

            breaker = get_node_circuit_breaker("health_check")
            restored_circuits = 0

            for node_id, health_state in peer_health_states.items():
                if node_id == self.node_id:
                    continue

                # Restore circuit breaker state if circuit was open
                if health_state.circuit_state == "open":
                    breaker.force_open(node_id)
                    restored_circuits += 1
                    logger.debug(
                        f"[P2P] Restored open circuit for {node_id} "
                        f"(failures: {health_state.failure_count})"
                    )

                # Update peer's last_seen if we have fresh data
                if node_id in self.peers and health_state.last_seen > 0:
                    peer = self.peers[node_id]
                    if hasattr(peer, "last_heartbeat"):
                        # Only update if our persisted data is fresher
                        if health_state.last_seen > (peer.last_heartbeat or 0):
                            peer.last_heartbeat = health_state.last_seen

            if restored_circuits > 0:
                logger.info(f"[P2P] Restored {restored_circuits} open circuit breakers from state")

            # Restore gossip health tracker state if available
            if hasattr(self, "_gossip_health_tracker"):
                for node_id, health_state in peer_health_states.items():
                    if health_state.gossip_failure_count >= 5:
                        # Mark as suspected in gossip tracker
                        for _ in range(health_state.gossip_failure_count):
                            self._gossip_health_tracker.record_gossip_failure(node_id)

        except ImportError:
            logger.debug("[P2P] Node circuit breaker not available for health state restoration")
        except Exception as e:  # noqa: BLE001
            logger.warning(f"[P2P] Error applying peer health state: {e}")

    def _collect_peer_health_states(self) -> list:
        """Collect peer health states from circuit breakers and gossip tracker.

        Dec 28, 2025 (Phase 7): Gathers health state for persistence.

        Returns:
            List of PeerHealthState objects to persist
        """
        from scripts.p2p.managers.state_manager import PeerHealthState

        health_states = []

        # Get circuit breaker states
        circuit_states = {}
        try:
            from app.coordination.node_circuit_breaker import get_node_circuit_breaker

            breaker = get_node_circuit_breaker("health_check")
            for node_id, status in breaker.get_all_states().items():
                circuit_states[node_id] = {
                    "state": status.state.value,
                    "failure_count": status.failure_count,
                    "opened_at": status.opened_at or 0.0,
                    "last_failure": status.last_failure_time or 0.0,
                }
        except ImportError:
            pass
        except Exception as e:  # noqa: BLE001
            if self.verbose:
                logger.debug(f"[P2P] Error collecting circuit states: {e}")

        # Get gossip health tracker states
        gossip_failures = {}
        try:
            if hasattr(self, "_gossip_health_tracker"):
                tracker = self._gossip_health_tracker
                for node_id in tracker.get_suspected_peers():
                    gossip_failures[node_id] = tracker.get_failure_count(node_id)
        except Exception as e:  # noqa: BLE001
            if self.verbose:
                logger.debug(f"[P2P] Error collecting gossip states: {e}")

        # Collect peer states
        with self.peers_lock:
            for node_id, peer in self.peers.items():
                if node_id == self.node_id:
                    continue

                # Determine peer state
                is_retired = getattr(peer, "retired", False)
                is_alive = peer.is_alive() if hasattr(peer, "is_alive") else True

                if is_retired:
                    peer_state = "retired"
                elif not is_alive:
                    peer_state = "dead"
                else:
                    peer_state = "alive"

                # Get circuit info
                circuit_info = circuit_states.get(node_id, {})
                gossip_fail_count = gossip_failures.get(node_id, 0)

                # Adjust state if circuit is open
                if circuit_info.get("state") == "open" and peer_state == "alive":
                    peer_state = "suspect"

                health_states.append(
                    PeerHealthState(
                        node_id=node_id,
                        state=peer_state,
                        failure_count=circuit_info.get("failure_count", 0),
                        gossip_failure_count=gossip_fail_count,
                        last_seen=getattr(peer, "last_heartbeat", 0.0) or 0.0,
                        last_failure=circuit_info.get("last_failure", 0.0),
                        circuit_state=circuit_info.get("state", "closed"),
                        circuit_opened_at=circuit_info.get("opened_at", 0.0),
                    )
                )

        return health_states

    def _save_state(self):
        """Save current state to database.

        Phase 1 Refactoring: Delegated to StateManager.
        Creates a PersistedLeaderState from instance variables and
        passes it to the StateManager for persistence.
        """
        try:
            # Build leader state from instance variables
            role_value = self.role.value if hasattr(self.role, "value") else str(self.role)
            leader_state = PersistedLeaderState(
                leader_id=self.leader_id or "",
                leader_lease_id=self.leader_lease_id or "",
                leader_lease_expires=float(self.leader_lease_expires or 0.0),
                last_lease_renewal=float(self.last_lease_renewal or 0.0),
                role=role_value,
                voter_grant_leader_id=str(getattr(self, "voter_grant_leader_id", "") or ""),
                voter_grant_lease_id=str(getattr(self, "voter_grant_lease_id", "") or ""),
                voter_grant_expires=float(getattr(self, "voter_grant_expires", 0.0) or 0.0),
                voter_node_ids=list(getattr(self, "voter_node_ids", []) or []),
                voter_config_source=str(getattr(self, "voter_config_source", "") or ""),
                # Phase 15.1.1: Fenced lease token state
                lease_epoch=int(getattr(self, "_lease_epoch", 0) or 0),
                fence_token=str(getattr(self, "_fence_token", "") or ""),
                last_seen_epoch=int(getattr(self, "_last_seen_epoch", 0) or 0),
            )

            # Delegate to StateManager
            self.state_manager.save_state(
                node_id=self.node_id,
                peers=self.peers,
                jobs=self.local_jobs,
                leader_state=leader_state,
                peers_lock=self.peers_lock,
                jobs_lock=self.jobs_lock,
            )

            # Dec 28, 2025 (Phase 7): Save peer health state
            try:
                peer_health_states = self._collect_peer_health_states()
                if peer_health_states:
                    saved = self.state_manager.save_peer_health_batch(peer_health_states)
                    if saved > 0 and self.verbose:
                        logger.debug(f"[P2P] Saved {saved} peer health records")
            except Exception as e:  # noqa: BLE001
                if self.verbose:
                    logger.debug(f"[P2P] Error saving peer health state: {e}")

        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to save state: {e}")

    # =========================================================================
    # Phase 27: Peer Cache and Reputation Tracking
    # Provided by PeerManagerMixin:
    # - _update_peer_reputation: EMA-based reputation updates
    # - _save_peer_to_cache: SQLite peer persistence with pruning
    # - _get_bootstrap_peers_by_reputation: Prioritized peer list for bootstrap
    # - _get_cached_peer_count, _clear_peer_cache, _prune_stale_peers
    # =========================================================================

    # =========================================================================
    # Phase 29: Cluster Epoch Persistence
    # Phase 1 Refactoring: Delegated to StateManager
    # =========================================================================

    def _load_cluster_epoch(self) -> None:
        """Load cluster epoch from database.

        Phase 1 Refactoring: Delegated to StateManager.
        Kept for backward compatibility.
        """
        self._cluster_epoch = self.state_manager.load_cluster_epoch()

    def _save_cluster_epoch(self) -> None:
        """Save cluster epoch to database.

        Phase 1 Refactoring: Delegated to StateManager.
        Kept for backward compatibility.
        """
        self.state_manager.set_cluster_epoch(self._cluster_epoch)
        self.state_manager.save_cluster_epoch()

    def _increment_cluster_epoch(self) -> None:
        """Increment cluster epoch (called on leader change).

        Phase 1 Refactoring: Delegated to StateManager.
        Kept for backward compatibility.
        """
        self._cluster_epoch = self.state_manager.increment_cluster_epoch()

    # Class-level metrics buffer (legacy, kept for backward compatibility)
    # Phase 1 Refactoring: Delegated to MetricsManager
    _metrics_buffer: list[tuple] = []
    _metrics_buffer_lock = threading.Lock()
    _metrics_last_flush: float = 0.0
    _metrics_flush_interval: float = 30.0
    _metrics_max_buffer: int = 100

    def record_metric(
        self,
        metric_type: str,
        value: float,
        board_type: str | None = None,
        num_players: int | None = None,
        metadata: dict[str, Any] | None = None,
    ):
        """Record a metric to the history table for observability.

        Phase 1 Refactoring: Delegated to MetricsManager.

        Metric types:
        - training_loss: NNUE training loss
        - elo_rating: Model Elo rating
        - gpu_utilization: GPU utilization percentage
        - selfplay_games_per_hour: Game generation rate
        - validation_rate: GPU selfplay validation rate
        - tournament_win_rate: Tournament win rate for new model
        """
        self.metrics_manager.record_metric(
            metric_type=metric_type,
            value=value,
            board_type=board_type,
            num_players=num_players,
            metadata=metadata,
        )

    def get_metrics_history(
        self,
        metric_type: str,
        board_type: str | None = None,
        num_players: int | None = None,
        hours: float = 24,
        limit: int = 1000,
    ) -> list[dict[str, Any]]:
        """Get metrics history for a specific metric type."""
        conn = None
        try:
            conn = sqlite3.connect(str(self.db_path), timeout=10.0)
            cursor = conn.cursor()

            since = time.time() - (hours * 3600)
            query = """
                SELECT timestamp, value, board_type, num_players, metadata
                FROM metrics_history
                WHERE metric_type = ? AND timestamp > ?
            """
            params: list[Any] = [metric_type, since]

            if board_type:
                query += " AND board_type = ?"
                params.append(board_type)
            if num_players:
                query += " AND num_players = ?"
                params.append(num_players)

            query += " ORDER BY timestamp DESC LIMIT ?"
            params.append(limit)

            cursor.execute(query, params)
            results = []
            for row in cursor.fetchall():
                results.append({
                    "timestamp": row[0],
                    "value": row[1],
                    "board_type": row[2],
                    "num_players": row[3],
                    "metadata": json.loads(row[4]) if row[4] else None,
                })
            return results
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to get metrics history: {e}")
            return []
        finally:
            if conn:
                conn.close()

    def get_metrics_summary(self, hours: float = 24) -> dict[str, Any]:
        """Get summary of all metrics over the specified time period."""
        conn = None
        try:
            conn = sqlite3.connect(str(self.db_path), timeout=10.0)
            cursor = conn.cursor()

            since = time.time() - (hours * 3600)

            cursor.execute("""
                SELECT metric_type, COUNT(*), AVG(value), MIN(value), MAX(value)
                FROM metrics_history
                WHERE timestamp > ?
                GROUP BY metric_type
            """, (since,))

            summary: dict[str, Any] = {}
            for row in cursor.fetchall():
                summary[row[0]] = {
                    "count": row[1],
                    "avg": row[2],
                    "min": row[3],
                    "max": row[4],
                }

            cursor.execute("""
                SELECT metric_type, value, timestamp
                FROM metrics_history m1
                WHERE timestamp = (
                    SELECT MAX(timestamp) FROM metrics_history m2
                    WHERE m2.metric_type = m1.metric_type
                )
            """)
            for row in cursor.fetchall():
                if row[0] in summary:
                    summary[row[0]]["latest"] = row[1]
                    summary[row[0]]["latest_time"] = row[2]

            return {"period_hours": hours, "since": since, "metrics": summary}
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to get metrics summary: {e}")
            return {}
        finally:
            if conn:
                conn.close()

    def _create_self_info(self) -> NodeInfo:
        """Create NodeInfo for this node."""
        # Detect GPU
        has_gpu, gpu_name = self._detect_gpu()

        cpu_count = int(os.cpu_count() or 0)

        # Detect memory
        memory_gb = self._detect_memory()

        # Detect capabilities based on hardware
        # Dec 2025: RINGRIFT_IS_COORDINATOR=true restricts to coordinator-only
        # Dec 29, 2025: Also check distributed_hosts.yaml for role/enabled flags
        is_coordinator = os.environ.get("RINGRIFT_IS_COORDINATOR", "").lower() in ("true", "1", "yes")

        # Check YAML config for this node's settings
        if not is_coordinator:
            try:
                from app.config.cluster_config import load_cluster_config
                config = load_cluster_config()
                nodes = config.get("nodes", {})
                node_cfg = nodes.get(self.node_id, {})
                # Check role or explicit enabled flags
                if node_cfg.get("role") == "coordinator":
                    is_coordinator = True
                    logger.info(f"[P2P] Node {self.node_id} is coordinator (from YAML)")
                elif node_cfg.get("selfplay_enabled") is False and node_cfg.get("training_enabled") is False:
                    is_coordinator = True
                    logger.info(f"[P2P] Node {self.node_id} has selfplay/training disabled (from YAML)")
            except Exception as e:
                logger.debug(f"[P2P] Could not load cluster config: {e}")

        if is_coordinator:
            capabilities = []  # Coordinator nodes don't run compute tasks
            logger.info("[P2P] Coordinator-only mode: no selfplay/training/cmaes capabilities")
        else:
            capabilities = ["selfplay"]
            if has_gpu:
                capabilities.extend(["training", "cmaes"])
            if memory_gb >= 64:
                capabilities.append("large_boards")

        info = NodeInfo(
            node_id=self.node_id,
            host=self.advertise_host,
            port=self.advertise_port,
            role=self.role,
            last_heartbeat=time.time(),
            cpu_count=cpu_count,
            has_gpu=has_gpu,
            gpu_name=gpu_name,
            memory_gb=memory_gb,
            capabilities=capabilities,
            version=self.build_version,
        )
        # Advertise an alternate mesh endpoint (Tailscale) for NAT traversal and
        # multi-path retries. Peers persist the observed reachable endpoint in
        # `host`/`port` but keep our `reported_host`/`reported_port` as an
        # additional candidate (see `_heartbeat_loop` multi-path retry).
        ts_ip = self._get_tailscale_ip()
        if ts_ip and ts_ip != info.host:
            info.reported_host = ts_ip
            # Use the actual listening port for mesh endpoints (port-mapped
            # advertise ports may not be reachable inside overlays).
            info.reported_port = int(self.port)
        return info

    # NOTE: _detect_gpu, _detect_memory, _get_local_ip, _get_tailscale_ip
    # delegated to ResourceDetectorMixin (Dec 28, 2025). ~75 LOC removed.
    # Use: self._detect_gpu(), self._detect_memory(), etc.

    # _is_tailscale_host provided by NetworkUtilsMixin

    def _local_has_tailscale(self) -> bool:
        """Best-effort: True when this node appears to have a Tailscale address."""
        try:
            info = getattr(self, "self_info", None)
            if not info:
                return False
            host = str(getattr(info, "host", "") or "").strip()
            reported_host = str(getattr(info, "reported_host", "") or "").strip()
            return self._is_tailscale_host(host) or self._is_tailscale_host(reported_host)
        except (AttributeError):
            return False

    # _get_tailscale_ip_for_peer: Provided by NetworkUtilsMixin

    def _detect_network_partition(self) -> bool:
        """Detect if we're in a network partition (>50% peers unreachable via primary IP).

        Used to trigger Tailscale-first connectivity mode when the public network
        is fragmented but mesh connectivity remains intact.

        Returns:
            True if partition detected (majority of peers unreachable)
        """
        with self.peers_lock:
            peers_snapshot = [p for p in self.peers.values() if p.node_id != self.node_id]

        if len(peers_snapshot) < 2:
            return False

        # Count peers with recent heartbeat failures
        now = time.time()
        unreachable = 0
        for peer in peers_snapshot:
            if peer.consecutive_failures >= 3 or (now - peer.last_heartbeat > PEER_TIMEOUT):
                unreachable += 1

        partition_ratio = unreachable / len(peers_snapshot)
        if partition_ratio > 0.5:
            logger.info(f"Network partition detected: {unreachable}/{len(peers_snapshot)} peers unreachable ({partition_ratio:.0%})")
            return True
        return False

    def _get_tailscale_priority_mode(self) -> bool:
        """Check if Tailscale-first mode is enabled (partition recovery)."""
        return getattr(self, "_tailscale_priority", False)

    def _enable_tailscale_priority(self) -> None:
        """Enable Tailscale-first mode for heartbeats during partition recovery."""
        if not getattr(self, "_tailscale_priority", False):
            logger.info("Enabling Tailscale-priority mode for partition recovery")
            self._tailscale_priority = True
            self._tailscale_priority_until = time.time() + 300  # 5 minutes

    def _disable_tailscale_priority(self) -> None:
        """Disable Tailscale-first mode when connectivity recovers."""
        if getattr(self, "_tailscale_priority", False):
            logger.info("Disabling Tailscale-priority mode (connectivity recovered)")
            self._tailscale_priority = False

    # =========================================================================
    # Network Health Methods (December 30, 2025)
    # Required by NetworkHealthMixin for cross-verification of P2P vs Tailscale
    # =========================================================================

    async def _get_tailscale_status(self) -> dict[str, bool]:
        """Query Tailscale status and return peer online status.

        Returns:
            Dict mapping Tailscale IP to online status {ip: is_online}
        """
        try:
            proc = await asyncio.create_subprocess_exec(
                "tailscale",
                "status",
                "--json",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
            stdout, stderr = await asyncio.wait_for(
                proc.communicate(),
                timeout=10.0,
            )

            if proc.returncode != 0:
                logger.debug(f"Tailscale status failed: {stderr.decode()[:100]}")
                return {}

            data = json.loads(stdout.decode())

            # Extract peer IPs and online status
            result: dict[str, bool] = {}
            for peer_key, peer_data in data.get("Peer", {}).items():
                is_online = peer_data.get("Online", False)
                # Extract Tailscale IPs
                for ip in peer_data.get("TailscaleIPs", []):
                    result[ip] = is_online

            return result

        except asyncio.TimeoutError:
            logger.debug("Tailscale status timed out")
            return {}
        except json.JSONDecodeError as e:
            logger.debug(f"Failed to parse Tailscale status JSON: {e}")
            return {}
        except FileNotFoundError:
            logger.debug("Tailscale command not found")
            return {}
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Error querying Tailscale status: {e}")
            return {}

    async def _reconnect_discovered_peer(
        self, node_id: str, host: str, port: int
    ) -> bool:
        """Attempt to reconnect to a peer discovered via Tailscale.

        Probes the peer's health endpoint and sends a heartbeat to establish
        P2P connection.

        Args:
            node_id: Peer node identifier
            host: Tailscale IP address
            port: P2P port (usually 8770)

        Returns:
            True if reconnection successful, False otherwise
        """
        try:
            # Probe health endpoint
            url = f"http://{host}:{port}/health"
            timeout = ClientTimeout(total=5)
            async with get_client_session(timeout) as session:
                async with session.get(url) as resp:
                    if resp.status != 200:
                        return False
                    data, error = await safe_json_response(resp, default={}, log_errors=False)
                    if error:
                        return False

            # Extract node_id from response if available
            actual_node_id = data.get("node_id", node_id)

            # Send heartbeat to establish connection
            await self._send_heartbeat_to_peer(host, port)

            # Check if peer is now in our peers dict
            async with AsyncLockWrapper(self.peers_lock):
                if actual_node_id not in self.peers or not self.peers[actual_node_id].is_alive():
                    # Register the peer
                    self.peers[actual_node_id] = PeerInfo(
                        node_id=actual_node_id,
                        host=host,
                        port=port,
                        last_heartbeat=time.time(),
                        state="alive",
                    )
                    logger.info(f"Reconnected peer via network health: {actual_node_id} ({host}:{port})")
                    await self._emit_host_online(actual_node_id)
                    return True

            return True  # Already connected

        except Exception as e:  # noqa: BLE001
            logger.debug(f"Failed to reconnect {node_id}: {e}")
            return False

    async def reconnect_missing_peers(self) -> list[str]:
        """Reconnect to all peers that are online in Tailscale but not in P2P.

        Returns:
            List of node IDs that were successfully reconnected
        """
        ts_peers = await self._get_tailscale_status()
        config_hosts = self._load_distributed_hosts().get("hosts", {})

        # Build IP to node mapping
        ip_to_node: dict[str, tuple[str, dict]] = {}
        for name, h in config_hosts.items():
            ts_ip = h.get("tailscale_ip")
            if ts_ip and h.get("p2p_enabled", True):
                ip_to_node[ts_ip] = (name, h)

        # Get current alive peer IDs
        current_ids: set[str] = set()
        with self.peers_lock:
            for peer in self.peers.values():
                if peer.is_alive():
                    current_ids.add(peer.node_id)

        # Find and reconnect missing peers
        reconnected: list[str] = []
        for ts_ip, is_online in ts_peers.items():
            if not is_online:
                continue

            if ts_ip not in ip_to_node:
                continue

            node_id, node_config = ip_to_node[ts_ip]

            # Skip if already connected
            if node_id in current_ids:
                continue

            # Skip self
            if node_id == self.node_id:
                continue

            # Attempt reconnection
            port = node_config.get("p2p_port", DEFAULT_PORT)
            if await self._reconnect_discovered_peer(node_id, ts_ip, port):
                reconnected.append(node_id)

        if reconnected:
            logger.info(f"Reconnected {len(reconnected)} missing peers: {reconnected}")

        return reconnected

    # =========================================================================
    # Partition Read-Only Mode (Phase 2.4 - Dec 29, 2025)
    # =========================================================================

    def _check_partition_mode(self) -> None:
        """Check partition status and enable/disable read-only mode.

        December 2025 (Phase 2.4): Prevent data divergence during network partitions.

        When this node is in a minority partition (<50% of peers alive):
        - Pause training job dispatch
        - Pause selfplay job dispatch
        - Continue serving existing data (read-only)
        - Allow sync operations to help recovery

        This prevents split-brain scenarios where both partitions continue
        generating training data that later conflicts during merge.
        """
        now = time.time()

        # Rate limit partition checks
        if now - self._last_partition_check < self._partition_check_interval:
            return
        self._last_partition_check = now

        # Use gossip protocol's partition detection
        status, ratio = self.detect_partition_status()

        if status in ("minority", "isolated"):
            if not self._partition_readonly_mode:
                logger.warning(
                    f"[P2P] Entering partition read-only mode: "
                    f"status={status}, health_ratio={ratio:.2%}"
                )
                self._partition_readonly_mode = True
                self._partition_readonly_since = now

                # Emit event for monitoring
                self._safe_emit_event("PARTITION_READONLY_ENTERED", {
                    "node_id": self.node_id,
                    "status": status,
                    "health_ratio": ratio,
                    "timestamp": now,
                })
        else:
            if self._partition_readonly_mode:
                readonly_duration = now - self._partition_readonly_since
                logger.info(
                    f"[P2P] Exiting partition read-only mode: "
                    f"status={status}, health_ratio={ratio:.2%}, "
                    f"was_readonly_for={readonly_duration:.0f}s"
                )
                self._partition_readonly_mode = False
                self._partition_readonly_since = 0.0

                # Emit event for monitoring
                self._safe_emit_event("PARTITION_READONLY_EXITED", {
                    "node_id": self.node_id,
                    "status": status,
                    "health_ratio": ratio,
                    "readonly_duration_seconds": readonly_duration,
                    "timestamp": now,
                })

    def is_partition_readonly(self) -> bool:
        """Check if this node is in partition read-only mode.

        December 2025 (Phase 2.4): Query method for dispatch gates.

        Returns:
            True if job dispatch should be paused due to partition status.
        """
        # Do a fresh check if it's been a while
        self._check_partition_mode()
        return self._partition_readonly_mode

    def get_partition_status(self) -> dict[str, Any]:
        """Get current partition status details.

        December 2025 (Phase 2.4): Status API for monitoring/debugging.

        Returns:
            Dict with partition status, mode, and duration.
        """
        status, ratio = self.detect_partition_status()
        now = time.time()

        result = {
            "partition_status": status,
            "health_ratio": round(ratio, 3),
            "readonly_mode": self._partition_readonly_mode,
            "readonly_since": self._partition_readonly_since,
            "readonly_duration_seconds": (
                now - self._partition_readonly_since
                if self._partition_readonly_mode else 0.0
            ),
            "last_check": self._last_partition_check,
        }

        # Add detailed peer info if available
        if hasattr(self, "get_partition_details"):
            result["details"] = self.get_partition_details()

        return result

    # _tailscale_urls_for_voter: Provided by NetworkUtilsMixin

    # NOTE: _is_in_startup_grace_period, _get_resource_usage, _check_nfs_accessible,
    # _detect_local_external_work delegated to ResourceDetectorMixin (Dec 28, 2025).
    # ~170 LOC removed. Use: self._is_in_startup_grace_period(), self._get_resource_usage(), etc.

    # NOTE: _get_diversity_metrics() and _track_selfplay_diversity() removed.
    # Delegated to self.selfplay_scheduler.get_diversity_metrics() and
    # self.selfplay_scheduler.track_diversity(). Removed ~66 LOC Dec 2025.

    def _count_local_jobs(self) -> tuple[int, int]:
        """Count running selfplay and training jobs on this node."""
        def _pid_alive(pid: int) -> bool:
            try:
                os.kill(pid, 0)
                return True
            except ProcessLookupError:
                return False
            except PermissionError:
                return True
            except (AttributeError):
                return False

        # Primary source of truth: jobs we started and are tracking.
        selfplay_pids: set[str] = set()
        training_pids: set[str] = set()

        stale_job_ids: list[str] = []
        try:
            with self.jobs_lock:
                jobs_snapshot = list(self.local_jobs.items())
            for job_id, job in jobs_snapshot:
                if job.status != "running":
                    continue
                pid = int(job.pid or 0)
                if pid <= 0:
                    continue
                if not _pid_alive(pid):
                    stale_job_ids.append(job_id)
                    continue
                if job.job_type in (JobType.SELFPLAY, JobType.GPU_SELFPLAY, JobType.HYBRID_SELFPLAY, JobType.CPU_SELFPLAY, JobType.GUMBEL_SELFPLAY):
                    selfplay_pids.add(str(pid))
                elif job.job_type == JobType.TRAINING:
                    training_pids.add(str(pid))

            if stale_job_ids:
                with self.jobs_lock:
                    for job_id in stale_job_ids:
                        self.local_jobs.pop(job_id, None)
        except (ValueError, AttributeError):
            pass

        # Secondary check: best-effort process scan for untracked jobs (e.g. manual runs).
        # IMPORTANT: never return (0,0) just because `pgrep` is missing or fails;
        # that can cause the leader to spawn runaway selfplay processes until disk fills.
        try:
            import shutil

            if shutil.which("pgrep"):
                # December 2025: Added selfplay.py pattern - the current unified selfplay entry point
                # December 2025: Added gumbel_selfplay and SelfplayRunner patterns for module invocations
                for pattern in (
                    "selfplay.py",
                    "run_self_play_soak.py",
                    "run_gpu_selfplay.py",
                    "run_hybrid_selfplay.py",
                    "gumbel_selfplay",  # screen session name
                    "SelfplayRunner",   # class-based invocation
                    "selfplay_runner",  # module invocation
                    "-m app.training.selfplay",  # module mode
                ):
                    out = subprocess.run(
                        ["pgrep", "-f", pattern],
                        capture_output=True,
                        text=True,
                        timeout=5,
                    )
                    if out.returncode == 0 and out.stdout.strip():
                        selfplay_pids.update([p for p in out.stdout.strip().split() if p])

                for pattern in ("train_", "train.py", "-m app.training.train"):
                    out = subprocess.run(
                        ["pgrep", "-f", pattern],
                        capture_output=True,
                        text=True,
                        timeout=5,
                    )
                    if out.returncode == 0 and out.stdout.strip():
                        training_pids.update([p for p in out.stdout.strip().split() if p])
        except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError, KeyError, IndexError, AttributeError, ImportError):
            pass

        return len(selfplay_pids), len(training_pids)

    def _cleanup_stale_processes(self) -> int:
        """Kill processes that have been running too long.

        Scans for known process patterns (tournaments, gauntlets, selfplay)
        and kills any that exceed their maximum runtime threshold.

        Returns:
            Number of processes killed.
        """
        import shutil

        if not shutil.which("pgrep") or not shutil.which("ps"):
            return 0

        killed_count = 0
        time.time()

        # Map patterns to their max runtimes
        # December 2025: Added selfplay.py - the current unified selfplay entry point
        pattern_max_runtime = {
            "run_model_elo_tournament.py": MAX_TOURNAMENT_RUNTIME,
            "run_gauntlet.py": MAX_GAUNTLET_RUNTIME,
            "selfplay.py": MAX_SELFPLAY_RUNTIME,  # Unified selfplay script
            "run_self_play_soak.py": MAX_SELFPLAY_RUNTIME,
            "run_gpu_selfplay.py": MAX_SELFPLAY_RUNTIME,
            "run_hybrid_selfplay.py": MAX_SELFPLAY_RUNTIME,
            "run_diverse_selfplay.py": MAX_SELFPLAY_RUNTIME,
            "train_nnue.py": MAX_TRAINING_RUNTIME,
            "train.py": MAX_TRAINING_RUNTIME,
        }

        for pattern, max_runtime in pattern_max_runtime.items():
            try:
                # Get PIDs matching the pattern
                pgrep_result = subprocess.run(
                    ["pgrep", "-f", pattern],
                    capture_output=True,
                    text=True,
                    timeout=5,
                )
                if pgrep_result.returncode != 0 or not pgrep_result.stdout.strip():
                    continue

                pids = [p.strip() for p in pgrep_result.stdout.strip().split() if p.strip()]

                for pid in pids:
                    try:
                        # Get process start time using ps
                        ps_result = subprocess.run(
                            ["ps", "-o", "etimes=", "-p", pid],
                            capture_output=True,
                            text=True,
                            timeout=5,
                        )
                        if ps_result.returncode != 0:
                            continue

                        elapsed_seconds = int(ps_result.stdout.strip())

                        if elapsed_seconds > max_runtime:
                            # Process has exceeded max runtime - kill it
                            logger.warning(
                                f"Killing stale process {pid} ({pattern}): "
                                f"running for {elapsed_seconds/3600:.1f}h, max={max_runtime/3600:.1f}h"
                            )
                            subprocess.run(
                                ["kill", "-9", pid],
                                capture_output=True,
                                timeout=5,
                            )
                            killed_count += 1

                            # Send alert
                            if hasattr(self, 'notifier') and self.notifier:
                                asyncio.create_task(
                                    self.notifier.send(
                                        title="Stale Process Killed",
                                        message=f"Killed {pattern} (PID {pid}) after {elapsed_seconds/3600:.1f} hours",
                                        level="warning",
                                        node_id=self.node_id,
                                    )
                                )

                    except (ValueError, subprocess.TimeoutExpired):
                        continue

            except Exception as e:  # noqa: BLE001
                logger.debug(f"Error checking pattern {pattern}: {e}")
                continue

        if killed_count > 0:
            logger.info(f"Stale process cleanup: killed {killed_count} processes")

        return killed_count

    # ============================================
    # Phase 2: Distributed Data Sync Methods
    # ============================================

    def _collect_local_data_manifest(self) -> NodeDataManifest:
        """Collect manifest of all data files on this node.

        REFACTORED (Dec 2025): Delegates to SyncPlanner.collect_local_manifest().
        See scripts/p2p/managers/sync_planner.py for implementation.

        Scans the data directory for:
        - selfplay/ - Game replay files (.jsonl, .db)
        - models/ - Trained model files (.pt, .onnx)
        - training/ - Training data files (.npz)
        - games/ - Synced game databases (.db)

        Uses get_data_directory() to support both disk and ramdrive storage.
        """
        # Phase 2A: Delegate to SyncPlanner (Dec 2025)
        # This eliminates ~150 lines of duplicate code
        return self.sync_planner.collect_local_manifest(use_cache=False)

    # NOTE: _collect_local_data_manifest_legacy() removed Dec 27, 2025
    # (150 LOC dead code - was never called, SyncPlanner.collect_local_manifest used instead)
    # NOTE: _compute_file_hash() removed Dec 28, 2025
    # (12 LOC dead code - zero callers, orphaned legacy method)

    def _request_peer_manifest_sync(self, peer_id: str) -> NodeDataManifest | None:
        """Synchronous wrapper for requesting peer manifest.

        Used by SyncPlanner which expects a sync callback.
        Runs the async version in a new event loop.

        Args:
            peer_id: The peer's node ID to request from

        Returns:
            NodeDataManifest or None if request failed
        """
        # Look up peer info
        with self.peers_lock:
            peer_info = self.peers.get(peer_id)

        if not peer_info:
            logger.debug(f"Peer {peer_id} not found in peers dict")
            return None

        # Run async version in event loop
        try:
            loop = asyncio.get_running_loop()
            # If we're in an async context, use run_coroutine_threadsafe
            import concurrent.futures
            future = asyncio.run_coroutine_threadsafe(
                self._request_peer_manifest(peer_info), loop
            )
            return future.result(timeout=15)
        except RuntimeError:
            # No running loop - use asyncio.run
            try:
                return asyncio.run(self._request_peer_manifest(peer_info))
            except Exception as e:  # noqa: BLE001
                logger.debug(f"Failed to request manifest from {peer_id}: {e}")
                return None
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Failed to request manifest from {peer_id}: {e}")
            return None

    async def _request_peer_manifest(self, peer_info: NodeInfo) -> NodeDataManifest | None:
        """Request data manifest from a peer node."""
        try:
            # Keep manifest requests snappy: these are advisory and should not
            # stall leader loops or external callers (e.g. the improvement
            # daemon). Prefer faster failure and rely on periodic retries.
            timeout = ClientTimeout(total=10, sock_connect=3, sock_read=7)
            async with get_client_session(timeout) as session:
                for url in self._urls_for_peer(peer_info, "/data_manifest"):
                    try:
                        async with session.get(url, headers=self._auth_headers()) as resp:
                            if resp.status != 200:
                                continue
                            data = await resp.json()
                        return NodeDataManifest.from_dict((data or {}).get("manifest", {}))
                    except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
                        continue
        except Exception as e:  # noqa: BLE001
            logger.error(f"requesting manifest from {peer_info.node_id}: {e}")
        return None

    # =========================================================================
    # Manifest Cache Methods - MOVED to SyncPlanner (Dec 2025)
    # =========================================================================
    # The following methods were moved to scripts/p2p/managers/sync_planner.py:
    # - get_manifest_cache_path() - disk cache path
    # - save_manifest_to_cache() - persist manifest
    # - load_manifest_from_cache() - load cached manifest
    # - collect_local_manifest_cached() - collect with disk caching
    # Access via: self.sync_planner.<method_name>()

    async def _collect_cluster_manifest(self) -> ClusterDataManifest:
        """Leader-only: Collect manifests from all peers and build cluster view."""
        cluster_manifest = ClusterDataManifest(
            collected_at=time.time(),
        )

        # Collect from self
        local_manifest = await asyncio.to_thread(self._collect_local_data_manifest)
        with self.manifest_lock:
            self.local_data_manifest = local_manifest
        cluster_manifest.node_manifests[self.node_id] = local_manifest

        # Collect from peers in parallel.
        #
        # Only probe peers that are currently alive and not retired; terminated
        # or long-dead nodes should not stall manifest collection. NAT-blocked
        # peers can't accept inbound /data_manifest, so they are excluded too.
        with self.peers_lock:
            peers = [
                p
                for p in self.peers.values()
                if p.is_alive()
                and not bool(getattr(p, "retired", False))
                and not bool(getattr(p, "nat_blocked", False))
            ]

        tasks = [self._request_peer_manifest(peer) for peer in peers]
        # December 2025: Add timeout to prevent hang if peers are unresponsive
        # Individual requests have 10s timeout, but aggregate needs overall limit
        # to prevent blocking leader loop. 45s covers ~30 peers with some slack.
        try:
            results = await asyncio.wait_for(
                asyncio.gather(*tasks, return_exceptions=True),
                timeout=45.0
            )
        except asyncio.TimeoutError:
            logger.warning(
                f"[ManifestCollection] Timed out after 45s collecting from {len(peers)} peers. "
                "Proceeding with partial data."
            )
            results = []  # Proceed with only local manifest

        for peer, result in zip(peers, results, strict=False):
            if isinstance(result, NodeDataManifest):
                cluster_manifest.node_manifests[peer.node_id] = result

        # Compute cluster-wide statistics
        cluster_manifest.total_nodes = len(cluster_manifest.node_manifests)

        all_files: set[str] = set()
        for node_id, node_manifest in cluster_manifest.node_manifests.items():
            cluster_manifest.total_files += node_manifest.total_files
            cluster_manifest.total_size_bytes += node_manifest.total_size_bytes
            cluster_manifest.total_selfplay_games += node_manifest.selfplay_games
            cluster_manifest.files_by_node[node_id] = node_manifest.total_files

            for file_info in node_manifest.files:
                all_files.add(file_info.path)

        cluster_manifest.unique_files = all_files

        # Find files missing from nodes (for sync planning)
        for file_path in all_files:
            nodes_with_file = []
            nodes_without_file = []
            for node_id, node_manifest in cluster_manifest.node_manifests.items():
                file_paths = {f.path for f in node_manifest.files}
                if file_path in file_paths:
                    nodes_with_file.append(node_id)
                else:
                    nodes_without_file.append(node_id)

            if nodes_without_file:
                cluster_manifest.missing_from_nodes[file_path] = nodes_without_file

        logger.info(f"Cluster manifest: {cluster_manifest.total_nodes} nodes, "
              f"{len(cluster_manifest.unique_files)} unique files, "
              f"{cluster_manifest.total_selfplay_games} total games")

        return cluster_manifest

    # ============================================
    # Phase 2: P2P Rsync Coordination Methods
    # ============================================

    # NOTE: _generate_sync_plan() removed Dec 2025 (61 LOC dead code).
    # Use self.sync_planner.generate_sync_plan(cluster_manifest) instead.

    async def _execute_sync_plan(self) -> None:
        """Leader executes the sync plan by dispatching jobs to nodes.

        Delegates to SyncPlanner.execute_sync_plan() with _request_node_sync as callback.
        Dec 2025: Refactored to delegate to SyncPlanner for consolidated logic.
        """
        if not self.current_sync_plan:
            return

        # Delegate to SyncPlanner with our network request callback
        result = await self.sync_planner.execute_sync_plan(
            plan=self.current_sync_plan,
            execute_job_callback_async=self._request_node_sync,
        )

        # Update local state from SyncPlanner result
        with self.sync_lock:
            self.last_sync_time = time.time()

        if not result.get("success", False):
            logger.warning(f"Sync plan execution issue: {result.get('error', 'unknown')}")

    async def _request_node_sync(self, job: DataSyncJob) -> bool:
        """Request a target node to pull files from a source node."""
        target_peer = self.peers.get(job.target_node)
        if job.target_node == self.node_id:
            target_peer = self.self_info

        source_peer = self.peers.get(job.source_node)
        if job.source_node == self.node_id:
            source_peer = self.self_info

        if not target_peer or not source_peer:
            job.status = "failed"
            job.error_message = "Source or target peer not found"
            return False

        job.status = "running"
        job.started_at = time.time()

        try:
            # Local target: execute the pull directly (no HTTP round-trip).
            if job.target_node == self.node_id:
                result = await self._handle_sync_pull_request(
                    source_host=source_peer.host,
                    source_port=source_peer.port,
                    source_reported_host=(getattr(source_peer, "reported_host", "") or None),
                    source_reported_port=(getattr(source_peer, "reported_port", 0) or None),
                    source_node_id=job.source_node,
                    files=job.files,
                )
            else:
                payload = {
                    "job_id": job.job_id,
                    # Back-compat: target will prefer source_node_id lookup.
                    "source_host": source_peer.host,
                    "source_port": source_peer.port,
                    "source_node_id": job.source_node,
                    "files": job.files,
                }
                rh = (getattr(source_peer, "reported_host", "") or "").strip()
                rp = int(getattr(source_peer, "reported_port", 0) or 0)
                if rh and rp and (rh != source_peer.host or rp != source_peer.port):
                    payload["source_reported_host"] = rh
                    payload["source_reported_port"] = rp

                timeout = ClientTimeout(total=600)
                async with get_client_session(timeout) as session:
                    result = None
                    last_err: str | None = None
                    for url in self._urls_for_peer(target_peer, "/sync/pull"):
                        try:
                            async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                                if resp.status != 200:
                                    last_err = f"http_{resp.status}"
                                    continue
                                result = await resp.json()
                                break
                        except Exception as e:  # noqa: BLE001
                            last_err = str(e)
                            continue
                    if result is None:
                        job.status = "failed"
                        job.error_message = last_err or "sync_pull_failed"
                        # Note: SyncPlanner tracks jobs_failed count
                        return False

            ok = bool(result.get("success"))
            job.status = "completed" if ok else "failed"
            job.completed_at = time.time()
            job.bytes_transferred = int(result.get("bytes_transferred", 0) or 0)
            job.files_completed = int(result.get("files_completed", 0) or 0)
            if not ok:
                job.error_message = str(result.get("error") or "Unknown error")

            # Note: SyncPlanner tracks jobs_completed/jobs_failed counts

            if ok:
                logger.info(f"Sync job {job.job_id[:8]} completed: {job.source_node} -> {job.target_node}")
            else:
                logger.info(f"Sync job {job.job_id[:8]} failed: {job.error_message}")

            return ok

        except Exception as e:  # noqa: BLE001
            job.status = "failed"
            job.error_message = str(e)
            job.completed_at = time.time()
            # Note: SyncPlanner tracks jobs_failed count
            logger.info(f"Sync job {job.job_id[:8]} failed: {e}")
            return False

    async def _handle_sync_pull_request(
        self,
        source_host: str,
        source_port: int,
        source_node_id: str,
        files: list[str],
        source_reported_host: str | None = None,
        source_reported_port: int | None = None,
    ) -> dict[str, Any]:
        """
        Handle incoming request to pull files from a source node.
        Pulls files over the P2P HTTP channel to avoid SSH/rsync dependencies.
        Uses get_data_directory() to support both disk and ramdrive storage.
        """
        # Check disk capacity before pulling files
        has_capacity, disk_percent = check_disk_has_capacity()
        if not has_capacity:
            return {
                "success": False,
                "error": f"Disk full ({disk_percent:.1f}% >= {MAX_DISK_USAGE_PERCENT}%)",
                "disk_percent": disk_percent,
                "bytes_transferred": 0,
                "files_completed": 0,
            }

        data_dir = self.get_data_directory()
        data_dir.mkdir(parents=True, exist_ok=True)

        bytes_transferred = 0
        files_completed = 0
        errors: list[str] = []

        # Multi-path sources: prefer observed endpoint but allow a self-reported
        # endpoint (e.g. Tailscale) when the public route fails.
        candidate_sources: list[tuple[str, int]] = []
        seen_sources: set[tuple[str, int]] = set()

        def _add_source(host: str | None, port: int | None) -> None:
            if not host:
                return
            h = str(host).strip()
            if not h:
                return
            try:
                p = int(port or 0)
            except (ValueError):
                return
            if p <= 0:
                return
            key = (h, p)
            if key in seen_sources:
                return
            seen_sources.add(key)
            candidate_sources.append(key)

        _add_source(source_host, source_port)
        _add_source(source_reported_host, source_reported_port)

        timeout = ClientTimeout(total=None, sock_connect=HTTP_CONNECT_TIMEOUT, sock_read=600)

        async with get_client_session(timeout) as session:
            for rel_path in files:
                rel_path = (rel_path or "").lstrip("/")
                if not rel_path:
                    errors.append("empty_path")
                    continue

                # Security: keep all writes within ai-service/data.
                dest_path = (data_dir / rel_path)
                try:
                    data_root = data_dir.resolve()
                    dest_resolved = dest_path.resolve()
                    dest_resolved.relative_to(data_root)
                except (AttributeError):
                    errors.append(f"invalid_path:{rel_path}")
                    continue

                dest_path.parent.mkdir(parents=True, exist_ok=True)
                tmp_path = dest_path.with_name(dest_path.name + ".partial")

                last_err: str | None = None
                success = False

                for host, base_port in candidate_sources:
                    # Back-compat: if caller passed an SSH-like port (22), try DEFAULT_PORT too.
                    ports_to_try: list[int] = []
                    try:
                        ports_to_try.append(int(base_port))
                    except (ValueError, AttributeError):
                        ports_to_try.append(DEFAULT_PORT)
                    if DEFAULT_PORT not in ports_to_try:
                        ports_to_try.append(DEFAULT_PORT)

                    for port in ports_to_try:
                        url = f"http://{host}:{port}/sync/file"
                        try:
                            async with session.get(
                                url,
                                params={"path": rel_path},
                                headers=self._auth_headers(),
                            ) as resp:
                                if resp.status != 200:
                                    text = ""
                                    try:
                                        text = (await resp.text())[:200]
                                    except (KeyError, IndexError, AttributeError):
                                        text = ""
                                    last_err = f"{resp.status} {text}".strip()
                                    continue

                                with open(tmp_path, "wb") as out_f:
                                    async for chunk in resp.content.iter_chunked(1024 * 1024):
                                        out_f.write(chunk)
                                        bytes_transferred += len(chunk)

                                tmp_path.replace(dest_path)
                                files_completed += 1
                                success = True
                                break

                        except Exception as e:  # noqa: BLE001
                            last_err = str(e)
                            continue
                    if success:
                        break

                if not success:
                    errors.append(f"{rel_path}: {last_err or 'download_failed'}")
                    try:
                        if tmp_path.exists():
                            tmp_path.unlink()
                    except (OSError, AttributeError):
                        pass

        if errors:
            return {
                "success": False,
                "files_completed": files_completed,
                "bytes_transferred": bytes_transferred,
                "error": "; ".join(errors[:5]),
            }

        return {
            "success": True,
            "files_completed": files_completed,
            "bytes_transferred": bytes_transferred,
        }

    async def start_cluster_sync(self) -> dict[str, Any]:
        """
        Leader initiates a full cluster data sync.
        Returns status of the sync operation.
        """
        if not self._is_leader():
            return {"success": False, "error": "Not the leader"}

        # First, collect fresh manifests
        logger.info("Collecting cluster manifest for sync...")
        self.cluster_data_manifest = await self._collect_cluster_manifest()

        # Generate sync plan (using SyncPlanner manager for consolidated logic)
        self.current_sync_plan = self.sync_planner.generate_sync_plan(self.cluster_data_manifest)
        if not self.current_sync_plan:
            return {"success": True, "message": "No sync needed, all nodes in sync"}

        # Execute the plan
        await self._execute_sync_plan()

        return {
            "success": True,
            "plan_id": self.current_sync_plan.plan_id,
            "total_jobs": len(self.current_sync_plan.sync_jobs),
            "jobs_completed": self.current_sync_plan.jobs_completed,
            "jobs_failed": self.current_sync_plan.jobs_failed,
            "status": self.current_sync_plan.status,
        }

    # ============================================
    # NodeSelector Wrapper Methods REMOVED (Dec 2025)
    # All call sites now use self.node_selector.* directly
    # ============================================

    async def _dispatch_gauntlet_to_cpu_node(
        self,
        config_key: str,
        model_id: str,
        baseline_id: str,
        games_per_side: int = 4
    ) -> dict[str, Any] | None:
        """Dispatch gauntlet evaluation to a CPU-rich node.

        This ensures gauntlets run on Vast instances with high CPU count
        rather than blocking GPU-rich nodes like GH200/H100.
        """
        cpu_node = self.node_selector.get_best_cpu_node_for_gauntlet()
        if not cpu_node:
            logger.info("No CPU node available for gauntlet, running locally")
            return None

        # If we're already the best CPU node, return None to run locally
        if cpu_node.node_id == self.node_id:
            return None

        try:
            # Dispatch to the CPU node's gauntlet endpoint
            payload = {
                "config_key": config_key,
                "model_id": model_id,
                "baseline_id": baseline_id,
                "games_per_side": games_per_side,
            }

            timeout = ClientTimeout(total=120)
            async with get_client_session(timeout) as session:
                for url in self._urls_for_peer(cpu_node, "/gauntlet/quick-eval"):
                    try:
                        async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                            if resp.status == 200:
                                result = await resp.json()
                                logger.info(f"Gauntlet dispatched to {cpu_node.node_id} "
                                      f"(cpu_power={cpu_node.cpu_power_score()})")
                                return result
                    except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
                        continue

            logger.error(f"Failed to dispatch gauntlet to {cpu_node.node_id}")
            return None
        except Exception as e:  # noqa: BLE001
            logger.error(f"dispatching gauntlet: {e}")
            return None

    def _should_sync_to_node(self, node: NodeInfo) -> bool:
        """Check if we should sync data TO this node based on disk space."""
        # Don't sync to nodes with critical disk usage
        if node.disk_percent >= DISK_CRITICAL_THRESHOLD:
            logger.info(f"Skipping sync to {node.node_id}: disk critical ({node.disk_percent:.1f}%)")
            return False
        # Warn but allow sync to nodes with warning-level disk
        if node.disk_percent >= DISK_WARNING_THRESHOLD:
            logger.warning(f"{node.node_id} disk at {node.disk_percent:.1f}%")
        return True

    def _should_cleanup_source(self, node: NodeInfo) -> bool:
        """Check if source node needs disk cleanup after sync."""
        return node.disk_percent >= DISK_CLEANUP_THRESHOLD

    async def _cleanup_synced_files(self, node_id: str, files: list[str]) -> bool:
        """Delete synced files from source node to free disk space.

        Only called after successful sync to training nodes.
        """
        with self.peers_lock:
            node = self.peers.get(node_id)
        if not node or not node.is_alive():
            return False

        try:
            if getattr(node, "nat_blocked", False):
                cmd_id = await self._enqueue_relay_command_for_peer(
                    node,
                    "cleanup_files",
                    {"files": list(files or []), "reason": "post_sync_cleanup"},
                )
                if cmd_id:
                    logger.info(f"Enqueued relay cleanup_files for {node_id} ({len(files)} files)")
                    return True
                logger.info(f"Relay queue full for {node_id}; skipping cleanup_files enqueue")
                return False

            timeout = ClientTimeout(total=60)
            async with get_client_session(timeout) as session:
                last_err: str | None = None
                for url in self._urls_for_peer(node, "/cleanup/files"):
                    try:
                        async with session.post(
                            url,
                            json={"files": files, "reason": "post_sync_cleanup"},
                            headers=self._auth_headers(),
                        ) as resp:
                            if resp.status != 200:
                                last_err = f"http_{resp.status}"
                                continue
                            result = await resp.json()
                            freed_bytes = result.get("freed_bytes", 0)
                            logger.info(f"Cleanup on {node_id}: freed {freed_bytes / 1e6:.1f}MB")
                            return True
                    except Exception as e:  # noqa: BLE001
                        last_err = str(e)
                        continue
                if last_err:
                    logger.info(f"Cleanup files request failed on {node_id}: {last_err}")
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to cleanup files on {node_id}: {e}")
        return False

    async def _sync_selfplay_to_training_nodes(self) -> dict[str, Any]:
        """Sync selfplay data to training primary nodes.

        December 2025: Delegated to SyncPlanner.sync_selfplay_to_training_nodes()
        """
        if not self._is_leader():
            return {"success": False, "error": "Not the leader"}

        # Use stale manifest if available, otherwise will be collected fresh
        manifest = self.cluster_data_manifest
        if (time.time() - self.last_manifest_collection > self.manifest_collection_interval
                or not manifest):
            manifest = None  # Will be collected by SyncPlanner

        result = await self.sync_planner.sync_selfplay_to_training_nodes(
            get_training_nodes=self.node_selector.get_training_primary_nodes,
            should_sync_to_node=self._should_sync_to_node,
            should_cleanup_source=self._should_cleanup_source,
            collect_manifest=self._collect_cluster_manifest,
            execute_sync_job=self._request_node_sync,
            cleanup_synced_files=self._cleanup_synced_files,
            get_sync_router=self._get_sync_router,
            cluster_manifest=manifest,
        )

        # Update orchestrator state
        if result.get("success"):
            self.last_training_sync_time = time.time()
            # Refresh manifest after sync
            if not manifest:
                # Dec 2025: Add 5-minute timeout for manifest collection
                try:
                    self.cluster_data_manifest = await asyncio.wait_for(
                        self._collect_cluster_manifest(),
                        timeout=300.0  # 5 minutes max
                    )
                    self.last_manifest_collection = time.time()
                except asyncio.TimeoutError:
                    logger.warning("Post-sync manifest collection timed out after 5 minutes")

        return result

    async def _execute_pending_sync_jobs(self):
        """Execute all pending sync jobs."""
        with self.sync_lock:
            pending_jobs = [
                job for job in self.active_sync_jobs.values()
                if job.status == "pending"
            ]

        for job in pending_jobs:
            try:
                success = await self._request_node_sync(job)
                if success:
                    job.status = "completed"
                    job.completed_at = time.time()
                else:
                    job.status = "failed"
            except Exception as e:  # noqa: BLE001
                logger.info(f"Sync job {job.job_id} failed: {e}")
                job.status = "failed"
                job.error_message = str(e)

    # NOTE: _training_sync_loop() removed Dec 2025 (72 LOC).
    # Now runs via LoopManager as TrainingSyncLoop.
    # See scripts/p2p/loops/training_sync_loop.py for implementation.
    # The loop calls self._sync_selfplay_to_training_nodes() via callback.

    async def _force_ip_refresh_all_sources(self) -> int:
        """Force immediate refresh of IPs from all CLI sources (Tailscale, Vast, AWS).

        Called when network partition is detected to aggressively discover
        alternative paths to reach peers.

        Returns:
            Total number of IPs updated across all sources
        """
        if not HAS_DYNAMIC_REGISTRY or get_registry is None:
            return 0

        registry = get_registry()
        total_updated = 0

        logger.info("Force-refreshing all IP sources for partition recovery...")

        # Refresh Tailscale first (most likely to help in partition)
        try:
            # Reset rate limit to force immediate check
            registry._last_tailscale_check = 0
            updated = await registry.update_tailscale_ips()
            if updated > 0:
                logger.info(f"Tailscale refresh: {updated} IPs updated")
                total_updated += updated
        except Exception as e:  # noqa: BLE001
            logger.info(f"Tailscale refresh error: {e}")

        # Refresh Vast IPs
        try:
            registry._last_vast_check = 0
            updated = await registry.update_vast_ips()
            if updated > 0:
                logger.info(f"Vast refresh: {updated} IPs updated")
                total_updated += updated
        except Exception as e:  # noqa: BLE001
            logger.info(f"Vast refresh error: {e}")

        # Refresh AWS IPs
        try:
            registry._last_aws_check = 0
            updated = await registry.update_aws_ips()
            if updated > 0:
                logger.info(f"AWS refresh: {updated} IPs updated")
                total_updated += updated
        except Exception as e:  # noqa: BLE001
            logger.info(f"AWS refresh error: {e}")

        if total_updated > 0:
            logger.info(f"Force refresh complete: {total_updated} total IPs updated")
        return total_updated

    async def _vast_ip_update_loop(self):
        """Background loop to periodically refresh Vast instance connection info.

        Uses VAST_API_KEY when available, otherwise falls back to the `vastai`
        CLI if installed (see DynamicHostRegistry.update_vast_ips).
        """
        if not HAS_DYNAMIC_REGISTRY:
            return

        logger.info("Vast IP update loop started")
        registry = get_registry()

        while self.running:
            try:
                await asyncio.sleep(300)  # Check every 5 minutes

                updated = await registry.update_vast_ips()
                if updated > 0:
                    logger.info(f"Updated {updated} Vast instance IPs from API")

            except asyncio.CancelledError:
                break
            except Exception as e:  # noqa: BLE001
                logger.info(f"Vast IP update loop error: {e}")
                await asyncio.sleep(60)

    async def _aws_ip_update_loop(self):
        """Background loop to periodically refresh AWS instance connection info.

        Uses the `aws` CLI (see DynamicHostRegistry.update_aws_ips). No-op when
        no AWS instances are configured in distributed_hosts.yaml properties.
        """
        if not HAS_DYNAMIC_REGISTRY:
            return

        logger.info("AWS IP update loop started")
        registry = get_registry()

        while self.running:
            try:
                await asyncio.sleep(300)  # Check every 5 minutes

                updated = await registry.update_aws_ips()
                if updated > 0:
                    logger.info(f"Updated {updated} AWS instance IPs via CLI")

            except asyncio.CancelledError:
                break
            except Exception as e:  # noqa: BLE001
                logger.info(f"AWS IP update loop error: {e}")
                await asyncio.sleep(60)

    async def _tailscale_ip_update_loop(self):
        """Background loop to discover and update Tailscale IPs for cluster nodes.

        Uses `tailscale status --json` to discover mesh network peers.
        Tailscale provides reliable connectivity even when public IPs change.
        """
        if not HAS_DYNAMIC_REGISTRY:
            return

        logger.info("Tailscale IP update loop started")
        registry = get_registry()

        while self.running:
            try:
                await asyncio.sleep(120)  # Check every 2 minutes

                updated = await registry.update_tailscale_ips()
                if updated > 0:
                    logger.info(f"Updated {updated} node Tailscale IPs")

            except asyncio.CancelledError:
                break
            except Exception as e:  # noqa: BLE001
                logger.info(f"Tailscale IP update loop error: {e}")
                await asyncio.sleep(60)

    async def _tailscale_peer_recovery_loop(self):
        """Proactively discover and connect to all Tailscale nodes.

        .. deprecated::
            December 2025 - This method is deprecated and now runs via LoopManager
            as TailscalePeerDiscoveryLoop. See scripts/p2p/loops/network_loops.py.
            This inline version is kept for fallback compatibility but is no longer
            invoked by default. Will be removed Q2 2026.

        LEARNED LESSONS: Cross-cloud nodes (Lambda, Vast, Hetzner) can lose
        connectivity intermittently. This loop ensures all nodes stay in the
        P2P network by:
        1. Running `tailscale status --json` to find all online nodes
        2. Attempting to connect to nodes not in the peer list
        3. Retrying dead/stale nodes more aggressively
        """
        import json
        import subprocess

        logger.info("Tailscale peer recovery loop started (interval=120s)")

        # Patterns for compute nodes we want in the P2P network
        COMPUTE_PATTERNS = [
            "lambda-", "vast-", "gh200", "h100", "a100", "a10",
            "192-222-", "aws-",  # Lambda public IPs and AWS nodes
        ]

        while self.running:
            try:
                await asyncio.sleep(120)  # Every 2 minutes

                # Phase 30: All nodes participate in discovery (not just leader)
                # This ensures isolated nodes can rejoin the cluster
                # Rate limit non-leaders to every 5 minutes (3 loops)
                is_leader = self.role == NodeRole.LEADER
                if not is_leader:
                    # Non-leaders do discovery less frequently
                    loop_count = getattr(self, "_ts_recovery_loop_count", 0) + 1
                    self._ts_recovery_loop_count = loop_count
                    if loop_count % 3 != 0:  # Every 3rd iteration = 6 minutes
                        # Unless we're isolated (few peers)
                        with self.peers_lock:
                            alive_count = sum(1 for p in self.peers.values() if p.is_alive())
                        if alive_count >= MIN_CONNECTED_PEERS:
                            continue  # Skip if we have enough peers

                # Get current peer node_ids
                current_peers = set()
                with self.peers_lock:
                    current_peers = {p.node_id for p in self.peers.values()}

                # Get Tailscale peers
                try:
                    result = subprocess.run(
                        ["tailscale", "status", "--json"],
                        capture_output=True, text=True, timeout=10
                    )
                    if result.returncode != 0:
                        continue
                    ts_data = json.loads(result.stdout)
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Tailscale status failed: {e}")
                    continue

                # Find compute nodes not in P2P network
                missing_nodes = []
                for _peer_id, peer_info in ts_data.get("Peer", {}).items():
                    hostname = peer_info.get("HostName", "").lower()
                    online = peer_info.get("Online", False)
                    ts_ips = peer_info.get("TailscaleIPs", [])

                    if not online or not ts_ips:
                        continue

                    # Check if this is a compute node
                    is_compute = any(pat in hostname for pat in COMPUTE_PATTERNS)
                    if not is_compute:
                        continue

                    # Check if already in P2P network
                    if hostname in current_peers or hostname.replace("-", "_") in current_peers:
                        continue

                    # Also check by IP prefix
                    ip = ts_ips[0] if ts_ips else ""
                    ip_based_id = ip.replace(".", "-")
                    if ip_based_id in current_peers:
                        continue

                    missing_nodes.append((hostname, ip))

                if missing_nodes:
                    logger.info(f"Tailscale peer recovery: {len(missing_nodes)} compute nodes not in P2P network")
                    for hostname, ip in missing_nodes[:10]:  # Limit to 10 per cycle
                        logger.info(f"  Attempting to connect: {hostname} ({ip})")
                        try:
                            # Try to establish connection via heartbeat
                            url = f"http://{ip}:{DEFAULT_PORT}/health"
                            timeout = ClientTimeout(total=10)
                            async with get_client_session(timeout) as session:
                                async with session.get(url) as resp:
                                    if resp.status == 200:
                                        data = await resp.json()
                                        node_id = data.get("node_id", hostname)
                                        logger.info(f"  Connected to {node_id}, sending join request")
                                        # Send heartbeat to register
                                        await self._send_heartbeat_to_peer(ip, DEFAULT_PORT)
                        except Exception as e:  # noqa: BLE001
                            logger.debug(f"  Failed to connect to {hostname}: {e}")

            except asyncio.CancelledError:
                break
            except Exception as e:  # noqa: BLE001
                logger.warning(f"Tailscale peer recovery loop error: {e}")
                await asyncio.sleep(60)

    async def _discover_tailscale_peers(self):
        """One-shot Tailscale peer discovery for bootstrap fallback.

        Phase 30: Called when bootstrap from seeds fails. Discovers peers
        via `tailscale status --json` and attempts to connect.
        """
        import json
        import subprocess

        logger.info("Running one-shot Tailscale peer discovery...")

        try:
            result = subprocess.run(
                ["tailscale", "status", "--json"],
                capture_output=True, text=True, timeout=10
            )
            if result.returncode != 0:
                logger.warning(f"Tailscale status failed: {result.stderr}")
                return

            ts_data = json.loads(result.stdout)
            peers = ts_data.get("Peer", {})

            # Get current peer node_ids
            current_peers = set()
            with self.peers_lock:
                current_peers = {p.node_id for p in self.peers.values()}

            # Patterns for compute nodes
            COMPUTE_PATTERNS = [
                "lambda-", "vast-", "gh200", "h100", "a100", "a10",
                "nebius-", "runpod-", "vultr-", "hetzner-",
            ]

            discovered = 0
            for peer_info in peers.values():
                hostname = peer_info.get("HostName", "").lower()
                is_compute = any(pat in hostname for pat in COMPUTE_PATTERNS)
                if not is_compute:
                    continue

                # Get IP from TailscaleIPs (prefer IPv4)
                ts_ips = peer_info.get("TailscaleIPs", [])
                ipv4s = [ip for ip in ts_ips if "." in ip]
                if not ipv4s:
                    continue
                ip = ipv4s[0]

                # Skip if we already know this IP
                known = False
                with self.peers_lock:
                    for p in self.peers.values():
                        if getattr(p, "tailscale_ip", None) == ip or p.host == ip:
                            known = True
                            break
                if known:
                    continue

                # Try to connect
                try:
                    async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=5)) as session:
                        async with session.get(f"http://{ip}:{DEFAULT_PORT}/status") as resp:
                            if resp.status == 200:
                                data = await resp.json()
                                node_id = data.get("node_id", hostname)
                                if node_id not in current_peers:
                                    logger.info(f"Discovered peer {node_id} via Tailscale at {ip}")
                                    await self._send_heartbeat_to_peer(ip, DEFAULT_PORT)
                                    discovered += 1
                except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError) as e:
                    # Debug log for Tailscale discovery - failures are normal for offline nodes
                    logger.debug(f"Tailscale discovery failed for {hostname}/{ip}: {type(e).__name__}")

            if discovered > 0:
                logger.info(f"Tailscale discovery: connected to {discovered} new peer(s)")
            else:
                logger.info("Tailscale discovery: no new peers found")

        except FileNotFoundError:
            logger.debug("Tailscale not installed on this node")
        except Exception as e:  # noqa: BLE001
            logger.warning(f"Tailscale discovery error: {e}")

    async def _convert_jsonl_to_db(self, data_dir: Path, games_dir: Path) -> int:
        """Convert JSONL selfplay files to SQLite DB format for training.

        This enables the training pipeline to access games stored in JSONL format.
        Converted files are tracked to avoid re-processing.

        Features:
        - Chunked reading to handle large files without memory issues
        - Limited files per cycle to avoid blocking the event loop
        - Spawns external converter for large backlogs

        Returns:
            Number of games converted.
        """
        import json
        import sqlite3

        # Configuration
        MAX_FILES_PER_CYCLE = 50  # Process max 50 files per cycle to avoid blocking
        CHUNK_SIZE = 500  # Lines to read at a time
        LARGE_BACKLOG_THRESHOLD = 200  # Spawn external converter if more files

        # Track converted files to avoid re-processing
        converted_marker_file = data_dir / ".jsonl_converted"
        converted_files: set = set()
        if converted_marker_file.exists():
            with contextlib.suppress(Exception):
                converted_files = set(converted_marker_file.read_text().strip().split("\n"))

        total_converted = 0
        newly_converted = []
        selfplay_dir = data_dir / "selfplay"

        if not selfplay_dir.exists():
            return 0

        # Find all JSONL files in selfplay subdirectories
        jsonl_files = list(selfplay_dir.rglob("*.jsonl"))
        if not jsonl_files:
            return 0

        # Filter to unconverted files and sort by size (smallest first for quick wins)
        unconverted_files = []
        for jsonl_file in jsonl_files:
            try:
                file_size = jsonl_file.stat().st_size
                if file_size < 100:
                    continue
                file_key = str(jsonl_file.relative_to(data_dir))
                if file_key not in converted_files:
                    unconverted_files.append((jsonl_file, file_size, file_key))
            except (AttributeError):
                continue

        if not unconverted_files:
            return 0

        # Sort by size (smallest first) and limit per cycle
        unconverted_files.sort(key=lambda x: x[1])
        files_this_cycle = unconverted_files[:MAX_FILES_PER_CYCLE]

        # If large backlog, spawn external converter in background
        if len(unconverted_files) > LARGE_BACKLOG_THRESHOLD:
            logger.info(f"Large JSONL backlog ({len(unconverted_files)} files), spawning background converter")
            converter_script = self.ringrift_path / "ai-service" / "scripts" / "chunked_jsonl_converter.py"
            if converter_script.exists():
                try:
                    import subprocess
                    subprocess.Popen(
                        ["python3", str(converter_script),
                         "--input-dir", str(selfplay_dir),
                         "--output-dir", str(games_dir),
                         "--workers", "2",
                         "--chunk-size", "500"],
                        stdout=open("/tmp/chunked_converter.log", "a"),
                        stderr=subprocess.STDOUT,
                        cwd=str(self.ringrift_path / "ai-service"),
                    )
                except Exception as e:  # noqa: BLE001
                    logger.error(f"Failed to spawn background converter: {e}")

        # Group files by board type
        board_type_files: dict[str, list[tuple[Path, str]]] = {}
        for jsonl_file, _file_size, file_key in files_this_cycle:
            path_str = str(jsonl_file).lower()
            if "hex" in path_str:
                if "4p" in path_str:
                    board_key = "hexagonal_4p"
                elif "3p" in path_str:
                    board_key = "hexagonal_3p"
                else:
                    board_key = "hexagonal_2p"
            elif "square19" in path_str or "sq19" in path_str:
                if "4p" in path_str:
                    board_key = "square19_4p"
                elif "3p" in path_str:
                    board_key = "square19_3p"
                else:
                    board_key = "square19_2p"
            else:
                if "4p" in path_str:
                    board_key = "square8_4p"
                elif "3p" in path_str:
                    board_key = "square8_3p"
                else:
                    board_key = "square8_2p"

            if board_key not in board_type_files:
                board_type_files[board_key] = []
            board_type_files[board_key].append((jsonl_file, file_key))

        # Convert each board type to a consolidated DB (chunked reading)
        for board_key, files in board_type_files.items():
            if not files:
                continue

            db_path = games_dir / f"jsonl_converted_{board_key}.db"
            conn = None
            try:
                conn = sqlite3.connect(str(db_path), timeout=30.0)
                conn.execute("PRAGMA journal_mode=WAL")
                conn.execute("PRAGMA synchronous=NORMAL")
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS games (
                        game_id TEXT PRIMARY KEY,
                        board_type TEXT,
                        num_players INTEGER,
                        winner INTEGER,
                        move_count INTEGER,
                        game_status TEXT,
                        victory_type TEXT,
                        created_at TEXT,
                        source TEXT,
                        metadata_json TEXT
                    )
                """)
                conn.commit()

                games_added = 0
                for jsonl_file, file_key in files:
                    try:
                        # Read file in chunks to avoid memory issues
                        chunk_buffer = []
                        with open_jsonl_file(jsonl_file) as f:
                            for line_num, line in enumerate(f, 1):
                                line = line.strip()
                                if not line:
                                    continue
                                try:
                                    record = json.loads(line)
                                    game_id = f"{jsonl_file.stem}_{record.get('game_id', line_num)}"
                                    chunk_buffer.append((
                                        game_id,
                                        record.get("board_type", board_key.split("_")[0]),
                                        record.get("num_players", int(board_key[-2]) if board_key[-2].isdigit() else 2),
                                        record.get("winner", 0),
                                        record.get("move_count", 0),
                                        record.get("status", "completed"),
                                        record.get("victory_type", "unknown"),
                                        record.get("timestamp", ""),
                                        f"jsonl:{jsonl_file.name}",
                                        json.dumps(record),
                                    ))

                                    # Flush chunk when buffer is full
                                    if len(chunk_buffer) >= CHUNK_SIZE:
                                        conn.executemany("""
                                            INSERT OR IGNORE INTO games
                                            (game_id, board_type, num_players, winner, move_count,
                                             game_status, victory_type, created_at, source, metadata_json)
                                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                                        """, chunk_buffer)
                                        games_added += len(chunk_buffer)
                                        chunk_buffer = []

                                except (json.JSONDecodeError, Exception):
                                    continue

                        # Flush remaining records
                        if chunk_buffer:
                            conn.executemany("""
                                INSERT OR IGNORE INTO games
                                (game_id, board_type, num_players, winner, move_count,
                                 game_status, victory_type, created_at, source, metadata_json)
                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                            """, chunk_buffer)
                            games_added += len(chunk_buffer)

                        newly_converted.append(file_key)
                    except Exception as e:  # noqa: BLE001
                        logger.error(f"converting {jsonl_file.name}: {e}")
                        continue

                conn.commit()
                total_converted += games_added

                if games_added > 0:
                    logger.info(f"Converted {games_added} games to {db_path.name}")

            except Exception as e:  # noqa: BLE001
                logger.error(f"creating DB for {board_key}: {e}")
            finally:
                if conn:
                    conn.close()

        # Update converted files marker
        if newly_converted:
            try:
                all_converted = converted_files | set(newly_converted)
                converted_marker_file.write_text("\n".join(sorted(all_converted)))
            except (AttributeError):
                pass

        if total_converted > 0:
            logger.info(f"JSONL conversion complete: {total_converted} total games converted")

        return total_converted

    async def _convert_jsonl_to_npz_for_training(self, data_dir: Path, training_dir: Path) -> int:
        """Convert JSONL selfplay files directly to NPZ for training.

        This bypasses the DB intermediate step and creates training-ready NPZ files
        directly from JSONL. Called periodically when JSONL backlog exists.

        Returns:
            Number of NPZ files created.
        """
        import json as json_module
        import subprocess

        # Configuration
        JSONL_THRESHOLD_GAMES = 50  # Only convert when > 50 games accumulated
        MAX_CONVERSIONS_PER_CYCLE = 3  # Limit conversions per cycle

        selfplay_dir = data_dir / "selfplay"
        canonical_dir = selfplay_dir / "canonical"

        # Track converted files
        npz_marker_file = data_dir / ".jsonl_to_npz_converted"
        converted_files: set = set()
        if npz_marker_file.exists():
            with contextlib.suppress(Exception):
                converted_files = set(npz_marker_file.read_text().strip().split("\n"))

        conversions_done = 0
        newly_converted = []

        # Board configs to check
        board_configs = [
            ("square8", 2), ("square8", 3), ("square8", 4),
            ("square19", 2), ("square19", 3), ("square19", 4),
            ("hex8", 2), ("hex8", 3), ("hex8", 4),
            ("hexagonal", 2), ("hexagonal", 3), ("hexagonal", 4),
        ]

        for board_type, num_players in board_configs:
            if conversions_done >= MAX_CONVERSIONS_PER_CYCLE:
                break

            config_key = f"{board_type}_{num_players}p"

            # Skip if already converted recently
            if config_key in converted_files:
                continue

            # Find JSONL files for this config
            jsonl_files = []
            search_dirs = [canonical_dir, selfplay_dir, data_dir / "games"]

            for search_dir in search_dirs:
                if not search_dir.exists():
                    continue
                for jsonl_file in search_dir.rglob("*.jsonl"):
                    if jsonl_file.stat().st_size < 100:
                        continue
                    jsonl_files.append(jsonl_file)

            if not jsonl_files:
                continue

            # Count games matching this config (quick check - just count lines with board type)
            game_count = 0
            valid_files = []
            for jsonl_file in jsonl_files:
                try:
                    with open_jsonl_file(jsonl_file) as f:
                        for line in f:
                            if not line.strip():
                                continue
                            try:
                                game = json_module.loads(line)
                                game_board = game.get("board_type", "")
                                game_players = game.get("num_players", 0)
                                has_moves = "moves" in game and len(game.get("moves", [])) > 0

                                # Check if matches config
                                board_match = board_type in game_board.lower() or game_board.lower() in board_type
                                if board_type == "hexagonal":
                                    board_match = "hex" in game_board.lower()

                                if board_match and game_players == num_players and has_moves:
                                    game_count += 1
                                    if jsonl_file not in valid_files:
                                        valid_files.append(jsonl_file)
                            except json_module.JSONDecodeError:
                                continue
                except (AttributeError):
                    continue

            if game_count < JSONL_THRESHOLD_GAMES:
                continue

            if not valid_files:
                continue

            # Convert to NPZ using jsonl_to_npz.py
            output_npz = training_dir / f"jsonl_auto_{config_key}_{int(time.time())}.npz"
            converter_script = self.ringrift_path / "ai-service" / "scripts" / "jsonl_to_npz.py"

            if not converter_script.exists():
                logger.info(f"JSONLNPZ converter not found: {converter_script}")
                continue

            cmd = [
                sys.executable, str(converter_script),
                "--output", str(output_npz),
                "--board-type", board_type,
                "--num-players", str(num_players),
                "--sample-every", "5",
                "--max-games", "100",
            ]
            for vf in valid_files[:10]:  # Limit input files
                cmd.extend(["--input", str(vf)])

            env = os.environ.copy()
            env["PYTHONPATH"] = str(self.ringrift_path / "ai-service")

            try:
                logger.info(f"Converting {game_count} {config_key} JSONL games to NPZ...")
                result = subprocess.run(
                    cmd, capture_output=True, text=True, timeout=600, env=env,
                    cwd=str(self.ringrift_path / "ai-service")
                )
                if result.returncode == 0 and output_npz.exists():
                    logger.info(f"Created {output_npz.name} from JSONL")
                    conversions_done += 1
                    newly_converted.append(config_key)
                else:
                    logger.info(f"JSONLNPZ conversion failed for {config_key}: {result.stderr[:200] if result.stderr else 'no error'}")
            except subprocess.TimeoutExpired:
                logger.info(f"JSONLNPZ conversion timeout for {config_key}")
            except Exception as e:  # noqa: BLE001
                logger.info(f"JSONLNPZ conversion error for {config_key}: {e}")

        # Update marker file
        if newly_converted:
            try:
                all_converted = converted_files | set(newly_converted)
                npz_marker_file.write_text("\n".join(sorted(all_converted)))
            except (AttributeError):
                pass

        return conversions_done

    # NOTE: _data_management_loop_DEPRECATED() removed Dec 28, 2025 (~180 LOC).
    # See scripts/p2p/loops/data_loops.py::DataManagementLoop for replacement.

    # NOTE: _model_sync_loop_DEPRECATED() removed Dec 28, 2025 (~143 LOC).
    # See scripts/p2p/loops/data_loops.py::ModelSyncLoop for replacement.

    async def _consolidate_selfplay_data(self):
        """Consolidate siloed job databases AND JSONL files into training DB.

        LEARNED LESSONS: Selfplay jobs write to job-specific databases for isolation.
        GPU selfplay jobs write JSONL files for efficiency.
        These need to be periodically merged into the training DB for:
        1. Training triggers to see accurate game counts
        2. Cross-node data sync to work correctly
        3. Training scripts to find all available data

        Runs every ~5 minutes (every 5th job check cycle) to avoid overhead.
        """
        # Only run every 5th cycle (~5 minutes with JOB_CHECK_INTERVAL=60)
        cycle_counter = getattr(self, "_consolidation_cycle", 0)
        self._consolidation_cycle = cycle_counter + 1
        if cycle_counter % 5 != 0:
            return

        try:
            import sqlite3
            import subprocess
            from pathlib import Path

            data_dir = Path(self.ringrift_path) / "ai-service" / "data"
            selfplay_dir = data_dir / "selfplay"
            games_dir = data_dir / "games"
            main_db_path = games_dir / "selfplay.db"
            jsonl_db_path = games_dir / "jsonl_aggregated.db"

            if not selfplay_dir.exists():
                return

            env = os.environ.copy()
            env["PYTHONPATH"] = str(Path(self.ringrift_path) / "ai-service")

            # --- PART 1: Aggregate JSONL files (GPU selfplay output) ---
            jsonl_files = list(selfplay_dir.glob("**/games.jsonl"))
            # Filter to files > 1KB and modified in last 24 hours
            recent_jsonl = []
            for jf in jsonl_files:
                try:
                    if jf.stat().st_size > 1024:
                        recent_jsonl.append(jf)
                except (AttributeError):
                    pass

            if recent_jsonl:
                total_lines = 0
                for jf in recent_jsonl[:20]:  # Sample first 20
                    try:
                        with open(jf) as f:
                            total_lines += sum(1 for _ in f)
                    except (OSError):
                        pass

                if total_lines > 100:  # Only run if there's meaningful data
                    aggregate_script = Path(self.ringrift_path) / "ai-service" / "scripts" / "aggregate_jsonl_to_db.py"
                    if aggregate_script.exists() and not getattr(self, "_jsonl_aggregation_running", False):
                        self._jsonl_aggregation_running = True
                        logger.info(f"JSONL aggregation: ~{total_lines * len(recent_jsonl) // 20} games in {len(recent_jsonl)} files")
                        cmd = [
                            sys.executable, str(aggregate_script),
                            "--input-dir", str(selfplay_dir),
                            "--output-db", str(jsonl_db_path),
                        ]
                        proc = subprocess.Popen(
                            cmd,
                            env=env,
                            stdout=open("/tmp/jsonl_aggregate.log", "w"),
                            stderr=subprocess.STDOUT,
                            cwd=str(Path(self.ringrift_path) / "ai-service"),
                        )
                        logger.info(f"Started JSONL aggregation (PID: {proc.pid})")
                        # Reset flag after ~10 minutes
                        asyncio.get_running_loop().call_later(
                            600, lambda: setattr(self, "_jsonl_aggregation_running", False)
                        )

            # --- PART 1b: Export NPZ from aggregated DB for training ---
            # Only run if we have a decent sized aggregated DB and not already exporting
            # LEARNED LESSONS: NPZ export is CPU-intensive. Route to high-CPU nodes
            # (vast nodes have 256-512 CPUs vs lambda's 64) to free GPU nodes for training.
            if jsonl_db_path.exists() and not getattr(self, "_npz_export_running", False):
                try:
                    with safe_db_connection(jsonl_db_path, timeout=5) as conn:
                        game_count = conn.execute("SELECT COUNT(*) FROM games").fetchone()[0]

                    # Only export if we have enough games and it's been a while
                    training_dir = data_dir / "training"
                    training_dir.mkdir(exist_ok=True)
                    npz_output = training_dir / "auto_training_sq8_2p.npz"

                    # Check if existing NPZ is stale (older than 1 hour) or small
                    should_export = False
                    if not npz_output.exists():
                        should_export = game_count > 500
                    else:
                        npz_age_hours = (time.time() - npz_output.stat().st_mtime) / 3600
                        npz_size_mb = npz_output.stat().st_size / (1024 * 1024)
                        should_export = game_count > 1000 and (npz_age_hours > 1 or npz_size_mb < 1)

                    if should_export:
                        self._npz_export_running = True

                        # Find best CPU node for export (prefer vast nodes)
                        # Phase 2B: Direct call to NodeSelector
                        cpu_nodes = self.node_selector.get_cpu_primary_nodes(count=3)
                        export_node = None
                        for node in cpu_nodes:
                            # Skip nodes that are already very loaded
                            if node.get_load_score() < 80 and node.cpu_percent < 90:
                                export_node = node
                                break

                        if export_node and export_node.node_id != self.node_id:
                            # Dispatch to high-CPU node
                            logger.info(f"Dispatching NPZ export ({game_count} games) to {export_node.node_id} "
                                  f"(cpu_power={export_node.cpu_power_score()}, cpus={export_node.cpu_count})")
                            asyncio.create_task(self._dispatch_export_job(
                                node=export_node,
                                input_path=str(jsonl_db_path),
                                output_path=str(npz_output),
                                board_type="square8",
                                num_players=2,
                                encoder_version="v3",
                                max_games=5000,
                                is_jsonl=False,
                            ))
                        else:
                            # Fall back to local export if no suitable CPU node
                            export_script = Path(self.ringrift_path) / "ai-service" / "scripts" / "export_replay_dataset.py"
                            if export_script.exists():
                                logger.info(f"Starting local NPZ export ({game_count} games) -> {npz_output}")
                                cmd = [
                                    sys.executable, str(export_script),
                                    "--db", str(jsonl_db_path),
                                    "--board-type", "square8",
                                    "--num-players", "2",
                                    "--output", str(npz_output),
                                    "--encoder-version", "v3",
                                    "--max-games", "5000",
                                ]
                                subprocess.Popen(
                                    cmd,
                                    env=env,
                                    stdout=open("/tmp/npz_export.log", "w"),
                                    stderr=subprocess.STDOUT,
                                    cwd=str(Path(self.ringrift_path) / "ai-service"),
                                )

                        # Reset flag after 30 minutes (export is slow)
                        asyncio.get_running_loop().call_later(
                            1800, lambda: setattr(self, "_npz_export_running", False)
                        )
                except Exception as e:  # noqa: BLE001
                    logger.info(f"NPZ export check error: {e}")

            # --- PART 2: Merge job DBs (CPU selfplay output) ---
            dbs_to_merge = []
            for db_path in selfplay_dir.glob("**/games.db"):
                # Skip if it's in a .tmp directory or is the main DB
                if ".tmp" in str(db_path) or db_path == main_db_path:
                    continue
                try:
                    with safe_db_connection(db_path, timeout=5) as conn:
                        count = conn.execute("SELECT COUNT(*) FROM games").fetchone()[0]
                        if count > 0:
                            dbs_to_merge.append((db_path, count))
                except (KeyError, IndexError, AttributeError, sqlite3.Error):
                    pass

            if dbs_to_merge:
                total_games = sum(c for _, c in dbs_to_merge)
                logger.info(f"DB consolidation: {len(dbs_to_merge)} DBs with {total_games} games to merge")

                # Use merge script if available
                merge_script = Path(self.ringrift_path) / "ai-service" / "scripts" / "merge_game_dbs.py"
                if merge_script.exists():
                    # Build command with all DBs
                    cmd = [
                        sys.executable, str(merge_script),  # Use venv Python
                        "--output", str(main_db_path),
                        "--dedupe-by-game-id",
                    ]
                    for db_path, _ in dbs_to_merge[:50]:  # Limit to 50 DBs at a time
                        cmd.extend(["--db", str(db_path)])

                    # Run merge in background
                    proc = subprocess.Popen(
                        cmd,
                        env=env,
                        stdout=subprocess.DEVNULL,
                        stderr=subprocess.DEVNULL,
                        cwd=str(Path(self.ringrift_path) / "ai-service"),
                    )
                    logger.info(f"Started DB merge (PID: {proc.pid})")

        except Exception as e:  # noqa: BLE001
            logger.info(f"Data consolidation error: {e}")

    async def _start_auto_training(self, data_path: str):
        """Start automatic training job on local node."""
        try:
            run_dir = f"{self.ringrift_path}/ai-service/models/auto_train_{int(time.time())}"
            Path(run_dir).mkdir(parents=True, exist_ok=True)

            cmd = [
                sys.executable,  # Use venv Python
                f"{self.ringrift_path}/ai-service/scripts/run_nn_training_baseline.py",
                "--board", "square8",
                "--num-players", "2",
                "--run-dir", run_dir,
                "--data-path", data_path,
                "--epochs", "50",
                "--model-version", "v3",
            ]

            env = os.environ.copy()
            env["PYTHONPATH"] = f"{self.ringrift_path}/ai-service"

            subprocess.Popen(
                cmd,
                stdout=open(f"{run_dir}/training.log", "w"),
                stderr=subprocess.STDOUT,
                env=env,
                cwd=f"{self.ringrift_path}/ai-service",
            )
            logger.info(f"Started auto-training job in {run_dir}")
            self.self_info.training_jobs += 1

        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to start auto-training: {e}")

    async def _request_data_from_peers(self):
        """Sync training data (NPZ files) from peers with large datasets.

        This ensures training data is distributed across the cluster so any
        GPU node can run training jobs.
        """
        try:
            # Check disk capacity before requesting data
            has_capacity, disk_pct = check_disk_has_capacity(DISK_CRITICAL_THRESHOLD)
            if not has_capacity:
                if self.verbose:
                    logger.info(f"Skipping data sync request: disk at {disk_pct:.1f}% (limit {DISK_CRITICAL_THRESHOLD}%)")
                return

            # Only sync if we're a GPU node (training capable)
            if not self.self_info.is_gpu_node():
                return

            # Rate limit: only sync every 10 minutes
            last_sync = getattr(self, "_last_training_data_sync", 0)
            if time.time() - last_sync < 600:
                return

            # Load hosts config to get SSH details
            if not HAS_HOSTS_FOR_SYNC:
                return

            hosts = load_remote_hosts()
            if not hosts:
                return

            local_training_dir = Path(self.ringrift_path) / "ai-service" / "data" / "training"
            local_training_dir.mkdir(parents=True, exist_ok=True)

            # Calculate local training data size
            local_training_mb = sum(
                f.stat().st_size for f in local_training_dir.glob("*.npz")
            ) / (1024 * 1024) if local_training_dir.exists() else 0

            # Find peers with more training data
            synced_from = []
            for host_name, host_config in hosts.items():
                if host_name == self.node_id:
                    continue

                # Skip hosts without SSH access
                ssh_host = host_config.ssh_host
                if not ssh_host:
                    continue

                # Check if host is alive via P2P
                with self.peers_lock:
                    peer = self.peers.get(host_name)
                if not peer or not peer.is_alive():
                    continue

                # Get remote training data size via SSH
                try:
                    ssh_user = getattr(host_config, 'ssh_user', 'ubuntu')
                    remote_path = getattr(host_config, 'ringrift_path', '/home/ubuntu/ringrift')
                    if remote_path.startswith('~'):
                        remote_path = remote_path.replace('~', f'/home/{ssh_user}')

                    # Check remote training data size
                    cmd = [
                        "ssh", "-o", "ConnectTimeout=10", "-o", "StrictHostKeyChecking=no",
                        f"{ssh_user}@{ssh_host}",
                        f"du -sb {remote_path}/ai-service/data/training/*.npz 2>/dev/null | awk '{{sum+=$1}}END{{print sum}}'"
                    ]
                    result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
                    remote_size = int(result.stdout.strip() or 0)
                    remote_mb = remote_size / (1024 * 1024)

                    # Sync if remote has significantly more data (>20MB more)
                    if remote_mb > local_training_mb + 20:
                        logger.info(f"Syncing training data from {host_name}: {remote_mb:.1f}MB -> local {local_training_mb:.1f}MB")

                        # Use rsync to sync NPZ files
                        rsync_cmd = [
                            "rsync", "-avz", "--progress",
                            "-e", "ssh -o ConnectTimeout=30 -o StrictHostKeyChecking=no",
                            f"{ssh_user}@{ssh_host}:{remote_path}/ai-service/data/training/*.npz",
                            str(local_training_dir) + "/"
                        ]

                        sync_result = subprocess.run(
                            rsync_cmd, capture_output=True, text=True, timeout=300
                        )

                        if sync_result.returncode == 0:
                            synced_from.append(host_name)
                            logger.info(f"Successfully synced training data from {host_name}")
                        else:
                            logger.error(f"Failed to sync from {host_name}: {sync_result.stderr[:200]}")

                except subprocess.TimeoutExpired:
                    logger.info(f"Timeout checking training data on {host_name}")
                except Exception as e:  # noqa: BLE001
                    logger.error(f"syncing from {host_name}: {e}")

            if synced_from:
                # Recalculate local size after sync
                new_local_mb = sum(
                    f.stat().st_size for f in local_training_dir.glob("*.npz")
                ) / (1024 * 1024)
                logger.info(f"Training data sync complete: {local_training_mb:.1f}MB -> {new_local_mb:.1f}MB")

            self._last_training_data_sync = time.time()

        except Exception as e:  # noqa: BLE001
            logger.info(f"Data sync request error: {e}")
            import traceback
            traceback.print_exc()

    # ============================================
    # Git Auto-Update Methods
    # ============================================

    def _get_local_git_commit(self) -> str | None:
        """Get the current local git commit hash."""
        try:
            result = subprocess.run(
                self._git_cmd("rev-parse", "HEAD"),
                cwd=self.ringrift_path,
                capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0:
                return result.stdout.strip()
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to get local git commit: {e}")
        return None

    def _get_local_git_branch(self) -> str | None:
        """Get the current local git branch name."""
        try:
            result = subprocess.run(
                self._git_cmd("rev-parse", "--abbrev-ref", "HEAD"),
                cwd=self.ringrift_path,
                capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0:
                return result.stdout.strip()
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to get local git branch: {e}")
        return None

    def _get_remote_git_commit(self) -> str | None:
        """Fetch and get the remote branch's latest commit hash."""
        try:
            # First fetch to update remote refs
            fetch_result = subprocess.run(
                self._git_cmd("fetch", GIT_REMOTE_NAME, GIT_BRANCH_NAME),
                cwd=self.ringrift_path,
                capture_output=True, text=True, timeout=60
            )
            if fetch_result.returncode != 0:
                logger.info(f"Git fetch failed: {fetch_result.stderr}")
                return None

            # Get remote branch commit
            result = subprocess.run(
                self._git_cmd("rev-parse", f"{GIT_REMOTE_NAME}/{GIT_BRANCH_NAME}"),
                cwd=self.ringrift_path,
                capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0:
                return result.stdout.strip()
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to get remote git commit: {e}")
        return None

    def _check_for_updates(self) -> tuple[bool, str | None, str | None]:
        """Check if there are updates available from GitHub.

        Returns: (has_updates, local_commit, remote_commit)
        """
        local_commit = self._get_local_git_commit()
        remote_commit = self._get_remote_git_commit()

        if not local_commit or not remote_commit:
            return False, local_commit, remote_commit

        has_updates = local_commit != remote_commit
        return has_updates, local_commit, remote_commit

    def _get_commits_behind(self, local_commit: str, remote_commit: str) -> int:
        """Get the number of commits the local branch is behind remote."""
        try:
            result = subprocess.run(
                self._git_cmd("rev-list", "--count", f"{local_commit}..{remote_commit}"),
                cwd=self.ringrift_path,
                capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0:
                return int(result.stdout.strip())
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to count commits behind: {e}")
        return 0

    def _check_local_changes(self) -> bool:
        """Check if there are uncommitted local changes.

        Notes:
        - Ignore untracked files by default. Cluster nodes often accumulate local
          artifacts (logs, data, env backups) that should not block git updates.
        - Still blocks on tracked/staged modifications to avoid stomping on
          local hotfixes.
        """
        try:
            result = subprocess.run(
                self._git_cmd("status", "--porcelain", "--untracked-files=no"),
                cwd=self.ringrift_path,
                capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0:
                # If there's output, there are uncommitted changes
                return bool(result.stdout.strip())
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to check local changes: {e}")
        return True  # Assume changes exist on error (safer)

    async def _stop_all_local_jobs(self) -> int:
        """Stop all local jobs gracefully before update.

        Returns: Number of jobs stopped
        """
        stopped = 0
        with self.jobs_lock:
            for job_id, job in list(self.local_jobs.items()):
                try:
                    if job.pid > 0:
                        os.kill(job.pid, signal.SIGTERM)
                        logger.info(f"Sent SIGTERM to job {job_id} (PID {job.pid})")
                        stopped += 1
                        job.status = "stopping"
                except ProcessLookupError:
                    # Process already gone
                    job.status = "stopped"
                except Exception as e:  # noqa: BLE001
                    logger.error(f"Failed to stop job {job_id}: {e}")

        # Wait for processes to terminate gracefully
        # GPU games can take 1-10 minutes, so use a longer timeout (Dec 2025 fix)
        grace_period = int(os.environ.get("RINGRIFT_JOB_GRACE_PERIOD", "60"))
        if stopped > 0:
            await asyncio.sleep(grace_period)

            # Force kill any remaining
            with self.jobs_lock:
                for job_id, job in list(self.local_jobs.items()):
                    if job.status == "stopping" and job.pid > 0:
                        try:
                            os.kill(job.pid, signal.SIGKILL)
                            logger.info(f"Force killed job {job_id}")
                        except OSError:
                            pass  # Process already dead
                        job.status = "stopped"

        return stopped

    async def _perform_git_update(self) -> tuple[bool, str]:
        """Perform git pull to update the codebase.

        Returns: (success, message)
        """
        # Check for local changes
        if self._check_local_changes():
            return False, "Local changes detected. Cannot auto-update. Please commit or stash changes."

        # Stop jobs if configured
        if GRACEFUL_SHUTDOWN_BEFORE_UPDATE:
            stopped = await self._stop_all_local_jobs()
            if stopped > 0:
                logger.info(f"Stopped {stopped} jobs before update")

        try:
            # Perform git pull
            result = subprocess.run(
                self._git_cmd("pull", GIT_REMOTE_NAME, GIT_BRANCH_NAME),
                cwd=self.ringrift_path,
                capture_output=True, text=True, timeout=120
            )

            if result.returncode != 0:
                return False, f"Git pull failed: {result.stderr}"

            logger.info(f"Git pull successful: {result.stdout}")
            return True, result.stdout

        except subprocess.TimeoutExpired:
            return False, "Git pull timed out"
        except Exception as e:  # noqa: BLE001
            return False, f"Git pull error: {e}"

    async def _restart_orchestrator(self):
        """Restart the orchestrator process after update."""
        logger.info("Restarting orchestrator to apply updates...")

        # Save state before restart
        self._save_state()

        # Get current script path and arguments
        script_path = Path(__file__).resolve()
        args = sys.argv[1:]

        # Schedule restart
        await asyncio.sleep(2)

        # Use exec to replace current process
        os.execv(sys.executable, [sys.executable, str(script_path), *args])

    async def _git_update_loop(self):
        """Background loop to periodically check for and apply updates."""
        if not AUTO_UPDATE_ENABLED:
            logger.info("Auto-update disabled")
            return

        logger.info(f"Git auto-update loop started (interval: {GIT_UPDATE_CHECK_INTERVAL}s)")

        while self.running:
            try:
                await asyncio.sleep(GIT_UPDATE_CHECK_INTERVAL)

                if not self.running:
                    break

                # Check for updates
                has_updates, local_commit, remote_commit = self._check_for_updates()

                if has_updates and local_commit and remote_commit:
                    commits_behind = self._get_commits_behind(local_commit, remote_commit)
                    logger.info(f"Update available: {commits_behind} commits behind")
                    logger.info(f"Local:  {local_commit[:8]}")
                    logger.info(f"Remote: {remote_commit[:8]}")

                    # Perform update
                    success, message = await self._perform_git_update()

                    if success:
                        logger.info("Update successful, restarting...")
                        await self._restart_orchestrator()
                    else:
                        logger.info(f"Update failed: {message}")

            except asyncio.CancelledError:
                break
            except Exception as e:  # noqa: BLE001
                logger.info(f"Git update loop error: {e}")
                await asyncio.sleep(60)  # Wait before retry on error

    # ============================================
    # HTTP API Handlers
    # ============================================

    async def handle_heartbeat(self, request: web.Request) -> web.Response:
        """Handle heartbeat from peer node."""
        try:
            data = await request.json()
            incoming_voters = data.get("voter_node_ids") or data.get("voters") or None
            if incoming_voters:
                voters_list: list[str] = []
                if isinstance(incoming_voters, list):
                    voters_list = [str(v).strip() for v in incoming_voters if str(v).strip()]
                elif isinstance(incoming_voters, str):
                    voters_list = [t.strip() for t in incoming_voters.split(",") if t.strip()]
                if voters_list:
                    self._maybe_adopt_voter_node_ids(voters_list, source="learned")
            peer_info = NodeInfo.from_dict(data)
            # Ignore self-heartbeats so NAT detection + leader election aren't
            # distorted when COORDINATOR_URL includes this node's own endpoint(s).
            if peer_info.node_id == self.node_id:
                self._update_self_info()
                payload = self.self_info.to_dict()
                voter_node_ids = list(getattr(self, "voter_node_ids", []) or [])
                if voter_node_ids:
                    payload["voter_node_ids"] = voter_node_ids
                    payload["voter_quorum_size"] = int(getattr(self, "voter_quorum_size", 0) or 0)
                    payload["voter_config_source"] = str(getattr(self, "voter_config_source", "") or "")
                return web.json_response(payload)
            # Receiving any inbound heartbeat implies we're reachable inbound.
            self.last_inbound_heartbeat = time.time()
            # Preserve the node's self-reported endpoint for multi-path retries.
            if not peer_info.reported_host:
                peer_info.reported_host = peer_info.host
            if not peer_info.reported_port:
                peer_info.reported_port = peer_info.port
            peer_info.last_heartbeat = time.time()
            # Prefer the remote socket address over self-reported host so that
            # nodes behind overlays (e.g., Tailscale) use a reachable address.
            forwarded_for = (
                request.headers.get("X-Forwarded-For")
                or request.headers.get("X-Real-IP")
                or request.headers.get("CF-Connecting-IP")
            )
            if forwarded_for:
                peer_info.host = forwarded_for.split(",")[0].strip()
            elif request.remote:
                peer_info.host = request.remote

            # Preserve local reachability diagnostics: a peer can be "alive" (it can
            # send us heartbeats) while still being unreachable for inbound HTTP
            # (e.g. NAT/firewall). Our outbound heartbeat failures track that.
            # Use AsyncLockWrapper to avoid blocking event loop under high load
            async with AsyncLockWrapper(self.peers_lock):
                existing = self.peers.get(peer_info.node_id)
                # Dec 2025: Track first-contact for HOST_ONLINE emission
                is_first_contact = existing is None
                if existing:
                    # Dec 29, 2025: Merge alternate IPs for peer deduplication
                    all_ips = set(existing.alternate_ips) if existing.alternate_ips else set()
                    if existing.host:
                        all_ips.add(existing.host)
                    if peer_info.host and peer_info.host != existing.host:
                        all_ips.add(peer_info.host)
                    all_ips.discard("")
                    # Remove primary host from alternates
                    all_ips.discard(peer_info.host)
                    peer_info.alternate_ips = all_ips
                    peer_info.consecutive_failures = int(getattr(existing, "consecutive_failures", 0) or 0)
                    peer_info.last_failure_time = float(getattr(existing, "last_failure_time", 0.0) or 0.0)
                    # Sticky NAT/relay routing with recovery:
                    # - Receiving a direct heartbeat does NOT imply the peer is reachable inbound.
                    # - If a peer has ever registered via /relay/heartbeat, preserve nat_blocked
                    #   and relay_via so leaders can continue routing commands through the relay hub.
                    # - BUT: allow recovery after NAT_BLOCKED_RECOVERY_TIMEOUT if peer becomes reachable
                    existing_nat_blocked = getattr(existing, "nat_blocked", False)
                    existing_nat_blocked_since = float(getattr(existing, "nat_blocked_since", 0.0) or 0.0)
                    existing_last_nat_probe = float(getattr(existing, "last_nat_probe", 0.0) or 0.0)

                    if existing_nat_blocked and not getattr(peer_info, "nat_blocked", False):
                        # Peer was NAT-blocked, incoming says not blocked - preserve unless recovery triggered
                        peer_info.nat_blocked = True
                        peer_info.nat_blocked_since = existing_nat_blocked_since or time.time()
                        peer_info.last_nat_probe = existing_last_nat_probe
                    elif peer_info.nat_blocked and not existing_nat_blocked:
                        # Peer newly marked as NAT-blocked - record timestamp
                        peer_info.nat_blocked_since = time.time()
                    elif existing_nat_blocked and peer_info.nat_blocked:
                        # Both agree NAT-blocked - preserve original timestamp
                        peer_info.nat_blocked_since = existing_nat_blocked_since or time.time()
                        peer_info.last_nat_probe = existing_last_nat_probe

                    if (getattr(existing, "relay_via", "") or "") and not (getattr(peer_info, "relay_via", "") or ""):
                        peer_info.relay_via = str(getattr(existing, "relay_via", "") or "")
                    # Preserve retirement state across updates.
                    if getattr(existing, "retired", False):
                        peer_info.retired = True
                        peer_info.retired_at = float(getattr(existing, "retired_at", 0.0) or 0.0)

                # STABILITY FIX: Correct stale leader role claims
                # If a peer claims to be leader but we have an elected leader that's different,
                # downgrade their role to follower to prevent split-brain confusion.
                # This prevents stale leader info from propagating through gossip.
                if peer_info.role == NodeRole.LEADER and peer_info.node_id != self.node_id:
                    actual_leader = self.leader_id
                    if actual_leader and actual_leader != peer_info.node_id:
                        # Peer claims leader but we have a different elected leader
                        peer_info.role = NodeRole.FOLLOWER

                # Dec 2025: Leader discovery from peer heartbeats
                # If we don't have a leader but peer reports one, consider adopting it
                peer_leader = getattr(peer_info, "leader_id", "") or ""
                if peer_leader and not self.leader_id:
                    # Peer reports a leader and we don't have one - check if valid
                    potential_leader = self.peers.get(peer_leader)
                    if potential_leader and potential_leader.is_alive() and potential_leader.role == NodeRole.LEADER:
                        self.leader_id = peer_leader
                        self.role = NodeRole.FOLLOWER
                        logger.info(f"Adopted leader {peer_leader} from heartbeat via {peer_info.node_id}")

                self.peers[peer_info.node_id] = peer_info

            # Dec 2025: Emit HOST_ONLINE for first-contact peers
            # This enables SelfplayScheduler, SyncRouter, and DataPipelineOrchestrator
            # to detect newly available nodes for work distribution
            if is_first_contact:
                capabilities = []
                if getattr(peer_info, "has_gpu", False):
                    gpu_type = getattr(peer_info, "gpu_type", "") or "gpu"
                    capabilities.append(gpu_type)
                else:
                    capabilities.append("cpu")
                await self._emit_host_online(peer_info.node_id, capabilities)
                logger.info(f"First-contact peer registered: {peer_info.node_id} (caps: {capabilities})")

            # Return our info
            self._update_self_info()
            payload = self.self_info.to_dict()
            voter_node_ids = list(getattr(self, "voter_node_ids", []) or [])
            if voter_node_ids:
                payload["voter_node_ids"] = voter_node_ids
                payload["voter_quorum_size"] = int(getattr(self, "voter_quorum_size", 0) or 0)
                payload["voter_config_source"] = str(getattr(self, "voter_config_source", "") or "")
            return web.json_response(payload)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=400)

    async def handle_status(self, request: web.Request) -> web.Response:
        """Return cluster status.

        Query parameters:
            alive_only: If "true" (default), only show alive peers. Set to "false" to include dead/stale peers.
            include_stale_jobs: If "false" (default), dead peers show 0 jobs. Set to "true" to show stale job counts.

        December 30, 2025: Made non-blocking with timeout-based lock acquisition.
        If locks can't be acquired within 2 seconds, returns partial status with
        "unavailable" markers instead of blocking indefinitely.
        """
        self._update_self_info()

        # Parse query parameters for filtering
        alive_only = request.query.get("alive_only", "true").lower() != "false"
        include_stale_jobs = request.query.get("include_stale_jobs", "false").lower() == "true"

        # Non-blocking peers lock acquisition (December 30, 2025)
        # CRITICAL: threading.RLock must be acquired AND released in the same thread
        # Wrap the entire lock-protected operation in asyncio.to_thread
        def _get_peers_snapshot_with_lock() -> list | None:
            """Get peers snapshot with lock - runs in thread pool."""
            if self.peers_lock.acquire(True, 2.0):  # blocking=True, timeout=2.0
                try:
                    return list(self.peers.values())
                finally:
                    self.peers_lock.release()
            return None

        peers_snapshot: list | None = None
        try:
            peers_snapshot = await asyncio.wait_for(
                asyncio.to_thread(_get_peers_snapshot_with_lock),
                timeout=3.0
            )
        except asyncio.TimeoutError:
            logger.warning("handle_status: peers_lock acquisition timed out")

        # Handle case when peers lock acquisition failed
        if peers_snapshot is None:
            peers_snapshot = []  # Empty list for graceful degradation

        conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])
        effective_leader = self._get_leader_peer()

        now = time.time()
        peers: dict[str, Any] = {}
        for node_id, info in ((p.node_id, p) for p in peers_snapshot):
            is_alive = info.is_alive()

            # Skip dead peers if alive_only is set
            if alive_only and not is_alive:
                continue

            d = info.to_dict()
            d["endpoint_conflict"] = self._endpoint_key(info) in conflict_keys
            d["leader_eligible"] = self._is_leader_eligible(info, conflict_keys, require_alive=False)

            # Add explicit alive status and staleness info
            d["is_alive"] = is_alive
            last_hb = float(getattr(info, "last_heartbeat", 0.0) or 0.0)
            d["seconds_since_heartbeat"] = int(now - last_hb) if last_hb > 0 else -1

            # Zero out job counts for dead peers unless explicitly requested
            if not is_alive and not include_stale_jobs:
                d["selfplay_jobs"] = 0
                d["training_jobs"] = 0
                d["active_job_count"] = 0

            peers[node_id] = d

        # Convenience diagnostics: reported leaders vs eligible leaders.
        leaders_reported = sorted(
            [p.node_id for p in peers_snapshot if p.role == NodeRole.LEADER and p.is_alive()]
        )
        leaders_eligible = sorted(
            [
                p.node_id
                for p in peers_snapshot
                if p.role == NodeRole.LEADER and self._is_leader_eligible(p, conflict_keys)
            ]
        )

        # Non-blocking jobs lock acquisition (December 30, 2025)
        # CRITICAL: threading.RLock must be acquired AND released in the same thread
        def _get_jobs_snapshot_with_lock() -> dict:
            """Get jobs snapshot with lock - runs in thread pool."""
            if self.jobs_lock.acquire(True, 2.0):  # blocking=True, timeout=2.0
                try:
                    return {k: v.to_dict() for k, v in self.local_jobs.items()}
                finally:
                    self.jobs_lock.release()
            return {"error": "lock_timeout"}

        jobs: dict = {}
        try:
            jobs = await asyncio.wait_for(
                asyncio.to_thread(_get_jobs_snapshot_with_lock),
                timeout=3.0
            )
        except asyncio.TimeoutError:
            logger.warning("handle_status: jobs_lock acquisition timed out")
            jobs = {"error": "lock_timeout"}

        # Get improvement cycle manager status
        improvement_status = None
        if self.improvement_cycle_manager:
            try:
                improvement_status = self.improvement_cycle_manager.get_status()
            except Exception as e:  # noqa: BLE001
                improvement_status = {"error": str(e)}

        # Get diversity metrics (delegated to SelfplayScheduler)
        # December 27, 2025: Added try-except to prevent 500 errors on memory-constrained nodes
        try:
            diversity_metrics = self.selfplay_scheduler.get_diversity_metrics()
        except Exception as e:  # noqa: BLE001
            diversity_metrics = {"error": str(e)}

        voter_ids = list(getattr(self, "voter_node_ids", []) or [])
        voters_alive = 0
        if voter_ids:
            peer_map = {p.node_id: p for p in peers_snapshot}
            for vid in voter_ids:
                if vid == self.node_id:
                    voters_alive += 1
                    continue
                peer = peer_map.get(vid)
                if peer and peer.is_alive():
                    voters_alive += 1

        # Get P2P sync metrics (with error handling for new features)
        # December 27, 2025: Wrapped all metric calls to prevent cascading 500 errors
        p2p_sync_metrics = getattr(self, "_p2p_sync_metrics", {})
        try:
            gossip_metrics = self._get_gossip_metrics_summary()
        except Exception as e:  # noqa: BLE001
            gossip_metrics = {"error": str(e)}
        try:
            distributed_training = self._get_distributed_training_summary()
        except Exception as e:  # noqa: BLE001
            distributed_training = {"error": str(e)}
        try:
            cluster_elo = self._get_cluster_elo_summary()
        except Exception as e:  # noqa: BLE001
            cluster_elo = {"error": str(e)}
        try:
            node_recovery = self._get_node_recovery_metrics()
        except Exception as e:  # noqa: BLE001
            node_recovery = {"error": str(e)}
        try:
            leader_consensus = self._get_cluster_leader_consensus()
        except Exception as e:  # noqa: BLE001
            leader_consensus = {"error": str(e)}
        try:
            peer_reputation = self._get_cluster_peer_reputation()
        except Exception as e:  # noqa: BLE001
            peer_reputation = {"error": str(e)}
        try:
            sync_intervals = self._get_sync_interval_summary()  # ADAPTIVE SYNC INTERVALS
        except Exception as e:  # noqa: BLE001
            sync_intervals = {"error": str(e)}
        try:
            tournament_scheduling = self._get_distributed_tournament_summary()  # DISTRIBUTED TOURNAMENTS
        except Exception as e:  # noqa: BLE001
            tournament_scheduling = {"error": str(e)}
        try:
            data_dedup = self._get_dedup_summary()  # DATA DEDUPLICATION
        except Exception as e:  # noqa: BLE001
            data_dedup = {"error": str(e)}

        # Phase 5: SWIM/Raft protocol status (Dec 26, 2025)
        swim_raft_status = self._get_swim_raft_status()

        # Dec 2025: Get event subscription status for health monitoring
        event_subscriptions = getattr(self, "_event_subscription_status", {
            "daemon_events": False,
            "feedback_signals": False,
            "manager_events": False,
            "all_healthy": False,
            "timestamp": 0,
        })

        # Phase 2.4 (Dec 29, 2025): Partition status for cluster monitoring
        try:
            partition_status = self.get_partition_status()
        except Exception as e:  # noqa: BLE001
            partition_status = {"error": str(e)}

        # Phase 4 (Dec 2025): Background loop status from LoopManager
        try:
            loop_manager = self._get_loop_manager()
            if loop_manager is not None:
                background_loops = loop_manager.get_all_status()
            else:
                background_loops = {"error": "LoopManager not initialized"}
        except Exception as e:  # noqa: BLE001
            background_loops = {"error": str(e)}

        return web.json_response({
            "node_id": self.node_id,
            "role": self.role.value,
            "leader_id": self.leader_id,
            "effective_leader_id": (effective_leader.node_id if effective_leader else None),
            "leaders_reported": leaders_reported,
            "leaders_eligible": leaders_eligible,
            "voter_node_ids": voter_ids,
            "voter_quorum_size": int(getattr(self, "voter_quorum_size", 0) or 0),
            "voters_alive": voters_alive,
            "voter_quorum_ok": self._has_voter_quorum(),
            "self": self.self_info.to_dict(),
            "peers": peers,
            "local_jobs": jobs,
            "alive_peers": len([p for p in self.peers.values() if p.is_alive()]),
            "improvement_cycle_manager": improvement_status,
            "diversity_metrics": diversity_metrics,
            "gossip_metrics": gossip_metrics,
            "p2p_sync_metrics": p2p_sync_metrics,
            "distributed_training": distributed_training,
            "cluster_elo": cluster_elo,
            "node_recovery": node_recovery,
            "leader_consensus": leader_consensus,
            "peer_reputation": peer_reputation,
            "sync_intervals": sync_intervals,
            "tournament_scheduling": tournament_scheduling,
            "data_dedup": data_dedup,
            "swim_raft": swim_raft_status,
            "event_subscriptions": event_subscriptions,
            "partition": partition_status,
            "background_loops": background_loops,
            # December 30, 2025: Lock acquisition status for debugging
            "_lock_status": {
                "peers_lock_acquired": peers_snapshot is not None,
                "jobs_lock_acquired": "error" not in jobs,
            },
        })

    async def handle_external_work(self, request: web.Request) -> web.Response:
        """Return external work status across the cluster.

        This endpoint shows work running outside P2P orchestrator tracking:
        - CMA-ES optimization jobs
        - Gauntlet runs
        - ELO tournaments
        - Data merge/aggregation

        Also identifies misrouted nodes (GPU nodes running CPU-bound work).
        """
        self._update_self_info()

        async with AsyncLockWrapper(self.peers_lock):
            peers_snapshot = list(self.peers.values())

        # Collect external work info
        nodes_with_external = []
        misrouted_nodes = []

        # Check self
        self.self_info.to_dict()
        if self.self_info.has_external_work():
            nodes_with_external.append({
                'node_id': self.node_id,
                'cmaes': self.self_info.cmaes_running,
                'gauntlet': self.self_info.gauntlet_running,
                'tournament': self.self_info.tournament_running,
                'data_merge': self.self_info.data_merge_running,
                'gpu_percent': self.self_info.gpu_percent,
                'cpu_percent': self.self_info.cpu_percent,
            })
        if self.self_info.is_misrouted():
            misrouted_nodes.append({
                'node_id': self.node_id,
                'gpu_name': self.self_info.gpu_name,
                'gpu_percent': self.self_info.gpu_percent,
                'cpu_percent': self.self_info.cpu_percent,
                'external_work': {
                    'cmaes': self.self_info.cmaes_running,
                    'gauntlet': self.self_info.gauntlet_running,
                    'tournament': self.self_info.tournament_running,
                }
            })

        # Check peers
        for peer in peers_snapshot:
            if peer.has_external_work():
                nodes_with_external.append({
                    'node_id': peer.node_id,
                    'cmaes': peer.cmaes_running,
                    'gauntlet': peer.gauntlet_running,
                    'tournament': peer.tournament_running,
                    'data_merge': peer.data_merge_running,
                    'gpu_percent': peer.gpu_percent,
                    'cpu_percent': peer.cpu_percent,
                })
            if peer.is_misrouted():
                misrouted_nodes.append({
                    'node_id': peer.node_id,
                    'gpu_name': peer.gpu_name,
                    'gpu_percent': peer.gpu_percent,
                    'cpu_percent': peer.cpu_percent,
                    'external_work': {
                        'cmaes': peer.cmaes_running,
                        'gauntlet': peer.gauntlet_running,
                        'tournament': peer.tournament_running,
                    }
                })

        return web.json_response({
            'nodes_with_external_work': nodes_with_external,
            'misrouted_nodes': misrouted_nodes,
            'total_external_work': len(nodes_with_external),
            'total_misrouted': len(misrouted_nodes),
        })

    # Work Queue Handlers moved to scripts/p2p/handlers/work_queue.py
    # Inherited from WorkQueueHandlersMixin: handle_work_*, handle_populator_status

    # Election Handlers moved to scripts/p2p/handlers/election.py
    # Inherited from ElectionHandlersMixin: handle_election, handle_lease_request,
    # handle_voter_grant_status, handle_election_reset, handle_election_force_leader

    # ============================================================
    # SERF INTEGRATION - Battle-tested membership/failure detection
    # ============================================================

    async def handle_serf_event(self, request: web.Request) -> web.Response:
        """POST /serf/event - Receive events from Serf event handler.

        SERF INTEGRATION: HashiCorp Serf provides battle-tested SWIM gossip
        for membership and failure detection. This endpoint receives events
        from the serf_event_handler.py script.

        Event types:
        - member-join: New node joined the cluster
        - member-leave: Node gracefully left
        - member-failed: Node failed (detected by SWIM)
        - member-update: Node tags changed
        - member-reap: Failed node was reaped from membership list
        - user: Custom user event (training-complete, model-promoted, etc.)

        Request body:
        {
            "event_type": "member-join",
            "timestamp": "2025-12-26T...",
            "payload": { event-specific data }
        }
        """
        try:
            if self.auth_token and not self._is_request_authorized(request):
                return web.json_response({"error": "unauthorized"}, status=401)

            data = await request.json()
            event_type = data.get("event_type", "")
            timestamp = data.get("timestamp", "")
            payload = data.get("payload", {})

            logger.info(f"Serf event received: {event_type} at {timestamp}")

            # Process based on event type
            if event_type == "member-join":
                await self._handle_serf_member_join(payload.get("members", []))
            elif event_type == "member-leave":
                await self._handle_serf_member_leave(payload.get("members", []))
            elif event_type == "member-failed":
                await self._handle_serf_member_failed(payload.get("members", []))
            elif event_type == "member-update":
                await self._handle_serf_member_update(payload.get("members", []))
            elif event_type == "member-reap":
                await self._handle_serf_member_reap(payload.get("members", []))
            elif event_type == "user":
                await self._handle_serf_user_event(payload)
            else:
                logger.warning(f"Unknown Serf event type: {event_type}")

            return web.json_response({
                "status": "processed",
                "event_type": event_type,
                "node_id": self.node_id,
            })

        except Exception as e:  # noqa: BLE001
            logger.error(f"Error processing Serf event: {e}")
            return web.json_response({"error": str(e)}, status=500)

    async def _handle_serf_member_join(self, members: list) -> None:
        """Handle Serf member-join events.

        When Serf detects new members, update our peer list and mark them alive.
        This is more reliable than our custom gossip because Serf uses SWIM
        with indirect probing.
        """
        for member in members:
            node_name = member.get("name", "")
            addr = member.get("addr", "")
            tags = member.get("tags", {})

            if not node_name or node_name == self.node_id:
                continue

            logger.info(f"Serf: member joined: {node_name} @ {addr}")

            # Update peer state
            now = time.time()
            if node_name not in self.peers:
                # Parse addr to get host:port (format: "ip:port")
                host, port = (addr.rsplit(":", 1) + ["8770"])[:2] if ":" in addr else (addr, "8770")
                try:
                    port_int = int(port)
                except ValueError:
                    port_int = 8770
                self.peers[node_name] = NodeInfo(
                    node_id=node_name,
                    host=host or "unknown",
                    port=port_int,
                    last_heartbeat=now,
                )
            else:
                peer = self.peers[node_name]
                if isinstance(peer, NodeInfo):
                    peer.last_heartbeat = now
                    if addr:
                        host, _ = (addr.rsplit(":", 1) + [""])[:2] if ":" in addr else (addr, "")
                        if host:
                            peer.host = host

            # Extract tags into peer info (store as capability hints)
            # Note: Serf tags are for reference only, NodeInfo uses capabilities list

    async def _handle_serf_member_leave(self, members: list) -> None:
        """Handle Serf member-leave events (graceful departure)."""
        for member in members:
            node_name = member.get("name", "")

            if not node_name or node_name == self.node_id:
                continue

            logger.info(f"Serf: member left gracefully: {node_name}")

            if node_name in self.peers:
                peer = self.peers[node_name]
                if isinstance(peer, NodeInfo):
                    # Mark as retired (NodeInfo equivalent of "left")
                    peer.retired = True
                    peer.retired_at = time.time()

    async def _handle_serf_member_failed(self, members: list) -> None:
        """Handle Serf member-failed events (SWIM failure detection).

        SWIM's failure detection is more reliable than our custom ping/pong
        because it uses indirect probing through multiple peers.
        """
        for member in members:
            node_name = member.get("name", "")
            addr = member.get("addr", "")

            if not node_name or node_name == self.node_id:
                continue

            logger.warning(f"Serf: member FAILED (SWIM detected): {node_name} @ {addr}")

            if node_name in self.peers:
                peer = self.peers[node_name]
                if isinstance(peer, NodeInfo):
                    # Mark with consecutive failures (triggers dead/suspect state)
                    peer.consecutive_failures += 1
                    peer.last_failure_time = time.time()

            # If the failed node was leader, trigger election
            if node_name == self.leader_id:
                logger.warning(f"Leader {node_name} failed (Serf detected) - triggering election")
                self.leader_id = None
                self.election_in_progress = False  # Allow new election
                self._save_state()

    async def _handle_serf_member_update(self, members: list) -> None:
        """Handle Serf member-update events (tag changes)."""
        for member in members:
            node_name = member.get("name", "")
            tags = member.get("tags", {})

            if not node_name or node_name == self.node_id:
                continue

            logger.info(f"Serf: member updated: {node_name}")

            if node_name in self.peers:
                peer = self.peers[node_name]
                if isinstance(peer, NodeInfo):
                    peer.last_heartbeat = time.time()
                    # Tags can update capabilities if structured appropriately
                    if "capabilities" in tags and isinstance(tags["capabilities"], list):
                        peer.capabilities = tags["capabilities"]

    async def _handle_serf_member_reap(self, members: list) -> None:
        """Handle Serf member-reap events (failed nodes removed from list)."""
        for member in members:
            node_name = member.get("name", "")

            if not node_name or node_name == self.node_id:
                continue

            logger.info(f"Serf: member reaped (final cleanup): {node_name}")

            # Mark as retired (reaped means permanently gone)
            if node_name in self.peers:
                peer = self.peers[node_name]
                if isinstance(peer, NodeInfo):
                    peer.retired = True
                    peer.retired_at = time.time()

    async def _handle_serf_user_event(self, payload: dict) -> None:
        """Handle Serf user events (custom RingRift events).

        User events include:
        - training-complete: Training job finished
        - model-promoted: Model was promoted to canonical
        - selfplay-started: Selfplay jobs started on a node
        - node-status: Periodic node status broadcast
        """
        event_name = payload.get("name", "")
        event_payload = payload.get("payload", {})
        ltime = payload.get("ltime", "0")

        logger.info(f"Serf user event: {event_name} (ltime={ltime})")

        if event_name == "training-complete":
            config_key = event_payload.get("config_key", "")
            model_path = event_payload.get("model_path", "")
            metrics = event_payload.get("metrics", {})
            logger.info(f"Training complete via Serf: {config_key} -> {model_path}")
            # Could trigger evaluation here

        elif event_name == "model-promoted":
            config_key = event_payload.get("config_key", "")
            model_path = event_payload.get("model_path", "")
            elo_gain = event_payload.get("elo_gain", 0)
            logger.info(f"Model promoted via Serf: {config_key} (+{elo_gain} Elo)")
            # Could trigger model distribution here

        elif event_name == "selfplay-started":
            node = event_payload.get("node", "")
            config_key = event_payload.get("config_key", "")
            job_count = event_payload.get("job_count", 1)
            logger.info(f"Selfplay started via Serf: {node} running {config_key} x{job_count}")

        elif event_name == "node-status":
            # Status updates from nodes - could merge with gossip state
            node_id = event_payload.get("node_id", "")
            if node_id and node_id in self.peers:
                # Update peer with status info
                status_fields = ["gpu_util", "gpu_mem_used", "cpu_percent", "memory_percent"]
                for field in status_fields:
                    if field in event_payload:
                        self.peers[node_id][field] = event_payload[field]

    # ============================================================
    # SWIM Native Integration - swim-p2p membership status
    # ============================================================

    async def handle_swim_status(self, request: web.Request) -> web.Response:
        """GET /swim/status - Return SWIM membership protocol status.

        SWIM provides leaderless gossip-based membership with:
        - O(1) bandwidth per node (constant message complexity)
        - <5 second failure detection (vs 60+ seconds with heartbeats)
        - Suspicion mechanism to reduce false positives
        """
        try:
            if not self._swim_manager:
                return web.json_response({
                    "status": "disabled",
                    "reason": "swim-p2p not installed or SWIM adapter not available",
                    "node_id": self.node_id,
                    "fallback": "http_heartbeats",
                })

            summary = self._swim_manager.get_membership_summary()
            alive_peers = self._swim_manager.get_alive_peers() if self._swim_started else []

            return web.json_response({
                "status": "enabled" if self._swim_started else "initialized",
                "node_id": self.node_id,
                "swim": summary,
                "alive_peers": alive_peers,
                "peer_count": len(alive_peers),
            })

        except Exception as e:  # noqa: BLE001
            logger.error(f"Error getting SWIM status: {e}")
            return web.json_response({
                "status": "error",
                "error": str(e),
                "node_id": self.node_id,
            }, status=500)

    async def _swim_membership_loop(self) -> None:
        """Background task that integrates SWIM membership with P2P peer tracking.

        This task:
        1. Starts the SWIM manager if available
        2. Periodically syncs SWIM membership with our peers dict
        3. Uses SWIM failure detection to mark peers as failed faster
        """
        if not self._swim_manager:
            logger.info("SWIM membership loop: disabled (swim-p2p not available)")
            return

        try:
            # Start SWIM manager
            started = await self._swim_manager.start()
            if not started:
                logger.warning("SWIM membership loop: failed to start SWIM manager")
                return

            self._swim_started = True
            logger.info("SWIM membership loop: started successfully")

            # Sync SWIM membership with our peer tracking every 10 seconds
            while self.running:
                try:
                    alive_peers = self._swim_manager.get_alive_peers()

                    # Update peers from SWIM detection
                    now = time.time()
                    with self.peers_lock:
                        for peer_id in alive_peers:
                            if peer_id not in self.peers:
                                # New peer detected by SWIM
                                # peer_id format is typically "host:port" from SWIM
                                host, port_str = (peer_id.rsplit(":", 1) + ["7947"])[:2] if ":" in peer_id else (peer_id, "7947")
                                try:
                                    port_int = int(port_str)
                                except ValueError:
                                    port_int = 8770  # Use P2P port, not SWIM port
                                # SWIM detection creates a minimal NodeInfo; HTTP handshake fills details
                                self.peers[peer_id] = NodeInfo(
                                    node_id=peer_id,
                                    host=host or "unknown",
                                    port=8770,  # P2P API port (SWIM uses 7947)
                                    last_heartbeat=now,
                                )
                            else:
                                # Update existing peer's heartbeat
                                peer = self.peers[peer_id]
                                if isinstance(peer, NodeInfo):
                                    peer.last_heartbeat = now

                except Exception as e:  # noqa: BLE001
                    logger.warning(f"SWIM sync error: {e}")

                await asyncio.sleep(10)  # Sync every 10 seconds

        except asyncio.CancelledError:
            logger.info("SWIM membership loop: cancelled")
            raise
        except Exception as e:  # noqa: BLE001
            logger.error(f"SWIM membership loop error: {e}", exc_info=True)
        finally:
            if self._swim_manager and self._swim_started:
                try:
                    await self._swim_manager.stop()
                except Exception as e:  # noqa: BLE001
                    logger.warning(f"Error stopping SWIM manager: {e}")
                self._swim_started = False

    async def handle_coordinator(self, request: web.Request) -> web.Response:
        """Handle coordinator announcement from new leader.

        LEARNED LESSONS - Only accept leadership from higher-priority nodes (Bully algorithm).
        Also handles lease-based leadership updates.
        """
        try:
            self._update_self_info()
            data = await request.json()
            new_leader_raw = data.get("leader_id")
            if not new_leader_raw:
                return web.json_response(
                    {"accepted": False, "reason": "missing_leader_id"},
                    status=400,
                )
            new_leader = str(new_leader_raw)
            lease_id = data.get("lease_id", "")
            lease_expires = data.get("lease_expires", 0)
            is_renewal = data.get("lease_renewal", False)
            incoming_voters = data.get("voter_node_ids") or data.get("voters") or None
            if incoming_voters:
                voters_list: list[str] = []
                if isinstance(incoming_voters, list):
                    voters_list = [str(v).strip() for v in incoming_voters if str(v).strip()]
                elif isinstance(incoming_voters, str):
                    voters_list = [t.strip() for t in incoming_voters.split(",") if t.strip()]
                if voters_list:
                    self._maybe_adopt_voter_node_ids(voters_list, source="learned")

            voters = list(getattr(self, "voter_node_ids", []) or [])
            if voters and new_leader not in voters:
                return web.json_response(
                    {"accepted": False, "reason": "leader_not_voter", "voters": voters},
                    status=403,
                )

            # Voter-side safety: if we've granted a still-valid lease to a different leader,
            # do not accept a conflicting coordinator announcement. This prevents a voter
            # from "following" a non-quorum leader during transient partitions.
            if voters and self.node_id in voters:
                grant_leader = str(getattr(self, "voter_grant_leader_id", "") or "")
                grant_expires = float(getattr(self, "voter_grant_expires", 0.0) or 0.0)
                if grant_leader and grant_expires > time.time() and grant_leader != new_leader:
                    return web.json_response(
                        {
                            "accepted": False,
                            "reason": "voter_lease_conflict",
                            "granted_to": grant_leader,
                            "granted_until": grant_expires,
                        },
                        status=409,
                    )

            # If quorum gating is not configured, fall back to bully ordering
            # (lexicographically highest node_id wins).
            if not voters and self.role == NodeRole.LEADER and new_leader < self.node_id:
                # Exception: accept if our lease has expired
                if self.leader_lease_expires > 0 and time.time() >= self.leader_lease_expires:
                    logger.info(f"Our lease expired, accepting leader: {new_leader}")
                else:
                    logger.info(f"Rejecting leader announcement from lower-priority node: {new_leader} < {self.node_id}")
                    return web.json_response({"accepted": False, "reason": "lower_priority"})

            # Reject leadership from nodes that are not directly reachable / uniquely addressable.
            if new_leader != self.node_id:
                with self.peers_lock:
                    peer = self.peers.get(new_leader)
                    peers_snapshot = [p for p in self.peers.values() if p.node_id != self.node_id]
                if peer:
                    conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])
                    if not self._is_leader_eligible(peer, conflict_keys, require_alive=False):
                        return web.json_response({"accepted": False, "reason": "leader_ineligible"})

            if is_renewal and new_leader == self.leader_id:
                self.leader_lease_expires = lease_expires
                self.leader_lease_id = lease_id
                return web.json_response({"accepted": True})

            logger.info(f"Accepting leader announcement: {new_leader}")
            self.leader_id = new_leader
            self.leader_lease_id = lease_id
            self.leader_lease_expires = lease_expires if lease_expires else time.time() + LEADER_LEASE_DURATION

            if new_leader == self.node_id:
                self.role = NodeRole.LEADER
            else:
                self.role = NodeRole.FOLLOWER

            self._save_state()
            return web.json_response({"accepted": True})
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=400)

    async def handle_start_job(self, request: web.Request) -> web.Response:
        """Handle request to start a job (from leader)."""
        try:
            data = await request.json()
            job_type = JobType(data.get("job_type", "selfplay"))
            board_type = data.get("board_type", "square8")
            num_players = data.get("num_players", 2)
            engine_mode = data.get("engine_mode", "gumbel-mcts")  # GPU-accelerated default
            job_id = data.get("job_id")
            cuda_visible_devices = data.get("cuda_visible_devices")

            # Extra params for DATA_EXPORT jobs
            export_params = None
            if job_type == JobType.DATA_EXPORT:
                export_params = {
                    "input_path": data.get("input_path"),
                    "output_path": data.get("output_path"),
                    "encoder_version": data.get("encoder_version", "v3"),
                    "max_games": data.get("max_games", 5000),
                    "is_jsonl": data.get("is_jsonl", False),
                }

            job = await self._start_local_job(
                job_type,
                board_type=board_type,
                num_players=num_players,
                engine_mode=engine_mode,
                job_id=job_id,
                cuda_visible_devices=cuda_visible_devices,
                export_params=export_params,
            )

            if job:
                return web.json_response({"success": True, "job": job.to_dict()})
            else:
                return web.json_response({"success": False, "error": "Failed to start job"}, status=500)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=400)

    async def handle_stop_job(self, request: web.Request) -> web.Response:
        """Handle request to stop a job."""
        try:
            data = await request.json()
            job_id = data.get("job_id")

            async with AsyncLockWrapper(self.jobs_lock):
                if job_id in self.local_jobs:
                    job = self.local_jobs[job_id]
                    try:
                        os.kill(job.pid, signal.SIGTERM)
                        job.status = "stopped"
                    except OSError:
                        pass  # Process already dead
                    return web.json_response({"success": True})

            return web.json_response({"success": False, "error": "Job not found"}, status=404)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=400)

    async def handle_job_kill(self, request: web.Request) -> web.Response:
        """Handle request to forcefully kill a stuck job (SIGKILL).

        Used by the leader's self-healing system to kill stuck jobs remotely.
        Supports killing by job_id or by job_type pattern.
        """
        try:
            data = await request.json()
            job_id = data.get("job_id")
            job_type = data.get("job_type")  # "training", "selfplay", etc.
            reason = data.get("reason", "unknown")

            killed = 0

            # Try to kill by job_id first
            if job_id:
                async with AsyncLockWrapper(self.jobs_lock):
                    if job_id in self.local_jobs:
                        job = self.local_jobs[job_id]
                        try:
                            os.kill(job.pid, signal.SIGKILL)
                            job.status = "killed"
                            killed += 1
                            logger.info(f"Killed job {job_id} (pid {job.pid}): {reason}")
                        except Exception as e:  # noqa: BLE001
                            logger.error(f"Failed to kill job {job_id}: {e}")

            # Kill by job_type pattern (for stuck training, etc.)
            if job_type and killed == 0:
                import subprocess
                patterns = {
                    "training": ["train_nnue", "train.*model"],
                    "selfplay": ["selfplay", "run_hybrid_selfplay"],
                }
                for pattern in patterns.get(job_type, [job_type]):
                    try:
                        result = subprocess.run(
                            ["pkill", "-9", "-f", pattern],
                            timeout=5,
                            capture_output=True,
                        )
                        if result.returncode == 0:
                            killed += 1
                            logger.info(f"Killed processes matching '{pattern}': {reason}")
                    except Exception as e:  # noqa: BLE001
                        logger.info(f"pkill error for {pattern}: {e}")

            return web.json_response({
                "success": killed > 0,
                "killed": killed,
                "reason": reason,
            })

        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=400)

    async def handle_cleanup(self, request: web.Request) -> web.Response:
        """Handle cleanup request (from leader or manual).

        LEARNED LESSONS - This endpoint allows remote nodes to trigger disk cleanup
        when the leader detects disk usage approaching critical thresholds.
        """
        try:
            logger.info("Cleanup request received")

            # Run cleanup in background to avoid blocking the request
            asyncio.create_task(self._cleanup_local_disk())

            # Return current disk usage
            usage = self._get_resource_usage()
            return web.json_response({
                "success": True,
                "disk_percent_before": usage["disk_percent"],
                "message": "Cleanup initiated",
            })
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=500)

    async def handle_restart_stuck_jobs(self, request: web.Request) -> web.Response:
        """Handle request to restart stuck selfplay jobs.

        LEARNED LESSONS - Called by leader when it detects GPU idle with running processes.
        Kills all selfplay processes and clears job tracking so they restart.
        """
        try:
            logger.info("Restart stuck jobs request received")

            # Run in background to avoid blocking
            asyncio.create_task(self._restart_local_stuck_jobs())

            return web.json_response({
                "success": True,
                "message": "Stuck job restart initiated",
            })
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=500)

    async def handle_reduce_selfplay(self, request: web.Request) -> web.Response:
        """Stop excess selfplay jobs on this node (load shedding).

        Used by leaders when a node is under memory/disk pressure so the node
        can recover without requiring manual intervention.
        """
        try:
            data = await request.json()
            target_raw = data.get("target_selfplay_jobs", data.get("target", 0))
            reason = str(data.get("reason") or "remote_request")
            try:
                target = int(target_raw)
            except (ValueError):
                target = 0

            result = await self._reduce_local_selfplay_jobs(target, reason=reason)
            return web.json_response({"success": True, **result})
        except Exception as e:  # noqa: BLE001
            return web.json_response({"success": False, "error": str(e)}, status=400)

    async def handle_selfplay_start(self, request: web.Request) -> web.Response:
        """POST /selfplay/start - Start GPU selfplay job on this node.

        Called by leader to dispatch GPU selfplay work to worker nodes.
        Uses run_hybrid_selfplay.py for GPU-accelerated game generation.
        """
        try:
            # Phase 2.4 (Dec 29, 2025): Block dispatch if in partition readonly mode
            if self.is_partition_readonly():
                status = self.get_partition_status()
                logger.warning(
                    f"[P2P] Rejecting selfplay start: partition readonly mode "
                    f"(status={status['partition_status']}, ratio={status['health_ratio']:.2%})"
                )
                return web.json_response({
                    "success": False,
                    "error": "Node is in partition readonly mode",
                    "partition_status": status["partition_status"],
                    "health_ratio": status["health_ratio"],
                    "retry_after_seconds": 60,
                }, status=503)

            data = await request.json()
            board_type = data.get("board_type", "square8")
            num_players = data.get("num_players", 2)
            num_games = data.get("num_games", 500)
            engine_mode = data.get("engine_mode", "gumbel-mcts")  # GPU-accelerated MCTS
            engine_extra_args = data.get("engine_extra_args")  # December 2025: for budget override
            data.get("auto_scaled", False)

            job_id = f"selfplay-{self.node_id}-{int(time.time())}"

            # Start the selfplay job in background
            # Delegate to JobManager (Phase 2B refactoring, Dec 2025)
            asyncio.create_task(self.job_manager.run_gpu_selfplay_job(
                job_id=job_id,
                board_type=board_type,
                num_players=num_players,
                num_games=num_games,
                engine_mode=engine_mode,
                engine_extra_args=engine_extra_args,
            ))

            logger.info(f"Started GPU selfplay job {job_id}: {board_type}/{num_players}p, {num_games} games")

            # Track diversity metrics for monitoring (Phase 2B, Dec 2025)
            self.selfplay_scheduler.track_diversity({
                "board_type": board_type,
                "num_players": num_players,
                "engine_mode": engine_mode,
            })

            return web.json_response({
                "success": True,
                "job_id": job_id,
                "board_type": board_type,
                "num_players": num_players,
                "num_games": num_games,
                "node_id": self.node_id,
            })
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to start selfplay: {e}")
            return web.json_response({"success": False, "error": str(e)}, status=500)

    # NOTE: _run_gpu_selfplay_job() removed Dec 2025 (123 LOC).
    # Use self.job_manager.run_gpu_selfplay_job() instead.
    # See scripts/p2p/managers/job_manager.py for implementation.

    async def handle_dispatch_selfplay(self, request: web.Request) -> web.Response:
        """POST /dispatch_selfplay - Request selfplay jobs for a config.

        Called from coordinator or any node to request selfplay generation.
        If this node is leader, adds work to the queue directly.
        If not leader, proxies the request to the current leader.

        Request body:
        {
            "board_type": "hex8",
            "num_players": 4,
            "num_games": 200,
            "engine_mode": "gumbel-mcts",  # optional, defaults to gumbel-mcts
            "priority": 50,  # optional, 1-100, higher = more urgent
            "force": false  # optional, bypass backpressure
        }

        Response:
        {
            "success": true,
            "work_ids": ["abc123", ...],
            "count": 2,
            "dispatched_to": "leader-node-id"
        }
        """
        try:
            data = await request.json()
        except (json.JSONDecodeError, ValueError):
            return web.json_response(
                {"success": False, "error": "Invalid JSON body"},
                status=400,
            )

        board_type = data.get("board_type", "square8")
        num_players = data.get("num_players", 2)
        num_games = data.get("num_games", 200)
        engine_mode = data.get("engine_mode", "gumbel-mcts")
        priority = data.get("priority", 50)
        force = data.get("force", False)

        # If we're the leader, add work directly to queue
        if self.is_leader:
            try:
                from app.coordination.work_queue import get_work_queue, WorkItem, WorkType

                wq = get_work_queue()
                if wq is None:
                    return web.json_response(
                        {"success": False, "error": "Work queue not available"},
                        status=503,
                    )

                # Calculate number of work items (split into 100-game chunks for better distribution)
                games_per_job = 100
                num_jobs = max(1, num_games // games_per_job)
                games_remainder = num_games % games_per_job

                work_ids = []
                for i in range(num_jobs):
                    games_this_job = games_per_job if i < num_jobs - 1 or games_remainder == 0 else games_remainder
                    if games_this_job == 0 and i == num_jobs - 1:
                        games_this_job = games_per_job  # Use full batch for last if no remainder

                    config_key = f"{board_type}_{num_players}p"
                    item = WorkItem(
                        work_type=WorkType.SELFPLAY,
                        priority=priority,
                        config={
                            "board_type": board_type,
                            "num_players": num_players,
                            "num_games": games_this_job,
                            "engine_mode": engine_mode,
                            "config_key": config_key,
                        },
                        timeout_seconds=3600.0,
                    )
                    work_id = wq.add_work(item, force=force)
                    work_ids.append(work_id)

                logger.info(
                    f"[P2P] Dispatched {len(work_ids)} selfplay jobs for {board_type}_{num_players}p "
                    f"({num_games} total games, engine={engine_mode})"
                )

                return web.json_response({
                    "success": True,
                    "work_ids": work_ids,
                    "count": len(work_ids),
                    "total_games": num_games,
                    "dispatched_to": self.node_id,
                    "config_key": f"{board_type}_{num_players}p",
                })

            except RuntimeError as e:
                if "BACKPRESSURE" in str(e):
                    logger.warning(f"Selfplay dispatch rejected due to backpressure: {e}")
                    return web.json_response(
                        {"success": False, "error": str(e), "retry_after_seconds": 60},
                        status=429,
                    )
                raise
            except Exception as e:
                logger.error(f"Failed to dispatch selfplay: {e}")
                return web.json_response({"success": False, "error": str(e)}, status=500)

        # Not leader - proxy to leader
        return await self._proxy_to_leader(request)

    async def handle_cleanup_files(self, request: web.Request) -> web.Response:
        """Delete specific files from this node (for post-sync cleanup).

        Called by leader after successful sync to training nodes to free
        disk space on source nodes with high disk usage.
        """
        try:
            data = await request.json()
            files = data.get("files", [])
            reason = data.get("reason", "manual")

            if not files:
                return web.json_response({"success": False, "error": "No files specified"}, status=400)

            logger.info(f"Cleanup files request: {len(files)} files, reason={reason}")

            data_dir = self.get_data_directory()
            freed_bytes = 0
            deleted_count = 0

            for file_path in files:
                # Security: only allow deletion within data directory
                full_path = data_dir / (file_path or "").lstrip("/")
                try:
                    data_root = data_dir.resolve()
                    resolved = full_path.resolve()
                    resolved.relative_to(data_root)
                except (AttributeError):
                    logger.info(f"Cleanup: skipping path outside data dir: {file_path}")
                    continue

                if resolved.exists():
                    try:
                        size = resolved.stat().st_size
                        resolved.unlink()
                        freed_bytes += size
                        deleted_count += 1
                    except Exception as e:  # noqa: BLE001
                        logger.error(f"Failed to delete {file_path}: {e}")

            logger.info(f"Cleanup complete: {deleted_count} files, {freed_bytes / 1e6:.1f}MB freed")

            return web.json_response({
                "success": True,
                "freed_bytes": freed_bytes,
                "deleted_count": deleted_count,
            })
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=500)

    # NOTE: Peer admin handlers (handle_purge_retired_peers, handle_purge_stale_peers,
    # handle_admin_unretire, handle_admin_reset_node_jobs) moved to AdminHandlersMixin
    # in scripts/p2p/handlers/admin.py (Dec 28, 2025)

    async def handle_training_sync(self, request: web.Request) -> web.Response:
        """Manually trigger sync of selfplay data to training nodes.

        Leader-only: Syncs selfplay data to the top GPU nodes for training.
        """
        try:
            result = await self._sync_selfplay_to_training_nodes()
            return web.json_response(result)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=500)

    async def handle_gpu_rankings(self, request: web.Request) -> web.Response:
        """Get GPU power rankings for all nodes in the cluster.

        Returns nodes sorted by GPU processing power for training priority.
        """
        try:
            # Phase 2B: Direct calls to NodeSelector
            rankings = self.node_selector.get_training_nodes_ranked()
            training_nodes = self.node_selector.get_training_primary_nodes()

            return web.json_response({
                "rankings": rankings,
                "training_primary_nodes": [n.node_id for n in training_nodes],
                "training_node_count": TRAINING_NODE_COUNT,
            })
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=500)

    async def handle_health(self, request: web.Request) -> web.Response:
        """Handle health check request.

        LEARNED LESSONS - Simple health endpoint for monitoring and load balancers.
        Returns node health status without full cluster state.
        Includes utilization status from resource_optimizer for cluster coordination.
        """
        try:
            self._update_self_info()
            is_healthy = self.self_info.is_healthy()

            # Calculate uptime and leader status
            uptime_seconds = time.time() - getattr(self, "start_time", time.time())
            leader_last_seen = time.time() - getattr(self, "last_leader_seen", time.time())
            active_peers = sum(1 for p in self.peers.values()
                             if time.time() - p.last_heartbeat < 120)

            response = {
                "healthy": is_healthy,
                "node_id": self.node_id,
                "role": self.role.value,
                "disk_percent": self.self_info.disk_percent,
                "memory_percent": self.self_info.memory_percent,
                "cpu_percent": self.self_info.cpu_percent,
                "gpu_percent": self.self_info.gpu_percent,
                "gpu_memory_percent": self.self_info.gpu_memory_percent,
                "selfplay_jobs": self.self_info.selfplay_jobs,
                "training_jobs": self.self_info.training_jobs,
                # Cluster health for alerting
                "leader_id": self.leader_id,
                "leader_last_seen_seconds": leader_last_seen if self.leader_id else None,
                "active_peers": active_peers,
                "total_peers": len(self.peers),
                "uptime_seconds": uptime_seconds,
                "timestamp": datetime.utcnow().isoformat(),
            }

            # Add cluster utilization status for cooperative 60-80% targeting
            if HAS_RATE_NEGOTIATION and get_utilization_status is not None:
                try:
                    util_status = get_utilization_status()
                    response["cluster_utilization"] = {
                        "cpu_util": util_status.get("cpu_util", 0),
                        "gpu_util": util_status.get("gpu_util", 0),
                        "selfplay_rate": util_status.get("current_rate", 1000),
                        "target_range": "60-80%",
                        "status": util_status.get("status", "unknown"),
                    }
                except (AttributeError):
                    pass

            return web.json_response(response)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e), "healthy": False}, status=500)

    async def handle_cluster_health(self, request: web.Request) -> web.Response:
        """Aggregate health status from all cluster nodes (leader-only).

        Phase 6: Health broadcasting - leader aggregates peer health data
        and reports unhealthy nodes for monitoring and alerting.
        """
        try:
            if not self._is_leader():
                return await self._proxy_to_leader(request)

            self._update_self_info()

            # Collect health from all peers
            unhealthy_nodes = []
            code_version_mismatches = []
            disk_warnings = []
            memory_warnings = []
            nfs_issues = []

            my_version = self.build_version

            with self.peers_lock:
                peers_snapshot = list(self.peers.values())

            for peer in peers_snapshot:
                # Check for health issues
                issues = peer.get_health_issues()
                if issues:
                    unhealthy_nodes.append({
                        "node_id": peer.node_id,
                        "issues": [{"code": code, "description": desc} for code, desc in issues],
                    })

                # Check code version mismatch
                if peer.code_version and peer.code_version != my_version:
                    code_version_mismatches.append({
                        "node_id": peer.node_id,
                        "version": peer.code_version,
                        "leader_version": my_version,
                    })

                # Check disk warnings
                if peer.disk_percent >= 85:
                    disk_warnings.append({
                        "node_id": peer.node_id,
                        "disk_percent": peer.disk_percent,
                        "disk_free_gb": peer.disk_free_gb,
                    })

                # Check memory warnings
                if peer.memory_percent >= 85:
                    memory_warnings.append({
                        "node_id": peer.node_id,
                        "memory_percent": peer.memory_percent,
                    })

                # Check NFS issues
                if not peer.nfs_accessible:
                    nfs_issues.append({
                        "node_id": peer.node_id,
                    })

            # Calculate cluster health summary
            total_nodes = len(peers_snapshot) + 1  # Include self
            healthy_nodes = total_nodes - len(unhealthy_nodes)
            cluster_health_pct = (healthy_nodes / total_nodes * 100) if total_nodes > 0 else 0

            return web.json_response({
                "success": True,
                "timestamp": time.time(),
                "leader_id": self.node_id,
                "leader_version": my_version,
                "cluster_health": {
                    "total_nodes": total_nodes,
                    "healthy_nodes": healthy_nodes,
                    "unhealthy_nodes": len(unhealthy_nodes),
                    "health_percent": cluster_health_pct,
                },
                "issues": {
                    "unhealthy_nodes": unhealthy_nodes,
                    "code_version_mismatches": code_version_mismatches,
                    "disk_warnings": disk_warnings,
                    "memory_warnings": memory_warnings,
                    "nfs_issues": nfs_issues,
                },
            })
        except Exception as e:  # noqa: BLE001
            logger.error(f"Cluster health check error: {e}")
            return web.json_response({"error": str(e)}, status=500)

    # Relay Handlers moved to scripts/p2p/handlers/relay.py
    # Inherited from RelayHandlersMixin: handle_relay_heartbeat, handle_relay_enqueue,
    # handle_relay_peers, handle_relay_status

    # Gossip Handlers moved to scripts/p2p/handlers/gossip.py
    # Inherited from GossipHandlersMixin: handle_gossip, handle_gossip_anti_entropy

    async def handle_register(self, request: web.Request) -> web.Response:
        """POST /register - Node self-registration for dynamic IP updates.

        Nodes call this endpoint to announce their current IP address.
        Useful when Vast.ai instances restart and get new IPs.

        Request body:
        {
            "node_id": "vast-5090-quad",
            "host": "211.72.13.202",
            "port": 45875,
            "vast_instance_id": "28654132"  // optional
        }
        """
        if not HAS_DYNAMIC_REGISTRY:
            return web.json_response({
                "error": "Dynamic registry not available"
            }, status=501)

        try:
            data = await request.json()
            node_id = data.get("node_id")
            host = data.get("host")
            port = data.get("port", 22)
            vast_instance_id = data.get("vast_instance_id")
            tailscale_ip = data.get("tailscale_ip")

            if not node_id or not host:
                return web.json_response({
                    "error": "Missing required fields: node_id, host"
                }, status=400)

            registry = get_registry()
            success = registry.register_node(node_id, host, port, vast_instance_id, tailscale_ip=tailscale_ip)

            if success:
                logger.info(f"Node registered: {node_id} at {host}:{port}")
                return web.json_response({
                    "success": True,
                    "node_id": node_id,
                    "registered_host": host,
                    "registered_port": port,
                })
            else:
                return web.json_response({
                    "error": "Registration failed"
                }, status=500)

        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=500)

    # NOTE: Registry handlers (handle_registry_status, handle_registry_update_vast,
    # handle_registry_update_aws, handle_registry_update_tailscale, handle_registry_save_yaml)
    # moved to RegistryHandlersMixin in scripts/p2p/handlers/registry.py (Dec 28, 2025)

    # ============================================
    # Connectivity Diagnosis Handlers (SSH/HTTP fallback)
    # ============================================

    async def handle_connectivity_diagnose(self, request: web.Request) -> web.Response:
        """GET /connectivity/diagnose/{node_id} - Diagnose connectivity to a specific node.

        Probes HTTP, Tailscale, and SSH transports and returns latency/reachability
        for each. Helps identify the best transport for communicating with a node.
        """
        node_id = request.match_info.get("node_id", "")
        if not node_id:
            return web.json_response({"error": "node_id required"}, status=400)

        # Find the node's address
        with self.peers_lock:
            peer = self.peers.get(node_id)

        if not peer:
            return web.json_response({
                "error": f"Node {node_id} not found in peers",
                "known_peers": list(self.peers.keys()),
            }, status=404)

        if not HAS_HYBRID_TRANSPORT or not self.hybrid_transport:
            # Fallback: just check if we can reach the node via HTTP
            try:
                info = await self._send_heartbeat_to_peer(peer.host, peer.port)
                return web.json_response({
                    "node_id": node_id,
                    "http_reachable": info is not None,
                    "hybrid_transport_available": False,
                })
            except Exception as e:  # noqa: BLE001
                return web.json_response({
                    "node_id": node_id,
                    "http_reachable": False,
                    "error": str(e),
                    "hybrid_transport_available": False,
                })

        try:
            diagnosis = await diagnose_node_connectivity(node_id, peer.host, peer.port)
            return web.json_response(diagnosis)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=500)

    async def handle_transport_stats(self, request: web.Request) -> web.Response:
        """GET /connectivity/transport_stats - Get transport statistics for all nodes.

        Returns per-node transport preferences and success rates.
        """
        if not HAS_HYBRID_TRANSPORT or not self.hybrid_transport:
            return web.json_response({
                "available": False,
                "message": "Hybrid transport not available",
            })

        try:
            stats = self.hybrid_transport.get_transport_stats()
            return web.json_response({
                "available": True,
                "node_count": len(stats),
                "nodes": stats,
            })
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=500)

    async def handle_probe_vast_nodes(self, request: web.Request) -> web.Response:
        """POST /connectivity/probe_vast - Probe all Vast nodes via SSH.

        Tests SSH connectivity to all vast-* nodes in the registry.
        Useful for diagnosing networking issues with Vast instances.
        """
        if not HAS_HYBRID_TRANSPORT:
            return web.json_response({
                "error": "Hybrid transport not available"
            }, status=501)

        try:
            results = await probe_vast_nodes_via_ssh()
            reachable = sum(1 for r, _ in results.values() if r)

            return web.json_response({
                "total_nodes": len(results),
                "reachable": reachable,
                "unreachable": len(results) - reachable,
                "nodes": {
                    node_id: {"reachable": r, "message": msg}
                    for node_id, (r, msg) in results.items()
                },
            })
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=500)

    # Gauntlet Handlers moved to scripts/p2p/handlers/gauntlet.py
    # Inherited from GauntletHandlersMixin: handle_gauntlet_execute, handle_gauntlet_status,
    # handle_gauntlet_quick_eval, _execute_gauntlet_batch, _execute_single_gauntlet_game,
    # _run_gauntlet_game_sync

    # Admin/Git Handlers moved to scripts/p2p/handlers/admin.py
    # Inherited from AdminHandlersMixin: handle_git_status, handle_git_update, handle_admin_restart

    # Manifest handlers moved to scripts/p2p/handlers/manifest.py (Dec 28, 2025 - Phase 8)
    # Inherited from ManifestHandlersMixin: handle_data_manifest, handle_cluster_data_manifest, handle_refresh_manifest

    # CMA-ES Handlers moved to scripts/p2p/handlers/cmaes.py
    # Inherited from CMAESHandlersMixin:
    # - handle_cmaes_start, handle_cmaes_evaluate
    # - handle_cmaes_status, handle_cmaes_result

    async def _run_distributed_cmaes(self, job_id: str):
        """Main coordinator loop for distributed CMA-ES.

        Integrates with CMA-ES algorithm to optimize heuristic weights.
        Distributes candidate evaluation across GPU workers in the cluster.
        """
        try:
            state = self.distributed_cmaes_state.get(job_id)
            if not state:
                return

            logger.info(f"CMA-ES coordinator started for job {job_id}")
            logger.info(f"Config: {state.generations} gens, pop={state.population_size}, {state.games_per_eval} games/eval")

            # Try to import CMA-ES library
            try:
                import cma
                import numpy as np
            except ImportError:
                logger.info("CMA-ES requires: pip install cma numpy")
                state.status = "error: cma not installed"
                return

            # Default heuristic weights to optimize
            weight_names = [
                "material_weight", "ring_count_weight", "stack_height_weight",
                "center_control_weight", "territory_weight", "mobility_weight",
                "line_potential_weight", "defensive_weight",
            ]
            default_weights = {
                "material_weight": 1.0, "ring_count_weight": 0.5,
                "stack_height_weight": 0.3, "center_control_weight": 0.4,
                "territory_weight": 0.8, "mobility_weight": 0.2,
                "line_potential_weight": 0.6, "defensive_weight": 0.3,
            }

            # Convert to vector for CMA-ES
            x0 = np.array([default_weights[n] for n in weight_names])

            # Initialize CMA-ES
            es = cma.CMAEvolutionStrategy(x0, 0.5, {
                'popsize': state.population_size,
                'maxiter': state.generations,
                'bounds': [0, 2],  # Weights between 0 and 2
            })

            state.current_generation = 0

            while not es.stop() and state.status == "running":
                state.current_generation += 1
                state.last_update = time.time()

                # Get candidate solutions
                solutions = es.ask()

                # Distribute evaluations across workers
                fitness_results = {}
                pending_evals = {}

                for idx, sol in enumerate(solutions):
                    weights = {name: float(sol[i]) for i, name in enumerate(weight_names)}

                    # Round-robin assign to workers
                    if state.worker_nodes:
                        worker_idx = idx % len(state.worker_nodes)
                        worker_id = state.worker_nodes[worker_idx]

                        # Send evaluation request to worker
                        eval_id = f"{job_id}_gen{state.current_generation}_idx{idx}"
                        pending_evals[eval_id] = idx

                        try:
                            with self.peers_lock:
                                worker = self.peers.get(worker_id)
                            if worker:
                                timeout = ClientTimeout(total=300)
                                async with get_client_session(timeout) as session:
                                    url = self._url_for_peer(worker, "/cmaes/evaluate")
                                    await session.post(url, json={
                                        "job_id": job_id,
                                        "weights": weights,
                                        "generation": state.current_generation,
                                        "individual_idx": idx,
                                        "games_per_eval": state.games_per_eval,
                                        "board_type": state.board_type,
                                        "num_players": state.num_players,
                                    }, headers=self._auth_headers())
                        except Exception as e:  # noqa: BLE001
                            logger.error(f"Failed to send eval to {worker_id}: {e}")
                            # Fall back to local evaluation
                            fitness = await self._evaluate_cmaes_weights_local(
                                weights, state.games_per_eval, state.board_type, state.num_players
                            )
                            fitness_results[idx] = fitness

                # Wait for results with timeout
                wait_start = time.time()
                len(solutions) - len(fitness_results)
                while len(fitness_results) < len(solutions) and (time.time() - wait_start) < 300:
                    await asyncio.sleep(1)
                    state.last_update = time.time()

                    # Check for results that came in via /cmaes/result endpoint
                    # Results are stored in state.pending_results by handle_cmaes_result
                    for idx in range(len(solutions)):
                        if idx in fitness_results:
                            continue
                        result_key = f"{state.current_generation}_{idx}"
                        if result_key in state.pending_results:
                            fitness_results[idx] = state.pending_results[result_key]
                            del state.pending_results[result_key]  # Clean up

                    # Progress logging every 30 seconds
                    elapsed = time.time() - wait_start
                    if int(elapsed) % 30 == 0 and elapsed > 1:
                        received = len(fitness_results)
                        logger.info(f"Gen {state.current_generation}: {received}/{len(solutions)} results received ({elapsed:.0f}s elapsed)")

                # Fill in any missing results with default fitness
                fitnesses = []
                for idx in range(len(solutions)):
                    fitness = fitness_results.get(idx, 0.5)  # Default to 0.5 if no result
                    fitnesses.append(-fitness)  # CMA-ES minimizes, so negate

                # Update CMA-ES
                es.tell(solutions, fitnesses)

                # Track best
                best_idx = np.argmin(fitnesses)
                if -fitnesses[best_idx] > state.best_fitness:
                    state.best_fitness = -fitnesses[best_idx]
                    state.best_weights = {name: float(solutions[best_idx][i]) for i, name in enumerate(weight_names)}

                logger.info(f"Gen {state.current_generation}: best_fitness={state.best_fitness:.4f}")

            state.status = "completed"
            logger.info(f"CMA-ES job {job_id} completed: best_fitness={state.best_fitness:.4f}")
            logger.info(f"Best weights: {state.best_weights}")

            # Feed CMA-ES results back to improvement cycle manager
            if self.improvement_cycle_manager and state.best_weights:
                try:
                    agent_id = self.improvement_cycle_manager.handle_cmaes_complete(
                        state.board_type, state.num_players, state.best_weights
                    )
                    logger.info(f"CMA-ES weights registered as agent: {agent_id}")
                    self.diversity_metrics["cmaes_triggers"] += 1

                    # Save weights to file for future use
                    weights_file = Path(self.ringrift_path) / "ai-service" / "data" / "cmaes" / f"best_weights_{state.board_type}_{state.num_players}p.json"
                    weights_file.parent.mkdir(parents=True, exist_ok=True)
                    import json as json_mod
                    with open(weights_file, "w") as f:
                        json_mod.dump({
                            "weights": state.best_weights,
                            "fitness": state.best_fitness,
                            "job_id": job_id,
                            "generation": state.current_generation,
                            "timestamp": time.time(),
                        }, f, indent=2)
                    logger.info(f"Saved CMA-ES weights to {weights_file}")

                    # Propagate new weights to selfplay jobs
                    asyncio.create_task(self._propagate_cmaes_weights(
                        state.board_type, state.num_players, state.best_weights
                    ))
                except Exception as e:  # noqa: BLE001
                    logger.error(f"Failed to register CMA-ES weights: {e}")

        except Exception as e:  # noqa: BLE001
            import traceback
            logger.info(f"CMA-ES coordinator error: {e}")
            traceback.print_exc()
            if job_id in self.distributed_cmaes_state:
                self.distributed_cmaes_state[job_id].status = f"error: {e}"

    async def _evaluate_cmaes_weights_local(
        self, weights: dict, num_games: int, board_type: str, num_players: int
    ) -> float:
        """Evaluate weights locally by running selfplay games."""
        try:
            sem = getattr(self, "_cmaes_eval_semaphore", None)
            if sem is None:
                sem = asyncio.Semaphore(1)

            async with sem:
                # Run selfplay subprocess to evaluate weights
                import json as json_mod
                import tempfile

                with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                    json_mod.dump(weights, f)
                    weights_file = f.name

                ai_service_path = str(Path(self.ringrift_path) / "ai-service")
                cmd = [
                    sys.executable, "-c", f"""
import sys
sys.path.insert(0, '{ai_service_path}')
from app.game_engine import GameEngine
from app.ai.heuristic_ai import HeuristicAI
from app.models import AIConfig, BoardType, GameStatus
from app.training.generate_data import create_initial_state
import json

weights = json.load(open('{weights_file}'))
board_type = BoardType('{board_type}')
wins = 0
total = {num_games}

for i in range(total):
    state = create_initial_state(board_type, num_players={num_players})
    engine = GameEngine()

    # Candidate with custom weights vs baseline
    config_candidate = AIConfig(difficulty=5, randomness=0.1, think_time=500, custom_weights=weights)
    config_baseline = AIConfig(difficulty=5, randomness=0.1, think_time=500)

    ai_candidate = HeuristicAI(1, config_candidate)
    ai_baseline = HeuristicAI(2, config_baseline)

    move_count = 0
    while state.game_status == GameStatus.ACTIVE and move_count < 300:
        current_ai = ai_candidate if state.current_player == 1 else ai_baseline
        move = current_ai.select_move(state)
        if move is None:
            break
        state = engine.apply_move(state, move)
        move_count += 1

    if state.winner == 1:
        wins += 1
    elif state.winner is None:
        wins += 0.5  # Draw counts as half

print(wins / total)
"""
                ]

                proc = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                    env={**os.environ, "PYTHONPATH": ai_service_path},
                )
                stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=300)

                # Clean up temp file
                os.unlink(weights_file)

                if proc.returncode == 0:
                    return float(stdout.decode().strip())
                else:
                    logger.info(f"Local eval error: {stderr.decode()}")
                    return 0.5

        except Exception as e:  # noqa: BLE001
            logger.info(f"Local CMA-ES evaluation error: {e}")
            return 0.5

    async def _evaluate_cmaes_weights(
        self, job_id: str, weights: dict, generation: int, individual_idx: int,
        games_per_eval: int = 5, board_type: str = "square8", num_players: int = 2
    ):
        """Evaluate weights locally and report result to coordinator."""
        try:
            # Run local evaluation using passed parameters (workers don't have state)
            fitness = await self._evaluate_cmaes_weights_local(
                weights, games_per_eval, board_type, num_players
            )

            logger.info(f"Completed local CMA-ES evaluation: job={job_id}, gen={generation}, idx={individual_idx}, fitness={fitness:.4f}")

            # If we're not the coordinator, report result back
            if self.role != NodeRole.LEADER and self.leader_id:
                with self.peers_lock:
                    leader = self.peers.get(self.leader_id)
                if leader:
                    try:
                        timeout = ClientTimeout(total=30)
                        async with get_client_session(timeout) as session:
                            url = self._url_for_peer(leader, "/cmaes/result")
                            await session.post(url, json={
                                "job_id": job_id,
                                "generation": generation,
                                "individual_idx": individual_idx,
                                "fitness": fitness,
                                "weights": weights,
                                "worker_id": self.node_id,
                            }, headers=self._auth_headers())
                    except Exception as e:  # noqa: BLE001
                        logger.error(f"Failed to report CMA-ES result to leader: {e}")

        except Exception as e:  # noqa: BLE001
            logger.info(f"CMA-ES evaluation error: {e}")

    # Tournament Handlers moved to scripts/p2p/handlers/tournament.py
    # Inherited from TournamentHandlersMixin:
    # - handle_tournament_start, handle_tournament_match
    # - handle_tournament_status, handle_tournament_result

    async def handle_play_elo_match(self, request: web.Request) -> web.Response:
        """Play a single Elo calibration match between AI configurations.

        This endpoint supports playing games between different AI types
        (random, heuristic, minimax, mcts, policy_only, gumbel_mcts, descent)
        for Elo calibration purposes. Supports 2-4 player games.

        Request body:
            match_id: Unique match identifier
            agent_a: Agent A identifier (e.g., "random", "mcts_neural")
            agent_b: Agent B identifier
            agent_a_config: Full AI configuration for agent A
            agent_b_config: Full AI configuration for agent B
            agents: List of agent identifiers for multiplayer (optional)
            agent_configs: List of AI configs for multiplayer (optional)
            board_type: Board type (default: square8)
            num_players: Number of players (default: 2)

        Returns:
            success: True if match completed
            winner: "agent_a", "agent_b", "agent_c", "agent_d", or "draw"
            game_length: Number of moves
            duration_sec: Game duration in seconds
        """
        try:
            logger.info("Tournament endpoint called, parsing request...")
            data = await request.json()
            logger.info(f"Request parsed: {data}")

            match_id = data.get("match_id", str(uuid.uuid4())[:8])
            board_type_str = data.get("board_type", "square8")
            num_players = data.get("num_players", 2)

            # Support both legacy 2-player (agent_a/agent_b) and multiplayer (agents list)
            agents_list = data.get("agents")
            agent_configs_list = data.get("agent_configs")

            if agents_list and len(agents_list) >= 2:
                # Multiplayer mode: use agents list
                agents = agents_list[:num_players]
                if agent_configs_list and len(agent_configs_list) >= num_players:
                    agent_configs = agent_configs_list[:num_players]
                else:
                    # Build configs from agent names
                    agent_configs = [{"ai_type": a} for a in agents]
            else:
                # Legacy 2-player mode
                agent_a = data.get("agent_a", "random")
                agent_b = data.get("agent_b", "heuristic")
                agent_a_config = data.get("agent_a_config", {"ai_type": agent_a})
                agent_b_config = data.get("agent_b_config", {"ai_type": agent_b})
                agents = [agent_a, agent_b]
                agent_configs = [agent_a_config, agent_b_config]

            # Pad with random if fewer agents than players
            while len(agents) < num_players:
                agents.append("random")
                agent_configs.append({"ai_type": "random"})

            agents_desc = " vs ".join(agents)
            logger.info(f"Playing Elo match {match_id}: {agents_desc}")
            start_time = time.time()

            # Acquire semaphore to prevent concurrent matches (OOM protection)
            # Create lazily in async context to avoid event loop issues
            if self._tournament_match_semaphore is None:
                logger.info("Creating tournament semaphore...")
                self._tournament_match_semaphore = asyncio.Semaphore(1)

            # Try to acquire semaphore with timeout to avoid deadlocks
            # If we can't get the semaphore within 30 seconds, fail fast
            logger.info(f"Acquiring semaphore (current holder: {getattr(self, '_current_match_holder', 'none')})...")
            try:
                await asyncio.wait_for(
                    self._tournament_match_semaphore.acquire(),
                    timeout=30.0
                )
            except asyncio.TimeoutError:
                logger.info(f"Semaphore acquisition timed out after 30s (holder: {getattr(self, '_current_match_holder', 'unknown')})")
                return web.json_response({
                    "success": False,
                    "error": f"Server busy - another match in progress (holder: {getattr(self, '_current_match_holder', 'unknown')})",
                    "match_id": match_id,
                }, status=503)

            # Track who holds the semaphore for debugging
            self._current_match_holder = f"{match_id} ({agents_desc})"
            try:
                logger.info(f"Semaphore acquired, running match {match_id}...")
                # Run the match in a thread pool to avoid blocking
                # Add 5-minute timeout to prevent hung matches
                try:
                    result = await asyncio.wait_for(
                        asyncio.to_thread(
                            self._play_elo_match_sync,
                            agent_configs,
                            board_type_str,
                            num_players,
                            match_id,
                            agents,
                        ),
                        timeout=300.0,  # 5 minute timeout for tournament matches
                    )
                except asyncio.TimeoutError:
                    logger.info(f"Elo match {match_id} timed out after 5 minutes")
                    return web.json_response({
                        "success": False,
                        "error": "Match timed out after 5 minutes",
                        "match_id": match_id,
                    }, status=504)
            finally:
                # Always release semaphore and clear holder
                self._current_match_holder = None
                self._tournament_match_semaphore.release()
                logger.info(f"Semaphore released for match {match_id}")

            duration = time.time() - start_time

            if result is None:
                return web.json_response({
                    "success": False,
                    "error": "Match failed to complete",
                    "match_id": match_id,
                }, status=500)

            # Map winner player number to agent label
            winner_player = result.get("winner_player")
            if winner_player is not None and winner_player > 0:
                agent_labels = ["agent_a", "agent_b", "agent_c", "agent_d"]
                winner = agent_labels[winner_player - 1] if winner_player <= len(agent_labels) else "draw"
            else:
                # Legacy format support
                winner_map = {"model_a": "agent_a", "model_b": "agent_b", "draw": "draw"}
                winner = winner_map.get(result.get("winner", "draw"), "draw")

            response = {
                "success": True,
                "match_id": match_id,
                "agents": agents,
                "agent_a": agents[0] if len(agents) > 0 else "unknown",
                "agent_b": agents[1] if len(agents) > 1 else "unknown",
                "winner": winner,
                "winner_player": winner_player,
                "game_length": result.get("game_length", 0),
                "duration_sec": duration,
                "worker_node": self.node_id,
            }

            logger.info(f"Elo match {match_id} complete: {agents_desc} -> {response['winner']} ({result.get('game_length', 0)} moves)")
            return web.json_response(response)

        except Exception as e:  # noqa: BLE001
            import traceback
            logger.info(f"Elo match error: {e}")
            traceback.print_exc()
            return web.json_response({"error": str(e)}, status=500)

    def _play_elo_match_sync(
        self,
        agent_configs: list[dict],
        board_type_str: str,
        num_players: int,
        match_id: str,
        agent_ids: list[str] | None = None,
    ) -> dict | None:
        """Synchronous wrapper for playing an Elo match.

        Uses a lightweight implementation for simple AI types (random, heuristic, minimax)
        to avoid loading heavy neural network dependencies that cause OOM.

        Args:
            agent_configs: List of AI configurations for each player
            board_type_str: Board type string
            num_players: Number of players in the game
        """
        try:
            import time as time_mod

            from app.db.unified_recording import (
                RecordingConfig,
                RecordSource,
                UnifiedGameRecorder,
                is_recording_enabled,
            )
            from app.game_engine import GameEngine
            from app.models import AIConfig, AIType, BoardType, GameStatus
            from app.training.initial_state import create_initial_state

            board_type = BoardType(board_type_str)
            start_time = time_mod.time()

            # Generate unique random seeds for each player
            import random as rand_mod
            match_seed = int(time_mod.time() * 1000000) % (2**31)
            rand_mod.seed(match_seed)
            seeds = [rand_mod.randint(0, 2**31 - 1) for _ in range(num_players)]

            # Create initial state
            state = create_initial_state(board_type, num_players)
            engine = GameEngine()

            # Map agent names to AI types
            def get_ai_type(agent_config: dict) -> str:
                ai_type = agent_config.get("ai_type", "random")
                if isinstance(ai_type, str):
                    return ai_type.lower()
                return str(ai_type).lower()

            def create_lightweight_ai(agent_config: dict, player_num: int, rng_seed: int):
                """Create AI without loading heavy dependencies."""
                ai_type = get_ai_type(agent_config)

                if ai_type in ("random", "aitype.random"):
                    from app.ai.random_ai import RandomAI
                    config = AIConfig(ai_type=AIType.RANDOM, board_type=board_type, difficulty=1, rng_seed=rng_seed)
                    return RandomAI(player_num, config)

                elif ai_type in ("heuristic", "aitype.heuristic"):
                    from app.ai.heuristic_ai import HeuristicAI
                    config = AIConfig(ai_type=AIType.HEURISTIC, board_type=board_type, difficulty=3, rng_seed=rng_seed)
                    return HeuristicAI(player_num, config)

                elif ai_type in ("minimax", "minimax_heuristic", "aitype.minimax"):
                    from app.ai.minimax_ai import MinimaxAI
                    use_nn = agent_config.get("use_neural_net", False)
                    max_depth = agent_config.get("max_depth", 3)
                    config = AIConfig(
                        ai_type=AIType.MINIMAX,
                        board_type=board_type,
                        difficulty=agent_config.get("difficulty", 3),
                        use_neural_net=use_nn,
                        max_depth=max_depth,
                        rng_seed=rng_seed,
                    )
                    return MinimaxAI(player_num, config)

                elif ai_type in ("mcts", "mcts_heuristic", "aitype.mcts"):
                    from app.ai.mcts_ai import MCTSAI
                    use_nn = agent_config.get("use_neural_net", False)
                    iters = agent_config.get("mcts_iterations", 100)
                    config = AIConfig(
                        ai_type=AIType.MCTS,
                        board_type=board_type,
                        difficulty=agent_config.get("difficulty", 5),
                        use_neural_net=use_nn,
                        mcts_iterations=iters,
                        rng_seed=rng_seed,
                    )
                    return MCTSAI(player_num, config)

                elif ai_type in ("descent", "aitype.descent"):
                    # Descent AI is CPU-based but can be slow at high difficulty
                    # Cap at difficulty 5 for tournament matches (~1.1s/move)
                    from app.ai.descent_ai import DescentAI
                    requested_diff = agent_config.get("difficulty", 5)
                    capped_diff = min(requested_diff, 5)  # Cap at 5 for tournaments
                    if capped_diff < requested_diff:
                        logger.info(f"Descent AI difficulty capped from {requested_diff} to {capped_diff} for tournament")
                    config = AIConfig(
                        ai_type=AIType.DESCENT,
                        board_type=board_type,
                        difficulty=capped_diff,
                        rng_seed=rng_seed,
                    )
                    return DescentAI(player_num, config)

                else:
                    # For neural-net based types (policy_only, gumbel_mcts, mcts_neural),
                    # check available memory before loading
                    import psutil
                    mem = psutil.virtual_memory()
                    available_gb = mem.available / (1024**3)

                    # Require at least 8GB free for neural network loading (conservative to prevent OOM)
                    if available_gb < 8.0:
                        logger.info(f"Skipping NN-based AI {ai_type}: only {available_gb:.1f}GB available (need 8GB)")
                        # Fall back to descent AI (CPU-based, no NN)
                        from app.ai.descent_ai import DescentAI
                        config = AIConfig(ai_type=AIType.DESCENT, board_type=board_type, difficulty=7, rng_seed=rng_seed)
                        return DescentAI(player_num, config)

                    # Safe to load neural network AI
                    try:
                        from scripts.run_model_elo_tournament import create_ai_from_model
                        return create_ai_from_model(agent_config, player_num, board_type)
                    except Exception as e:  # noqa: BLE001
                        logger.error(f"Failed to create NN AI {ai_type}: {e}, falling back to heuristic")
                        from app.ai.heuristic_ai import HeuristicAI
                        config = AIConfig(ai_type=AIType.HEURISTIC, board_type=board_type, difficulty=7, rng_seed=rng_seed)
                        return HeuristicAI(player_num, config)

            # Create AIs for all players with unique seeds
            ais = {}
            for i in range(num_players):
                player_num = i + 1
                config = agent_configs[i] if i < len(agent_configs) else {"ai_type": "random"}
                seed = seeds[i] if i < len(seeds) else 0
                ais[player_num] = create_lightweight_ai(config, player_num, seed)

            # Keep initial state for training record
            initial_state = state

            if not agent_ids:
                agent_ids = [
                    str(cfg.get("agent_id") or cfg.get("ai_type") or f"player_{idx + 1}")
                    for idx, cfg in enumerate(agent_configs[:num_players])
                ]
            while len(agent_ids) < num_players:
                agent_ids.append(f"player_{len(agent_ids) + 1}")

            # Play game and record actual Move objects for training
            move_count = 0
            max_moves = 500
            recorded_moves = []  # List of actual Move objects for training
            termination_reason = "completed"
            recording_enabled = is_recording_enabled()
            tags = [
                "elo_tournament",
                f"node_{self.node_id}",
                f"board_{board_type.value}",
                f"players_{num_players}",
            ]
            for idx, cfg in enumerate(agent_configs[:num_players]):
                ai_type = cfg.get("ai_type", "unknown")
                tags.append(f"p{idx + 1}_{ai_type}")

            recording_config = RecordingConfig(
                board_type=board_type.value,
                num_players=num_players,
                source=RecordSource.TOURNAMENT,
                engine_mode="p2p_elo",
                db_prefix="tournament",
                db_dir="data/games",
                store_history_entries=True,
                fsm_validation=True,
                tags=tags,
            )
            recorder = UnifiedGameRecorder(recording_config, state, game_id=match_id) if recording_enabled else None

            try:
                if recorder is not None:
                    recorder.__enter__()

                while state.game_status == GameStatus.ACTIVE and move_count < max_moves:
                    current_player = state.current_player

                    requirement = GameEngine.get_phase_requirement(state, current_player)
                    if requirement is not None:
                        move = GameEngine.synthesize_bookkeeping_move(requirement, state)
                    else:
                        current_ai = ais.get(current_player)
                        if current_ai is None:
                            logger.warning(f"No AI for player {current_player}, using random fallback")
                            from app.ai.random_ai import RandomAI
                            config = AIConfig(ai_type=AIType.RANDOM, board_type=board_type, difficulty=1)
                            current_ai = RandomAI(current_player, config)
                            ais[current_player] = current_ai
                        move = current_ai.select_move(state)

                    if move is None:
                        termination_reason = "no_move"
                        break

                    # Get soft policy targets for training data
                    move_probs = None
                    if hasattr(current_ai, 'get_visit_distribution'):
                        try:
                            moves_dist, probs_dist = current_ai.get_visit_distribution()
                            if moves_dist and probs_dist:
                                move_probs = {}
                                for mv, prob in zip(moves_dist, probs_dist, strict=False):
                                    # Create move key in format: "{from_x},{from_y}->{to_x},{to_y}"
                                    if hasattr(mv, 'to') and mv.to is not None:
                                        move_key = f"{mv.to.x},{mv.to.y}"
                                        if hasattr(mv, 'from_pos') and mv.from_pos is not None:
                                            move_key = f"{mv.from_pos.x},{mv.from_pos.y}->{move_key}"
                                        move_probs[move_key] = float(prob)
                        except (ValueError, KeyError, IndexError, AttributeError):
                            pass  # Silently ignore if visit distribution fails

                    # Record actual Move object for training
                    recorded_moves.append(move)

                    state_before = state
                    state = engine.apply_move(state, move, trace_mode=True)
                    move_count += 1
                    if recorder is not None:
                        recorder.add_move(
                            move,
                            state_after=state,
                            state_before=state_before,
                            available_moves_count=None,
                            move_probs=move_probs,
                        )
            finally:
                if move_count >= max_moves and state.game_status == GameStatus.ACTIVE:
                    termination_reason = "max_moves"

                if recorder is not None:
                    winner_player = state.winner if state.winner else 0
                    winner_agent = None
                    if isinstance(winner_player, int) and winner_player > 0 and winner_player <= len(agent_ids):
                        winner_agent = agent_ids[winner_player - 1]

                    extra_metadata = {
                        "match_id": match_id,
                        "tournament_id": f"p2p_elo_{match_id}",
                        "node_id": self.node_id,
                        "match_seed": match_seed,
                        "agent_ids": agent_ids,
                        "agent_configs": agent_configs,
                        "winner_player": winner_player,
                        "winner_agent": winner_agent,
                        "game_length": move_count,
                        "duration_sec": time_mod.time() - start_time,
                        "termination_reason": termination_reason,
                    }
                    try:
                        recorder.finalize(state, extra_metadata=extra_metadata)
                    finally:
                        recorder.__exit__(None, None, None)

            duration = time_mod.time() - start_time

            # Determine winner (as player number)
            winner_player = state.winner if state.winner else 0

            # Legacy format for 2-player backward compatibility
            winner = "draw"
            if winner_player == 1:
                winner = "model_a"
            elif winner_player == 2:
                winner = "model_b"

            # Save game for training using proper GameRecord format
            if recorded_moves and winner_player > 0:
                try:
                    self._save_tournament_game_for_training(
                        initial_state=initial_state,
                        final_state=state,
                        moves=recorded_moves,
                        match_seed=match_seed,
                        agent_configs=agent_configs,
                    )
                except Exception as e:  # noqa: BLE001
                    logger.warning(f"Failed to save tournament game for training: {e}")

            return {
                "winner": winner,
                "winner_player": winner_player,
                "game_length": move_count,
                "duration_sec": duration,
            }

        except Exception as e:  # noqa: BLE001
            import traceback
            logger.info(f"_play_elo_match_sync error: {e}")
            traceback.print_exc()
            return None

    def _save_tournament_game_for_training(
        self,
        initial_state,  # GameState
        final_state,  # GameState
        moves: list,  # List of Move objects
        match_seed: int,
        agent_configs: list[dict] | None = None,  # Optional AI configs for metadata
    ) -> None:
        """Save a tournament game to JSONL format for training.

        Uses build_training_game_record to create proper GameRecord format
        compatible with the training pipeline. The saved games can be ingested
        by the training system alongside selfplay games.

        Saves games to data/tournament_games/{board_type}_{num_players}p/ directory.

        Args:
            initial_state: The initial game state
            final_state: The final game state after all moves
            moves: List of Move objects representing the game
            match_seed: RNG seed used for this match
            agent_configs: Optional list of AI configs for each player
        """
        import json
        import sys
        from datetime import datetime, timezone
        from pathlib import Path

        # Ensure app module is importable
        ai_service_path = str(Path(self.ringrift_path) / "ai-service")
        if ai_service_path not in sys.path:
            sys.path.insert(0, ai_service_path)

        try:
            from app.models.game_record import RecordSource
            from app.training.game_record_export import build_training_game_record
        except ImportError as e:
            logger.warning(f"Cannot import game record modules: {e}")
            return

        board_type_str = initial_state.board_type.value if hasattr(initial_state.board_type, 'value') else str(initial_state.board_type)
        num_players = len(initial_state.players)

        # Create output directory
        data_dir = Path(self.ringrift_path) / "ai-service" / "data" / "tournament_games"
        config_dir = data_dir / f"{board_type_str}_{num_players}p"
        config_dir.mkdir(parents=True, exist_ok=True)

        # Create game ID with full metadata
        timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
        game_id = f"tournament_{self.node_id}_{timestamp}_{uuid.uuid4().hex[:8]}"

        # Build tags with detailed metadata for training filtering
        tags = [
            "elo_tournament",
            f"node_{self.node_id}",
            f"board_{board_type_str}",
            f"players_{num_players}",
        ]
        if agent_configs:
            for i, cfg in enumerate(agent_configs[:num_players]):
                ai_type = cfg.get("ai_type", "unknown")
                tags.append(f"player{i+1}_{ai_type}")

        # Build proper GameRecord using the training export function
        try:
            game_record = build_training_game_record(
                game_id=game_id,
                initial_state=initial_state,
                final_state=final_state,
                moves=moves,
                source=RecordSource.TOURNAMENT,
                rng_seed=match_seed,
                terminated_by_budget_only=False,
                created_at=datetime.now(timezone.utc),
                tags=tags,
                fsm_validated=None,  # Not FSM validated in tournament context
            )

            # Use the canonical to_jsonl_line() method for proper serialization
            jsonl_line = game_record.to_jsonl_line()

            # Append to daily file for this config
            daily_file = config_dir / f"tournament_{timestamp[:8]}.jsonl"
            with open(daily_file, "a", encoding="utf-8") as f:
                f.write(jsonl_line + "\n")

            logger.info(f"Saved tournament game {game_id} to {daily_file} ({len(moves)} moves, winner={final_state.winner})")

        except Exception as e:  # noqa: BLE001
            import traceback
            logger.warning(f"Failed to build/save tournament game record: {e}")
            traceback.print_exc()

    # -------------------------------------------------------------------------
    # SSH Tournament Handlers - EXTRACTED to scripts/p2p/handlers/ssh_tournament.py
    # Provides: handle_ssh_tournament_start, handle_ssh_tournament_status,
    #           handle_ssh_tournament_cancel, _monitor_ssh_tournament_process
    # See SSHTournamentHandlersMixin
    # -------------------------------------------------------------------------

    # NOTE: _run_distributed_tournament() removed Dec 27, 2025 (~9 LOC)
    # Use self.job_manager.run_distributed_tournament() directly.

    async def _send_match_to_worker(self, job_id: str, worker_id: str, match: dict):
        """Send a match to a worker node."""
        try:
            with self.peers_lock:
                worker = self.peers.get(worker_id)
            if not worker:
                return

            timeout = ClientTimeout(total=10)
            async with get_client_session(timeout) as session:
                url = self._url_for_peer(worker, "/tournament/match")
                await session.post(url, json={"job_id": job_id, "match": match}, headers=self._auth_headers())
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to send match to worker {worker_id}: {e}")

    async def _play_tournament_match(self, job_id: str, match_info: dict) -> dict | None:
        """Play a tournament match locally using subprocess selfplay.

        Dec 28, 2025: Fixed to accept both field name conventions:
        - agent1/agent2 (from tournament handler)
        - player1_model/player2_model (from JobManager)

        Returns:
            Match result dict or None on error
        """
        try:
            import json as json_module
            import sys

            # Support both naming conventions
            agent1 = match_info.get("agent1") or match_info.get("player1_model")
            agent2 = match_info.get("agent2") or match_info.get("player2_model")
            game_num = match_info.get("game_num", 0)
            board_type = match_info.get("board_type", "square8")
            num_players = match_info.get("num_players", 2)

            logger.info(f"Playing tournament match: {agent1} vs {agent2} (game {game_num})")

            # Build the subprocess command to run a single game
            # Agent IDs map to model paths or heuristic configurations
            # Dec 28, 2025: Fixed imports and game playing logic
            game_script = f"""
import sys
sys.path.insert(0, '{self.ringrift_path}/ai-service')
from app.training.initial_state import create_initial_state
from app.rules import get_rules_engine
from app.ai.heuristic_ai import HeuristicAI
from app.ai.random_ai import RandomAI
from app.models import AIConfig
import json
import os

# Skip shadow contracts for performance
os.environ['RINGRIFT_SKIP_SHADOW_CONTRACTS'] = 'true'

def load_agent(agent_id: str, player_idx: int, board_type: str, num_players: int):
    '''Load agent by ID - supports random, heuristic, or model paths.'''
    # Dec 29, 2025: Added difficulty=5 (medium) as required by AIConfig
    config = AIConfig(board_type=board_type, num_players=num_players, difficulty=5)
    if agent_id == 'random':
        return RandomAI(player_idx, config=config)
    elif agent_id == 'heuristic':
        return HeuristicAI(player_idx, config=config)
    elif agent_id.startswith('heuristic:'):
        # Parse weights from agent ID: "heuristic:w1,w2,w3,..."
        weight_str = agent_id.split(':')[1]
        weights = [float(w) for w in weight_str.split(',')]
        weight_names = [
            "material_weight", "ring_count_weight", "stack_height_weight",
            "center_control_weight", "territory_weight", "mobility_weight",
            "line_potential_weight", "defensive_weight",
        ]
        weight_dict = dict(zip(weight_names, weights))
        config.heuristic_weights = weight_dict
        return HeuristicAI(player_idx, config=config)
    elif agent_id.startswith('model:') or agent_id.startswith('canonical_'):
        # Neural network model - for now, fall back to heuristic
        # TODO: Load actual neural network models
        return HeuristicAI(player_idx, config=config)
    else:
        # Default heuristic agent
        return HeuristicAI(player_idx, config=config)

# Initialize game state and engine
engine = get_rules_engine(skip_shadow_contracts=True)
state = create_initial_state(board_type='{board_type}', num_players={num_players})
agents = [
    load_agent('{agent1}', 0, '{board_type}', {num_players}),
    load_agent('{agent2}', 1, '{board_type}', {num_players}),
]

# Play until completion
max_moves = 10000
move_count = 0
while not state.game_over and move_count < max_moves:
    current_player = state.current_player_index
    agent = agents[current_player]
    move = agent.select_move(state)
    if move is None:
        break
    state = engine.apply_move(state, move)
    move_count += 1

# Get result
winner_idx = None
victory_type = 'unknown'
if state.game_over:
    # Find winner from scores
    scores = state.player_scores
    if scores:
        max_score = max(scores)
        if scores.count(max_score) == 1:
            winner_idx = scores.index(max_score)

# Map winner index to agent ID
winner_agent = None
if winner_idx == 0:
    winner_agent = '{agent1}'
elif winner_idx == 1:
    winner_agent = '{agent2}'

result = {{
    'agent1': '{agent1}',
    'agent2': '{agent2}',
    'winner': winner_agent,
    'winner_idx': winner_idx,
    'victory_type': victory_type,
    'move_count': move_count,
    'game_num': {game_num},
}}
print(json.dumps(result))
"""
            # Run the game in subprocess
            cmd = [sys.executable, "-c", game_script]
            env = os.environ.copy()
            env["PYTHONPATH"] = f"{self.ringrift_path}/ai-service"
            env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

            proc = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env,
            )

            stdout, stderr = await asyncio.wait_for(
                proc.communicate(),
                timeout=300  # 5 minute timeout per game
            )

            if proc.returncode != 0:
                logger.info(f"Tournament match subprocess error: {stderr.decode()}")
                result = {
                    "agent1": agent1,
                    "agent2": agent2,
                    "winner": None,
                    "error": stderr.decode()[:200],
                    "game_num": game_num,
                }
            else:
                # Parse result from stdout
                output_lines = stdout.decode().strip().split('\n')
                result_line = output_lines[-1] if output_lines else '{}'
                result = json_module.loads(result_line)

            logger.info(f"Match result: {agent1} vs {agent2} -> winner={result.get('winner')}")

            # Report result back to coordinator (leader)
            if self.role != NodeRole.LEADER and self.leader_id:
                with self.peers_lock:
                    leader = self.peers.get(self.leader_id)
                if leader:
                    try:
                        timeout = ClientTimeout(total=10)
                        async with get_client_session(timeout) as session:
                            url = self._url_for_peer(leader, "/tournament/result")
                            await session.post(url, json={
                                "job_id": job_id,
                                "result": result,
                                "worker_id": self.node_id,
                            }, headers=self._auth_headers())
                    except Exception as e:  # noqa: BLE001
                        logger.error(f"Failed to report tournament result to leader: {e}")
            else:
                # We are the leader, update state directly
                if job_id in self.distributed_tournament_state:
                    state = self.distributed_tournament_state[job_id]
                    state.results.append(result)
                    state.completed_matches += 1
                    state.last_update = time.time()

            # Dec 28, 2025: Return result for synchronous handler usage
            return result

        except asyncio.TimeoutError:
            logger.info(f"Tournament match timed out: {match_info}")
            return None
        except Exception as e:  # noqa: BLE001
            logger.info(f"Tournament match error: {e}")
            return None

    # NOTE: _calculate_tournament_ratings removed Dec 27, 2025 (dead code, never called)
    # Elo rating calculation is now handled in JobManager.run_distributed_tournament()

    # =========================================================================
    # NOTE: Improvement handlers moved to scripts/p2p/handlers/improvement.py (Dec 28, 2025 - Phase 8)
    # Inherited from ImprovementHandlersMixin:
    # - handle_improvement_start, handle_improvement_status, handle_improvement_phase_complete
    # - handle_improvement_cycles_status, handle_improvement_cycles_leaderboard
    # - handle_improvement_training_complete, handle_improvement_evaluation_complete
    # =========================================================================

    # =========================================================================
    # NOTE: Sync handlers (handle_sync_*) extracted to SyncHandlersMixin
    # See: scripts/p2p/handlers/sync.py (Dec 28, 2025 - Phase 8)
    # Removed: handle_sync_start, handle_sync_status, handle_sync_push,
    #          handle_sync_receipt, handle_sync_receipts_status, handle_sync_pull,
    #          handle_sync_file, handle_sync_job_update (~625 LOC)
    # =========================================================================

    async def handle_subscriptions(self, request: web.Request) -> web.Response:
        """GET /subscriptions - Get event subscription dashboard.

        Phase 5 (December 2025): Visibility into feedback loop event wiring.

        Returns list of events and their subscribers to verify the feedback
        loop is properly wired. Critical for debugging dead-end events.

        Returns:
            JSON with event types and their subscriber counts/names
        """
        try:
            from app.coordination.event_router import DataEventType

            subscriptions: dict[str, dict] = {}
            router_info: dict = {"available": False, "type": "unknown"}

            try:
                from app.coordination.event_router import get_router

                router = get_router()
                if router is not None:
                    router_info["available"] = True
                    router_info["type"] = type(router).__name__

                    # Get all subscriptions from router
                    if hasattr(router, '_subscribers'):
                        for event_key, handlers in router._subscribers.items():
                            handler_names = []
                            for handler in handlers:
                                if hasattr(handler, '__name__'):
                                    handler_names.append(handler.__name__)
                                elif hasattr(handler, '__class__'):
                                    handler_names.append(handler.__class__.__name__)
                                else:
                                    handler_names.append(str(type(handler)))

                            subscriptions[event_key] = {
                                "count": len(handlers),
                                "handlers": handler_names[:10],  # Limit to first 10
                            }
            except Exception as e:  # noqa: BLE001
                router_info["error"] = str(e)

            # Define critical events for feedback loop
            critical_events = [
                "hyperparameter_updated",
                "curriculum_advanced",
                "adaptive_params_changed",
                "regression_critical",
                "evaluation_completed",
                "model_promoted",
                "training_complete",
                "selfplay_complete",
            ]

            critical_status: dict[str, dict] = {}
            for event in critical_events:
                if event in subscriptions:
                    critical_status[event] = {
                        "status": "active",
                        "subscribers": subscriptions[event]["count"],
                    }
                else:
                    critical_status[event] = {
                        "status": "missing",
                        "subscribers": 0,
                    }

            missing_count = sum(1 for e in critical_status.values() if e["status"] == "missing")

            return web.json_response({
                "node_id": self.node_id,
                "router": router_info,
                "feedback_loop_health": "healthy" if missing_count == 0 else f"{missing_count} missing",
                "critical_events": critical_status,
                "all_subscriptions": subscriptions,
                "total_event_types": len(subscriptions),
                "phase": "Phase 5 - December 2025",
            })

        except Exception as e:  # noqa: BLE001
            logger.error(f"in handle_subscriptions: {e}")
            return web.json_response({"error": str(e)}, status=500)


    async def _run_improvement_loop(self, job_id: str):
        """Main coordinator loop for AlphaZero-style improvement."""
        try:
            state = self.improvement_loop_state.get(job_id)
            if not state:
                return

            logger.info(f"Improvement loop coordinator started for job {job_id}")

            while state.current_iteration < state.max_iterations and state.status == "running":
                state.current_iteration += 1
                logger.info(f"Improvement iteration {state.current_iteration}/{state.max_iterations}")

                # Phase 1: Selfplay
                state.phase = "selfplay"
                state.selfplay_progress = {}
                await self.job_manager.run_distributed_selfplay(job_id)

                # Phase 2: Export training data
                state.phase = "export"
                await self.job_manager.export_training_data(job_id)

                # Phase 3: Training
                state.phase = "train"
                await self.job_manager.run_training(job_id)

                # Phase 4: Evaluation
                state.phase = "evaluate"
                await self._run_evaluation(job_id)

                # Phase 5: Promote if better
                state.phase = "promote"
                await self._promote_model_if_better(job_id)

                state.last_update = time.time()

            state.status = "completed"
            state.phase = "idle"
            logger.info(f"Improvement loop {job_id} completed after {state.current_iteration} iterations")

        except Exception as e:  # noqa: BLE001
            logger.info(f"Improvement loop error: {e}")
            if job_id in self.improvement_loop_state:
                self.improvement_loop_state[job_id].status = f"error: {e}"

    # NOTE: The following methods were removed Dec 27, 2025 (call sites updated to use job_manager directly):
    # - _run_distributed_selfplay() -> self.job_manager.run_distributed_selfplay()
    # - _export_training_data() -> self.job_manager.export_training_data()
    # - _run_training() -> self.job_manager.run_training()
    # - _run_local_selfplay() -> self.job_manager.run_local_selfplay() (removed Dec 2025)
    # - _run_local_training() -> self.job_manager.run_local_training() (removed Dec 2025)

    # ============================================
    # Phase 3: Training Pipeline Integration Methods
    # ============================================

    # NOTE: _check_training_readiness() removed Dec 2025 (95 LOC).
    # Use self.training_coordinator.check_training_readiness() instead.
    # See scripts/p2p/managers/training_coordinator.py for implementation.

    # NOTE: _find_running_training_job() and _find_resumable_training_job() removed Dec 2025 (29 LOC).
    # Use self.training_coordinator.find_running_training_job() and
    # self.training_coordinator.find_resumable_training_job() instead.

    # NOTE: _dispatch_training_job() removed Dec 2025 (9 LOC).
    # Use self.training_coordinator.dispatch_training_job() directly.

    async def _check_and_trigger_training(self):
        """Periodic check for training readiness (leader only)."""
        if self.role != NodeRole.LEADER:
            return

        # Phase 2.4 (Dec 29, 2025): Skip training dispatch in partition readonly mode
        if self.is_partition_readonly():
            logger.debug("[P2P] Skipping training check: partition readonly mode")
            return

        current_time = time.time()
        if current_time - self.last_training_check < self.training_check_interval:
            return

        self.last_training_check = current_time

        # Get jobs that should be started (delegated to TrainingCoordinator manager)
        jobs_to_start = self.training_coordinator.check_training_readiness()

        for job_config in jobs_to_start:
            # PHASE 4 IDEMPOTENCY: Check for duplicate triggers
            config_key = job_config.get("config_key", "")
            game_count = job_config.get("total_games", 0)
            can_proceed, trigger_hash = self._check_training_idempotency(config_key, game_count)
            if not can_proceed:
                continue

            logger.info(f"Auto-triggering {job_config['job_type']} training for {config_key} ({game_count} games)")
            await self.training_coordinator.dispatch_training_job(job_config)
            self._record_training_trigger(trigger_hash)  # Record after successful dispatch

    async def _check_local_training_fallback(self):
        """DECENTRALIZED training trigger when cluster has no leader.

        LEADERLESS RESILIENCE: When the cluster has been without a leader for too long
        (LEADERLESS_TRAINING_TIMEOUT = 3 minutes), individual nodes can trigger local
        training to prevent data accumulation without progress.

        This makes the system more resilient to leader election failures while avoiding
        duplicate training by:
        1. Only triggering after a brief leaderless period (3 minutes)
        2. Using random jitter so nodes don't all train simultaneously
        3. Only training on local data (no cluster-wide coordination needed)
        4. Using reasonable cooldowns between fallback training runs
        """
        # Skip if we ARE the leader or have a known leader
        if self.role == NodeRole.LEADER or self.leader_id:
            self.last_leader_seen = time.time()  # Update leader seen time
            return

        current_time = time.time()
        leaderless_duration = current_time - self.last_leader_seen

        # Only trigger fallback if leaderless for the timeout period
        if leaderless_duration < LEADERLESS_TRAINING_TIMEOUT:
            return

        # Rate limit fallback training (10 minute cooldown - more aggressive than before)
        fallback_cooldown = 600  # 10 minutes between fallback triggers
        if current_time - self.last_local_training_fallback < fallback_cooldown:
            return

        # Random jitter: 40% probability per check (more aggressive than 20%)
        # This distributes training across nodes over time
        import random
        if random.random() > 0.4:
            return

        # Check if we have a GPU (training needs GPU)
        if not getattr(self.self_info, "has_gpu", False):
            return

        # Check local data manifest (use cached version for speed)
        local_manifest = getattr(self, "local_data_manifest", None)
        if not local_manifest:
            # Try to load from cache or collect if we don't have one
            try:
                local_manifest = self.sync_planner.collect_local_manifest_cached(max_cache_age=600)
                with self.manifest_lock:
                    self.local_data_manifest = local_manifest
            except (AttributeError):
                return

        # Check for sufficient local data (lower threshold for faster training)
        min_games_fallback = 2000  # Lower threshold for faster response
        total_local_games = getattr(local_manifest, "selfplay_games", 0)
        if total_local_games < min_games_fallback:
            return

        # Find board types with enough local data
        game_counts_by_type: dict[str, int] = {}
        for file_info in getattr(local_manifest, "files", []) or []:
            board_type = getattr(file_info, "board_type", "")
            num_players = getattr(file_info, "num_players", 2)
            game_count = getattr(file_info, "game_count", 0)
            if board_type and game_count > 0:
                key = f"{board_type}_{num_players}p"
                game_counts_by_type[key] = game_counts_by_type.get(key, 0) + game_count

        # Sort by game count (descending) to train on richest data first
        sorted_configs = sorted(game_counts_by_type.items(), key=lambda x: x[1], reverse=True)

        # Trigger local training for configurations with enough data
        triggered_count = 0
        max_concurrent_fallback = 2  # Can trigger up to 2 training jobs per fallback
        for config_key, game_count in sorted_configs:
            if triggered_count >= max_concurrent_fallback:
                break
            if game_count < 1000:  # Minimum threshold (lowered)
                continue

            # Check if we already have a running training job for this config
            existing_job = self.training_coordinator.find_running_training_job("nnue", config_key)
            if existing_job:
                continue

            # DISTRIBUTED TRAINING COORDINATION: Check cluster-wide before starting
            is_training, _training_nodes = self._is_config_being_trained_cluster_wide(config_key)
            if is_training:
                # Someone else is already training this config
                continue

            # Use distributed slot claiming to avoid race conditions
            if not self._should_claim_training_slot(config_key):
                continue

            # Parse board type and player count
            parts = config_key.split("_")
            if len(parts) < 2:
                continue
            board_type = parts[0]
            num_players = int(parts[1].replace("p", ""))

            # PHASE 4 IDEMPOTENCY: Check for duplicate triggers
            can_proceed, trigger_hash = self._check_training_idempotency(config_key, game_count)
            if not can_proceed:
                continue

            logger.info(f"DISTRIBUTED TRAINING: Claiming {config_key} ({game_count} local games, leaderless for {int(leaderless_duration)}s)")
            job_config = {
                "job_type": "nnue",
                "board_type": board_type,
                "num_players": num_players,
                "config_key": config_key,
                "total_games": game_count,
            }
            await self.training_coordinator.dispatch_training_job(job_config)
            self._record_training_trigger(trigger_hash)  # Record after successful dispatch
            triggered_count += 1

        if triggered_count > 0:
            self.last_local_training_fallback = current_time
            logger.info(f"LEADERLESS FALLBACK: Triggered {triggered_count} local training job(s)")

    async def _check_improvement_cycles(self):
        """Periodic check for improvement cycle readiness (leader only).

        This integrates with the ImprovementCycleManager to:
        1. Check if any cycles need training based on data thresholds
        2. Trigger export/training jobs for ready cycles
        3. Run evaluations and update Elo ratings
        4. Schedule CMA-ES optimization when needed
        5. Schedule diverse tournaments for AI calibration
        """
        if self.role != NodeRole.LEADER:
            return

        if not self.improvement_cycle_manager:
            return

        current_time = time.time()
        if current_time - self.last_improvement_cycle_check < self.improvement_cycle_check_interval:
            return

        self.last_improvement_cycle_check = current_time

        # Check which cycles are ready for training
        training_ready = self.improvement_cycle_manager.check_training_needed()

        # Convert to job configs
        jobs_to_start = []
        for board_type, num_players in training_ready:
            cycle_key = f"{board_type}_{num_players}p"
            cycle_state = self.improvement_cycle_manager.state.cycles.get(cycle_key)
            if cycle_state and self.improvement_cycle_manager.trigger_training(board_type, num_players):
                jobs_to_start.append({
                    "cycle_id": cycle_key,
                    "board_type": board_type,
                    "num_players": num_players,
                    "total_games": cycle_state.games_since_last_training,
                    "iteration": cycle_state.current_iteration + 1,
                })

        # Also check for CMA-ES optimization opportunities
        cmaes_ready = self.improvement_cycle_manager.check_cmaes_needed()
        for board_type, num_players in cmaes_ready:
            # Trigger distributed CMA-ES
            logger.info(f"CMA-ES optimization ready for {board_type}_{num_players}p")
            asyncio.create_task(self._trigger_auto_cmaes(board_type, num_players))

        # Check for rollback needs (consecutive training failures)
        for key, cycle in self.improvement_cycle_manager.state.cycles.items():
            if not cycle.pending_training and not cycle.pending_evaluation:
                should_rollback, reason = self.improvement_cycle_manager.check_rollback_needed(
                    cycle.board_type, cycle.num_players
                )
                if should_rollback:
                    logger.info(f"ROLLBACK NEEDED for {key}: {reason}")
                    if self.improvement_cycle_manager.execute_rollback(cycle.board_type, cycle.num_players):
                        self.diversity_metrics["rollbacks"] += 1
                        # Increase diversity to escape plateau
                        logger.info(f"Increasing diversity to escape training plateau for {key}")

        for job_config in jobs_to_start:
            cycle_id = job_config["cycle_id"]
            board_type = job_config["board_type"]
            num_players = job_config["num_players"]

            logger.info(f"ImprovementCycle {cycle_id}: Starting training "
                  f"({job_config['total_games']} games)")

            # Find GPU worker for training
            gpu_worker = None
            candidates: list[NodeInfo] = []
            with self.peers_lock:
                candidates.extend([p for p in self.peers.values() if p.is_gpu_node() and p.is_healthy()])
            if self.self_info.is_gpu_node() and self.self_info.is_healthy():
                candidates.append(self.self_info)
            if candidates:
                candidates.sort(
                    key=lambda p: (-p.gpu_power_score(), p.get_load_score(), str(p.node_id))
                )
                gpu_worker = candidates[0]

            if not gpu_worker:
                logger.info(f"ImprovementCycle {cycle_id}: No GPU worker available, deferring")
                self.improvement_cycle_manager.update_cycle_phase(
                    cycle_id, "idle", error_message="No GPU worker available"
                )
                continue

            # Create training job
            job_id = f"cycle_{cycle_id}_{int(time.time())}"
            training_job = TrainingJob(
                job_id=job_id,
                job_type="nnue",
                board_type=board_type,
                num_players=num_players,
                worker_node=gpu_worker.node_id,
                epochs=job_config.get("epochs", 100),
                batch_size=job_config.get("batch_size", 4096),
                learning_rate=job_config.get("learning_rate", 0.001),
                data_games_count=job_config.get("total_games", 0),
            )

            with self.training_lock:
                self.training_jobs[job_id] = training_job

            # Update cycle state
            self.improvement_cycle_manager.update_cycle_phase(
                cycle_id, "training", training_job_id=job_id
            )

            # Dispatch training to worker
            await self._dispatch_improvement_training(training_job, cycle_id)

    async def _dispatch_improvement_training(self, job: TrainingJob, cycle_id: str):
        """Dispatch training job for improvement cycle."""
        try:
            # Find the worker node
            worker_node = None
            if job.worker_node == self.node_id:
                worker_node = self.self_info
            else:
                with self.peers_lock:
                    worker_node = self.peers.get(job.worker_node)

            if not worker_node:
                logger.info(f"ImprovementCycle {cycle_id}: Worker {job.worker_node} not found")
                self.improvement_cycle_manager.update_cycle_phase(
                    cycle_id, "idle", error_message=f"Worker {job.worker_node} not found"
                )
                return

            # Build training payload
            payload = {
                "job_id": job.job_id,
                "cycle_id": cycle_id,
                "board_type": job.board_type,
                "num_players": job.num_players,
                "epochs": job.epochs,
                "batch_size": job.batch_size,
                "learning_rate": job.learning_rate,
            }

            # Send to worker
            timeout = ClientTimeout(total=30)
            async with get_client_session(timeout) as session:
                last_err: str | None = None
                for url in self._urls_for_peer(worker_node, "/training/nnue/start"):
                    try:
                        async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                            if resp.status != 200:
                                last_err = f"http_{resp.status}"
                                continue
                            result = await resp.json()
                        if result.get("success"):
                            job.status = "running"
                            job.started_at = time.time()
                            logger.info(f"ImprovementCycle {cycle_id}: Training started on {worker_node.node_id}")
                            return
                        self.improvement_cycle_manager.update_cycle_phase(
                            cycle_id, "idle", error_message=result.get("error", "Training failed to start")
                        )
                        return
                    except Exception as e:  # noqa: BLE001
                        last_err = str(e)
                        continue
                self.improvement_cycle_manager.update_cycle_phase(
                    cycle_id, "idle", error_message=last_err or "dispatch_failed"
                )

        except Exception as e:  # noqa: BLE001
            logger.info(f"ImprovementCycle {cycle_id}: Training dispatch failed: {e}")
            self.improvement_cycle_manager.update_cycle_phase(
                cycle_id, "idle", error_message=str(e)
            )

    # Phase 3 HTTP Handlers

    async def handle_training_start(self, request: web.Request) -> web.Response:
        """Handle request to start a training job (from external or leader)."""
        try:
            data = await request.json()
            job_type = data.get("job_type", "nnue")
            board_type = data.get("board_type", "square8")
            num_players = data.get("num_players", 2)

            if self.role != NodeRole.LEADER:
                return web.json_response({
                    "success": False,
                    "error": "Only leader can dispatch training jobs"
                })

            job_config = {
                "job_type": job_type,
                "board_type": board_type,
                "num_players": num_players,
                "config_key": f"{board_type}_{num_players}p",
                "total_games": data.get("total_games", 0),
            }

            job = await self.training_coordinator.dispatch_training_job(job_config)
            if job:
                return web.json_response({
                    "success": True,
                    "job_id": job.job_id,
                    "worker": job.worker_node,
                })
            else:
                return web.json_response({
                    "success": False,
                    "error": "No suitable worker available"
                })

        except Exception as e:  # noqa: BLE001
            return web.json_response({"success": False, "error": str(e)})

    async def handle_training_status(self, request: web.Request) -> web.Response:
        """Return status of all training jobs."""
        with self.training_lock:
            jobs = [job.to_dict() for job in self.training_jobs.values()]

        return web.json_response({
            "success": True,
            "jobs": jobs,
            "thresholds": self.training_thresholds.to_dict(),
        })

    async def handle_training_update(self, request: web.Request) -> web.Response:
        """Handle training progress/completion update from worker."""
        try:
            data = await request.json()
            job_id = data.get("job_id")

            with self.training_lock:
                job = self.training_jobs.get(job_id)
                if not job:
                    return web.json_response({
                        "success": False,
                        "error": f"Job {job_id} not found"
                    })

                # Update job status
                if data.get("status"):
                    job.status = data["status"]
                if data.get("completed"):
                    job.status = "completed"
                    job.completed_at = time.time()
                if data.get("output_model_path"):
                    job.output_model_path = data["output_model_path"]
                if data.get("final_loss"):
                    job.final_loss = data["final_loss"]
                if data.get("final_accuracy"):
                    job.final_accuracy = data["final_accuracy"]
                if data.get("error"):
                    job.status = "failed"
                    job.error_message = data["error"]
                    # ALERTING: Notify on training failure
                    asyncio.create_task(self.notifier.send(
                        title="Training Job Failed",
                        message=f"Training job {job.job_id} failed: {data['error'][:100]}",
                        level="error",
                        fields={
                            "Job ID": job.job_id,
                            "Type": job.job_type,
                            "Config": f"{job.board_type}_{job.num_players}p",
                            "Worker": job.worker_node or "unknown",
                            "Error": data["error"][:200],
                            "Checkpoint": job.checkpoint_path or "none",
                        },
                        node_id=self.node_id,
                    ))

                # TRAINING CHECKPOINTING: Track checkpoint progress
                if data.get("checkpoint_path"):
                    job.checkpoint_path = data["checkpoint_path"]
                    job.checkpoint_updated_at = time.time()
                if data.get("checkpoint_epoch"):
                    job.checkpoint_epoch = int(data["checkpoint_epoch"])
                if data.get("checkpoint_loss"):
                    job.checkpoint_loss = float(data["checkpoint_loss"])

                # Check if we should trigger evaluation after training
                should_trigger_eval = (
                    data.get("completed") and
                    job.output_model_path and
                    self.improvement_cycle_manager
                )

            self._save_state()

            # Auto-trigger tournament evaluation when training completes
            # Delegate to TrainingCoordinator (Phase 2B refactoring, Dec 2025)
            if should_trigger_eval:
                asyncio.create_task(self.training_coordinator.handle_training_job_completion(job))

            return web.json_response({"success": True})

        except Exception as e:  # noqa: BLE001
            return web.json_response({"success": False, "error": str(e)})

    # NOTE: _handle_training_job_completion() removed Dec 2025 (9 LOC).
    # Use self.training_coordinator.handle_training_job_completion() directly.

    # NOTE: _schedule_model_comparison_tournament() removed Dec 2025 (9 LOC).
    # Use self.training_coordinator._schedule_model_comparison_tournament() directly.

    # =========================================================================
    # TRAINING TRIGGER RPC (December 30, 2025)
    # =========================================================================

    async def handle_training_trigger(self, request: web.Request) -> web.Response:
        """Handle request to trigger training for a config (December 30, 2025).

        Endpoint: POST /training/trigger

        This provides programmatic control over training triggers, allowing
        operators to bypass the daemon's automatic trigger logic when needed.

        Request body:
            {
                "config_key": "hex8_2p",
                "priority": "normal",  // "low", "normal", "high", "urgent"
                "force": false,        // Skip freshness/cooldown checks
                "data_path": null      // Optional: override NPZ path
            }

        Returns:
            Success: job_id, worker_node, trigger_reason, data_info
            Failure: success=false with reason
        """
        try:
            data = await request.json()
            config_key = data.get("config_key")
            priority = data.get("priority", "normal")
            force = data.get("force", False)
            data_path = data.get("data_path")

            # Validate config_key format
            if not config_key or "_" not in config_key:
                return web.json_response({
                    "success": False,
                    "error": "Invalid config_key format (expected: board_Np, e.g., hex8_2p)"
                }, status=400)

            # Only leader can trigger training
            if self.role != NodeRole.LEADER:
                return web.json_response({
                    "success": False,
                    "error": "Only leader can trigger training"
                }, status=403)

            # Get training trigger daemon for decision checking
            try:
                from app.coordination.training_trigger_daemon import (
                    get_training_trigger_daemon,
                    TrainingDecision,
                )
                daemon = get_training_trigger_daemon()
            except ImportError:
                daemon = None

            trigger_reason = "force=true" if force else "conditions met"
            decision_info = None

            # If not forcing, check conditions via daemon
            if not force and daemon:
                decision = await daemon.get_training_decision(config_key)
                decision_info = decision.to_dict()

                if not decision.can_trigger:
                    return web.json_response({
                        "success": False,
                        "triggered": False,
                        "reason": decision.reason,
                        "decision": decision_info,
                    })

                trigger_reason = decision.reason

            # Parse config_key to get board_type and num_players
            parts = config_key.rsplit("_", 1)
            if len(parts) != 2:
                return web.json_response({
                    "success": False,
                    "error": f"Cannot parse config_key: {config_key}"
                }, status=400)

            board_type = parts[0]
            try:
                num_players = int(parts[1].rstrip("p"))
            except ValueError:
                return web.json_response({
                    "success": False,
                    "error": f"Invalid player count in config_key: {config_key}"
                }, status=400)

            # Dispatch training job
            job_config = {
                "job_type": "nnue",
                "board_type": board_type,
                "num_players": num_players,
                "config_key": config_key,
                "priority": priority,
                "data_path": data_path,
                "triggered_by": "rpc",
            }

            job = await self.training_coordinator.dispatch_training_job(job_config)

            if job:
                return web.json_response({
                    "success": True,
                    "triggered": True,
                    "job_id": job.job_id,
                    "worker_node": job.worker_node,
                    "trigger_reason": trigger_reason,
                    "decision": decision_info,
                })
            else:
                return web.json_response({
                    "success": False,
                    "triggered": False,
                    "error": "No suitable worker available",
                    "decision": decision_info,
                })

        except Exception as e:  # noqa: BLE001
            logger.exception(f"[P2POrchestrator] Training trigger failed: {e}")
            return web.json_response({
                "success": False,
                "error": str(e)
            }, status=500)

    async def handle_training_trigger_decision(self, request: web.Request) -> web.Response:
        """Get training trigger decision for a config (December 30, 2025).

        Endpoint: GET /training/trigger-decision/{config_key}

        Returns full condition details explaining why training would or
        wouldn't trigger for the specified config.
        """
        try:
            config_key = request.match_info.get("config_key", "")

            if not config_key or "_" not in config_key:
                return web.json_response({
                    "success": False,
                    "error": "Invalid config_key format"
                }, status=400)

            # Get training trigger daemon
            try:
                from app.coordination.training_trigger_daemon import (
                    get_training_trigger_daemon,
                )
                daemon = get_training_trigger_daemon()
            except ImportError:
                return web.json_response({
                    "success": False,
                    "error": "TrainingTriggerDaemon not available"
                }, status=503)

            decision = await daemon.get_training_decision(config_key)

            return web.json_response({
                "success": True,
                **decision.to_dict(),
            })

        except Exception as e:  # noqa: BLE001
            logger.exception(f"[P2POrchestrator] Training trigger decision failed: {e}")
            return web.json_response({
                "success": False,
                "error": str(e)
            }, status=500)

    async def handle_training_trigger_configs(self, request: web.Request) -> web.Response:
        """Get list of all tracked training configs (December 30, 2025).

        Endpoint: GET /training/trigger-configs

        Returns list of all config keys that the training trigger daemon is tracking.
        """
        try:
            from app.coordination.training_trigger_daemon import get_training_trigger_daemon
            daemon = get_training_trigger_daemon()
            configs = daemon.get_tracked_configs()

            return web.json_response({
                "success": True,
                "configs": configs,
                "count": len(configs),
            })

        except ImportError:
            return web.json_response({
                "success": False,
                "error": "TrainingTriggerDaemon not available"
            }, status=503)
        except Exception as e:  # noqa: BLE001
            return web.json_response({
                "success": False,
                "error": str(e)
            }, status=500)

    # =========================================================================
    # POST-TRAINING GAUNTLET: Immediate evaluation after training
    # =========================================================================

    def _get_median_model(self, config_key: str) -> str | None:
        """Get the median-rated model for a config from ELO database.

        Returns the model_id at the 50th percentile by rating, or None if
        no models exist for this config.
        """
        elo_db_path = Path(self.ringrift_path) / "ai-service" / "data" / "unified_elo.db"
        if not elo_db_path.exists():
            return None

        # Parse config_key like "square8_2p"
        parts = config_key.rsplit("_", 1)
        if len(parts) != 2:
            return None
        board_type = parts[0]
        num_players = int(parts[1].rstrip("p"))

        try:
            import sqlite3
            with safe_db_connection(elo_db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT participant_id FROM elo_ratings
                    WHERE board_type = ? AND num_players = ? AND archived_at IS NULL
                    ORDER BY rating
                """, (board_type, num_players))
                rows = cursor.fetchall()

                if not rows:
                    return None

                # Return median model (middle of sorted list)
                median_idx = len(rows) // 2
                return rows[median_idx][0]
        except Exception as e:  # noqa: BLE001
            logger.error(f"getting median model: {e}")
            return None

    # NOTE: _run_post_training_gauntlet() removed Dec 2025 (9 LOC).
    # Use self.training_coordinator._run_post_training_gauntlet() directly.

    async def _archive_failed_model(self, model_path: str, board_type: str,
                                     num_players: int, reason: str) -> None:
        """Archive a model that failed gauntlet evaluation.

        Moves the model file to models/archived/{config_key}/ and updates
        the ELO database to mark it as archived.
        """
        if not model_path or not os.path.exists(model_path):
            return

        config_key = f"{board_type}_{num_players}p"
        archive_dir = os.path.join(self.ringrift_path, "ai-service", "models",
                                   "archived", config_key)
        os.makedirs(archive_dir, exist_ok=True)

        # Move model to archive
        model_name = os.path.basename(model_path)
        archive_path = os.path.join(archive_dir, model_name)

        try:
            shutil.move(model_path, archive_path)
            logger.info(f"Archived {model_name} to {archive_dir} ({reason})")
        except Exception as e:  # noqa: BLE001
            logger.error(f"moving model to archive: {e}")
            return

        # Update ELO database to mark as archived
        model_id = os.path.splitext(model_name)[0]
        elo_db_path = Path(self.ringrift_path) / "ai-service" / "data" / "unified_elo.db"

        if elo_db_path.exists():
            try:
                import sqlite3
                with safe_db_connection(elo_db_path) as conn:
                    cursor = conn.cursor()
                    cursor.execute("""
                        UPDATE elo_ratings
                        SET archived_at = ?, archive_reason = ?
                        WHERE participant_id = ? AND board_type = ? AND num_players = ?
                    """, (time.time(), reason, model_id, board_type, num_players))
                    conn.commit()
            except Exception as e:  # noqa: BLE001
                logger.error(f"updating ELO database for archived model: {e}")

    async def handle_nnue_start(self, request: web.Request) -> web.Response:
        """Handle NNUE training start request (worker endpoint)."""
        try:
            data = await request.json()
            job_id = data.get("job_id")
            board_type = data.get("board_type", "square8")
            num_players = data.get("num_players", 2)
            epochs = data.get("epochs", 100)
            batch_size = data.get("batch_size", 4096)
            learning_rate = data.get("learning_rate", None)

            # Start NNUE training subprocess
            # December 29, 2025: Use canonical model paths for consistent naming
            output_path = os.path.join(
                self.ringrift_path, "ai-service", "models",
                f"canonical_{board_type}_{num_players}p.pth"
            )

            # Collect local selfplay databases. The NNUE trainer requires at
            # least one DB (it can replay moves when snapshots are absent).
            data_dir = self.get_data_directory()
            board_tokens = [str(board_type).lower()]
            if "hex" in board_tokens[0]:
                board_tokens = ["hexagonal", "hex"]
            players_token = f"_{int(num_players)}p"

            candidate_dbs: list[Path] = []
            for pattern in ("selfplay/**/*.db", "games/**/*.db"):
                for db_path in data_dir.glob(pattern):
                    if not db_path.is_file():
                        continue
                    path_lower = str(db_path).lower()
                    if players_token not in path_lower:
                        continue
                    if not any(tok in path_lower for tok in board_tokens):
                        continue
                    candidate_dbs.append(db_path)

            # Fallback: if naming conventions differ, use any selfplay DBs.
            if not candidate_dbs:
                candidate_dbs = [p for p in data_dir.glob("selfplay/**/*.db") if p.is_file()]

            # De-dupe + prefer newest DBs (avoid overlong argv on large clusters).
            unique_dbs = list({p.resolve() for p in candidate_dbs})
            unique_dbs.sort(key=lambda p: p.stat().st_mtime if p.exists() else 0.0, reverse=True)
            max_dbs = 64
            unique_dbs = unique_dbs[:max_dbs]

            if not unique_dbs:
                return web.json_response(
                    {
                        "success": False,
                        "error": f"No selfplay DBs found under {data_dir} for {board_type} {num_players}p",
                    },
                    status=400,
                )

            cmd = [
                sys.executable, "-m", "scripts.train_nnue",
                "--db", *[str(p) for p in unique_dbs],
                "--board-type", board_type,
                "--num-players", str(num_players),
                "--epochs", str(epochs),
                "--batch-size", str(batch_size),
                "--save-path", output_path,
                # Phase 1: Core Training Optimizations
                "--spectral-norm",  # Gradient stability
                "--cyclic-lr", "--cyclic-lr-period", "5",  # Cyclic LR
                "--mixed-precision", "--amp-dtype", "bfloat16",  # BF16 speed
                "--warmup-epochs", "3",  # LR warmup
                # Phase 2: Advanced Training
                "--value-whitening",  # Value head stability
                "--ema",  # Exponential Moving Average
                "--stochastic-depth", "--stochastic-depth-prob", "0.1",
                "--adaptive-warmup",  # Dataset-aware warmup
                "--hard-example-mining", "--hard-example-top-k", "0.3",
                # Phase 2: Optimizer Enhancements
                "--lookahead", "--lookahead-k", "5", "--lookahead-alpha", "0.5",
                "--adaptive-clip",  # Adaptive gradient clipping
                "--board-nas",  # Board-specific NAS
                "--online-bootstrap", "--bootstrap-temperature", "1.5",
                # Phase 2: Data Pipeline
                "--prefetch-gpu",  # GPU prefetching
                "--difficulty-curriculum",  # Curriculum learning
                "--quantized-eval",  # Fast validation
                # Phase 3: Advanced Learning
                "--grokking-detection",  # Detect delayed generalization
                "--policy-label-smoothing", "0.05",  # Prevent overconfidence
                "--sampling-weights", "victory_type",  # Balanced sampling
                # Phase 4: Training Stability (optional, enabled for production)
                "--adaptive-accumulation",  # Dynamic gradient accumulation
                # Phase 5: Production Optimization (selective)
                "--dynamic-loss-scaling",  # Adaptive FP16 loss scaling
            ]
            # Add hex symmetry augmentation for hex boards (12x effective data)
            if board_type in ('hex8', 'hexagonal', 'hex'):
                cmd.append("--augment-hex-symmetry")
            # Add profiling for debug jobs
            if os.environ.get("RINGRIFT_PROFILE_TRAINING"):
                cmd.extend(["--profile", "--profile-dir", str(Path(output_path).parent / "profile")])
            if learning_rate is not None:
                cmd.extend(["--learning-rate", str(learning_rate)])

            env = os.environ.copy()
            env["PYTHONPATH"] = os.path.join(self.ringrift_path, "ai-service")

            proc = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env,
                cwd=os.path.join(self.ringrift_path, "ai-service"),
            )

            logger.info(f"Started NNUE training subprocess (PID {proc.pid}) for job {job_id}")

            # Don't wait - let it run in background
            asyncio.create_task(self._monitor_training_process(job_id, proc, output_path))

            return web.json_response({
                "success": True,
                "pid": proc.pid,
            })

        except Exception as e:  # noqa: BLE001
            return web.json_response({"success": False, "error": str(e)})

    async def _trigger_auto_cmaes(self, board_type: str, num_players: int):
        """Automatically trigger CMA-ES optimization for a configuration.

        Called by improvement cycle manager when optimization is due.
        """
        try:
            job_id = f"auto_cmaes_{board_type}_{num_players}p_{int(time.time())}"
            logger.info(f"Auto-triggering CMA-ES: {job_id}")

            # Check for GPU workers
            gpu_workers = []
            with self.peers_lock:
                for peer in self.peers.values():
                    if peer.is_healthy() and peer.has_gpu and peer.node_id != self.node_id:
                        gpu_workers.append(peer)

            if self.self_info.has_gpu:
                gpu_workers.append(self.self_info)

            if len(gpu_workers) >= 2:
                # DISTRIBUTED MODE
                cmaes_job_id = f"cmaes_auto_{job_id}"
                state = DistributedCMAESState(
                    job_id=cmaes_job_id,
                    board_type=board_type,
                    num_players=num_players,
                    generations=100,
                    population_size=max(32, len(gpu_workers) * 8),
                    games_per_eval=100,
                    status="running",
                    started_at=time.time(),
                    last_update=time.time(),
                    worker_nodes=[w.node_id for w in gpu_workers],
                )
                self.distributed_cmaes_state[cmaes_job_id] = state
                asyncio.create_task(self._run_distributed_cmaes(cmaes_job_id))
                logger.info(f"Started distributed CMA-ES with {len(gpu_workers)} workers")
            else:
                # LOCAL MODE - use GPU CMA-ES script
                output_dir = os.path.join(
                    self.ringrift_path, "ai-service", "data", "cmaes",
                    f"{board_type}_{num_players}p_auto_{int(time.time())}"
                )
                os.makedirs(output_dir, exist_ok=True)

                cmd = [
                    sys.executable,
                    os.path.join(self.ringrift_path, "ai-service", "scripts", "run_gpu_cmaes.py"),
                    "--board", board_type,
                    "--num-players", str(num_players),
                    "--generations", "100",
                    "--population-size", "32",
                    "--games-per-eval", "100",
                    "--max-moves", "10000",
                    "--output-dir", output_dir,
                    "--multi-gpu",
                ]

                env = os.environ.copy()
                env["PYTHONPATH"] = os.path.join(self.ringrift_path, "ai-service")
                env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

                proc = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                    env=env,
                )
                logger.info(f"Started local CMA-ES optimization (PID {proc.pid})")

        except Exception as e:  # noqa: BLE001
            logger.info(f"Auto CMA-ES trigger failed: {e}")

    async def handle_cmaes_start_auto(self, request: web.Request) -> web.Response:
        """Handle CMA-ES optimization start request.

        Uses distributed GPU CMA-ES across all cluster GPU nodes for maximum throughput.
        Falls back to local GPU CMA-ES if no remote workers available.
        """
        try:
            data = await request.json()
            job_id = data.get("job_id")
            board_type = data.get("board_type", "square8")
            num_players = data.get("num_players", 2)

            # Check for available GPU workers in the cluster
            gpu_workers = []
            with self.peers_lock:
                for peer in self.peers.values():
                    if peer.is_healthy() and peer.has_gpu and peer.node_id != self.node_id:
                        gpu_workers.append(peer)

            # Include self if we have GPU
            if self.self_info.has_gpu:
                gpu_workers.append(self.self_info)

            if len(gpu_workers) >= 2:
                # DISTRIBUTED MODE: Use P2P distributed CMA-ES across cluster
                logger.info(f"Starting DISTRIBUTED GPU CMA-ES with {len(gpu_workers)} workers")

                # Create distributed CMA-ES state
                cmaes_job_id = f"cmaes_auto_{job_id}_{int(time.time())}"
                state = DistributedCMAESState(
                    job_id=cmaes_job_id,
                    board_type=board_type,
                    num_players=num_players,
                    generations=100,  # More generations for better optimization
                    population_size=max(32, len(gpu_workers) * 8),  # Scale with workers
                    games_per_eval=100,  # More games for accurate fitness
                    status="running",
                    started_at=time.time(),
                    last_update=time.time(),
                    worker_nodes=[w.node_id for w in gpu_workers],
                )
                self.distributed_cmaes_state[cmaes_job_id] = state

                # Launch distributed coordinator task
                asyncio.create_task(self._run_distributed_cmaes(cmaes_job_id))

                # Track as training job
                with self.training_lock:
                    if job_id in self.training_jobs:
                        self.training_jobs[job_id].status = "running"
                        self.training_jobs[job_id].started_at = time.time()

                return web.json_response({
                    "success": True,
                    "mode": "distributed",
                    "job_id": cmaes_job_id,
                    "workers": [w.node_id for w in gpu_workers],
                })

            else:
                # LOCAL MODE: Run GPU CMA-ES on this node only
                logger.info("Starting LOCAL GPU CMA-ES (no remote workers available)")

                output_dir = os.path.join(
                    self.ringrift_path, "ai-service", "data", "cmaes",
                    f"{board_type}_{num_players}p_auto_{int(time.time())}"
                )
                os.makedirs(output_dir, exist_ok=True)

                cmd = [
                    sys.executable,
                    os.path.join(self.ringrift_path, "ai-service", "scripts", "run_gpu_cmaes.py"),
                    "--board", board_type,
                    "--num-players", str(num_players),
                    "--generations", "100",
                    "--population-size", "32",
                    "--games-per-eval", "100",
                    "--max-moves", "10000",
                    "--output-dir", output_dir,
                    "--multi-gpu",
                ]

                env = os.environ.copy()
                env["PYTHONPATH"] = os.path.join(self.ringrift_path, "ai-service")
                env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

                proc = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                    env=env,
                )

                logger.info(f"Started local GPU CMA-ES (PID {proc.pid}) for job {job_id}")
                asyncio.create_task(self._monitor_training_process(job_id, proc, output_dir))

                return web.json_response({
                    "success": True,
                    "mode": "local",
                    "pid": proc.pid,
                })

        except Exception as e:  # noqa: BLE001
            return web.json_response({"success": False, "error": str(e)})

    def _get_training_timeout(self, job_id: str) -> int:
        """Get dynamic timeout based on job configuration.

        Returns timeout in seconds based on board type and model complexity:
        - square19: 6 hours (large board, 361 cells)
        - hexagonal: 5 hours (469 cells)
        - square8/hex8: 2 hours (small boards)
        Default: 3 hours if job not found
        """
        with self.training_lock:
            job = self.training_jobs.get(job_id)
            if not job:
                return 10800  # 3 hours default

            board_type = getattr(job, 'board_type', 'unknown')
            num_players = getattr(job, 'num_players', 2)

            # Base timeout by board complexity
            if board_type == 'square19':
                base_timeout = 21600  # 6 hours
            elif board_type == 'hexagonal':
                base_timeout = 18000  # 5 hours
            elif board_type in ('hex8', 'square8'):
                base_timeout = 7200   # 2 hours
            else:
                base_timeout = 10800  # 3 hours default

            # Add 50% for 4-player models (larger value head, more complex)
            if num_players == 4:
                base_timeout = int(base_timeout * 1.5)
            elif num_players == 3:
                base_timeout = int(base_timeout * 1.25)

            return base_timeout

    async def _monitor_training_process(self, job_id: str, proc, output_path: str):
        """Monitor training subprocess and report completion to leader."""
        try:
            timeout = self._get_training_timeout(job_id)
            _stdout, stderr = await asyncio.wait_for(
                proc.communicate(),
                timeout=timeout
            )

            success = proc.returncode == 0

            # Report to leader with retry logic
            if self.leader_id and self.leader_id != self.node_id:
                leader = self.peers.get(self.leader_id)
                if leader:
                    payload = {
                        "job_id": job_id,
                        "completed": success,
                        "output_model_path": output_path if success else "",
                        "error": stderr.decode()[:500] if not success else "",
                    }
                    # Retry with exponential backoff (3 attempts: 5s, 10s, 20s)
                    max_retries = 3
                    base_delay = 5.0
                    for attempt in range(max_retries):
                        try:
                            http_timeout = ClientTimeout(total=30)
                            async with get_client_session(http_timeout) as session:
                                url = self._url_for_peer(leader, "/training/update")
                                resp = await session.post(url, json=payload, headers=self._auth_headers())
                                if resp.status < 400:
                                    logger.info(f"Training completion reported to leader (attempt {attempt + 1})")
                                    break
                                else:
                                    logger.warning(f"Leader returned {resp.status}, retrying...")
                        except Exception as e:  # noqa: BLE001
                            delay = base_delay * (2 ** attempt)
                            if attempt < max_retries - 1:
                                logger.warning(f"Failed to report training completion (attempt {attempt + 1}): {e}, retrying in {delay}s")
                                await asyncio.sleep(delay)
                            else:
                                logger.error(f"Failed to report training completion after {max_retries} attempts: {e}")
            else:
                # We are the leader, update directly
                with self.training_lock:
                    job = self.training_jobs.get(job_id)
                    if job:
                        if success:
                            job.status = "completed"
                            job.completed_at = time.time()
                            job.output_model_path = output_path
                            # LEARNED LESSONS - Schedule tournament to compare new model against baseline
                            asyncio.create_task(self._schedule_model_comparison(job, output_path))
                            # Update improvement cycle manager with training completion
                            if self.improvement_cycle_manager:
                                self.improvement_cycle_manager.handle_training_complete(
                                    job.board_type, job.num_players,
                                    output_path, job.data_games_count or 0
                                )
                            # PFSP: Add trained model to opponent pool for diverse selfplay
                            config_key = f"{job.board_type}_{job.num_players}p"
                            if HAS_PFSP and config_key in self.pfsp_pools:
                                try:
                                    model_id = Path(output_path).stem
                                    self.pfsp_pools[config_key].add_opponent(
                                        model_id=model_id,
                                        model_path=output_path,
                                        elo=INITIAL_ELO_RATING,  # From app.config.thresholds
                                        win_rate=0.5,
                                    )
                                    logger.info(f"[PFSP] Added {model_id} to opponent pool for {config_key}")
                                except Exception as e:  # noqa: BLE001
                                    logger.error(f"[PFSP] Error adding model to pool: {e}")
                            # CMA-ES: Check for Elo plateau and trigger auto-tuning
                            asyncio.create_task(self._check_cmaes_auto_tuning(config_key))
                        else:
                            job.status = "failed"
                            job.error_message = stderr.decode()[:500]
                        job.completed_at = time.time()

            logger.info(f"Training job {job_id} {'completed' if success else 'failed'}")

        except asyncio.TimeoutError:
            logger.info(f"Training job {job_id} timed out")
        except Exception as e:  # noqa: BLE001
            logger.info(f"Training monitor error for {job_id}: {e}")

    async def _monitor_gpu_selfplay_and_validate(
        self,
        job_id: str,
        proc: subprocess.Popen,
        output_dir: Path,
        board_type: str,
        num_players: int,
    ) -> None:
        """Monitor GPU selfplay completion and run CPU validation.

        When GPU selfplay completes, this runs import_gpu_selfplay_to_db.py to:
        1. Replay each game with CPU GameEngine
        2. Validate all moves against legal move lists
        3. Discard games with invalid moves
        4. Store only validated games in canonical DB format

        This ensures GPU-generated games are safe for training.
        """
        try:
            # Wait for GPU selfplay to complete (with timeout)
            return_code = await asyncio.wait_for(
                asyncio.to_thread(proc.wait),
                timeout=7200,  # 2 hour max
            )

            # Update job status
            with self.jobs_lock:
                job = self.local_jobs.get(job_id)
                if job:
                    job.status = "completed" if return_code == 0 else "failed"
                    job.completed_at = time.time()

            if return_code != 0:
                logger.info(f"GPU selfplay job {job_id} failed (exit code {return_code})")
                return

            # Find the generated JSONL file
            jsonl_files = list(output_dir.glob("*.jsonl"))
            if not jsonl_files:
                logger.warning(f"GPU selfplay job {job_id}: No JSONL output found")
                with self.jobs_lock:
                    job = self.local_jobs.get(job_id)
                    if job:
                        job.status = "failed"
                        job.completed_at = time.time()
                        job.error_message = "missing_jsonl_output"
                return

            input_jsonl = jsonl_files[0]
            try:
                if input_jsonl.stat().st_size == 0:
                    logger.warning(f"GPU selfplay job {job_id}: JSONL output is empty ({input_jsonl})")
                    with self.jobs_lock:
                        job = self.local_jobs.get(job_id)
                        if job:
                            job.status = "failed"
                            job.completed_at = time.time()
                            job.error_message = "empty_jsonl_output"
                    return
            except OSError as e:
                logger.warning(f"GPU selfplay job {job_id}: Failed to stat JSONL output ({input_jsonl}): {e}")
                with self.jobs_lock:
                    job = self.local_jobs.get(job_id)
                    if job:
                        job.status = "failed"
                        job.completed_at = time.time()
                        job.error_message = "jsonl_stat_failed"
                return
            validated_db = output_dir / "validated_games.db"

            logger.info(f"GPU selfplay job {job_id} completed, running CPU validation...")

            # Run CPU validation import
            validate_cmd = [
                sys.executable,  # Use venv Python
                f"{self.ringrift_path}/ai-service/scripts/import_gpu_selfplay_to_db.py",
                "--input", str(input_jsonl),
                "--output", str(validated_db),
            ]

            env = os.environ.copy()
            env["PYTHONPATH"] = f"{self.ringrift_path}/ai-service"

            validate_proc = await asyncio.create_subprocess_exec(
                *validate_cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env,
                cwd=self.ringrift_path,
            )

            stdout, stderr = await asyncio.wait_for(
                validate_proc.communicate(),
                timeout=1800,  # 30 min validation timeout
            )

            if validate_proc.returncode == 0:
                # Parse validation results from output
                output_text = stdout.decode()
                imported = 0
                failed = 0
                for line in output_text.split("\n"):
                    if "Successfully imported:" in line:
                        imported = int(line.split(":")[-1].strip())
                    elif "Failed:" in line:
                        failed = int(line.split(":")[-1].strip())

                validation_rate = imported / (imported + failed) * 100 if (imported + failed) > 0 else 0

                logger.info(f"GPU selfplay {job_id} CPU validation complete:")
                logger.info(f"  Valid games: {imported}, Invalid: {failed}, Validation rate: {validation_rate:.1f}%")

                # Track validation metrics for diversity reporting
                if hasattr(self, 'diversity_metrics'):
                    if "gpu_validation_stats" not in self.diversity_metrics:
                        self.diversity_metrics["gpu_validation_stats"] = {
                            "total_generated": 0,
                            "total_validated": 0,
                            "total_failed": 0,
                        }
                    self.diversity_metrics["gpu_validation_stats"]["total_generated"] += imported + failed
                    self.diversity_metrics["gpu_validation_stats"]["total_validated"] += imported
                    self.diversity_metrics["gpu_validation_stats"]["total_failed"] += failed

                # Record validation rate metric for observability
                self.record_metric(
                    "validation_rate",
                    validation_rate,
                    board_type=board_type,
                    num_players=num_players,
                    metadata={
                        "job_id": job_id,
                        "imported": imported,
                        "failed": failed,
                    },
                )

                # Auto-import to canonical database if validation rate is high enough
                if validation_rate >= 95 and imported > 0:
                    asyncio.create_task(self._import_gpu_selfplay_to_canonical(
                        validated_db, board_type, num_players, imported
                    ))
                elif validation_rate < 95:
                    logger.info(f"WARNING: GPU selfplay validation rate {validation_rate:.1f}% is below 95%")
                    logger.info("  This indicates potential GPU/CPU rule divergence")
                    logger.info("  Skipping auto-import to canonical database")
                    # Alert on low validation rate
                    asyncio.create_task(self.notifier.send(
                        title="Low GPU Validation Rate",
                        message=f"GPU selfplay validation rate {validation_rate:.1f}% is below 95% threshold",
                        level="warning",
                        fields={
                            "Config": f"{board_type}_{num_players}p",
                            "Valid": str(imported),
                            "Invalid": str(failed),
                            "Rate": f"{validation_rate:.1f}%",
                        },
                        node_id=self.node_id,
                    ))

            else:
                logger.info(f"GPU selfplay {job_id} CPU validation failed:")
                logger.info(f"  {stderr.decode()[:500]}")

        except asyncio.TimeoutError:
            logger.info(f"GPU selfplay job {job_id} timed out")
            with self.jobs_lock:
                job = self.local_jobs.get(job_id)
                if job:
                    job.status = "failed"
        except Exception as e:  # noqa: BLE001
            logger.info(f"GPU selfplay monitor error for {job_id}: {e}")
            with self.jobs_lock:
                job = self.local_jobs.get(job_id)
                if job:
                    job.status = "failed"

    async def _schedule_model_comparison(self, job: TrainingJob, new_model_path: str):
        """Schedule a tournament to compare new model against current baseline.

        LEARNED LESSONS - After training, automatically run tournament to:
        1. Compare new model against current best baseline
        2. Update Elo ratings
        3. Promote to best baseline if new model wins
        """
        try:
            config_key = f"{job.board_type}_{job.num_players}p"
            logger.info(f"Scheduling model comparison tournament for {config_key}")

            # Find current baseline model
            baseline_dir = Path(self.ringrift_path) / "ai-service" / "models" / job.job_type
            baseline_pattern = f"{job.board_type}_{job.num_players}p_best*"

            baseline_model = None
            for f in baseline_dir.glob(baseline_pattern):
                baseline_model = str(f)
                break

            if not baseline_model:
                # No baseline - this model becomes baseline
                logger.info(f"No baseline found for {config_key}, new model becomes baseline")
                await self._promote_to_baseline(new_model_path, job.board_type, job.num_players, job.job_type)
                return

            # Schedule tournament via SSH tournament system
            tournament_id = f"autoeval_{config_key}_{int(time.time())}"

            # Use existing SSH tournament infrastructure
            with self.ssh_tournament_lock:
                self.ssh_tournament_runs[tournament_id] = SSHTournamentRun(
                    tournament_id=tournament_id,
                    board_type=job.board_type,
                    num_players=job.num_players,
                    status="pending",
                    started_at=time.time(),
                )

            # Start tournament in background
            tournament_config = {
                "tournament_id": tournament_id,
                "board_type": job.board_type,
                "num_players": job.num_players,
                "model_a": new_model_path,
                "model_b": baseline_model,
                "games_per_matchup": 50,
            }
            asyncio.create_task(self._run_model_comparison_tournament(tournament_config))

        except Exception as e:  # noqa: BLE001
            logger.info(f"Model comparison scheduling error: {e}")

    async def _run_model_comparison_tournament(self, config: dict):
        """Run a model comparison tournament and update baseline if new model wins."""
        tournament_id = config["tournament_id"]
        try:
            logger.info(f"Running model comparison tournament {tournament_id}")

            results_dir = Path(self.ringrift_path) / "ai-service" / "results" / "tournaments"
            results_dir.mkdir(parents=True, exist_ok=True)

            cmd = [
                sys.executable,
                os.path.join(self.ringrift_path, "ai-service", "scripts", "run_tournament.py"),
                "--player1", f"nn:{config['model_a']}",
                "--player2", f"nn:{config['model_b']}",
                "--board", config["board_type"],
                "--num-players", str(config["num_players"]),
                "--games", str(config["games_per_matchup"]),
                "--output", str(results_dir / f"{tournament_id}.json"),
            ]

            env = os.environ.copy()
            env["PYTHONPATH"] = os.path.join(self.ringrift_path, "ai-service")
            env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

            proc = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env,
            )

            _stdout, _stderr = await asyncio.wait_for(proc.communicate(), timeout=3600)

            if proc.returncode == 0:
                results_file = results_dir / f"{tournament_id}.json"
                if results_file.exists():
                    import json as json_module
                    results = json_module.loads(results_file.read_text())
                    new_model_wins = results.get("player1_wins", 0)
                    baseline_wins = results.get("player2_wins", 0)
                    total_games = new_model_wins + baseline_wins

                    win_rate = new_model_wins / total_games if total_games > 0 else 0.5
                    logger.info(f"Tournament {tournament_id}: new model win rate = {win_rate:.1%}")

                    promoted = win_rate >= 0.55
                    if promoted:
                        logger.info("New model beats baseline! Promoting to best baseline.")
                        await self._promote_to_baseline(
                            config["model_a"], config["board_type"],
                            config["num_players"], "nnue" if "nnue" in config["model_a"].lower() else "cmaes"
                        )

                    # Update improvement cycle manager with tournament result
                    await self._handle_tournament_completion(
                        tournament_id,
                        config["board_type"],
                        config["num_players"],
                        config["model_a"],
                        config["model_b"],
                        win_rate,
                        promoted,
                    )

            with self.ssh_tournament_lock:
                if tournament_id in self.ssh_tournament_runs:
                    self.ssh_tournament_runs[tournament_id].status = "completed"
                    self.ssh_tournament_runs[tournament_id].completed_at = time.time()

        except Exception as e:  # noqa: BLE001
            logger.info(f"Tournament {tournament_id} error: {e}")
            with self.ssh_tournament_lock:
                if tournament_id in self.ssh_tournament_runs:
                    self.ssh_tournament_runs[tournament_id].status = "failed"
                    self.ssh_tournament_runs[tournament_id].error = str(e)

    async def _promote_to_baseline(self, model_path: str, board_type: str, num_players: int, model_type: str):
        """Promote a model to the best baseline for its board type."""
        try:
            import shutil
            baseline_dir = Path(self.ringrift_path) / "ai-service" / "models" / model_type
            baseline_dir.mkdir(parents=True, exist_ok=True)

            baseline_path = baseline_dir / f"{board_type}_{num_players}p_best.pt"
            if baseline_path.exists():
                backup_path = baseline_dir / f"{board_type}_{num_players}p_prev_{int(time.time())}.pt"
                shutil.copy2(baseline_path, backup_path)
                logger.info(f"Backed up previous baseline to {backup_path}")

            shutil.copy2(model_path, baseline_path)
            logger.info(f"Promoted {model_path} to baseline at {baseline_path}")

            # Dec 2025: Emit MODEL_PROMOTED event for coordination layer
            # Enables: model distribution, model selector hot-reload, temperature adjustment
            config_key = f"{board_type}_{num_players}p"
            model_id = Path(model_path).name
            await self._emit_model_promoted(
                model_id=model_id,
                config_key=config_key,
                elo=0.0,  # Elo not available in this context
                elo_gain=0.0,
                source="p2p_orchestrator._promote_to_baseline",
            )

        except Exception as e:  # noqa: BLE001
            logger.info(f"Baseline promotion error: {e}")

    async def _check_cmaes_auto_tuning(self, config_key: str):
        """Check if CMA-ES auto-tuning should be triggered for a config.

        Monitors Elo progression and triggers hyperparameter optimization
        when the model's improvement plateaus.
        """
        if not HAS_PFSP or config_key not in self.cmaes_auto_tuners:
            return

        try:
            # Get current Elo from unified database
            from app.tournament import get_elo_database
            db = get_elo_database()

            parts = config_key.rsplit("_", 1)
            board_type = parts[0]
            num_players = int(parts[1].replace("p", ""))

            # Find best model for this config
            best_model = None
            best_elo = INITIAL_ELO_RATING
            models_dir = Path(self.ringrift_path) / "ai-service" / "models" / "nnue"
            pattern = f"nnue_{board_type}_{num_players}p*.pt"

            for model_path in models_dir.glob(pattern):
                model_id = model_path.stem
                elo = db.get_elo(model_id)
                if elo and elo > best_elo:
                    best_elo = elo
                    best_model = model_id

            if not best_model:
                return

            # Check for plateau
            auto_tuner = self.cmaes_auto_tuners[config_key]
            self.last_cmaes_elo.get(config_key, INITIAL_ELO_RATING)

            # Record Elo history for plateau detection
            should_tune = auto_tuner.check_plateau(best_elo)
            self.last_cmaes_elo[config_key] = best_elo

            if should_tune:
                logger.info(f"[CMA-ES] Elo plateau detected for {config_key} (Elo: {best_elo:.0f})")
                logger.info("[CMA-ES] Triggering auto hyperparameter optimization...")

                # Trigger CMA-ES via existing distributed infrastructure
                await self._trigger_auto_cmaes(board_type, num_players)

        except Exception as e:  # noqa: BLE001
            logger.info(f"[CMA-ES] Auto-tuning check error for {config_key}: {e}")

    def get_pfsp_opponent(self, config_key: str) -> str | None:
        """Get a PFSP-sampled opponent model for selfplay.

        Returns path to an opponent model sampled from the PFSP pool,
        weighted by difficulty (harder opponents sampled more frequently).
        """
        if not HAS_PFSP or config_key not in self.pfsp_pools:
            return None

        try:
            pool = self.pfsp_pools[config_key]
            opponent = pool.sample_opponent()
            if opponent:
                return opponent.model_path
        except Exception as e:  # noqa: BLE001
            logger.error(f"[PFSP] Error sampling opponent: {e}")
        return None

    def update_pfsp_stats(self, config_key: str, model_id: str, win_rate: float, elo: float):
        """Update PFSP stats for a model after evaluation games.

        Called after tournament/evaluation to update opponent difficulty metrics.
        """
        if not HAS_PFSP or config_key not in self.pfsp_pools:
            return

        try:
            self.pfsp_pools[config_key].update_stats(model_id, win_rate=win_rate, elo=elo)
            logger.info(f"[PFSP] Updated stats for {model_id}: win_rate={win_rate:.2f}, elo={elo:.0f}")
        except Exception as e:  # noqa: BLE001
            logger.error(f"[PFSP] Error updating stats: {e}")

    async def _handle_tournament_completion(
        self,
        tournament_id: str,
        board_type: str,
        num_players: int,
        new_model: str,
        baseline_model: str,
        win_rate: float,
        promoted: bool,
    ):
        """Handle tournament completion - update cycle state and trigger next iteration.

        This closes the feedback loop by:
        1. Updating improvement cycle manager with evaluation result
        2. Recording result to unified Elo database
        3. Updating diversity metrics
        4. Boosting selfplay for this config if model was promoted
        """
        try:
            # 1. Update improvement cycle manager
            if self.improvement_cycle_manager:
                self.improvement_cycle_manager.handle_evaluation_complete(
                    board_type, num_players, win_rate, new_model
                )
                logger.info(f"Updated improvement cycle for {board_type}_{num_players}p")

            # 2. Record to unified Elo database
            try:
                from app.tournament import get_elo_database
                db = get_elo_database()
                # Rankings: 0 = winner, 1 = loser
                rankings = [0, 1] if win_rate > 0.5 else [1, 0]
                db.record_match_and_update(
                    participant_ids=[new_model, baseline_model],
                    rankings=rankings,
                    board_type=board_type,
                    num_players=num_players,
                    tournament_id=tournament_id,
                )
                logger.info("Recorded tournament result to unified Elo DB")

                # Trigger Elo sync to propagate to cluster
                if HAS_ELO_SYNC and self.elo_sync_manager:
                    asyncio.create_task(self._trigger_elo_sync_after_matches(1))
            except Exception as e:  # noqa: BLE001
                logger.info(f"Elo database update failed (non-fatal): {e}")

            # 3. Update diversity metrics
            if hasattr(self, 'diversity_metrics'):
                self.diversity_metrics["tournament_runs"] = self.diversity_metrics.get("tournament_runs", 0) + 1
                if promoted:
                    self.diversity_metrics["promotions"] = self.diversity_metrics.get("promotions", 0) + 1

            # 4. Record metrics for observability
            self.record_metric(
                "tournament_win_rate",
                win_rate,
                board_type=board_type,
                num_players=num_players,
                metadata={
                    "new_model": new_model,
                    "baseline_model": baseline_model,
                    "promoted": promoted,
                    "tournament_id": tournament_id,
                },
            )

            # 5. Boost selfplay for this config if promoted (more data for next iteration)
            if promoted:
                asyncio.create_task(self._boost_selfplay_for_config(board_type, num_players))
                # Alert on successful promotion
                asyncio.create_task(self.notifier.send(
                    title="Model Promoted",
                    message=f"New model promoted for {board_type}_{num_players}p with {win_rate*100:.1f}% win rate",
                    level="info",
                    fields={"Model": new_model, "Win Rate": f"{win_rate*100:.1f}%"},
                    node_id=self.node_id,
                ))
            elif win_rate < 0.5:
                # Alert on failed promotion (new model lost)
                asyncio.create_task(self.notifier.send(
                    title="Model Promotion Failed",
                    message=f"New model failed tournament for {board_type}_{num_players}p with only {win_rate*100:.1f}% win rate",
                    level="warning",
                    fields={
                        "Model": new_model,
                        "Win Rate": f"{win_rate*100:.1f}%",
                        "Baseline": baseline_model,
                    },
                    node_id=self.node_id,
                ))

        except Exception as e:  # noqa: BLE001
            logger.info(f"Tournament completion handler error: {e}")
            asyncio.create_task(self.notifier.send(
                title="Tournament Handler Error",
                message=str(e),
                level="error",
                node_id=self.node_id,
            ))

    async def _boost_selfplay_for_config(self, board_type: str, num_players: int):
        """Temporarily boost selfplay for a configuration after model promotion.

        This accelerates data generation for the next training iteration.
        """
        try:
            config_key = f"{board_type}_{num_players}p"
            logger.info(f"Boosting selfplay for {config_key} after promotion")

            # Schedule additional selfplay jobs for this configuration
            # This will be picked up by the next job scheduling cycle
            if hasattr(self, 'selfplay_boost_configs'):
                self.selfplay_boost_configs[config_key] = {
                    "boost_until": time.time() + 3600,  # Boost for 1 hour
                    "multiplier": 1.5,  # 50% more jobs
                }
            else:
                self.selfplay_boost_configs = {
                    config_key: {
                        "boost_until": time.time() + 3600,
                        "multiplier": 1.5,
                    }
                }

        except Exception as e:  # noqa: BLE001
            logger.info(f"Selfplay boost error: {e}")

    async def _propagate_cmaes_weights(
        self, board_type: str, num_players: int, weights: dict[str, float]
    ):
        """Propagate new CMA-ES weights to selfplay workers.

        After CMA-ES optimization finds better weights, this:
        1. Saves weights to shared config file
        2. Restarts selfplay jobs for this config with new weights
        """
        try:
            config_key = f"{board_type}_{num_players}p"
            logger.info(f"Propagating CMA-ES weights for {config_key}")

            # 1. Save to shared heuristic weights config
            config_path = Path(self.ringrift_path) / "ai-service" / "config" / "heuristic_weights.json"
            config_path.parent.mkdir(parents=True, exist_ok=True)

            import json as json_mod
            existing = {}
            if config_path.exists():
                with contextlib.suppress(Exception):
                    existing = json_mod.loads(config_path.read_text())

            existing[config_key] = {
                "weights": weights,
                "updated_at": time.time(),
            }
            config_path.write_text(json_mod.dumps(existing, indent=2))
            logger.info(f"Updated heuristic_weights.json with {config_key} weights")

            # 2. Track config for weight-aware selfplay scheduling
            if not hasattr(self, 'cmaes_weight_configs'):
                self.cmaes_weight_configs = {}

            self.cmaes_weight_configs[config_key] = {
                "weights": weights,
                "updated_at": time.time(),
            }

            # 3. Stop existing selfplay jobs for this config (they'll restart with new weights)
            jobs_to_stop = []
            with self.jobs_lock:
                for job_id, job in self.local_jobs.items():
                    if (job.job_type in (JobType.SELFPLAY, JobType.GPU_SELFPLAY, JobType.HYBRID_SELFPLAY, JobType.CPU_SELFPLAY, JobType.GUMBEL_SELFPLAY)
                        and getattr(job, 'board_type', None) == board_type
                        and getattr(job, 'num_players', None) == num_players
                        and job.status == "running"):
                        jobs_to_stop.append(job_id)

            for job_id in jobs_to_stop:
                await self._stop_local_job(job_id)
                logger.info(f"Stopped selfplay job {job_id} for weight update")

            # 4. Boost selfplay to generate data with new weights
            asyncio.create_task(self._boost_selfplay_for_config(board_type, num_players))

            logger.info(f"Weight propagation complete for {config_key}")

        except Exception as e:  # noqa: BLE001
            logger.info(f"CMA-ES weight propagation error: {e}")

    async def _stop_local_job(self, job_id: str):
        """Stop a local job by job ID."""
        try:
            with self.jobs_lock:
                job = self.local_jobs.get(job_id)
                if job and hasattr(job, 'process') and job.process:
                    job.process.terminate()
                    job.status = "stopped"
        except Exception as e:  # noqa: BLE001
            logger.error(f"stopping job {job_id}: {e}")

    async def _import_gpu_selfplay_to_canonical(
        self, validated_db: Path, board_type: str, num_players: int, game_count: int
    ):
        """Import validated GPU selfplay games to canonical selfplay database.

        After GPU selfplay games pass CPU validation (>=95% validation rate),
        this merges them into the canonical selfplay database for training.
        """
        try:
            # Determine canonical DB path
            canonical_db = Path(self.ringrift_path) / "ai-service" / "data" / "games" / "selfplay.db"
            if not canonical_db.parent.exists():
                canonical_db.parent.mkdir(parents=True, exist_ok=True)

            logger.info(f"Auto-importing {game_count} validated GPU games to canonical DB...")

            # Use sqlite3 to merge games from validated_db to canonical_db
            import sqlite3

            # Phase 3.4 Dec 29, 2025: Use context managers to prevent connection leaks
            with safe_db_connection(validated_db) as src_conn, \
                 safe_db_connection(canonical_db) as dst_conn:

                # Ensure destination tables exist
                dst_conn.execute("""
                    CREATE TABLE IF NOT EXISTS games (
                        game_id TEXT PRIMARY KEY,
                        board_type TEXT NOT NULL,
                        num_players INTEGER NOT NULL,
                        winner INTEGER,
                        move_count INTEGER,
                        game_time_ms INTEGER,
                        created_at REAL,
                        source TEXT DEFAULT 'selfplay'
                    )
                """)
                dst_conn.execute("""
                    CREATE TABLE IF NOT EXISTS moves (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        game_id TEXT NOT NULL,
                        move_number INTEGER NOT NULL,
                        player INTEGER NOT NULL,
                        move_type TEXT NOT NULL,
                        from_pos TEXT,
                        to_pos TEXT,
                        direction TEXT,
                        captured_pos TEXT,
                        state_before TEXT,
                        policy_probs TEXT,
                        value_est REAL,
                        FOREIGN KEY (game_id) REFERENCES games(game_id)
                    )
                """)
                dst_conn.execute("""
                    CREATE INDEX IF NOT EXISTS idx_moves_game_id ON moves(game_id)
                """)
                dst_conn.commit()

                # Check source schema and copy games
                src_cursor = src_conn.cursor()
                src_cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                src_tables = {row[0] for row in src_cursor.fetchall()}

                imported = 0
                if "games" in src_tables:
                    # Get existing game IDs in destination to avoid duplicates
                    dst_cursor = dst_conn.cursor()
                    dst_cursor.execute("SELECT game_id FROM games")
                    existing_ids = {row[0] for row in dst_cursor.fetchall()}

                    # Copy games that don't already exist
                    src_cursor.execute("SELECT * FROM games")
                    src_columns = [desc[0] for desc in src_cursor.description]

                    for row in src_cursor.fetchall():
                        game_id_idx = src_columns.index("game_id") if "game_id" in src_columns else 0
                        game_id = row[game_id_idx]

                        if game_id in existing_ids:
                            continue

                        # Insert game with proper column mapping
                        placeholders = ", ".join(["?"] * len(row))
                        columns = ", ".join(src_columns)
                        try:
                            dst_conn.execute(
                                f"INSERT OR IGNORE INTO games ({columns}) VALUES ({placeholders})",
                                row
                            )
                            imported += 1
                        except (AttributeError):
                            continue

                    # Copy moves for new games
                    if "moves" in src_tables and imported > 0:
                        src_cursor.execute("SELECT * FROM moves")
                        move_columns = [desc[0] for desc in src_cursor.description]
                        move_placeholders = ", ".join(["?"] * len(move_columns))
                        move_col_str = ", ".join(move_columns)

                        for row in src_cursor.fetchall():
                            game_id_idx = move_columns.index("game_id") if "game_id" in move_columns else 1
                            game_id = row[game_id_idx]
                            if game_id not in existing_ids:
                                try:
                                    dst_conn.execute(
                                        f"INSERT OR IGNORE INTO moves ({move_col_str}) VALUES ({move_placeholders})",
                                        row
                                    )
                                except (AttributeError):
                                    continue

                    dst_conn.commit()

            logger.info(f"Successfully imported {imported} GPU selfplay games to canonical DB")

            # Update cluster data manifest to reflect new games
            config_key = f"{board_type}_{num_players}p"
            if hasattr(self, 'cluster_data_manifest') and self.cluster_data_manifest and config_key in self.cluster_data_manifest.by_board_type:
                self.cluster_data_manifest.by_board_type[config_key]["total_games"] = (
                    self.cluster_data_manifest.by_board_type[config_key].get("total_games", 0) + imported
                )

            # Notify improvement cycle manager of new games
            if self.improvement_cycle_manager and imported > 0:
                self.improvement_cycle_manager.record_games(board_type, num_players, imported)

        except Exception as e:  # noqa: BLE001
            logger.info(f"GPU selfplay import error: {e}")
            import traceback
            traceback.print_exc()

    # =========================================================================

    # =========================================================================
    # NOTE: Improvement Cycle handlers moved to ImprovementHandlersMixin
    # See: scripts/p2p/handlers/improvement.py (Dec 28, 2025 - Phase 8)
    # =========================================================================

    async def handle_metrics(self, request: web.Request) -> web.Response:
        """GET /metrics - Get metrics summary and history.

        Content negotiation:
        - Accept: text/plain -> Prometheus format (same as /metrics/prometheus)
        - Accept: application/json -> JSON format
        - Default (no header) -> Prometheus format for Prometheus scraper compatibility
        """
        try:
            # Content negotiation for Prometheus compatibility
            accept = request.headers.get("Accept", "")
            # Prometheus sends "text/plain" or "application/openmetrics-text"
            # Also check for explicit format param
            format_param = request.query.get("format", "").lower()
            if format_param == "prometheus" or "text/plain" in accept or "openmetrics" in accept or not accept:
                # Return Prometheus format
                return await self.handle_metrics_prometheus(request)

            hours = float(request.query.get("hours", "24"))
            metric_type = request.query.get("type")
            board_type = request.query.get("board_type")
            num_players_str = request.query.get("num_players")
            num_players = int(num_players_str) if num_players_str else None

            if metric_type:
                # Get specific metric history
                history = self.get_metrics_history(
                    metric_type=metric_type,
                    board_type=board_type,
                    num_players=num_players,
                    hours=hours,
                )
                return web.json_response({
                    "success": True,
                    "metric_type": metric_type,
                    "period_hours": hours,
                    "count": len(history),
                    "history": history,
                })
            else:
                # Get summary of all metrics
                summary = self.get_metrics_summary(hours=hours)
                return web.json_response({
                    "success": True,
                    **summary,
                })

        except Exception as e:  # noqa: BLE001
            return web.json_response({"success": False, "error": str(e)})

    async def handle_metrics_prometheus(self, request: web.Request) -> web.Response:
        """GET /metrics/prometheus - Prometheus-compatible metrics export.

        Returns metrics in Prometheus text exposition format for scraping.
        """
        try:
            lines = []
            now = time.time()

            # Cluster metrics
            with self.peers_lock:
                alive_peers = len([p for p in self.peers.values() if p.is_alive()])
                total_peers = len(self.peers)

            lines.append("# HELP ringrift_cluster_peers_total Total number of known peers")
            lines.append("# TYPE ringrift_cluster_peers_total gauge")
            lines.append(f"ringrift_cluster_peers_total {total_peers}")

            lines.append("# HELP ringrift_cluster_peers_alive Number of alive peers")
            lines.append("# TYPE ringrift_cluster_peers_alive gauge")
            lines.append(f"ringrift_cluster_peers_alive {alive_peers}")

            lines.append("# HELP ringrift_is_leader Whether this node is the leader")
            lines.append("# TYPE ringrift_is_leader gauge")
            lines.append(f"ringrift_is_leader {1 if self.role == NodeRole.LEADER else 0}")

            # Job counts
            with self.jobs_lock:
                selfplay_jobs = len([j for j in self.local_jobs.values()
                                    if j.job_type in (JobType.SELFPLAY, JobType.GPU_SELFPLAY, JobType.HYBRID_SELFPLAY, JobType.CPU_SELFPLAY, JobType.GUMBEL_SELFPLAY)
                                    and j.status == "running"])
                training_jobs = len([j for j in self.local_jobs.values()
                                    if j.job_type == JobType.TRAINING and j.status == "running"])

            lines.append("# HELP ringrift_selfplay_jobs_running Number of running selfplay jobs")
            lines.append("# TYPE ringrift_selfplay_jobs_running gauge")
            lines.append(f"ringrift_selfplay_jobs_running {selfplay_jobs}")

            lines.append("# HELP ringrift_training_jobs_running Number of running training jobs")
            lines.append("# TYPE ringrift_training_jobs_running gauge")
            lines.append(f"ringrift_training_jobs_running {training_jobs}")

            # Games per hour metric - aggregate from all peers
            lines.append("# HELP ringrift_selfplay_games_per_hour Estimated games generated per hour")
            lines.append("# TYPE ringrift_selfplay_games_per_hour gauge")

            # Calculate games/hour from peer data
            total_cluster_selfplay_jobs = 0
            with self.peers_lock:
                for peer in self.peers.values():
                    if peer.is_alive():
                        jobs = getattr(peer, 'selfplay_jobs', 0) or 0
                        total_cluster_selfplay_jobs += jobs

            # Estimate games/hour based on running jobs (rough heuristic: ~30 games/hour per job)
            # This is a rough estimate; actual rate depends on board size and GPU speed
            estimated_games_per_hour = total_cluster_selfplay_jobs * 30
            lines.append(f"ringrift_selfplay_games_per_hour {estimated_games_per_hour}")

            # Also report total cluster selfplay jobs
            lines.append("# HELP ringrift_cluster_selfplay_jobs_total Total selfplay jobs across cluster")
            lines.append("# TYPE ringrift_cluster_selfplay_jobs_total gauge")
            lines.append(f"ringrift_cluster_selfplay_jobs_total {total_cluster_selfplay_jobs}")

            # Resource utilization - include node labels for all nodes
            lines.append("# HELP ringrift_cpu_percent CPU utilization percentage per node")
            lines.append("# TYPE ringrift_cpu_percent gauge")

            lines.append("# HELP ringrift_memory_percent Memory utilization percentage per node")
            lines.append("# TYPE ringrift_memory_percent gauge")

            lines.append("# HELP ringrift_disk_percent Disk utilization percentage per node")
            lines.append("# TYPE ringrift_disk_percent gauge")

            lines.append("# HELP ringrift_gpu_percent GPU utilization percentage per node")
            lines.append("# TYPE ringrift_gpu_percent gauge")

            lines.append("# HELP ringrift_selfplay_jobs Selfplay jobs per node")
            lines.append("# TYPE ringrift_selfplay_jobs gauge")

            lines.append("# HELP ringrift_node_alive Whether node is alive (1) or not (0)")
            lines.append("# TYPE ringrift_node_alive gauge")

            # Cluster cost metrics (for Grafana dashboards)
            # GPU hourly rates (Lambda Labs pricing)
            GPU_HOURLY_RATES = {
                "GH200": 2.49, "H100": 2.49, "A100": 1.99, "A10": 0.75,
                "RTX_4090": 0.50, "RTX4090": 0.50, "4090": 0.50,
                "RTX_3090": 0.30, "RTX3090": 0.30, "3090": 0.30,
                "unknown": 0.50,
            }

            lines.append("# HELP ringrift_cluster_node_up Whether cluster node is active (1=up, 0=down)")
            lines.append("# TYPE ringrift_cluster_node_up gauge")
            lines.append("# HELP ringrift_cluster_node_cost_per_hour Estimated hourly cost in USD")
            lines.append("# TYPE ringrift_cluster_node_cost_per_hour gauge")
            lines.append("# HELP ringrift_cluster_gpu_utilization GPU utilization as fraction (0-1)")
            lines.append("# TYPE ringrift_cluster_gpu_utilization gauge")
            lines.append("# HELP ringrift_cluster_cpu_utilization CPU utilization as fraction (0-1)")
            lines.append("# TYPE ringrift_cluster_cpu_utilization gauge")
            lines.append("# HELP ringrift_cluster_gpu_memory_used_bytes GPU memory used in bytes")
            lines.append("# TYPE ringrift_cluster_gpu_memory_used_bytes gauge")
            lines.append("# HELP ringrift_cluster_memory_used_bytes System memory used in bytes")
            lines.append("# TYPE ringrift_cluster_memory_used_bytes gauge")

            # Export self metrics with node label
            node_name = self.node_id or "unknown"
            cpu = getattr(self.self_info, 'cpu_percent', 0)
            mem = getattr(self.self_info, 'memory_percent', 0)
            disk = getattr(self.self_info, 'disk_percent', 0)
            gpu = getattr(self.self_info, 'gpu_percent', 0) if self.self_info.has_gpu else 0
            role = "leader" if self.role == NodeRole.LEADER else "worker"
            gpu_type = getattr(self.self_info, 'gpu_type', 'unknown') or 'unknown'
            # Normalize GPU type for lookup
            gpu_type_key = gpu_type.replace(' ', '_').upper() if gpu_type else 'unknown'
            hourly_cost = GPU_HOURLY_RATES.get(gpu_type_key, GPU_HOURLY_RATES.get(gpu_type, GPU_HOURLY_RATES['unknown']))
            gpu_mem_bytes = getattr(self.self_info, 'gpu_memory_used_bytes', 0) or 0
            sys_mem_bytes = getattr(self.self_info, 'memory_used_bytes', 0) or 0

            lines.append(f'ringrift_cpu_percent{{node="{node_name}",role="{role}"}} {cpu}')
            lines.append(f'ringrift_memory_percent{{node="{node_name}",role="{role}"}} {mem}')
            lines.append(f'ringrift_disk_percent{{node="{node_name}",role="{role}"}} {disk}')
            lines.append(f'ringrift_gpu_percent{{node="{node_name}",role="{role}"}} {gpu}')
            lines.append(f'ringrift_selfplay_jobs{{node="{node_name}",role="{role}"}} {selfplay_jobs}')
            lines.append(f'ringrift_node_alive{{node="{node_name}",role="{role}"}} 1')

            # Export cluster cost metrics for self (for Grafana cost dashboard)
            lines.append(f'ringrift_cluster_node_up{{node="{node_name}",gpu_type="{gpu_type}"}} 1')
            lines.append(f'ringrift_cluster_node_cost_per_hour{{node="{node_name}",gpu_type="{gpu_type}"}} {hourly_cost}')
            lines.append(f'ringrift_cluster_gpu_utilization{{node="{node_name}",gpu_type="{gpu_type}"}} {gpu / 100.0 if gpu else 0}')
            lines.append(f'ringrift_cluster_cpu_utilization{{node="{node_name}"}} {cpu / 100.0 if cpu else 0}')
            lines.append(f'ringrift_cluster_gpu_memory_used_bytes{{node="{node_name}",gpu_type="{gpu_type}"}} {gpu_mem_bytes}')
            lines.append(f'ringrift_cluster_memory_used_bytes{{node="{node_name}"}} {sys_mem_bytes}')

            # Export peer metrics with node labels
            with self.peers_lock:
                for peer_id, peer in self.peers.items():
                    peer_name = peer_id or "unknown"
                    peer_role = "worker"
                    is_alive = 1 if peer.is_alive() else 0

                    # Get peer resource info if available
                    peer_cpu = getattr(peer, 'cpu_percent', 0) or 0
                    peer_mem = getattr(peer, 'memory_percent', 0) or 0
                    peer_gpu = getattr(peer, 'gpu_percent', 0) or 0
                    peer_jobs = getattr(peer, 'selfplay_jobs', 0) or 0
                    peer_gpu_type = getattr(peer, 'gpu_type', 'unknown') or 'unknown'
                    peer_gpu_type_key = peer_gpu_type.replace(' ', '_').upper() if peer_gpu_type else 'unknown'
                    peer_hourly_cost = GPU_HOURLY_RATES.get(peer_gpu_type_key, GPU_HOURLY_RATES.get(peer_gpu_type, GPU_HOURLY_RATES['unknown']))
                    peer_gpu_mem = getattr(peer, 'gpu_memory_used_bytes', 0) or 0
                    peer_sys_mem = getattr(peer, 'memory_used_bytes', 0) or 0

                    lines.append(f'ringrift_cpu_percent{{node="{peer_name}",role="{peer_role}"}} {peer_cpu}')
                    lines.append(f'ringrift_memory_percent{{node="{peer_name}",role="{peer_role}"}} {peer_mem}')
                    lines.append(f'ringrift_gpu_percent{{node="{peer_name}",role="{peer_role}"}} {peer_gpu}')
                    lines.append(f'ringrift_selfplay_jobs{{node="{peer_name}",role="{peer_role}"}} {peer_jobs}')
                    lines.append(f'ringrift_node_alive{{node="{peer_name}",role="{peer_role}"}} {is_alive}')

                    # Export cluster cost metrics for peer
                    lines.append(f'ringrift_cluster_node_up{{node="{peer_name}",gpu_type="{peer_gpu_type}"}} {is_alive}')
                    lines.append(f'ringrift_cluster_node_cost_per_hour{{node="{peer_name}",gpu_type="{peer_gpu_type}"}} {peer_hourly_cost if is_alive else 0}')
                    lines.append(f'ringrift_cluster_gpu_utilization{{node="{peer_name}",gpu_type="{peer_gpu_type}"}} {peer_gpu / 100.0 if peer_gpu else 0}')
                    lines.append(f'ringrift_cluster_cpu_utilization{{node="{peer_name}"}} {peer_cpu / 100.0 if peer_cpu else 0}')
                    lines.append(f'ringrift_cluster_gpu_memory_used_bytes{{node="{peer_name}",gpu_type="{peer_gpu_type}"}} {peer_gpu_mem}')
                    lines.append(f'ringrift_cluster_memory_used_bytes{{node="{peer_name}"}} {peer_sys_mem}')

            # Elo metrics with config labels
            try:
                from scripts.run_model_elo_tournament import ELO_DB_PATH, init_elo_database
                if ELO_DB_PATH and ELO_DB_PATH.exists():
                    db = init_elo_database()
                    conn = db._get_connection()
                    cursor = conn.cursor()
                    cursor.execute("""
                        SELECT board_type, num_players, MAX(rating) as best_elo
                        FROM elo_ratings
                        WHERE games_played >= 10
                        GROUP BY board_type, num_players
                    """)
                    lines.append("# HELP ringrift_best_elo Best Elo rating per configuration")
                    lines.append("# TYPE ringrift_best_elo gauge")
                    for row in cursor.fetchall():
                        bt, np, elo = row
                        config = f"{bt}_{np}p"
                        lines.append(f'ringrift_best_elo{{config="{config}",board_type="{bt}",num_players="{np}"}} {elo}')
                    db.close()
            except (OSError, AttributeError, ImportError):
                pass

            # Diversity metrics
            if hasattr(self, 'diversity_metrics'):
                dm = self.diversity_metrics
                lines.append("# HELP ringrift_tournament_runs_total Total tournament runs")
                lines.append("# TYPE ringrift_tournament_runs_total counter")
                lines.append(f"ringrift_tournament_runs_total {dm.get('tournament_runs', 0)}")

                lines.append("# HELP ringrift_promotions_total Total model promotions")
                lines.append("# TYPE ringrift_promotions_total counter")
                lines.append(f"ringrift_promotions_total {dm.get('promotions', 0)}")

                lines.append("# HELP ringrift_rollbacks_total Total model rollbacks")
                lines.append("# TYPE ringrift_rollbacks_total counter")
                lines.append(f"ringrift_rollbacks_total {dm.get('rollbacks', 0)}")

                # GPU validation stats
                gpu_stats = dm.get('gpu_validation_stats', {})
                if gpu_stats:
                    lines.append("# HELP ringrift_gpu_games_validated_total Total GPU games validated")
                    lines.append("# TYPE ringrift_gpu_games_validated_total counter")
                    lines.append(f"ringrift_gpu_games_validated_total {gpu_stats.get('total_validated', 0)}")

                    lines.append("# HELP ringrift_gpu_games_failed_total Total GPU games failed validation")
                    lines.append("# TYPE ringrift_gpu_games_failed_total counter")
                    lines.append(f"ringrift_gpu_games_failed_total {gpu_stats.get('total_failed', 0)}")

            # Recent metrics from database (last hour averages)
            try:
                summary = self.get_metrics_summary(hours=1)
                metrics_data = summary.get("metrics", {})

                for metric_name, metric_info in metrics_data.items():
                    safe_name = metric_name.replace("-", "_").replace(".", "_")
                    if metric_info.get("latest") is not None:
                        lines.append(f"# HELP ringrift_{safe_name} Latest {metric_name} value")
                        lines.append(f"# TYPE ringrift_{safe_name} gauge")
                        lines.append(f"ringrift_{safe_name} {metric_info['latest']}")
            except (AttributeError):
                pass

            # Data manifest totals
            if hasattr(self, 'cluster_data_manifest') and self.cluster_data_manifest:
                for config_key, config_data in self.cluster_data_manifest.by_board_type.items():
                    total_games = config_data.get("total_games", 0)
                    parts = config_key.split("_")
                    if len(parts) >= 2:
                        board_type = parts[0]
                        num_players = parts[1].replace("p", "")
                        lines.append(f'ringrift_games_total{{board_type="{board_type}",num_players="{num_players}"}} {total_games}')

            # Add header for games total
            if hasattr(self, 'cluster_data_manifest') and self.cluster_data_manifest:
                lines.insert(-len(self.cluster_data_manifest.by_board_type),
                           "# HELP ringrift_games_total Total games per board configuration")
                lines.insert(-len(self.cluster_data_manifest.by_board_type),
                           "# TYPE ringrift_games_total gauge")

            # === CRITICAL SELF-IMPROVEMENT LOOP METRICS ===

            # Training Progress Metrics
            lines.append("# HELP ringrift_training_loss Current model training loss")
            lines.append("# TYPE ringrift_training_loss gauge")
            lines.append("# HELP ringrift_training_val_loss Current model validation loss")
            lines.append("# TYPE ringrift_training_val_loss gauge")
            lines.append("# HELP ringrift_training_epoch Current training epoch")
            lines.append("# TYPE ringrift_training_epoch gauge")
            if hasattr(self, 'training_metrics'):
                for config, metrics in self.training_metrics.items():
                    loss = metrics.get('loss', 0)
                    val_loss = metrics.get('val_loss', 0)
                    epoch = metrics.get('epoch', 0)
                    lines.append(f'ringrift_training_loss{{config="{config}"}} {loss}')
                    lines.append(f'ringrift_training_val_loss{{config="{config}"}} {val_loss}')
                    lines.append(f'ringrift_training_epoch{{config="{config}"}} {epoch}')

            # Data Freshness Metrics
            lines.append("# HELP ringrift_data_freshness_hours Age of newest training data in hours")
            lines.append("# TYPE ringrift_data_freshness_hours gauge")
            lines.append("# HELP ringrift_data_staleness_hours Age of oldest training data in hours")
            lines.append("# TYPE ringrift_data_staleness_hours gauge")
            try:
                from pathlib import Path
                selfplay_dir = Path("data/selfplay")
                if selfplay_dir.exists():
                    for config_dir in selfplay_dir.iterdir():
                        if config_dir.is_dir() and not config_dir.name.startswith('.'):
                            jsonl_files = list(config_dir.glob("*.jsonl"))
                            if jsonl_files:
                                newest = max(f.stat().st_mtime for f in jsonl_files)
                                oldest = min(f.stat().st_mtime for f in jsonl_files)
                                freshness_hours = (now - newest) / 3600
                                staleness_hours = (now - oldest) / 3600
                                config_name = config_dir.name
                                lines.append(f'ringrift_data_freshness_hours{{config="{config_name}"}} {freshness_hours:.2f}')
                                lines.append(f'ringrift_data_staleness_hours{{config="{config_name}"}} {staleness_hours:.2f}')
            except (OSError, AttributeError, ImportError):
                pass

            # Selfplay Throughput Metrics
            lines.append("# HELP ringrift_selfplay_games_per_hour Selfplay game generation rate")
            lines.append("# TYPE ringrift_selfplay_games_per_hour gauge")
            lines.append("# HELP ringrift_selfplay_games_total_24h Total games generated in last 24h")
            lines.append("# TYPE ringrift_selfplay_games_total_24h gauge")
            if hasattr(self, 'selfplay_throughput'):
                for config, rate in self.selfplay_throughput.items():
                    lines.append(f'ringrift_selfplay_games_per_hour{{config="{config}"}} {rate}')

            # Cost Efficiency Metrics
            lines.append("# HELP ringrift_gpu_hours_total Total GPU hours consumed")
            lines.append("# TYPE ringrift_gpu_hours_total counter")
            lines.append("# HELP ringrift_estimated_cost_usd Estimated cost in USD")
            lines.append("# TYPE ringrift_estimated_cost_usd gauge")
            lines.append("# HELP ringrift_elo_per_gpu_hour Elo improvement per GPU hour")
            lines.append("# TYPE ringrift_elo_per_gpu_hour gauge")
            if hasattr(self, 'cost_metrics'):
                gpu_hours = self.cost_metrics.get('gpu_hours_total', 0)
                cost_usd = self.cost_metrics.get('estimated_cost_usd', 0)
                elo_per_hour = self.cost_metrics.get('elo_per_gpu_hour', 0)
                lines.append(f"ringrift_gpu_hours_total {gpu_hours}")
                lines.append(f"ringrift_estimated_cost_usd {cost_usd}")
                lines.append(f"ringrift_elo_per_gpu_hour {elo_per_hour}")

            # Promotion Quality Metrics
            lines.append("# HELP ringrift_promotion_success_rate Promotion success rate (0-1)")
            lines.append("# TYPE ringrift_promotion_success_rate gauge")
            lines.append("# HELP ringrift_promotion_elo_gain Average Elo gain on successful promotion")
            lines.append("# TYPE ringrift_promotion_elo_gain gauge")
            lines.append("# HELP ringrift_promotion_rejections_total Total promotion rejections by reason")
            lines.append("# TYPE ringrift_promotion_rejections_total counter")
            if hasattr(self, 'promotion_metrics'):
                success_rate = self.promotion_metrics.get('success_rate', 0)
                avg_gain = self.promotion_metrics.get('avg_elo_gain', 0)
                lines.append(f"ringrift_promotion_success_rate {success_rate}")
                lines.append(f"ringrift_promotion_elo_gain {avg_gain}")
                for reason, count in self.promotion_metrics.get('rejections', {}).items():
                    lines.append(f'ringrift_promotion_rejections_total{{reason="{reason}"}} {count}')

            # Model Evaluation Quality Metrics
            lines.append("# HELP ringrift_eval_games_played Games played in model evaluation")
            lines.append("# TYPE ringrift_eval_games_played gauge")
            lines.append("# HELP ringrift_eval_confidence Evaluation confidence (0-1)")
            lines.append("# TYPE ringrift_eval_confidence gauge")
            lines.append("# HELP ringrift_elo_uncertainty Elo rating uncertainty margin")
            lines.append("# TYPE ringrift_elo_uncertainty gauge")
            try:
                from scripts.run_model_elo_tournament import ELO_DB_PATH, init_elo_database
                if ELO_DB_PATH and ELO_DB_PATH.exists():
                    db = init_elo_database()
                    conn = db._get_connection()
                    cursor = conn.cursor()
                    cursor.execute("""
                        SELECT board_type, num_players,
                               AVG(games_played) as avg_games,
                               AVG(rating_deviation) as avg_rd
                        FROM elo_ratings
                        WHERE games_played >= 5
                        GROUP BY board_type, num_players
                    """)
                    for row in cursor.fetchall():
                        bt, np, avg_games, avg_rd = row
                        config = f"{bt}_{np}p"
                        confidence = max(0, min(1, 1 - (avg_rd / 350)))  # RD 350 = 0% confidence
                        lines.append(f'ringrift_eval_games_played{{config="{config}"}} {avg_games:.1f}')
                        lines.append(f'ringrift_eval_confidence{{config="{config}"}} {confidence:.3f}')
                        lines.append(f'ringrift_elo_uncertainty{{config="{config}"}} {avg_rd:.1f}')
                    db.close()
            except (OSError, AttributeError, ImportError):
                pass

            # Improvement Loop Health Metrics
            lines.append("# HELP ringrift_improvement_cycles_total Total improvement cycles completed")
            lines.append("# TYPE ringrift_improvement_cycles_total counter")
            lines.append("# HELP ringrift_last_improvement_hours Hours since last Elo improvement")
            lines.append("# TYPE ringrift_last_improvement_hours gauge")
            lines.append("# HELP ringrift_training_queue_size Number of configs awaiting training")
            lines.append("# TYPE ringrift_training_queue_size gauge")
            if hasattr(self, 'improvement_cycle_manager') and self.improvement_cycle_manager:
                icm = self.improvement_cycle_manager
                # Count total training iterations across all cycles
                cycles_completed = sum(c.current_iteration for c in icm.state.cycles.values())
                lines.append(f"ringrift_improvement_cycles_total {cycles_completed}")

            # Victory Type Metrics by board config
            lines.append("# HELP ringrift_victory_type_total Games won by victory type")
            lines.append("# TYPE ringrift_victory_type_total counter")
            try:
                victory_stats = await self._get_victory_type_stats()
                for (board_type, num_players, victory_type), count in victory_stats.items():
                    lines.append(
                        f'ringrift_victory_type_total{{board_type="{board_type}",num_players="{num_players}",victory_type="{victory_type}"}} {count}'
                    )
            except (AttributeError):
                pass

            # Game Analytics Metrics
            lines.append("# HELP ringrift_game_length_avg Average game length by config")
            lines.append("# TYPE ringrift_game_length_avg gauge")
            lines.append("# HELP ringrift_games_per_hour Game generation throughput")
            lines.append("# TYPE ringrift_games_per_hour gauge")
            lines.append("# HELP ringrift_opening_diversity Unique opening moves seen")
            lines.append("# TYPE ringrift_opening_diversity gauge")
            try:
                # Use cached analytics if available
                analytics = await self._get_game_analytics_cached()
                for config, stats in analytics.get("configs", {}).items():
                    parts = config.rsplit("_", 1)
                    if len(parts) == 2:
                        board_type = parts[0]
                        num_players = parts[1].replace("p", "")
                        lines.append(f'ringrift_game_length_avg{{board_type="{board_type}",num_players="{num_players}"}} {stats.get("avg_length", 0)}')
                        lines.append(f'ringrift_games_per_hour{{board_type="{board_type}",num_players="{num_players}"}} {stats.get("throughput_per_hour", 0)}')
                        lines.append(f'ringrift_opening_diversity{{board_type="{board_type}",num_players="{num_players}"}} {stats.get("opening_diversity", 0)}')
            except (AttributeError):
                pass

            # Best Elo by Config
            lines.append("# HELP ringrift_best_elo Best Elo rating by config")
            lines.append("# TYPE ringrift_best_elo gauge")
            lines.append("# HELP ringrift_elo_games_played Games played by best model")
            lines.append("# TYPE ringrift_elo_games_played gauge")
            try:
                import sqlite3
                ai_root = Path(self.ringrift_path) / "ai-service"
                db_path = ai_root / "data" / "unified_elo.db"
                if not db_path.exists():
                    db_path = ai_root / "data" / "unified_elo.db"
                if db_path.exists():
                    # Phase 3.4 Dec 29, 2025: Use context manager to prevent connection leaks
                    with safe_db_connection(db_path) as conn:
                        cursor = conn.cursor()
                        # Check which column name is used (model_id vs participant_id)
                        cursor.execute("PRAGMA table_info(elo_ratings)")
                        columns = [col[1] for col in cursor.fetchall()]
                        id_col = "model_id" if "model_id" in columns else "participant_id"
                        cursor.execute(f"""
                            SELECT board_type, num_players, MAX(rating), {id_col}, games_played
                            FROM elo_ratings
                            WHERE games_played >= 10
                            GROUP BY board_type, num_players
                        """)
                        for row in cursor.fetchall():
                            bt, np, rating, model, games = row
                            lines.append(f'ringrift_best_elo{{board_type="{bt}",num_players="{np}",model="{model}"}} {rating:.1f}')
                            lines.append(f'ringrift_elo_games_played{{board_type="{bt}",num_players="{np}",model="{model}"}} {games}')
            except (OSError, KeyError, IndexError, AttributeError, ImportError, sqlite3.Error):
                pass

            # Training Loss Metrics (from latest training)
            lines.append("# HELP ringrift_training_loss Latest training loss")
            lines.append("# TYPE ringrift_training_loss gauge")
            lines.append("# HELP ringrift_training_epoch Current training epoch")
            lines.append("# TYPE ringrift_training_epoch gauge")
            try:
                training_metrics = await self._get_training_metrics_cached()
                for config, data in training_metrics.get("configs", {}).items():
                    parts = config.rsplit("_", 1)
                    if len(parts) == 2 and data.get("latest_loss"):
                        board_type = parts[0]
                        num_players = parts[1].replace("p", "")
                        lines.append(f'ringrift_training_loss{{board_type="{board_type}",num_players="{num_players}"}} {data["latest_loss"]}')
                        lines.append(f'ringrift_training_epoch{{board_type="{board_type}",num_players="{num_players}"}} {data.get("latest_epoch", 0)}')
            except (AttributeError):
                pass

            # === HOLDOUT VALIDATION METRICS ===
            lines.append("# HELP ringrift_holdout_games Number of games in holdout set")
            lines.append("# TYPE ringrift_holdout_games gauge")
            lines.append("# HELP ringrift_holdout_positions Number of positions in holdout set")
            lines.append("# TYPE ringrift_holdout_positions gauge")
            lines.append("# HELP ringrift_holdout_loss Model loss on holdout validation set")
            lines.append("# TYPE ringrift_holdout_loss gauge")
            lines.append("# HELP ringrift_holdout_accuracy Model accuracy on holdout validation set")
            lines.append("# TYPE ringrift_holdout_accuracy gauge")
            lines.append("# HELP ringrift_overfit_gap Gap between holdout and training loss (positive = overfitting)")
            lines.append("# TYPE ringrift_overfit_gap gauge")
            try:
                holdout_metrics = await self._get_holdout_metrics_cached()
                for config, data in holdout_metrics.get("configs", {}).items():
                    parts = config.rsplit("_", 1)
                    if len(parts) == 2:
                        board_type = parts[0]
                        num_players = parts[1].replace("p", "")
                        lines.append(f'ringrift_holdout_games{{board_type="{board_type}",num_players="{num_players}"}} {data.get("holdout_games", 0)}')
                        lines.append(f'ringrift_holdout_positions{{board_type="{board_type}",num_players="{num_players}"}} {data.get("holdout_positions", 0)}')
                        if data.get("holdout_loss") is not None:
                            lines.append(f'ringrift_holdout_loss{{board_type="{board_type}",num_players="{num_players}"}} {data["holdout_loss"]}')
                        if data.get("holdout_accuracy") is not None:
                            lines.append(f'ringrift_holdout_accuracy{{board_type="{board_type}",num_players="{num_players}"}} {data["holdout_accuracy"]}')
                        if data.get("overfit_gap") is not None:
                            lines.append(f'ringrift_overfit_gap{{board_type="{board_type}",num_players="{num_players}"}} {data["overfit_gap"]}')
            except (AttributeError):
                pass

            # === MCTS SEARCH STATISTICS ===
            lines.append("# HELP ringrift_mcts_avg_nodes Average MCTS nodes visited per move")
            lines.append("# TYPE ringrift_mcts_avg_nodes gauge")
            lines.append("# HELP ringrift_mcts_max_nodes Maximum MCTS nodes visited in a move")
            lines.append("# TYPE ringrift_mcts_max_nodes gauge")
            lines.append("# HELP ringrift_mcts_avg_depth Average MCTS search depth")
            lines.append("# TYPE ringrift_mcts_avg_depth gauge")
            lines.append("# HELP ringrift_mcts_max_depth Maximum MCTS search depth")
            lines.append("# TYPE ringrift_mcts_max_depth gauge")
            lines.append("# HELP ringrift_mcts_avg_time Average time per MCTS move (seconds)")
            lines.append("# TYPE ringrift_mcts_avg_time gauge")
            try:
                mcts_stats = await self._get_mcts_stats_cached()
                summary = mcts_stats.get("summary", {})
                if summary.get("avg_nodes_per_move"):
                    lines.append(f'ringrift_mcts_avg_nodes {summary["avg_nodes_per_move"]:.0f}')
                if summary.get("max_nodes_per_move"):
                    lines.append(f'ringrift_mcts_max_nodes {summary["max_nodes_per_move"]}')
                if summary.get("avg_search_depth"):
                    lines.append(f'ringrift_mcts_avg_depth {summary["avg_search_depth"]:.1f}')
                if summary.get("max_search_depth"):
                    lines.append(f'ringrift_mcts_max_depth {summary["max_search_depth"]}')
                if summary.get("avg_time_per_move"):
                    lines.append(f'ringrift_mcts_avg_time {summary["avg_time_per_move"]:.3f}')
                # Per-config MCTS stats
                for config, data in mcts_stats.get("configs", {}).items():
                    parts = config.rsplit("_", 1)
                    if len(parts) == 2:
                        board_type = parts[0]
                        num_players = parts[1].replace("p", "")
                        if data.get("avg_nodes"):
                            lines.append(f'ringrift_mcts_avg_nodes{{board_type="{board_type}",num_players="{num_players}"}} {data["avg_nodes"]:.0f}')
                        if data.get("avg_depth"):
                            lines.append(f'ringrift_mcts_avg_depth{{board_type="{board_type}",num_players="{num_players}"}} {data["avg_depth"]:.1f}')
            except (AttributeError):
                pass

            # === DATA QUALITY METRICS ===
            lines.append("# HELP ringrift_data_quality_games Total games analyzed for quality")
            lines.append("# TYPE ringrift_data_quality_games gauge")
            lines.append("# HELP ringrift_data_quality_short_rate Percentage of short games (<10 moves)")
            lines.append("# TYPE ringrift_data_quality_short_rate gauge")
            lines.append("# HELP ringrift_data_quality_issues Number of data quality issues detected")
            lines.append("# TYPE ringrift_data_quality_issues gauge")
            try:
                quality = await self._get_data_quality_cached()
                for config, data in quality.get("configs", {}).items():
                    parts = config.rsplit("_", 1)
                    if len(parts) == 2:
                        board_type = parts[0]
                        num_players = parts[1].replace("p", "")
                        lines.append(f'ringrift_data_quality_games{{board_type="{board_type}",num_players="{num_players}"}} {data.get("total_games", 0)}')
                        lines.append(f'ringrift_data_quality_short_rate{{board_type="{board_type}",num_players="{num_players}"}} {data.get("short_game_rate", 0)}')
                lines.append(f'ringrift_data_quality_issues {len(quality.get("issues", []))}')
            except (AttributeError):
                pass

            # === TRAINING EFFICIENCY METRICS ===
            lines.append("# HELP ringrift_gpu_hours_total Total GPU hours used for training")
            lines.append("# TYPE ringrift_gpu_hours_total gauge")
            lines.append("# HELP ringrift_elo_per_gpu_hour Elo points gained per GPU hour")
            lines.append("# TYPE ringrift_elo_per_gpu_hour gauge")
            lines.append("# HELP ringrift_training_cost_usd Estimated training cost in USD")
            lines.append("# TYPE ringrift_training_cost_usd gauge")
            try:
                efficiency = await self._get_training_efficiency_cached()
                for config, data in efficiency.get("configs", {}).items():
                    parts = config.rsplit("_", 1)
                    if len(parts) == 2:
                        board_type = parts[0]
                        num_players = parts[1].replace("p", "")
                        lines.append(f'ringrift_gpu_hours_total{{board_type="{board_type}",num_players="{num_players}"}} {data.get("gpu_hours", 0)}')
                        lines.append(f'ringrift_elo_per_gpu_hour{{board_type="{board_type}",num_players="{num_players}"}} {data.get("elo_per_gpu_hour", 0)}')
                        lines.append(f'ringrift_training_cost_usd{{board_type="{board_type}",num_players="{num_players}"}} {data.get("estimated_cost_usd", 0)}')
                summary = efficiency.get("summary", {})
                if summary:
                    lines.append(f'ringrift_gpu_hours_total {summary.get("total_gpu_hours", 0)}')
                    lines.append(f'ringrift_training_cost_usd {summary.get("total_estimated_cost_usd", 0)}')
            except (AttributeError):
                pass

            # === MODEL LINEAGE METRICS ===
            lines.append("# HELP ringrift_model_count Total number of trained models")
            lines.append("# TYPE ringrift_model_count gauge")
            lines.append("# HELP ringrift_model_generation Latest model generation per config")
            lines.append("# TYPE ringrift_model_generation gauge")
            try:
                lineage = await self._get_model_lineage_cached()
                lines.append(f'ringrift_model_count {lineage.get("total_models", 0)}')
                for config, data in lineage.get("configs", {}).items():
                    parts = config.rsplit("_", 1)
                    if len(parts) == 2:
                        board_type = parts[0]
                        num_players = parts[1].replace("p", "")
                        lines.append(f'ringrift_model_generation{{board_type="{board_type}",num_players="{num_players}"}} {data.get("latest_generation", 0)}')
            except (AttributeError):
                pass

            # === ROLLBACK STATUS METRICS ===
            lines.append("# HELP ringrift_rollback_candidates Number of configs recommended for rollback")
            lines.append("# TYPE ringrift_rollback_candidates gauge")
            try:
                rollback = await self._check_rollback_conditions()
                lines.append(f'ringrift_rollback_candidates {len(rollback.get("candidates", []))}')
            except (AttributeError):
                pass

            # === AUTOSCALING METRICS ===
            lines.append("# HELP ringrift_autoscale_suggested_workers Suggested worker count from autoscaling")
            lines.append("# TYPE ringrift_autoscale_suggested_workers gauge")
            lines.append("# HELP ringrift_cluster_games_per_hour Current cluster-wide game generation rate")
            lines.append("# TYPE ringrift_cluster_games_per_hour gauge")
            try:
                autoscale = await self._get_autoscaling_metrics()
                state = autoscale.get("current_state", {})
                lines.append(f'ringrift_cluster_games_per_hour {state.get("games_per_hour", 0)}')
                recs = autoscale.get("recommendations", [])
                if recs:
                    lines.append(f'ringrift_autoscale_suggested_workers {recs[0].get("suggested_workers", state.get("total_nodes", 1))}')
                else:
                    lines.append(f'ringrift_autoscale_suggested_workers {state.get("total_nodes", 1)}')
            except (AttributeError):
                pass

            # === P2P ENHANCEMENT METRICS ===

            # Adaptive Sync Intervals
            lines.append("# HELP ringrift_sync_interval_data Current data sync interval in seconds")
            lines.append("# TYPE ringrift_sync_interval_data gauge")
            lines.append("# HELP ringrift_sync_interval_model Current model sync interval in seconds")
            lines.append("# TYPE ringrift_sync_interval_model gauge")
            lines.append("# HELP ringrift_sync_activity_factor Cluster activity factor (lower = more active)")
            lines.append("# TYPE ringrift_sync_activity_factor gauge")
            try:
                sync_summary = self._get_sync_interval_summary()
                lines.append(f'ringrift_sync_interval_data {sync_summary.get("data_interval", 300)}')
                lines.append(f'ringrift_sync_interval_model {sync_summary.get("model_interval", 180)}')
                lines.append(f'ringrift_sync_activity_factor {sync_summary.get("activity_factor", 1.0)}')
            except (AttributeError):
                pass

            # Gossip Protocol Metrics
            lines.append("# HELP ringrift_gossip_messages_sent Total gossip messages sent")
            lines.append("# TYPE ringrift_gossip_messages_sent counter")
            lines.append("# HELP ringrift_gossip_messages_received Total gossip messages received")
            lines.append("# TYPE ringrift_gossip_messages_received counter")
            lines.append("# HELP ringrift_gossip_state_updates Total state updates from gossip")
            lines.append("# TYPE ringrift_gossip_state_updates counter")
            lines.append("# HELP ringrift_gossip_compression_ratio Gossip compression ratio (1.0 = 100% compressed)")
            lines.append("# TYPE ringrift_gossip_compression_ratio gauge")
            lines.append("# HELP ringrift_gossip_bytes_saved_kb Total bytes saved by compression")
            lines.append("# TYPE ringrift_gossip_bytes_saved_kb counter")
            try:
                gossip = self._get_gossip_metrics_summary()
                lines.append(f'ringrift_gossip_messages_sent {gossip.get("message_sent", 0)}')
                lines.append(f'ringrift_gossip_messages_received {gossip.get("message_received", 0)}')
                lines.append(f'ringrift_gossip_state_updates {gossip.get("state_updates", 0)}')
                lines.append(f'ringrift_gossip_compression_ratio {gossip.get("compression_ratio", 0)}')
                lines.append(f'ringrift_gossip_bytes_saved_kb {gossip.get("bytes_saved_kb", 0)}')
            except (AttributeError):
                pass

            # Leader Consensus Metrics
            lines.append("# HELP ringrift_leader_agreement Nodes agreeing on current leader")
            lines.append("# TYPE ringrift_leader_agreement gauge")
            try:
                consensus = self._get_cluster_leader_consensus()
                lines.append(f'ringrift_leader_agreement {consensus.get("leader_agreement", 0)}')
            except (AttributeError):
                pass

            # Data Deduplication Metrics
            lines.append("# HELP ringrift_dedup_files_skipped Files skipped due to deduplication")
            lines.append("# TYPE ringrift_dedup_files_skipped counter")
            lines.append("# HELP ringrift_dedup_bytes_saved_mb Megabytes saved by deduplication")
            lines.append("# TYPE ringrift_dedup_bytes_saved_mb gauge")
            lines.append("# HELP ringrift_dedup_known_hashes Number of file hashes tracked")
            lines.append("# TYPE ringrift_dedup_known_hashes gauge")
            try:
                dedup = self._get_dedup_summary()
                lines.append(f'ringrift_dedup_files_skipped {dedup.get("files_skipped", 0)}')
                lines.append(f'ringrift_dedup_bytes_saved_mb {dedup.get("bytes_saved_mb", 0)}')
                lines.append(f'ringrift_dedup_known_hashes {dedup.get("known_file_hashes", 0)}')
            except (AttributeError):
                pass

            # Tournament Scheduling Metrics
            lines.append("# HELP ringrift_tournament_proposals_pending Pending tournament proposals")
            lines.append("# TYPE ringrift_tournament_proposals_pending gauge")
            lines.append("# HELP ringrift_tournament_active Active distributed tournaments")
            lines.append("# TYPE ringrift_tournament_active gauge")
            try:
                tourney = self._get_distributed_tournament_summary()
                lines.append(f'ringrift_tournament_proposals_pending {tourney.get("pending_proposals", 0)}')
                lines.append(f'ringrift_tournament_active {tourney.get("active_tournaments", 0)}')
            except (AttributeError):
                pass

            # Work Queue Metrics (leader only)
            wq = get_work_queue()
            if self.is_leader and wq:
                lines.append("# HELP ringrift_work_queue_pending Work items pending in queue")
                lines.append("# TYPE ringrift_work_queue_pending gauge")
                lines.append("# HELP ringrift_work_queue_running Work items currently running")
                lines.append("# TYPE ringrift_work_queue_running gauge")
                lines.append("# HELP ringrift_work_queue_total Total work items by status")
                lines.append("# TYPE ringrift_work_queue_total gauge")
                lines.append("# HELP ringrift_work_queue_by_type Work items by type and status")
                lines.append("# TYPE ringrift_work_queue_by_type gauge")
                lines.append("# HELP ringrift_work_queue_completed_total Total completed work items")
                lines.append("# TYPE ringrift_work_queue_completed_total counter")
                lines.append("# HELP ringrift_work_queue_failed_total Total failed work items")
                lines.append("# TYPE ringrift_work_queue_failed_total counter")
                lines.append("# HELP ringrift_work_queue_timeout_total Total timed out work items")
                lines.append("# TYPE ringrift_work_queue_timeout_total counter")
                lines.append("# HELP ringrift_work_queue_cancelled_total Total cancelled work items")
                lines.append("# TYPE ringrift_work_queue_cancelled_total counter")
                lines.append("# HELP ringrift_work_queue_avg_wait_seconds Average wait time in queue")
                lines.append("# TYPE ringrift_work_queue_avg_wait_seconds gauge")
                lines.append("# HELP ringrift_work_queue_avg_run_seconds Average run time for work items")
                lines.append("# TYPE ringrift_work_queue_avg_run_seconds gauge")

                try:
                    status = wq.get_queue_status()
                    by_status = status.get("by_status", {})

                    # Basic queue counts from by_status dict
                    pending_count = by_status.get("pending", 0)
                    running_count = by_status.get("running", 0) + by_status.get("claimed", 0)
                    lines.append(f"ringrift_work_queue_pending {pending_count}")
                    lines.append(f"ringrift_work_queue_running {running_count}")
                    lines.append(f'ringrift_work_queue_total{{status="pending"}} {pending_count}')
                    lines.append(f'ringrift_work_queue_total{{status="running"}} {running_count}')

                    # Count by work type from by_type dict
                    by_type = status.get("by_type", {})
                    for wtype, count in by_type.items():
                        lines.append(f'ringrift_work_queue_by_type{{work_type="{wtype}"}} {count}')

                    # Historical counts from database
                    history = wq.get_history(limit=1000)
                    completed_count = sum(1 for h in history if h.get("status") == "completed")
                    failed_count = sum(1 for h in history if h.get("status") == "failed")
                    timeout_count = sum(1 for h in history if h.get("status") == "timeout")
                    cancelled_count = sum(1 for h in history if h.get("status") == "cancelled")

                    lines.append(f"ringrift_work_queue_completed_total {completed_count}")
                    lines.append(f"ringrift_work_queue_failed_total {failed_count}")
                    lines.append(f"ringrift_work_queue_timeout_total {timeout_count}")
                    lines.append(f"ringrift_work_queue_cancelled_total {cancelled_count}")

                    # Calculate average wait and run times from completed items
                    wait_times = []
                    run_times = []
                    for h in history:
                        if h.get("status") == "completed":
                            created = h.get("created_at", 0)
                            claimed = h.get("claimed_at", 0)
                            completed = h.get("completed_at", 0)
                            if claimed and created:
                                wait_times.append(claimed - created)
                            if completed and claimed:
                                run_times.append(completed - claimed)

                    avg_wait = sum(wait_times) / len(wait_times) if wait_times else 0
                    avg_run = sum(run_times) / len(run_times) if run_times else 0
                    lines.append(f"ringrift_work_queue_avg_wait_seconds {avg_wait:.2f}")
                    lines.append(f"ringrift_work_queue_avg_run_seconds {avg_run:.2f}")

                except (AttributeError, KeyError, ValueError, TypeError):
                    # If work queue metrics fail, just skip them
                    pass

            # Uptime metric
            if hasattr(self, 'start_time'):
                uptime = now - self.start_time
                lines.append("# HELP ringrift_orchestrator_uptime_seconds Orchestrator uptime in seconds")
                lines.append("# TYPE ringrift_orchestrator_uptime_seconds gauge")
                lines.append(f"ringrift_orchestrator_uptime_seconds {uptime:.0f}")

            return web.Response(
                text="\n".join(lines) + "\n",
                content_type="text/plain",
                charset="utf-8",
            )

        except Exception as e:  # noqa: BLE001
            return web.json_response({"success": False, "error": str(e)})

    # handle_improvement_training_complete and handle_improvement_evaluation_complete
    # moved to ImprovementHandlersMixin (Dec 28, 2025 - Phase 8)

    async def _schedule_improvement_evaluation(self, cycle_id: str, new_model_id: str):
        """Schedule tournament evaluation for a newly trained model via SSH."""
        if not self.improvement_cycle_manager:
            return
        try:
            cycle = self.improvement_cycle_manager.state.cycles.get(cycle_id)
            if not cycle:
                return

            config = cycle.config
            best_model_id = cycle.best_model_id or f"baseline_{config.board_type}_{config.num_players}p"

            logger.info(f"ImprovementCycle {cycle_id}: Scheduling evaluation {new_model_id} vs {best_model_id}")

            self.improvement_cycle_manager.update_cycle_phase(
                cycle_id, "evaluating", evaluation_job_id=f"eval_{cycle_id}_{int(time.time())}"
            )

            # Run SSH tournament evaluation
            eval_result = await self._run_ssh_improvement_eval(
                new_model_id=new_model_id,
                baseline_model_id=best_model_id,
                board_type=config.board_type,
                num_players=config.num_players,
                games=config.evaluation_games,
            )

            if eval_result.get("success"):
                new_model_wins = eval_result.get("new_model_wins", 0)
                baseline_wins = eval_result.get("baseline_wins", 0)
                draws = eval_result.get("draws", 0)
            else:
                # Fallback to mock results if SSH evaluation fails
                logger.info(f"ImprovementCycle {cycle_id}: SSH evaluation failed, using fallback")
                import random
                total_games = config.evaluation_games
                new_model_wins = random.randint(int(total_games * 0.4), int(total_games * 0.6))
                draws = random.randint(0, int(total_games * 0.1))
                baseline_wins = total_games - new_model_wins - draws

            self.improvement_cycle_manager.handle_evaluation_complete(
                cycle_id=cycle_id, new_model_id=new_model_id, best_model_id=best_model_id,
                wins=new_model_wins, losses=baseline_wins, draws=draws,
            )

        except Exception as e:  # noqa: BLE001
            logger.info(f"ImprovementCycle {cycle_id}: Evaluation scheduling failed: {e}")
            if self.improvement_cycle_manager:
                self.improvement_cycle_manager.update_cycle_phase(cycle_id, "idle", error_message=str(e))

    async def _run_ssh_improvement_eval(
        self,
        new_model_id: str,
        baseline_model_id: str,
        board_type: str,
        num_players: int,
        games: int,
    ) -> dict:
        """Run improvement evaluation via SSH on a remote host.

        Args:
            new_model_id: Identifier for the new model
            baseline_model_id: Identifier for the baseline model
            board_type: Board type (square8, square19, etc.)
            num_players: Number of players
            games: Number of games to play

        Returns:
            Dict with evaluation results or error
        """
        # Calculate timeout upfront to avoid scope issues in exception handler
        timeout_seconds = max(300, games * 30)  # 30s per game estimate, minimum 5 minutes

        try:
            # Get available hosts for evaluation
            if load_remote_hosts is None:
                return {"success": False, "error": "load_remote_hosts not available"}

            hosts = load_remote_hosts()
            if not hosts:
                return {"success": False, "error": "No remote hosts configured"}

            # Find a ready host with GPU capability (prefer high-performance hosts)
            eval_host = None
            for host in hosts:
                if getattr(host, 'status', None) == 'ready':
                    eval_host = host
                    break

            if not eval_host:
                # Try any host
                eval_host = hosts[0] if hosts else None

            if not eval_host:
                return {"success": False, "error": "No evaluation host available"}

            ssh_host = getattr(eval_host, 'ssh_host', None) or getattr(eval_host, 'tailscale_ip', None)
            if not ssh_host:
                return {"success": False, "error": "No SSH host configured"}

            ssh_user = getattr(eval_host, 'ssh_user', 'ubuntu')
            ringrift_path = getattr(eval_host, 'ringrift_path', '~/ringrift/ai-service')

            # Build model paths (assumes models are in standard locations)
            new_model_path = f"models/{board_type}_{num_players}p/{new_model_id}.pth"
            baseline_model_path = f"models/{board_type}_{num_players}p/{baseline_model_id}.pth"

            # Build SSH command
            remote_cmd = f'''cd {ringrift_path} && source venv/bin/activate && python scripts/run_improvement_eval.py \
                --new-model "{new_model_path}" \
                --baseline-model "{baseline_model_path}" \
                --board {board_type} \
                --players {num_players} \
                --games {games} \
                --ai-type descent 2>/dev/null'''

            logger.info(f"Running SSH evaluation on {eval_host.name}: {new_model_id} vs {baseline_model_id}")

            proc = await asyncio.create_subprocess_exec(
                "ssh",
                "-o", "ConnectTimeout=30",
                "-o", "BatchMode=yes",
                "-o", "StrictHostKeyChecking=no",
                f"{ssh_user}@{ssh_host}",
                remote_cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )

            stdout, stderr = await asyncio.wait_for(
                proc.communicate(),
                timeout=timeout_seconds
            )

            if proc.returncode != 0:
                stderr_text = stderr.decode()[:500] if stderr else ""
                logger.info(f"SSH evaluation failed on {eval_host.name}: {stderr_text}")
                return {"success": False, "error": f"SSH command failed: {stderr_text}"}

            # Parse JSON result from stdout
            stdout_text = stdout.decode().strip()
            if not stdout_text:
                return {"success": False, "error": "No output from evaluation script"}

            result = json.loads(stdout_text)
            logger.info(f"SSH evaluation complete: {result.get('new_model_wins', 0)}-{result.get('baseline_wins', 0)}-{result.get('draws', 0)}")
            return result

        except asyncio.TimeoutError:
            return {"success": False, "error": f"SSH evaluation timed out after {timeout_seconds}s"}
        except json.JSONDecodeError as e:
            return {"success": False, "error": f"Failed to parse evaluation result: {e}"}
        except Exception as e:  # noqa: BLE001
            return {"success": False, "error": str(e)}

    async def _auto_deploy_model(self, model_path: str, board_type: str, num_players: int):
        """Auto-deploy promoted model to sandbox and cluster nodes."""
        try:
            import subprocess
            logger.info(f"Auto-deploying model: {model_path}")

            # Build command args
            cmd_args = [
                sys.executable, "scripts/auto_deploy_models.py",
                "--model-path", model_path,
                "--board-type", board_type,
                "--num-players", str(num_players),
                "--skip-eval",  # Already evaluated
            ]
            if self._is_leader():
                cmd_args.append("--sync-cluster")

            # Run deployment script
            result = await asyncio.to_thread(
                subprocess.run,
                cmd_args,
                capture_output=True,
                text=True,
                timeout=300,
                cwd=str(Path(__file__).parent.parent),
            )

            if result.returncode == 0:
                logger.info(f"Model deployed successfully: {model_path}")
            else:
                logger.info(f"Model deployment failed: {result.stderr}")

        except Exception as e:  # noqa: BLE001
            logger.info(f"Auto-deploy error: {e}")

    # Canonical Pipeline Integration (for pipeline_orchestrator.py)
    # =========================================================================

    async def handle_pipeline_start(self, request: web.Request) -> web.Response:
        """POST /pipeline/start - Start a canonical pipeline phase."""
        try:
            if not self._is_leader() and request.query.get("local") != "1":
                return await self._proxy_to_leader(request)
            if not self._is_leader():
                return web.json_response({"success": False, "error": "Only leader can start pipeline phases",
                                         "leader_id": self.leader_id}, status=403)
            data = await request.json()
            phase = data.get("phase")
            board_type = data.get("board_type", "square8")
            num_players = data.get("num_players", 2)

            if phase == "canonical_selfplay":
                result = await self._start_canonical_selfplay_pipeline(
                    board_type,
                    num_players,
                    data.get("games_per_node", 500),
                    data.get("seed", 0),
                    include_gpu_nodes=bool(data.get("include_gpu_nodes", False)),
                )
            elif phase == "parity_validation":
                result = await self._start_parity_validation_pipeline(
                    board_type, num_players, data.get("db_paths"))
            elif phase == "npz_export":
                result = await self._start_npz_export_pipeline(
                    board_type, num_players, data.get("output_dir", "data/training"))
            else:
                return web.json_response({"success": False,
                    "error": f"Unknown phase: {phase}. Supported: canonical_selfplay, parity_validation, npz_export"}, status=400)
            return web.json_response(result)
        except Exception as e:  # noqa: BLE001
            logger.info(f"Pipeline start error: {e}")
            return web.json_response({"success": False, "error": str(e)}, status=500)

    async def handle_pipeline_status(self, request: web.Request) -> web.Response:
        """GET /pipeline/status - Get current pipeline phase status."""
        if not self._is_leader() and request.query.get("local") != "1":
            proxied = await self._proxy_to_leader(request)
            if proxied.status not in (502, 503):
                return proxied
        pipeline_status = getattr(self, '_pipeline_status', {})
        return web.json_response({"success": True, "node_id": self.node_id,
                                 "is_leader": self._is_leader(), "current_job": pipeline_status})

    async def handle_pipeline_selfplay_worker(self, request: web.Request) -> web.Response:
        """POST /pipeline/selfplay_worker - Worker endpoint for canonical selfplay."""
        try:
            data = await request.json()
            asyncio.create_task(self._run_local_canonical_selfplay(
                data.get("job_id"), data.get("board_type", "square8"), data.get("num_players", 2),
                data.get("num_games", 500), data.get("seed", 0)))
            return web.json_response({"success": True, "job_id": data.get("job_id"),
                                     "message": f"Started canonical selfplay: {data.get('num_games', 500)} games"})
        except Exception as e:  # noqa: BLE001
            return web.json_response({"success": False, "error": str(e)}, status=500)

    async def _start_canonical_selfplay_pipeline(
        self,
        board_type: str,
        num_players: int,
        games_per_node: int,
        seed: int,
        include_gpu_nodes: bool = False,
    ) -> dict[str, Any]:
        """Start canonical selfplay on healthy nodes in the cluster.

        Canonical selfplay is CPU-bound. By default, prefer CPU-only nodes so GPU
        machines remain available for GPU-utilizing tasks (training/hybrid selfplay).
        """
        job_id = f"pipeline-selfplay-{int(time.time())}"
        healthy_nodes: list[tuple[str, NodeInfo]] = []
        with self.peers_lock:
            for peer_id, peer in self.peers.items():
                if peer.is_alive() and peer.is_healthy():
                    healthy_nodes.append((peer_id, peer))
        if self.self_info.is_healthy():
            healthy_nodes.append((self.node_id, self.self_info))

        if not include_gpu_nodes:
            cpu_nodes = [(nid, n) for nid, n in healthy_nodes if n.is_cpu_only_node()]
            if cpu_nodes:
                healthy_nodes = cpu_nodes

        # Load-balance: least-loaded nodes first.
        healthy_nodes.sort(key=lambda pair: pair[1].get_load_score())

        if not healthy_nodes:
            return {"success": False, "error": "No healthy nodes available"}

        logger.info(f"Starting canonical selfplay pipeline: {len(healthy_nodes)} nodes, {games_per_node} games/node")
        dispatched = 0
        for i, (node_id, node) in enumerate(healthy_nodes):
            node_seed = seed + i * 10000 + hash(node_id) % 10000
            if node_id == self.node_id:
                asyncio.create_task(self._run_local_canonical_selfplay(
                    f"{job_id}-{node_id}", board_type, num_players, games_per_node, node_seed))
                dispatched += 1
            else:
                try:
                    if getattr(node, "nat_blocked", False):
                        payload = {
                            "job_id": f"{job_id}-{node_id}",
                            "board_type": board_type,
                            "num_players": num_players,
                            "num_games": games_per_node,
                            "seed": node_seed,
                        }
                        cmd_id = await self._enqueue_relay_command_for_peer(node, "canonical_selfplay", payload)
                        if cmd_id:
                            dispatched += 1
                        else:
                            logger.info(f"Relay queue full; skipping canonical selfplay enqueue for {node_id}")
                    else:
                        payload = {
                            "job_id": f"{job_id}-{node_id}",
                            "board_type": board_type,
                            "num_players": num_players,
                            "num_games": games_per_node,
                            "seed": node_seed,
                        }
                        async with get_client_session(ClientTimeout(total=30)) as session:
                            for url in self._urls_for_peer(node, "/pipeline/selfplay_worker"):
                                try:
                                    async with session.post(url, json=payload, headers=self._get_auth_headers()) as resp:
                                        if resp.status == 200:
                                            dispatched += 1
                                            break
                                except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
                                    continue
                except Exception as e:  # noqa: BLE001
                    logger.error(f"Failed to dispatch selfplay to {node_id}: {e}")

        self._pipeline_status = {"job_id": job_id, "phase": "canonical_selfplay", "status": "running",
            "dispatched_count": dispatched, "total_nodes": len(healthy_nodes),
            "board_type": board_type, "num_players": num_players,
            "games_per_node": games_per_node, "started_at": time.time()}
        return {"success": True, "job_id": job_id, "dispatched_count": dispatched, "total_nodes": len(healthy_nodes)}

    async def _run_local_canonical_selfplay(self, job_id: str, board_type: str, num_players: int,
                                            num_games: int, seed: int):
        """Run canonical selfplay locally."""
        try:
            db_file = os.path.join(self.ringrift_path, "ai-service", "data", "games",
                                   f"canonical_{board_type}_{num_players}p_{self.node_id}.db")
            log_file = os.path.join(self.ringrift_path, "ai-service", "logs", "selfplay",
                                    f"canonical_{job_id}.jsonl")
            os.makedirs(os.path.dirname(db_file), exist_ok=True)
            os.makedirs(os.path.dirname(log_file), exist_ok=True)

            cmd = [sys.executable, os.path.join(self.ringrift_path, "ai-service", "scripts", "run_self_play_soak.py"),
                "--num-games", str(num_games), "--board-type", board_type, "--num-players", str(num_players),
                "--max-moves", "10000",  # LEARNED LESSONS - Avoid draws due to move limit
                "--difficulty-band", "light", "--seed", str(seed), "--log-jsonl", log_file, "--record-db", db_file]
            env = os.environ.copy()
            env["PYTHONPATH"] = os.path.join(self.ringrift_path, "ai-service")
            env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

            logger.info(f"Starting canonical selfplay job {job_id}: {num_games} games -> {db_file}")
            proc = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE,
                                                        stderr=asyncio.subprocess.PIPE, env=env)
            _stdout, stderr = await proc.communicate()
            if proc.returncode == 0:
                logger.info(f"Canonical selfplay job {job_id} completed successfully")
            else:
                logger.info(f"Canonical selfplay job {job_id} failed: {stderr.decode()[:500]}")
        except Exception as e:  # noqa: BLE001
            logger.info(f"Canonical selfplay job {job_id} error: {e}")

    async def _start_parity_validation_pipeline(self, board_type: str, num_players: int,
                                                db_paths: list[str] | None) -> dict[str, Any]:
        """Start parity validation on the leader node."""
        job_id = f"pipeline-parity-{int(time.time())}"
        asyncio.create_task(self._run_parity_validation(job_id, board_type, num_players, db_paths))
        self._pipeline_status = {"job_id": job_id, "phase": "parity_validation", "status": "running",
                                "board_type": board_type, "num_players": num_players, "started_at": time.time()}
        return {"success": True, "job_id": job_id, "message": "Parity validation started"}

    async def _run_parity_validation(self, job_id: str, board_type: str, num_players: int,
                                     db_paths: list[str] | None):
        """Run parity validation."""
        try:
            if not db_paths:
                import glob
                db_paths = glob.glob(os.path.join(self.ringrift_path, "ai-service", "data", "games",
                                                  f"canonical_{board_type}_{num_players}p_*.db"))
            if not db_paths:
                self._pipeline_status["status"] = "failed"
                self._pipeline_status["error"] = "No databases found"
                return

            output_json = os.path.join(self.ringrift_path, "ai-service", "data", f"parity_validation_{job_id}.json")
            cmd = [sys.executable, os.path.join(self.ringrift_path, "ai-service", "scripts", "run_parity_validation.py"),
                "--databases", *db_paths, "--mode", "canonical", "--output-json", output_json, "--progress-every", "100"]
            env = os.environ.copy()
            env["PYTHONPATH"] = os.path.join(self.ringrift_path, "ai-service")
            env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

            logger.info(f"Starting parity validation job {job_id}: {len(db_paths)} databases")
            proc = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE,
                                                        stderr=asyncio.subprocess.PIPE, env=env)
            _stdout, stderr = await proc.communicate()
            if proc.returncode == 0:
                logger.info(f"Parity validation job {job_id} completed successfully")
                self._pipeline_status["status"] = "completed"
                if os.path.exists(output_json):
                    with open(output_json) as f:
                        self._pipeline_status["results"] = json.load(f)
            else:
                logger.info(f"Parity validation job {job_id} failed: {stderr.decode()[:500]}")
                self._pipeline_status["status"] = "failed"
                self._pipeline_status["error"] = stderr.decode()[:500]
        except Exception as e:  # noqa: BLE001
            logger.info(f"Parity validation job {job_id} error: {e}")
            self._pipeline_status["status"] = "failed"
            self._pipeline_status["error"] = str(e)

    async def _start_npz_export_pipeline(self, board_type: str, num_players: int,
                                         output_dir: str) -> dict[str, Any]:
        """Start NPZ export on the leader node."""
        job_id = f"pipeline-npz-{int(time.time())}"
        asyncio.create_task(self._run_npz_export(job_id, board_type, num_players, output_dir))
        self._pipeline_status = {"job_id": job_id, "phase": "npz_export", "status": "running",
                                "board_type": board_type, "num_players": num_players,
                                "output_dir": output_dir, "started_at": time.time()}
        return {"success": True, "job_id": job_id, "message": "NPZ export started"}

    async def _run_npz_export(self, job_id: str, board_type: str, num_players: int, output_dir: str):
        """Run NPZ export."""
        try:
            import glob
            db_paths = glob.glob(os.path.join(self.ringrift_path, "ai-service", "data", "games",
                                              f"canonical_{board_type}_{num_players}p_*.db"))
            if not db_paths:
                self._pipeline_status["status"] = "failed"
                self._pipeline_status["error"] = "No databases found"
                return

            full_output_dir = os.path.join(self.ringrift_path, "ai-service", output_dir)
            os.makedirs(full_output_dir, exist_ok=True)
            output_file = os.path.join(full_output_dir, f"canonical_{board_type}_{num_players}p_{job_id}.npz")

            cmd = [sys.executable, os.path.join(self.ringrift_path, "ai-service", "scripts", "export_replay_dataset.py"),
                "--databases", *db_paths, "--output", output_file, "--board-type", board_type,
                "--num-players", str(num_players)]
            env = os.environ.copy()
            env["PYTHONPATH"] = os.path.join(self.ringrift_path, "ai-service")

            logger.info(f"Starting NPZ export job {job_id}: {len(db_paths)} databases -> {output_file}")
            proc = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE,
                                                        stderr=asyncio.subprocess.PIPE, env=env)
            _stdout, stderr = await proc.communicate()
            if proc.returncode == 0:
                logger.info(f"NPZ export job {job_id} completed successfully")
                self._pipeline_status["status"] = "completed"
                self._pipeline_status["output_file"] = output_file
            else:
                logger.info(f"NPZ export job {job_id} failed: {stderr.decode()[:500]}")
                self._pipeline_status["status"] = "failed"
                self._pipeline_status["error"] = stderr.decode()[:500]
        except Exception as e:  # noqa: BLE001
            logger.info(f"NPZ export job {job_id} error: {e}")
            self._pipeline_status["status"] = "failed"
            self._pipeline_status["error"] = str(e)

    def _get_auth_headers(self) -> dict[str, str]:
        """Get authentication headers for peer requests."""
        return {"Authorization": f"Bearer {self.auth_token}"} if self.auth_token else {}

    # =========================================================================
    # Phase 4: REST API for External Job Submission and Dashboard
    # =========================================================================

    async def handle_root(self, request: web.Request) -> web.StreamResponse:
        """Redirect to the dashboard to avoid upstream 404s on `/`."""
        raise web.HTTPFound("/dashboard")

    async def handle_api_cluster_status(self, request: web.Request) -> web.Response:
        """Get comprehensive cluster status for external clients and dashboard."""
        try:
            if not self._is_leader() and request.query.get("local") != "1":
                proxied = await self._proxy_to_leader(request)
                if proxied.status not in (502, 503):
                    return proxied

            # Ensure local resource stats are fresh for dashboard consumers.
            with contextlib.suppress(Exception):
                self._update_self_info()

            is_leader = self._is_leader()
            effective_leader = self._get_leader_peer()
            effective_leader_id = effective_leader.node_id if effective_leader else None
            last_known_leader_id = self.leader_id
            leader_id = effective_leader_id or last_known_leader_id

            # Collect peer info (dashboard-oriented shape)
            peers_info: list[dict[str, Any]] = []
            include_retired = request.query.get("include_retired") == "1"
            with self.peers_lock:
                peers_snapshot = dict(self.peers)
            for peer_id, peer in peers_snapshot.items():
                if getattr(peer, "retired", False) and not include_retired:
                    continue
                status = "offline" if not peer.is_alive() else "online"
                key = self._endpoint_key(peer)
                effective_scheme, effective_host, effective_port = (None, None, None)
                if key:
                    effective_scheme, effective_host, effective_port = key
                peers_info.append(
                    {
                        "node_id": peer_id,
                        "host": peer.host,
                        "port": peer.port,
                        "scheme": getattr(peer, "scheme", "http"),
                        "reported_host": getattr(peer, "reported_host", ""),
                        "reported_port": getattr(peer, "reported_port", 0),
                        "effective_scheme": effective_scheme,
                        "effective_host": effective_host,
                        "effective_port": effective_port,
                        "nat_blocked": bool(getattr(peer, "nat_blocked", False)),
                        "relay_via": getattr(peer, "relay_via", ""),
                        "role": peer.role.value if hasattr(peer.role, "value") else str(peer.role),
                        "version": getattr(peer, "version", ""),
                        "status": status,
                        "last_seen": peer.last_heartbeat,
                        "capabilities": list(peer.capabilities) if peer.capabilities else [],
                        "current_job": "",
                        "has_gpu": bool(peer.has_gpu),
                        "cpu_percent": peer.cpu_percent,
                        "memory_percent": peer.memory_percent,
                        "disk_percent": peer.disk_percent,
                        "gpu_percent": peer.gpu_percent,
                        "gpu_memory_percent": peer.gpu_memory_percent,
                        "selfplay_jobs": peer.selfplay_jobs,
                        "training_jobs": peer.training_jobs,
                    }
                )

            # Collect local job info
            with self.jobs_lock:
                jobs_snapshot = list(self.local_jobs.values())
            jobs_info: list[dict[str, Any]] = [
                {
                    "job_id": job.job_id,
                    "job_type": job.job_type.value if hasattr(job.job_type, "value") else str(job.job_type),
                    "status": job.status,
                    "node_id": job.node_id,
                    "board_type": job.board_type,
                    "num_players": job.num_players,
                    "engine_mode": job.engine_mode,
                    "pid": job.pid,
                    "started_at": job.started_at,
                }
                for job in jobs_snapshot
            ]

            # Collect training job info
            training_info: list[dict[str, Any]] = []
            with self.training_lock:
                for job_id, job in self.training_jobs.items():
                    training_info.append(
                        {
                            "job_id": job_id,
                            "job_type": job.job_type,
                            "status": job.status,
                            "board_type": job.board_type,
                            "num_players": job.num_players,
                            "assigned_worker": job.worker_node,
                            "created_at": job.created_at,
                            "started_at": job.started_at,
                            "completed_at": job.completed_at,
                            "output_model_path": job.output_model_path,
                            "error_message": job.error_message,
                        }
                    )

            # Collect data manifest info (lightweight dashboard summary)
            # NOTE: Never block on manifest collection here - use cached data only.
            # The background _manifest_collection_loop will populate this shortly after startup.
            with self.manifest_lock:
                local_manifest = self.local_data_manifest
                cluster_manifest = self.cluster_data_manifest
                # Don't block on manifest collection - return what we have
                # local_manifest may be None during startup, which is fine

            manifest_info: dict[str, dict[str, Any]] = {}
            if cluster_manifest and getattr(cluster_manifest, "node_manifests", None):
                for node_id, node_manifest in cluster_manifest.node_manifests.items():
                    board_types = sorted(
                        {f.board_type for f in node_manifest.files if getattr(f, "board_type", "")}
                    )
                    manifest_info[node_id] = {
                        "game_count": node_manifest.selfplay_games,
                        "board_types": board_types,
                        "last_updated": node_manifest.collected_at,
                    }
            elif local_manifest:
                board_types = sorted(
                    {f.board_type for f in local_manifest.files if getattr(f, "board_type", "")}
                )
                manifest_info[local_manifest.node_id] = {
                    "game_count": local_manifest.selfplay_games,
                    "board_types": board_types,
                    "last_updated": local_manifest.collected_at,
                }

            voter_ids = list(getattr(self, "voter_node_ids", []) or [])
            voters_alive = 0
            if voter_ids:
                with self.peers_lock:
                    peers_by_id = dict(self.peers)
                for vid in voter_ids:
                    if vid == self.node_id:
                        voters_alive += 1
                        continue
                    p = peers_by_id.get(vid)
                    if p and p.is_alive():
                        voters_alive += 1

            self_payload = self.self_info.to_dict() if hasattr(self.self_info, "to_dict") else asdict(self.self_info)
            self_key = self._endpoint_key(self.self_info)
            if self_key:
                self_payload.update(
                    {
                        "effective_scheme": self_key[0],
                        "effective_host": self_key[1],
                        "effective_port": self_key[2],
                    }
                )

            return web.json_response({
                "success": True,
                "node_id": self.node_id,
                "role": self.role.value if hasattr(self.role, 'value') else str(self.role),
                "leader_id": leader_id,
                "effective_leader_id": effective_leader_id,
                "last_known_leader_id": last_known_leader_id,
                "is_leader": is_leader,
                "voter_node_ids": voter_ids,
                "voter_quorum_size": int(getattr(self, "voter_quorum_size", 0) or 0),
                "voters_alive": voters_alive,
                "voter_quorum_ok": self._has_voter_quorum(),
                "voter_config_source": str(getattr(self, "voter_config_source", "") or ""),
                "self": self_payload,
                "uptime_seconds": time.time() - self.start_time,
                "peers": peers_info,
                "peer_count": len(self.peers),
                "jobs": jobs_info,
                "job_count": len(jobs_info),
                "training_jobs": training_info,
                "training_job_count": len(training_info),
                "data_manifests": manifest_info,
                "timestamp": time.time(),
            })
        except Exception as e:  # noqa: BLE001
            return web.json_response({"success": False, "error": str(e)}, status=500)

    async def handle_api_cluster_git_update(self, request: web.Request) -> web.Response:
        """Leader-coordinated git updates for cluster nodes.

        Body (JSON):
            node_ids: list[str] | str (optional)
                If omitted, updates all known peers (online by default).
            include_self: bool (default False)
                If true and (node_ids omitted or includes this node_id), also update
                the leader node itself (performed last, triggers restart).
            include_offline: bool (default False)
                If true, attempt updates against offline peers as well.
            timeout_seconds: int (default 20, max 120)
                Per-peer request timeout.

        Notes:
            - This stops jobs and restarts orchestrators on nodes with updates
              available. Use with care.
        """
        try:
            if not self._is_leader() and request.query.get("local") != "1":
                return await self._proxy_to_leader(request)

            payload: dict[str, Any] = {}
            try:
                payload = await request.json()
            except (AttributeError):
                payload = {}

            node_ids_raw = payload.get("node_ids") or payload.get("nodes") or []
            node_ids: list[str] = []
            if isinstance(node_ids_raw, str):
                node_ids = [t.strip() for t in node_ids_raw.split(",") if t.strip()]
            elif isinstance(node_ids_raw, list):
                node_ids = [str(t).strip() for t in node_ids_raw if str(t).strip()]

            include_self = bool(payload.get("include_self", False))
            include_offline = bool(payload.get("include_offline", False))

            timeout_seconds = float(payload.get("timeout_seconds", 20) or 20)
            timeout_seconds = max(5.0, min(timeout_seconds, 120.0))

            with self.peers_lock:
                peers_by_id = dict(self.peers)

            targets: list[NodeInfo] = []

            def should_include_peer(peer: NodeInfo) -> bool:
                if peer.node_id == self.node_id:
                    return False
                return not (not include_offline and not peer.is_alive())

            if node_ids:
                for node_id in node_ids:
                    peer = peers_by_id.get(node_id)
                    if peer and should_include_peer(peer):
                        targets.append(peer)
            else:
                for peer in peers_by_id.values():
                    if should_include_peer(peer):
                        targets.append(peer)

            results: list[dict[str, Any]] = []
            timeout = ClientTimeout(total=timeout_seconds)
            async with get_client_session(timeout) as session:
                for peer in sorted(targets, key=lambda p: p.node_id):
                    peer_payload: dict[str, Any] = {
                        "node_id": peer.node_id,
                        "status": "online" if peer.is_alive() else "offline",
                        "success": False,
                        "attempted_urls": [],
                    }

                    if not include_offline and not peer.is_alive():
                        peer_payload["error"] = "offline"
                        results.append(peer_payload)
                        continue

                    last_error: str | None = None
                    for url in self._urls_for_peer(peer, "/git/update"):
                        peer_payload["attempted_urls"].append(url)
                        try:
                            async with session.post(url, json={}, headers=self._auth_headers()) as resp:
                                peer_payload["http_status"] = resp.status
                                try:
                                    data = await resp.json()
                                except (AttributeError):
                                    data = {"raw": await resp.text()}
                                peer_payload["response"] = data
                                if resp.status == 200:
                                    peer_payload["success"] = bool(data.get("success", True))
                                    break
                                last_error = (
                                    str(data.get("error") or "")
                                    or str(data.get("message") or "")
                                    or f"http_{resp.status}"
                                )
                        except Exception as exc:
                            last_error = str(exc)
                            continue

                    if last_error and not peer_payload.get("success"):
                        peer_payload["error"] = last_error

                    results.append(peer_payload)

            self_update: dict[str, Any] | None = None
            update_self = bool(include_self and (not node_ids or self.node_id in node_ids))
            if update_self:
                has_updates, local_commit, remote_commit = self._check_for_updates()
                if not has_updates:
                    self_update = {
                        "node_id": self.node_id,
                        "success": True,
                        "message": "Already up to date",
                        "local_commit": local_commit[:8] if local_commit else None,
                    }
                else:
                    success, message = await self._perform_git_update()
                    self_update = {
                        "node_id": self.node_id,
                        "success": success,
                        "message": message,
                        "old_commit": local_commit[:8] if local_commit else None,
                        "new_commit": remote_commit[:8] if remote_commit else None,
                    }
                    if success:
                        asyncio.create_task(self._restart_orchestrator())

            return web.json_response(
                {
                    "success": True,
                    "leader_id": self.node_id,
                    "updated_peers": results,
                    "self_update": self_update,
                    "timestamp": time.time(),
                }
            )
        except Exception as e:  # noqa: BLE001
            return web.json_response({"success": False, "error": str(e)}, status=500)

    async def handle_api_selfplay_stats(self, request: web.Request) -> web.Response:
        """Get aggregated selfplay game statistics for dashboard charts."""
        try:
            if not self._is_leader() and request.query.get("local") != "1":
                proxied = await self._proxy_to_leader(request)
                if proxied.status not in (502, 503):
                    return proxied

            with self.manifest_lock:
                cluster_manifest = self.cluster_data_manifest
                local_manifest = self.local_data_manifest
                history = list(self.selfplay_stats_history)

            by_board_type: dict[str, dict[str, Any]] = {}
            total_selfplay_games = 0
            manifest_collected_at = 0.0

            if cluster_manifest:
                by_board_type = cluster_manifest.by_board_type
                total_selfplay_games = int(cluster_manifest.total_selfplay_games or 0)
                manifest_collected_at = float(cluster_manifest.collected_at or 0.0)
            elif local_manifest:
                manifest_collected_at = float(local_manifest.collected_at or 0.0)
                totals: dict[str, int] = {}
                for f in getattr(local_manifest, "files", []) or []:
                    if getattr(f, "file_type", "") != "selfplay":
                        continue
                    board_type = getattr(f, "board_type", "") or ""
                    num_players = int(getattr(f, "num_players", 0) or 0)
                    if not board_type or not num_players:
                        continue
                    key = f"{board_type}_{num_players}p"
                    totals[key] = totals.get(key, 0) + int(getattr(f, "game_count", 0) or 0)
                by_board_type = {k: {"total_games": v, "nodes": [local_manifest.node_id]} for k, v in totals.items()}
                total_selfplay_games = sum(totals.values())

            return web.json_response(
                {
                    "success": True,
                    "node_id": self.node_id,
                    "is_leader": self._is_leader(),
                    "manifest_collected_at": manifest_collected_at,
                    "total_selfplay_games": total_selfplay_games,
                    "by_board_type": by_board_type,
                    "history": history,
                    "timestamp": time.time(),
                }
            )
        except Exception as e:  # noqa: BLE001
            return web.json_response({"success": False, "error": str(e)}, status=500)

    async def handle_api_elo_leaderboard(self, request: web.Request) -> web.Response:
        """Get Elo leaderboard for all board types from persistent database.

        Query params:
            board_type: Filter by board type (optional)
            num_players: Filter by number of players (optional)
            limit: Max results per config (default 20)
        """
        try:
            # Try to import Elo database functions
            try:
                from scripts.run_model_elo_tournament import (
                    ELO_DB_PATH,
                    get_leaderboard,
                    init_elo_database,
                )
            except ImportError:
                return web.json_response({
                    "success": False,
                    "error": "Elo database module not available",
                }, status=500)

            # Check if database exists
            if not ELO_DB_PATH or not ELO_DB_PATH.exists():
                return web.json_response({
                    "success": True,
                    "leaderboards": {},
                    "message": "No Elo database found yet. Run cross-model tournament to populate.",
                })

            board_type = request.query.get("board_type")
            num_players_str = request.query.get("num_players")
            num_players = int(num_players_str) if num_players_str else None
            limit = int(request.query.get("limit", "20"))

            db = init_elo_database()

            # If specific filter requested, return just that
            if board_type and num_players:
                leaderboard = get_leaderboard(db, board_type, num_players, limit=limit)
                db.close()
                return web.json_response({
                    "success": True,
                    "leaderboards": {f"{board_type}_{num_players}p": leaderboard},
                    "total_models": len(leaderboard),
                    "timestamp": time.time(),
                })

            # Otherwise return all board/player combinations
            # Query unique board_type/num_players combinations
            conn = db._get_connection()
            cursor = conn.cursor()
            cursor.execute("""
                SELECT DISTINCT board_type, num_players
                FROM elo_ratings
                WHERE board_type IS NOT NULL AND num_players IS NOT NULL
                ORDER BY board_type, num_players
            """)
            configs = cursor.fetchall()

            leaderboards = {}
            total_models = 0
            total_games = 0

            for bt, np in configs:
                key = f"{bt}_{np}p"
                lb = get_leaderboard(db, bt, np, limit=limit)
                if lb:
                    leaderboards[key] = lb
                    total_models += len(lb)
                    total_games += sum(entry.get("games_played", 0) for entry in lb)

            # Get match history stats
            cursor.execute("SELECT COUNT(*) FROM match_history")
            match_count = cursor.fetchone()[0]

            db.close()

            return web.json_response({
                "success": True,
                "leaderboards": leaderboards,
                "total_models": total_models,
                "total_matches": match_count,
                "total_games_recorded": total_games,
                "configs": [f"{bt}_{np}p" for bt, np in configs],
                "timestamp": time.time(),
            })

        except Exception as e:  # noqa: BLE001
            return web.json_response({"success": False, "error": str(e)}, status=500)

    # handle_elo_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)
    # handle_nodes_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    async def _get_victory_type_stats(self) -> dict[tuple[str, int, str], int]:
        """Aggregate victory types from recent game data.

        Returns dict mapping (board_type, num_players, victory_type) -> count.
        Caches results for 5 minutes to avoid excessive I/O.
        """
        import json
        from collections import defaultdict

        cache_key = "_victory_stats_cache"
        cache_time_key = "_victory_stats_cache_time"
        cache_ttl = 300  # 5 minutes

        # Check cache
        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        # Skip JSONL scanning during startup grace period
        if self._is_in_startup_grace_period():
            return {}

        stats: dict[tuple[str, int, str], int] = defaultdict(int)

        # Scan recent game files (last 24 hours)
        ai_root = Path(self.ringrift_path) / "ai-service"
        data_dirs = [
            ai_root / "data" / "games" / "daemon_sync",
            ai_root / "data" / "selfplay",
        ]

        cutoff_time = now - 86400  # 24 hours ago

        for data_dir in data_dirs:
            if not data_dir.exists():
                continue
            for jsonl_path in data_dir.rglob("*.jsonl"):
                try:
                    # Skip files older than 24h
                    if jsonl_path.stat().st_mtime < cutoff_time:
                        continue
                    with open_jsonl_file(jsonl_path) as f:
                        for line in f:
                            try:
                                game = json.loads(line)
                                board_type = game.get("board_type", "unknown")
                                num_players = game.get("num_players", 0)
                                victory_type = game.get("victory_type", "unknown")
                                if victory_type and victory_type != "unknown":
                                    stats[(board_type, num_players, victory_type)] += 1
                            except json.JSONDecodeError:
                                continue
                except (json.JSONDecodeError, AttributeError):
                    continue

        # Update cache
        setattr(self, cache_key, dict(stats))
        setattr(self, cache_time_key, now)

        return dict(stats)

    async def _get_game_analytics_cached(self) -> dict[str, Any]:
        """Get game analytics with caching (5 min TTL)."""
        import json
        from collections import defaultdict

        cache_key = "_game_analytics_cache"
        cache_time_key = "_game_analytics_cache_time"
        cache_ttl = 300

        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        # Skip JSONL scanning during startup grace period
        if self._is_in_startup_grace_period():
            return {"configs": {}}

        hours = 24
        cutoff = now - (hours * 3600)

        ai_root = Path(self.ringrift_path) / "ai-service"
        data_dirs = [
            ai_root / "data" / "games" / "daemon_sync",
            ai_root / "data" / "selfplay",
        ]

        game_lengths: dict[str, list[int]] = defaultdict(list)
        games_by_hour: dict[str, dict[int, int]] = defaultdict(lambda: defaultdict(int))
        opening_moves: dict[str, dict[str, int]] = defaultdict(lambda: defaultdict(int))

        for data_dir in data_dirs:
            if not data_dir.exists():
                continue
            for jsonl_path in data_dir.rglob("*.jsonl"):
                try:
                    if jsonl_path.stat().st_mtime < cutoff:
                        continue
                    with open_jsonl_file(jsonl_path) as f:
                        for line in f:
                            try:
                                game = json.loads(line)
                                board_type = game.get("board_type", "unknown")
                                num_players = game.get("num_players", 0)
                                config = f"{board_type}_{num_players}p"

                                length = game.get("length", 0)
                                if length > 0:
                                    game_lengths[config].append(length)

                                hour_bucket = int(jsonl_path.stat().st_mtime // 3600)
                                games_by_hour[config][hour_bucket] += 1

                                moves = game.get("moves", [])
                                if moves and len(moves) >= 1:
                                    first_move = str(moves[0].get("action", ""))[:20]
                                    if first_move:
                                        opening_moves[config][first_move] += 1
                            except json.JSONDecodeError:
                                continue
                except (json.JSONDecodeError, ValueError, AttributeError):
                    continue

        analytics = {"configs": {}}
        for config in set(list(game_lengths.keys()) + list(games_by_hour.keys())):
            lengths = game_lengths.get(config, [])
            hourly = games_by_hour.get(config, {})
            openings = opening_moves.get(config, {})
            throughput = sum(hourly.values()) / max(len(hourly), 1) if hourly else 0

            analytics["configs"][config] = {
                "avg_length": round(sum(lengths) / len(lengths), 1) if lengths else 0,
                "throughput_per_hour": round(throughput, 1),
                "opening_diversity": len(openings),
            }

        setattr(self, cache_key, analytics)
        setattr(self, cache_time_key, now)
        return analytics

    async def _get_training_metrics_cached(self) -> dict[str, Any]:
        """Get training metrics with caching (2 min TTL)."""
        import re

        cache_key = "_training_metrics_cache"
        cache_time_key = "_training_metrics_cache_time"
        cache_ttl = 120

        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        ai_root = Path(self.ringrift_path) / "ai-service"
        logs_dir = ai_root / "logs" / "training"

        metrics = {"configs": {}}

        if logs_dir.exists():
            log_files = sorted(logs_dir.glob("*.log"), key=lambda f: f.stat().st_mtime, reverse=True)[:10]

            for log_file in log_files:
                try:
                    content = log_file.read_text()
                    config_match = re.search(r"(square\d+|hexagonal|hex)_(\d+)p", log_file.name)
                    if not config_match:
                        continue
                    config = f"{config_match.group(1)}_{config_match.group(2)}p"

                    loss_pattern = re.compile(r"[Ee]poch\s+(\d+).*?loss[=:]\s*([\d.]+)")
                    epochs = []
                    for match in loss_pattern.finditer(content):
                        epochs.append({
                            "epoch": int(match.group(1)),
                            "loss": float(match.group(2)),
                        })

                    if epochs:
                        metrics["configs"][config] = {
                            "latest_loss": epochs[-1]["loss"],
                            "latest_epoch": epochs[-1]["epoch"],
                        }
                except (OSError, ValueError, KeyError, IndexError, AttributeError):
                    continue

        setattr(self, cache_key, metrics)
        setattr(self, cache_time_key, now)
        return metrics

    async def _get_holdout_metrics_cached(self) -> dict[str, Any]:
        """Get holdout validation metrics with caching (5 min TTL)."""
        import sqlite3

        cache_key = "_holdout_metrics_cache"
        cache_time_key = "_holdout_metrics_cache_time"
        cache_ttl = 300

        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        ai_root = Path(self.ringrift_path) / "ai-service"
        db_path = ai_root / "data" / "holdouts" / "holdout_validation.db"

        metrics = {"configs": {}, "evaluations": [], "summary": {}}

        if not db_path.exists():
            setattr(self, cache_key, metrics)
            setattr(self, cache_time_key, now)
            return metrics

        try:
            # Phase 3.4 Dec 29, 2025: Use context manager to prevent connection leaks
            with safe_db_connection(db_path) as conn:
                conn.row_factory = sqlite3.Row
                cursor = conn.cursor()

                # Get holdout game counts by config
                cursor.execute("""
                    SELECT board_type, num_players, COUNT(*) as game_count, SUM(num_positions) as total_positions
                    FROM holdout_games
                    GROUP BY board_type, num_players
                """)
                for row in cursor.fetchall():
                    config = f"{row['board_type']}_{row['num_players']}p"
                    metrics["configs"][config] = {
                        "holdout_games": row["game_count"],
                        "holdout_positions": row["total_positions"] or 0,
                    }

                # Get latest evaluations per config
                cursor.execute("""
                    SELECT model_path, board_type, num_players, holdout_loss, holdout_accuracy,
                           train_loss, num_samples, evaluated_at, overfit_gap
                    FROM evaluations
                    WHERE id IN (
                        SELECT MAX(id) FROM evaluations
                        GROUP BY board_type, num_players
                    )
                    ORDER BY evaluated_at DESC
                """)
                for row in cursor.fetchall():
                    config = f"{row['board_type']}_{row['num_players']}p"
                    eval_data = {
                        "config": config,
                        "model": row["model_path"],
                        "holdout_loss": row["holdout_loss"],
                        "holdout_accuracy": row["holdout_accuracy"],
                        "train_loss": row["train_loss"],
                        "overfit_gap": row["overfit_gap"],
                        "num_samples": row["num_samples"],
                        "evaluated_at": row["evaluated_at"],
                    }
                    metrics["evaluations"].append(eval_data)
                    # Update config metrics
                    if config in metrics["configs"]:
                        metrics["configs"][config].update({
                            "holdout_loss": row["holdout_loss"],
                            "holdout_accuracy": row["holdout_accuracy"],
                            "overfit_gap": row["overfit_gap"],
                        })

                # Get summary stats
                cursor.execute("SELECT COUNT(*) FROM holdout_games")
                metrics["summary"]["total_holdout_games"] = cursor.fetchone()[0]
                cursor.execute("SELECT COUNT(*) FROM evaluations")
                metrics["summary"]["total_evaluations"] = cursor.fetchone()[0]
        except (sqlite3.Error, OSError, KeyError, IndexError, TypeError):
            pass

        setattr(self, cache_key, metrics)
        setattr(self, cache_time_key, now)
        return metrics

    async def _get_mcts_stats_cached(self) -> dict[str, Any]:
        """Get MCTS search statistics with caching (2 min TTL)."""
        import json
        import re

        cache_key = "_mcts_stats_cache"
        cache_time_key = "_mcts_stats_cache_time"
        cache_ttl = 120

        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        # Skip JSONL scanning during startup grace period
        if self._is_in_startup_grace_period():
            return {"configs": {}, "summary": {}}

        ai_root = Path(self.ringrift_path) / "ai-service"
        stats = {"configs": {}, "summary": {}}

        # Parse selfplay logs for MCTS stats
        logs_dir = ai_root / "logs" / "selfplay"
        if logs_dir.exists():
            log_files = sorted(logs_dir.glob("*.log"), key=lambda f: f.stat().st_mtime, reverse=True)[:20]

            nodes_per_move = []
            depth_stats = []
            time_per_move = []

            for log_file in log_files:
                try:
                    content = log_file.read_text(errors='ignore')
                    # Parse MCTS stats patterns (nodes visited, search depth, time)
                    # Pattern: "nodes: 1234" or "nodes_visited: 1234"
                    for match in re.finditer(r'nodes[_\s]*(?:visited)?[:\s]*(\d+)', content, re.I):
                        nodes_per_move.append(int(match.group(1)))
                    # Pattern: "depth: 12" or "search_depth: 12"
                    for match in re.finditer(r'(?:search_)?depth[:\s]*(\d+)', content, re.I):
                        depth_stats.append(int(match.group(1)))
                    # Pattern: "time: 0.123s" or "move_time: 123ms"
                    for match in re.finditer(r'(?:move_)?time[:\s]*([\d.]+)\s*(?:s|ms)?', content, re.I):
                        time_per_move.append(float(match.group(1)))
                except (ValueError, KeyError, IndexError, AttributeError):
                    continue

            if nodes_per_move:
                stats["summary"]["avg_nodes_per_move"] = sum(nodes_per_move) / len(nodes_per_move)
                stats["summary"]["max_nodes_per_move"] = max(nodes_per_move)
            if depth_stats:
                stats["summary"]["avg_search_depth"] = sum(depth_stats) / len(depth_stats)
                stats["summary"]["max_search_depth"] = max(depth_stats)
            if time_per_move:
                stats["summary"]["avg_time_per_move"] = sum(time_per_move) / len(time_per_move)

        # Also check game JSONL files for MCTS metadata
        data_dirs = [
            ai_root / "data" / "games" / "daemon_sync",
            ai_root / "data" / "selfplay",
        ]
        cutoff = now - 3600  # Last hour

        for data_dir in data_dirs:
            if not data_dir.exists():
                continue
            for jsonl_path in data_dir.rglob("*.jsonl"):
                try:
                    if jsonl_path.stat().st_mtime < cutoff:
                        continue
                    with open_jsonl_file(jsonl_path) as f:
                        for line in f:
                            try:
                                game = json.loads(line)
                                board_type = game.get("board_type", "unknown")
                                num_players = game.get("num_players", 0)
                                config = f"{board_type}_{num_players}p"

                                # Check for MCTS metadata in game
                                mcts_data = game.get("mcts_stats", {})
                                if mcts_data:
                                    if config not in stats["configs"]:
                                        stats["configs"][config] = {
                                            "nodes_samples": [],
                                            "depth_samples": [],
                                        }
                                    if "avg_nodes" in mcts_data:
                                        stats["configs"][config]["nodes_samples"].append(mcts_data["avg_nodes"])
                                    if "avg_depth" in mcts_data:
                                        stats["configs"][config]["depth_samples"].append(mcts_data["avg_depth"])
                            except json.JSONDecodeError:
                                continue
                except (json.JSONDecodeError, AttributeError):
                    continue

        # Compute per-config averages
        for _config, data in stats["configs"].items():
            if data.get("nodes_samples"):
                data["avg_nodes"] = sum(data["nodes_samples"]) / len(data["nodes_samples"])
            if data.get("depth_samples"):
                data["avg_depth"] = sum(data["depth_samples"]) / len(data["depth_samples"])
            # Clean up sample lists
            data.pop("nodes_samples", None)
            data.pop("depth_samples", None)

        setattr(self, cache_key, stats)
        setattr(self, cache_time_key, now)
        return stats

    # =========================================================================
    # Feature 1: Tournament Matchup Analysis
    # =========================================================================

    async def _get_matchup_matrix_cached(self) -> dict[str, Any]:
        """Get head-to-head matchup statistics with caching (5 min TTL)."""
        import sqlite3
        from collections import defaultdict

        cache_key = "_matchup_matrix_cache"
        cache_time_key = "_matchup_matrix_cache_time"
        cache_ttl = 300

        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        ai_root = Path(self.ringrift_path) / "ai-service"
        db_path = ai_root / "data" / "unified_elo.db"

        matrix = {"matchups": [], "models": [], "configs": {}}

        if not db_path.exists():
            setattr(self, cache_key, matrix)
            setattr(self, cache_time_key, now)
            return matrix

        try:
            # Phase 3.4 Dec 29, 2025: Use context manager to prevent connection leaks
            with safe_db_connection(db_path) as conn:
                conn.row_factory = sqlite3.Row

                # Get all match history
                rows = conn.execute("""
                    SELECT participant_a, participant_b, winner, board_type, num_players,
                           game_length, duration_sec, timestamp
                    FROM match_history
                    WHERE timestamp > ?
                    ORDER BY timestamp DESC
                    LIMIT 10000
                """, (now - 86400 * 7,)).fetchall()  # Last 7 days

                # Build matchup stats
                h2h: dict[str, dict[str, dict[str, int]]] = defaultdict(lambda: defaultdict(lambda: {"wins": 0, "losses": 0, "draws": 0}))
                models = set()
                config_stats = defaultdict(lambda: {"total_matches": 0, "avg_game_length": [], "avg_duration": []})

                for row in rows:
                    a = row["participant_a"]
                    b = row["participant_b"]
                    winner = row["winner"]
                    config = f"{row['board_type']}_{row['num_players']}p"

                    if a and b:
                        models.add(a)
                        models.add(b)

                        if winner == a:
                            h2h[a][b]["wins"] += 1
                            h2h[b][a]["losses"] += 1
                        elif winner == b:
                            h2h[b][a]["wins"] += 1
                            h2h[a][b]["losses"] += 1
                        else:
                            h2h[a][b]["draws"] += 1
                            h2h[b][a]["draws"] += 1

                        config_stats[config]["total_matches"] += 1
                        if row["game_length"]:
                            config_stats[config]["avg_game_length"].append(row["game_length"])
                        if row["duration_sec"]:
                            config_stats[config]["avg_duration"].append(row["duration_sec"])

                # Convert to matchup list
                matchups = []
                for model_a in sorted(models):
                    for model_b in sorted(models):
                        if model_a < model_b:  # Avoid duplicates
                            stats = h2h[model_a][model_b]
                            total = stats["wins"] + stats["losses"] + stats["draws"]
                            if total > 0:
                                matchups.append({
                                    "model_a": model_a,
                                    "model_b": model_b,
                                    "a_wins": stats["wins"],
                                    "b_wins": stats["losses"],
                                    "draws": stats["draws"],
                                    "total": total,
                                    "a_win_rate": round(stats["wins"] / total, 3) if total > 0 else 0,
                                })

                # Compute config averages
                for _config, data in config_stats.items():
                    if data["avg_game_length"]:
                        data["avg_game_length"] = round(sum(data["avg_game_length"]) / len(data["avg_game_length"]), 1)
                    else:
                        data["avg_game_length"] = 0
                    if data["avg_duration"]:
                        data["avg_duration"] = round(sum(data["avg_duration"]) / len(data["avg_duration"]), 2)
                    else:
                        data["avg_duration"] = 0

                matrix["matchups"] = matchups
                matrix["models"] = sorted(models)
                matrix["configs"] = dict(config_stats)
                matrix["total_matches"] = sum(c["total_matches"] for c in config_stats.values())
        except (sqlite3.Error, OSError, KeyError, ValueError, TypeError):
            pass

        setattr(self, cache_key, matrix)
        setattr(self, cache_time_key, now)
        return matrix

    # =========================================================================
    # Feature 2: Model Lineage Tracking
    # =========================================================================

    async def _get_model_lineage_cached(self) -> dict[str, Any]:
        """Get model lineage and ancestry with caching (10 min TTL)."""
        import re

        cache_key = "_model_lineage_cache"
        cache_time_key = "_model_lineage_cache_time"
        cache_ttl = 600

        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        ai_root = Path(self.ringrift_path) / "ai-service"
        models_dir = ai_root / "models"

        lineage = {"models": [], "generations": {}, "configs": {}}

        if not models_dir.exists():
            setattr(self, cache_key, lineage)
            setattr(self, cache_time_key, now)
            return lineage

        try:
            # Discover all models
            model_files = list(models_dir.glob("**/*.pt")) + list(models_dir.glob("**/*.pth"))

            for model_path in model_files:
                model_name = model_path.stem
                model_stat = model_path.stat()

                # Parse model name for lineage info
                # Common patterns:
                #   - square8_2p_v5_gen12, nnue_square8_2p_epoch50
                #   - ringrift_best_sq8_2p, ringrift_best_sq19_2p
                #   - hex_3p_nn_baseline, ringrift_best_hex_2p
                # Handle both full names (square8, hexagonal) and abbreviations (sq8, hex)
                config_match = re.search(
                    r"(square\d+|sq\d+|hexagonal|hex)[\W_]*(\d+)p",
                    model_name,
                    re.I
                )
                gen_match = re.search(r"gen(\d+)|v(\d+)|epoch(\d+)", model_name, re.I)

                if config_match:
                    board = config_match.group(1).lower()
                    players = config_match.group(2)
                    # Normalize board names (only transform abbreviations, not full names)
                    if board.startswith("sq") and not board.startswith("square"):
                        # sq8 -> square8, sq19 -> square19
                        board = f"square{board[2:]}"
                    elif board == "hex":
                        board = "hexagonal"
                    config = f"{board}_{players}p"
                else:
                    config = "unknown"
                generation = int(gen_match.group(1) or gen_match.group(2) or gen_match.group(3) or 0) if gen_match else 0

                model_info = {
                    "name": model_name,
                    "path": str(model_path.relative_to(ai_root)),
                    "config": config,
                    "generation": generation,
                    "size_mb": round(model_stat.st_size / 1024 / 1024, 2),
                    "created_at": model_stat.st_mtime,
                    "age_hours": round((now - model_stat.st_mtime) / 3600, 1),
                }
                lineage["models"].append(model_info)

                # Track generations per config
                if config not in lineage["generations"]:
                    lineage["generations"][config] = []
                lineage["generations"][config].append(model_info)

            # Sort models by generation within each config
            for config in lineage["generations"]:
                lineage["generations"][config].sort(key=lambda m: m["generation"])

            # Summary per config
            for config, models in lineage["generations"].items():
                lineage["configs"][config] = {
                    "total_models": len(models),
                    "latest_generation": max(m["generation"] for m in models) if models else 0,
                    "latest_model": models[-1]["name"] if models else None,
                    "total_size_mb": round(sum(m["size_mb"] for m in models), 1),
                }

            lineage["total_models"] = len(lineage["models"])

        except (sqlite3.Error, OSError, KeyError, ValueError, TypeError):
            pass

        setattr(self, cache_key, lineage)
        setattr(self, cache_time_key, now)
        return lineage

    # =========================================================================
    # Feature 3: Data Quality Metrics
    # =========================================================================

    async def _get_data_quality_cached(self) -> dict[str, Any]:
        """Get data quality metrics with caching (5 min TTL)."""
        import json
        from collections import defaultdict

        cache_key = "_data_quality_cache"
        cache_time_key = "_data_quality_cache_time"
        cache_ttl = 300

        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        # Skip JSONL scanning during startup grace period
        if self._is_in_startup_grace_period():
            return {"configs": {}, "issues": [], "summary": {}}

        ai_root = Path(self.ringrift_path) / "ai-service"
        quality = {"configs": {}, "issues": [], "summary": {}}

        data_dirs = [
            ai_root / "data" / "games" / "daemon_sync",
            ai_root / "data" / "selfplay",
        ]
        cutoff = now - 86400  # Last 24 hours

        try:
            config_stats = defaultdict(lambda: {
                "total_games": 0,
                "game_lengths": [],
                "short_games": 0,  # < 10 moves
                "long_games": 0,   # > 500 moves
                "stalemates": 0,
                "unique_openings": set(),
                "player_wins": defaultdict(int),
                "parse_errors": 0,
            })

            for data_dir in data_dirs:
                if not data_dir.exists():
                    continue
                for jsonl_path in data_dir.rglob("*.jsonl"):
                    try:
                        if jsonl_path.stat().st_mtime < cutoff:
                            continue
                        with open_jsonl_file(jsonl_path) as f:
                            for line in f:
                                try:
                                    game = json.loads(line)
                                    board_type = game.get("board_type", "unknown")
                                    num_players = game.get("num_players", 0)
                                    config = f"{board_type}_{num_players}p"

                                    stats = config_stats[config]
                                    stats["total_games"] += 1

                                    length = game.get("length", 0)
                                    if length > 0:
                                        stats["game_lengths"].append(length)
                                        if length < 10:
                                            stats["short_games"] += 1
                                        elif length > 500:
                                            stats["long_games"] += 1

                                    victory_type = game.get("victory_type", "")
                                    if victory_type == "stalemate":
                                        stats["stalemates"] += 1

                                    # Track opening diversity
                                    moves = game.get("moves", [])
                                    if moves and len(moves) >= 2:
                                        opening = str(moves[0].get("action", ""))[:15] + "-" + str(moves[1].get("action", ""))[:15]
                                        stats["unique_openings"].add(opening)

                                    # Track winner distribution
                                    winner = game.get("winner")
                                    if winner is not None:
                                        stats["player_wins"][winner] += 1

                                except json.JSONDecodeError:
                                    config_stats["unknown"]["parse_errors"] += 1
                    except (OSError, ValueError, KeyError):
                        continue

            # Convert to quality metrics
            issues = []
            for config, stats in config_stats.items():
                total = stats["total_games"]
                if total == 0:
                    continue

                lengths = stats["game_lengths"]
                avg_length = sum(lengths) / len(lengths) if lengths else 0
                length_std = (sum((length - avg_length) ** 2 for length in lengths) / len(lengths)) ** 0.5 if len(lengths) > 1 else 0

                short_rate = stats["short_games"] / total
                long_rate = stats["long_games"] / total
                stalemate_rate = stats["stalemates"] / total
                opening_diversity = len(stats["unique_openings"])

                # Detect issues
                if short_rate > 0.1:
                    issues.append({"config": config, "issue": "high_short_game_rate", "value": round(short_rate * 100, 1), "severity": "warning"})
                if stalemate_rate > 0.3:
                    issues.append({"config": config, "issue": "high_stalemate_rate", "value": round(stalemate_rate * 100, 1), "severity": "warning"})
                if opening_diversity < 5 and total > 50:
                    issues.append({"config": config, "issue": "low_opening_diversity", "value": opening_diversity, "severity": "warning"})

                # Check for player bias
                wins = stats["player_wins"]
                if len(wins) >= 2 and total > 20:
                    max_win_rate = max(wins.values()) / total
                    if max_win_rate > 0.7:
                        issues.append({"config": config, "issue": "player_bias", "value": round(max_win_rate * 100, 1), "severity": "info"})

                quality["configs"][config] = {
                    "total_games": total,
                    "avg_length": round(avg_length, 1),
                    "length_std": round(length_std, 1),
                    "short_game_rate": round(short_rate * 100, 1),
                    "long_game_rate": round(long_rate * 100, 1),
                    "stalemate_rate": round(stalemate_rate * 100, 1),
                    "opening_diversity": opening_diversity,
                    "parse_errors": stats["parse_errors"],
                }

            quality["issues"] = issues
            quality["summary"] = {
                "total_configs": len(quality["configs"]),
                "total_issues": len(issues),
                "critical_issues": len([i for i in issues if i["severity"] == "critical"]),
                "warning_issues": len([i for i in issues if i["severity"] == "warning"]),
            }

        except (OSError, ValueError, KeyError, TypeError):
            pass

        setattr(self, cache_key, quality)
        setattr(self, cache_time_key, now)
        return quality

    # =========================================================================
    # Feature 4: Training Efficiency Dashboard
    # =========================================================================

    async def _get_training_efficiency_cached(self) -> dict[str, Any]:
        """Get training efficiency metrics with caching (5 min TTL)."""
        import re
        import sqlite3

        cache_key = "_training_efficiency_cache"
        cache_time_key = "_training_efficiency_cache_time"
        cache_ttl = 300

        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        ai_root = Path(self.ringrift_path) / "ai-service"
        efficiency = {"configs": {}, "summary": {}, "cost_tracking": {}}

        try:
            # Get Elo history to track improvements
            db_path = ai_root / "data" / "unified_elo.db"
            elo_history = {}

            if db_path.exists():
                # Phase 3.4 Dec 29, 2025: Use context manager to prevent connection leaks
                with safe_db_connection(db_path) as conn:
                    rows = conn.execute("""
                        SELECT board_type, num_players, participant_id, rating, timestamp
                        FROM rating_history
                        WHERE timestamp > ?
                        ORDER BY timestamp ASC
                    """, (now - 86400 * 7,)).fetchall()  # Last 7 days

                    for row in rows:
                        config = f"{row[0]}_{row[1]}p"
                        if config not in elo_history:
                            elo_history[config] = {"ratings": [], "timestamps": []}
                        elo_history[config]["ratings"].append(row[3])
                        elo_history[config]["timestamps"].append(row[4])

            # Parse training logs for GPU hours
            logs_dir = ai_root / "logs" / "training"
            gpu_hours_per_config = {}

            if logs_dir.exists():
                for log_file in logs_dir.glob("*.log"):
                    try:
                        content = log_file.read_text(errors='ignore')
                        config_match = re.search(r"(square\d+|hex\w*)_(\d+)p", log_file.name)
                        if not config_match:
                            continue
                        config = f"{config_match.group(1)}_{config_match.group(2)}p"

                        # Extract training duration
                        duration_match = re.search(r"(?:total[_\s]?time|duration)[:\s]*([\d.]+)\s*(?:s|sec|min|h)", content, re.I)
                        if duration_match:
                            duration = float(duration_match.group(1))
                            # Assume hours if > 100, else assume minutes
                            if duration > 100:
                                duration = duration / 3600  # seconds to hours
                            elif duration < 24:
                                duration = duration / 60  # minutes to hours

                            if config not in gpu_hours_per_config:
                                gpu_hours_per_config[config] = 0
                            gpu_hours_per_config[config] += duration
                    except (ValueError, KeyError, IndexError, AttributeError):
                        continue

            # Calculate efficiency metrics per config
            for config in set(list(elo_history.keys()) + list(gpu_hours_per_config.keys())):
                elo_data = elo_history.get(config, {"ratings": [], "timestamps": []})
                gpu_hours = gpu_hours_per_config.get(config, 0)

                if elo_data["ratings"]:
                    initial_elo = elo_data["ratings"][0] if elo_data["ratings"] else INITIAL_ELO_RATING
                    current_elo = elo_data["ratings"][-1] if elo_data["ratings"] else INITIAL_ELO_RATING
                    elo_gain = current_elo - initial_elo
                else:
                    initial_elo = current_elo = INITIAL_ELO_RATING
                    elo_gain = 0

                # Elo per GPU hour
                elo_per_hour = elo_gain / gpu_hours if gpu_hours > 0 else 0

                # Estimated cost (assuming $2/GPU-hour average)
                estimated_cost = gpu_hours * 2.0

                efficiency["configs"][config] = {
                    "gpu_hours": round(gpu_hours, 2),
                    "initial_elo": round(initial_elo, 1),
                    "current_elo": round(current_elo, 1),
                    "elo_gain": round(elo_gain, 1),
                    "elo_per_gpu_hour": round(elo_per_hour, 2),
                    "estimated_cost_usd": round(estimated_cost, 2),
                    "cost_per_elo_point": round(estimated_cost / max(elo_gain, 1), 2) if elo_gain > 0 else None,
                }

            # Summary
            total_gpu_hours = sum(c.get("gpu_hours", 0) for c in efficiency["configs"].values())
            total_elo_gain = sum(c.get("elo_gain", 0) for c in efficiency["configs"].values())
            total_cost = sum(c.get("estimated_cost_usd", 0) for c in efficiency["configs"].values())

            efficiency["summary"] = {
                "total_gpu_hours": round(total_gpu_hours, 2),
                "total_elo_gain": round(total_elo_gain, 1),
                "total_estimated_cost_usd": round(total_cost, 2),
                "overall_elo_per_gpu_hour": round(total_elo_gain / max(total_gpu_hours, 1), 2),
            }

        except (sqlite3.Error, OSError, ValueError, KeyError, TypeError):
            pass

        setattr(self, cache_key, efficiency)
        setattr(self, cache_time_key, now)
        return efficiency

    # =========================================================================
    # Feature 5: Automated Model Rollback
    # =========================================================================

    async def _check_rollback_conditions(self) -> dict[str, Any]:
        """Check if any models should be rolled back based on metrics."""
        rollback_status = {"candidates": [], "recent_rollbacks": [], "config_status": {}}

        try:
            # Get holdout metrics for overfitting detection
            holdout = await self._get_holdout_metrics_cached()

            # Get Elo data for regression detection
            ai_root = Path(self.ringrift_path) / "ai-service"
            db_path = ai_root / "data" / "unified_elo.db"

            elo_data = {}
            if db_path.exists():
                import sqlite3
                # Phase 3.4 Dec 29, 2025: Use context manager to prevent connection leaks
                with safe_db_connection(db_path) as conn:
                    rows = conn.execute("""
                        SELECT board_type, num_players, participant_id, rating, timestamp
                        FROM rating_history
                        ORDER BY timestamp DESC
                        LIMIT 1000
                    """).fetchall()

                    for row in rows:
                        config = f"{row[0]}_{row[1]}p"
                        if config not in elo_data:
                            elo_data[config] = []
                        elo_data[config].append({"model": row[2], "rating": row[3], "timestamp": row[4]})

            # Check each config for rollback conditions
            for config, holdout_data in holdout.get("configs", {}).items():
                status = {"config": config, "rollback_recommended": False, "reasons": []}

                # Check 1: Overfitting (overfit_gap > 0.15)
                overfit_gap = holdout_data.get("overfit_gap", 0)
                if overfit_gap and overfit_gap > 0.15:
                    status["rollback_recommended"] = True
                    status["reasons"].append(f"Overfitting detected: gap={overfit_gap:.3f}")

                # Check 2: Low holdout accuracy (< 60%)
                holdout_acc = holdout_data.get("holdout_accuracy", 1.0)
                if holdout_acc and holdout_acc < 0.6:
                    status["rollback_recommended"] = True
                    status["reasons"].append(f"Low holdout accuracy: {holdout_acc*100:.1f}%")

                # Check 3: Elo regression (dropped > 50 points recently)
                if config in elo_data and len(elo_data[config]) >= 2:
                    recent = elo_data[config][0]["rating"]
                    previous = max(e["rating"] for e in elo_data[config][:10])
                    if previous - recent > 50:
                        status["rollback_recommended"] = True
                        status["reasons"].append(f"Elo regression: {previous:.0f} -> {recent:.0f}")

                rollback_status["config_status"][config] = status
                if status["rollback_recommended"]:
                    rollback_status["candidates"].append(status)

            # Load recent rollback history if exists
            rollback_log = ai_root / "logs" / "rollbacks.json"
            if rollback_log.exists():
                import json
                with contextlib.suppress(json.JSONDecodeError, OSError, KeyError, IndexError):
                    rollback_status["recent_rollbacks"] = json.loads(rollback_log.read_text())[-10:]

        except (sqlite3.Error, OSError, ValueError, KeyError, TypeError):
            pass

        return rollback_status

    async def _execute_rollback(self, config: str, dry_run: bool = False) -> dict[str, Any]:
        """Execute a rollback for the given config by restoring previous model.

        Args:
            config: Config string like "square8_2p"
            dry_run: If True, only simulate the rollback without making changes

        Returns:
            Dict with rollback results (success, message, details)
        """
        import json
        import shutil

        result = {
            "success": False,
            "config": config,
            "dry_run": dry_run,
            "message": "",
            "details": {},
        }

        try:
            ai_root = Path(self.ringrift_path) / "ai-service"
            models_dir = ai_root / "models"
            archive_dir = models_dir / "archive"
            archive_dir.mkdir(parents=True, exist_ok=True)

            # Parse config to get board type and player count
            parts = config.rsplit("_", 1)
            if len(parts) != 2 or not parts[1].endswith("p"):
                result["message"] = f"Invalid config format: {config}"
                return result

            board = parts[0]
            players = parts[1][:-1]

            # Find the current best model alias
            # Common patterns: ringrift_best_sq8_2p, ringrift_best_square8_2p
            board_abbrev = board.replace("square", "sq").replace("hexagonal", "hex")
            best_patterns = [
                f"ringrift_best_{board_abbrev}_{players}p.pth",
                f"ringrift_best_{board}_{players}p.pth",
            ]

            current_best = None
            for pattern in best_patterns:
                candidate = models_dir / pattern
                if candidate.exists():
                    current_best = candidate
                    break

            if not current_best:
                result["message"] = f"No best model found for {config}"
                return result

            # Find previous checkpoints for this config
            checkpoint_dir = models_dir / "checkpoints"
            checkpoints = []
            if checkpoint_dir.exists():
                for ckpt in checkpoint_dir.glob(f"*{board_abbrev}*{players}p*.pth"):
                    try:
                        stat = ckpt.stat()
                        checkpoints.append({
                            "path": ckpt,
                            "mtime": stat.st_mtime,
                            "name": ckpt.name,
                        })
                    except (AttributeError):
                        continue

            # Also check archive for previous best models
            for archived in archive_dir.glob(f"*{board_abbrev}*{players}p*.pth"):
                try:
                    stat = archived.stat()
                    checkpoints.append({
                        "path": archived,
                        "mtime": stat.st_mtime,
                        "name": archived.name,
                    })
                except (AttributeError):
                    continue

            # Sort by modification time descending
            checkpoints.sort(key=lambda x: x["mtime"], reverse=True)

            # Filter out the current best model
            current_mtime = current_best.stat().st_mtime
            previous_checkpoints = [c for c in checkpoints if abs(c["mtime"] - current_mtime) > 60]

            if not previous_checkpoints:
                result["message"] = f"No previous checkpoints found for rollback of {config}"
                return result

            # Select the most recent previous checkpoint
            rollback_source = previous_checkpoints[0]

            result["details"] = {
                "current_model": current_best.name,
                "rollback_to": rollback_source["name"],
                "rollback_age_hours": round((time.time() - rollback_source["mtime"]) / 3600, 1),
                "available_checkpoints": len(previous_checkpoints),
            }

            if dry_run:
                result["success"] = True
                result["message"] = f"Dry run: Would rollback {current_best.name} to {rollback_source['name']}"
                return result

            # Archive the current model
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            archived_name = f"{current_best.stem}_archived_{timestamp}.pth"
            shutil.copy2(current_best, archive_dir / archived_name)

            # Restore the previous checkpoint
            shutil.copy2(rollback_source["path"], current_best)

            # Log the rollback
            rollback_log = ai_root / "logs" / "rollbacks.json"
            rollback_log.parent.mkdir(parents=True, exist_ok=True)

            rollback_entry = {
                "timestamp": time.strftime("%Y-%m-%dT%H:%M:%S"),
                "config": config,
                "previous_model": current_best.name,
                "rolled_back_to": rollback_source["name"],
                "archived_as": archived_name,
            }

            try:
                existing = json.loads(rollback_log.read_text()) if rollback_log.exists() else []
            except (json.JSONDecodeError, OSError, KeyError, IndexError, AttributeError):
                existing = []

            existing.append(rollback_entry)
            rollback_log.write_text(json.dumps(existing[-100:], indent=2))  # Keep last 100 rollbacks

            result["success"] = True
            result["message"] = f"Successfully rolled back {config} from {current_best.name} to {rollback_source['name']}"

            # Increment rollback counter
            self.diversity_metrics["rollbacks"] += 1

            # Send alert notification
            asyncio.create_task(self.notifier.send(
                title="Model Rollback Executed",
                message=f"Rolled back {config} from {current_best.name} to {rollback_source['name']}",
                level="warning",
                fields={
                    "Config": config,
                    "Previous": current_best.name,
                    "Restored": rollback_source["name"],
                    "Age": f"{result['details']['rollback_age_hours']:.1f}h",
                },
                node_id=self.node_id,
            ))

        except Exception as e:  # noqa: BLE001
            result["message"] = f"Rollback failed: {e!s}"

        return result

    async def _auto_rollback_check(self) -> list[dict[str, Any]]:
        """Automatically check and execute rollbacks for critical candidates.

        Returns list of executed rollbacks.
        """
        # Check if auto-rollback is enabled
        if os.environ.get("RINGRIFT_AUTO_ROLLBACK", "").lower() not in ("1", "true", "yes"):
            return []

        executed = []
        try:
            status = await self._check_rollback_conditions()
            for candidate in status.get("candidates", []):
                # Only auto-rollback if multiple serious conditions are met
                reasons = candidate.get("reasons", [])
                if len(reasons) >= 2 or any("Overfitting" in r for r in reasons):
                    config = candidate["config"]
                    result = await self._execute_rollback(config, dry_run=False)
                    executed.append(result)
                    if result["success"]:
                        logger.warning(f"[AUTO-ROLLBACK] Executed for {config}: {reasons}")
        except Exception as e:  # noqa: BLE001
            logger.error(f"[AUTO-ROLLBACK] Error: {e}")

        return executed

    # =========================================================================
    # Feature 6: Distributed Selfplay Autoscaling
    # =========================================================================

    async def _get_autoscaling_metrics(self) -> dict[str, Any]:
        """Get metrics for autoscaling decisions."""
        # Autoscaling thresholds tuned for 46-node cluster
        # These can be overridden via environment variables
        max_workers = int(os.environ.get("RINGRIFT_AUTOSCALE_MAX_WORKERS", "46"))
        min_workers = int(os.environ.get("RINGRIFT_AUTOSCALE_MIN_WORKERS", "2"))
        scale_up_threshold = int(os.environ.get("RINGRIFT_AUTOSCALE_SCALE_UP_GPH", "100"))
        scale_down_threshold = int(os.environ.get("RINGRIFT_AUTOSCALE_SCALE_DOWN_GPH", "500"))
        target_freshness = float(os.environ.get("RINGRIFT_AUTOSCALE_TARGET_FRESHNESS_HOURS", "2"))

        autoscale = {
            "current_state": {},
            "recommendations": [],
            "thresholds": {
                "scale_up_games_per_hour": scale_up_threshold,  # Scale up if below this
                "scale_down_games_per_hour": scale_down_threshold,  # Scale down if above this
                "max_workers": max_workers,
                "min_workers": min_workers,
                "target_data_freshness_hours": target_freshness,
            },
        }

        try:
            # Get current worker count
            with self.peers_lock:
                total_nodes = len(self.peers) + 1
                gpu_nodes = len([p for p in self.peers.values() if getattr(p, "has_gpu", False)])
                if self.self_info.has_gpu:
                    gpu_nodes += 1

            with self.jobs_lock:
                active_selfplay = len([j for j in self.local_jobs.values()
                                      if j.job_type in (JobType.SELFPLAY, JobType.GPU_SELFPLAY, JobType.HYBRID_SELFPLAY, JobType.CPU_SELFPLAY, JobType.GUMBEL_SELFPLAY)
                                      and j.status == "running"])

            autoscale["current_state"] = {
                "total_nodes": total_nodes,
                "gpu_nodes": gpu_nodes,
                "active_selfplay_jobs": active_selfplay,
            }

            # Get game generation throughput
            analytics = await self._get_game_analytics_cached()
            total_throughput = sum(c.get("throughput_per_hour", 0) for c in analytics.get("configs", {}).values())

            autoscale["current_state"]["games_per_hour"] = round(total_throughput, 1)

            # Get data freshness
            now = time.time()
            ai_root = Path(self.ringrift_path) / "ai-service"
            selfplay_dir = ai_root / "data" / "selfplay"

            freshest_data = 0
            if selfplay_dir.exists():
                for jsonl in selfplay_dir.rglob("*.jsonl"):
                    try:
                        mtime = jsonl.stat().st_mtime
                        if mtime > freshest_data:
                            freshest_data = mtime
                    except (AttributeError):
                        continue

            data_age_hours = (now - freshest_data) / 3600 if freshest_data > 0 else 999
            autoscale["current_state"]["data_freshness_hours"] = round(data_age_hours, 2)

            # Generate recommendations
            thresholds = autoscale["thresholds"]

            if total_throughput < thresholds["scale_up_games_per_hour"] and total_nodes < thresholds["max_workers"]:
                autoscale["recommendations"].append({
                    "action": "scale_up",
                    "reason": f"Low throughput ({total_throughput:.0f} games/h < {thresholds['scale_up_games_per_hour']})",
                    "suggested_workers": min(total_nodes + 2, thresholds["max_workers"]),
                })

            if total_throughput > thresholds["scale_down_games_per_hour"] and total_nodes > thresholds["min_workers"]:
                autoscale["recommendations"].append({
                    "action": "scale_down",
                    "reason": f"High throughput ({total_throughput:.0f} games/h > {thresholds['scale_down_games_per_hour']})",
                    "suggested_workers": max(total_nodes - 1, thresholds["min_workers"]),
                })

            if data_age_hours > thresholds["target_data_freshness_hours"]:
                autoscale["recommendations"].append({
                    "action": "scale_up",
                    "reason": f"Stale data ({data_age_hours:.1f}h > {thresholds['target_data_freshness_hours']}h)",
                    "suggested_workers": min(total_nodes + 1, thresholds["max_workers"]),
                })

            # Cost optimization recommendation
            efficiency = await self._get_training_efficiency_cached()
            elo_per_hour = efficiency.get("summary", {}).get("overall_elo_per_gpu_hour", 0)
            if elo_per_hour < 1 and total_nodes > 2:
                autoscale["recommendations"].append({
                    "action": "optimize",
                    "reason": f"Low efficiency ({elo_per_hour:.2f} Elo/GPU-h) - consider reducing workers",
                    "suggested_workers": max(total_nodes - 1, thresholds["min_workers"]),
                })

        except (AttributeError, KeyError, ValueError, TypeError):
            pass

        return autoscale

    # handle_victory_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    async def handle_elo_history(self, request: web.Request) -> web.Response:
        """GET /elo/history - Historical Elo ratings for time series visualization.

        Query params:
            - config: Filter by config (e.g., square8_2p)
            - model: Filter by model/participant_id (supports partial match)
            - nn_only: If "true", filter to NN models only
            - hours: Hours of history (default 168 = 1 week)
            - limit: Max entries to return (default 5000)
        """
        import sqlite3

        try:
            config_filter = request.query.get("config")
            model_filter = request.query.get("model")
            nn_only = request.query.get("nn_only", "").lower() == "true"
            hours = int(request.query.get("hours", "168"))
            limit = int(request.query.get("limit", "5000"))

            ai_root = Path(self.ringrift_path) / "ai-service"

            # Canonical Elo database for trained models
            db_paths = [
                ai_root / "data" / "unified_elo.db",
            ]

            data = []
            cutoff = time.time() - (hours * 3600)

            for db_path in db_paths:
                if not db_path.exists():
                    continue

                try:
                    # Phase 3.4 Dec 29, 2025: Use context manager to prevent connection leaks
                    with safe_db_connection(db_path) as conn:
                        cursor = conn.cursor()

                        # Check if this DB has data
                        cursor.execute("SELECT COUNT(*) FROM rating_history WHERE timestamp > ?", (cutoff,))
                        count = cursor.fetchone()[0]
                        if count == 0:
                            continue

                        # Build query - unified_elo.db has different schema (no board_type/num_players)
                        cursor.execute("PRAGMA table_info(rating_history)")
                        columns = {col[1] for col in cursor.fetchall()}

                        if "board_type" in columns:
                            # unified_elo.db schema
                            query = """
                                SELECT participant_id, board_type, num_players, rating, games_played, timestamp
                                FROM rating_history
                                WHERE timestamp > ?
                            """
                            params = [cutoff]

                            if config_filter:
                                parts = config_filter.replace("_", " ").split()
                                if len(parts) >= 2:
                                    board_type = parts[0]
                                    num_players = int(parts[1].replace("p", ""))
                                    query += " AND board_type = ? AND num_players = ?"
                                    params.extend([board_type, num_players])
                        else:
                            # unified_elo.db schema (model_id instead of participant_id)
                            query = """
                                SELECT model_id, rating, games_played, timestamp
                                FROM rating_history
                                WHERE timestamp > ?
                            """
                            params = [cutoff]

                        if model_filter:
                            col = "participant_id" if "participant_id" in columns else "model_id"
                            query += f" AND {col} LIKE ?"
                            params.append(f"%{model_filter}%")

                        if nn_only:
                            col = "participant_id" if "participant_id" in columns else "model_id"
                            query += f" AND ({col} LIKE '%nn%' OR {col} LIKE '%NN%')"

                        query += f" ORDER BY timestamp DESC LIMIT {limit}"

                        cursor.execute(query, params)
                        rows = cursor.fetchall()

                    # Format for Grafana time series
                    for row in rows:
                        if "board_type" in columns:
                            participant_id, board_type, num_players, rating, games_played, ts = row
                            config = f"{board_type}_{num_players}p"
                        else:
                            model_id, rating, games_played, ts = row
                            participant_id = model_id
                            # Extract config from model name (e.g., sq8_2p_nn_baseline -> square8_2p)
                            if "sq8" in model_id.lower() or "square8" in model_id.lower():
                                config = "square8_2p"
                            elif "sq19" in model_id.lower() or "square19" in model_id.lower():
                                config = "square19_2p"
                            else:
                                config = "unknown"

                        data.append({
                            "time": int(ts * 1000),  # Grafana expects ms
                            "model": participant_id,
                            "config": config,
                            "elo": round(rating, 1),
                            "games": games_played,
                        })

                    # If we got data from this DB, don't check others
                    if data:
                        break

                except sqlite3.Error:
                    continue

            # Sort by time ascending for time series
            data.sort(key=lambda x: x["time"])

            return web.json_response(data)

        except Exception as e:  # noqa: BLE001
            return web.json_response([{"error": str(e)}])

    # Elo Sync Handlers moved to scripts/p2p/handlers/elo_sync.py
    # Inherited from EloSyncHandlersMixin:
    # - handle_elo_sync_status, handle_elo_sync_trigger
    # - handle_elo_sync_download, handle_elo_sync_upload
    # - _trigger_elo_sync_after_matches

    # NOTE: _elo_sync_loop() removed Dec 2025 (29 LOC).
    # Now runs via LoopManager as EloSyncLoop.
    # See scripts/p2p/loops/elo_sync_loop.py for implementation.

    # NOTE: _worker_pull_loop() removed Dec 2025 (85 LOC).
    # Now runs via LoopManager as WorkerPullLoop.
    # See scripts/p2p/loops/job_loops.py for implementation.
    # Helper methods _claim_work_from_leader, _execute_claimed_work, _report_work_result
    # are retained and passed as callbacks to WorkerPullLoop.

    async def _claim_work_from_leader(self, capabilities: list[str]) -> dict[str, Any] | None:
        """Claim work from the leader's work queue."""
        if not self.leader_id or self.leader_id == self.node_id:
            return None

        # Find leader peer
        with self.peers_lock:
            leader_peer = self.peers.get(self.leader_id)

        if not leader_peer:
            return None

        try:
            timeout = ClientTimeout(total=15)
            async with get_client_session(timeout) as session:
                caps_str = ",".join(capabilities)
                url = self._url_for_peer(leader_peer, f"/work/claim?node_id={self.node_id}&capabilities={caps_str}")
                async with session.get(url, headers=self._auth_headers()) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        if data.get("status") == "claimed":
                            return data.get("work")
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Failed to claim work from leader: {e}")

        return None

    async def _execute_claimed_work(self, work_item: dict[str, Any]) -> bool:
        """Execute a claimed work item locally."""
        work_type = work_item.get("work_type", "")
        config = work_item.get("config", {})
        work_id = work_item.get("work_id", "")

        try:
            if work_type == "training":
                # Start training job
                board_type = config.get("board_type", "square8")
                num_players = config.get("num_players", 2)
                # Queue it for local training
                f"pull-{work_id}-{int(time.time())}"
                logger.info(f"Executing training work: {board_type}/{num_players}p")
                # Simplified: trigger training via existing mechanisms
                return True

            elif work_type == "selfplay":
                # Start selfplay job
                board_type = config.get("board_type", "square8")
                num_players = config.get("num_players", 2)
                num_games = config.get("num_games", 500)
                engine_mode = config.get("engine_mode", "mixed")
                engine_extra_args = config.get("engine_extra_args")  # December 2025: for budget override

                # Delegate to JobManager (Phase 2B refactoring, Dec 2025)
                asyncio.create_task(self.job_manager.run_gpu_selfplay_job(
                    job_id=f"pull-{work_id}",
                    board_type=board_type,
                    num_players=num_players,
                    num_games=num_games,
                    engine_mode=engine_mode,
                    engine_extra_args=engine_extra_args,
                ))

                # Track diversity metrics for monitoring (Phase 2B, Dec 2025)
                self.selfplay_scheduler.track_diversity({
                    "board_type": board_type,
                    "num_players": num_players,
                    "engine_mode": engine_mode,
                })
                return True

            elif work_type == "gpu_cmaes":
                # Start CMA-ES optimization
                logger.info(f"Executing GPU CMA-ES work: {config}")
                return True

            elif work_type == "tournament":
                # Start tournament
                logger.info(f"Executing tournament work: {config}")
                return True

            else:
                logger.warning(f"Unknown work type: {work_type}")
                return False

        except Exception as e:  # noqa: BLE001
            logger.error(f"Error executing work {work_id}: {e}")
            return False

    async def _report_work_result(self, work_item: dict[str, Any], success: bool) -> None:
        """Report work completion/failure to the leader."""
        if not self.leader_id or self.leader_id == self.node_id:
            return

        work_id = work_item.get("work_id", "")
        if not work_id:
            return

        with self.peers_lock:
            leader_peer = self.peers.get(self.leader_id)

        if not leader_peer:
            return

        try:
            timeout = ClientTimeout(total=15)
            async with get_client_session(timeout) as session:
                if success:
                    url = self._url_for_peer(leader_peer, "/work/complete")
                    payload = {"work_id": work_id, "result": {"node_id": self.node_id}}
                else:
                    url = self._url_for_peer(leader_peer, "/work/fail")
                    payload = {"work_id": work_id, "error": "execution_failed"}

                async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                    if resp.status == 200:
                        logger.debug(f"Reported work {work_id} result: {'success' if success else 'failed'}")
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Failed to report work result: {e}")

    # NOTE: _work_queue_maintenance_loop() removed Dec 2025 (42 LOC).
    # Now runs via LoopManager as WorkQueueMaintenanceLoop.
    # See scripts/p2p/loops/job_loops.py for implementation.

    # NOTE: _idle_detection_loop() removed Dec 2025 (128 LOC).
    # Now runs via LoopManager as IdleDetectionLoop.
    # See scripts/p2p/loops/job_loops.py for implementation.

    async def _auto_start_selfplay(self, peer, idle_duration: float):
        """Auto-start diverse hybrid selfplay on an idle node.

        Works with both NodeInfo (P2P peers) and DiscoveredNode (unified inventory).
        Uses diverse profiles for high-quality training data:
        - Multiple engine modes (gumbel-mcts, nnue-guided, policy-only, mcts)
        - Multiple board types (hex8, square8, square19)
        - Multiple player counts (2, 3, 4)
        - Multiple heuristic profiles (balanced, aggressive, territorial, defensive)
        """
        # Check for GPU - works with both NodeInfo and DiscoveredNode
        gpu_name = getattr(peer, "gpu_name", "") or ""

        # Don't auto-start on nodes that aren't GPU nodes
        has_gpu = bool(gpu_name) or getattr(peer, "has_gpu", False)
        is_gpu_node = getattr(peer, "is_gpu_node", lambda: has_gpu)()
        if not has_gpu and not is_gpu_node:
            return

        # GPU selfplay uses batch processing - scale based on GPU power
        if "GH200" in gpu_name.upper() or "H100" in gpu_name.upper() or "H200" in gpu_name.upper():
            num_processes = 4
            games_per_process = 10000
            gpu_tier = "high"
        elif "A100" in gpu_name.upper() or "A40" in gpu_name.upper():
            num_processes = 3
            games_per_process = 5000
            gpu_tier = "high"
        elif "4090" in gpu_name.upper() or "5090" in gpu_name.upper():
            num_processes = 3
            games_per_process = 5000
            gpu_tier = "mid"
        elif "4080" in gpu_name.upper() or "5080" in gpu_name.upper() or "5070" in gpu_name.upper():
            num_processes = 2
            games_per_process = 3000
            gpu_tier = "mid"
        elif "3090" in gpu_name.upper() or "4070" in gpu_name.upper():
            num_processes = 2
            games_per_process = 2500
            gpu_tier = "mid"
        else:
            num_processes = 2
            games_per_process = 2000
            gpu_tier = "low"

        logger.info(f"Auto-starting {num_processes} diverse selfplay processes on idle node {peer.node_id} "
                   f"(GPU={gpu_name}, tier={gpu_tier}, {games_per_process} games each, idle for {idle_duration:.0f}s)")

        # Send parallel requests to /selfplay/start endpoint
        try:
            url = self._url_for_peer(peer, "/selfplay/start")
            timeout = ClientTimeout(total=30)

            # Diverse profile configurations for high-quality training data
            # Each profile targets different aspects of game understanding
            # Dec 28, 2025: CRITICAL FIX - Changed "hex" to "hex8" in all profiles.
            # "hex" was being normalized to "hexagonal" (large board with 469 cells)
            # instead of "hex8" (small board with 61 cells), causing hex8_* configs
            # to receive no selfplay games for 52+ hours.
            DIVERSE_PROFILES = [
                # High-quality neural-guided profiles (50% of games)
                {
                    "engine_mode": "gumbel-mcts",
                    "board_type": "hex8",
                    "num_players": 2,
                    "profile": "balanced",
                    "weight": 0.18,
                    "description": "Gumbel MCTS 2P hex8 - highest quality",
                },
                {
                    "engine_mode": "policy-only",
                    "board_type": "hex8",
                    "num_players": 2,
                    "profile": "balanced",
                    "weight": 0.12,
                    "description": "Policy-only 2P hex8 - fast NN inference",
                },
                {
                    "engine_mode": "nnue-guided",
                    "board_type": "square8",
                    "num_players": 2,
                    "profile": "aggressive",
                    "weight": 0.08,
                    "description": "NNUE-guided 2P square - aggressive style",
                },
                {
                    "engine_mode": "gumbel-mcts",
                    "board_type": "square8",
                    "num_players": 3,
                    "profile": "balanced",
                    "weight": 0.06,
                    "description": "Gumbel MCTS 3P square - multiplayer strategy",
                },
                {
                    "engine_mode": "mcts",
                    "board_type": "hex8",
                    "num_players": 2,
                    "profile": "territorial",
                    "weight": 0.06,
                    "description": "MCTS 2P hex8 - territorial focus",
                },
                # MaxN/BRS multiplayer profiles (15% of games)
                # Benchmarks show: MaxN >> Descent in 3P/4P, MaxN  BRS
                {
                    "engine_mode": "maxn",
                    "board_type": "hex8",
                    "num_players": 3,
                    "profile": "balanced",
                    "weight": 0.05,
                    "description": "MaxN 3P hex8 - optimal multiplayer search",
                },
                {
                    "engine_mode": "maxn",
                    "board_type": "square8",
                    "num_players": 4,
                    "profile": "balanced",
                    "weight": 0.04,
                    "description": "MaxN 4P square - best for 4-player",
                },
                {
                    "engine_mode": "brs",
                    "board_type": "hex8",
                    "num_players": 3,
                    "profile": "aggressive",
                    "weight": 0.03,
                    "description": "BRS 3P hex8 - fast multiplayer search",
                },
                {
                    "engine_mode": "brs",
                    "board_type": "square8",
                    "num_players": 4,
                    "profile": "territorial",
                    "weight": 0.03,
                    "description": "BRS 4P square - territorial multiplayer",
                },
                # GPU-accelerated throughput profiles (25% of games)
                {
                    "engine_mode": "heuristic-only",
                    "board_type": "hex8",
                    "num_players": 2,
                    "profile": "balanced",
                    "weight": 0.10,
                    "description": "GPU heuristic 2P hex8 - fast throughput",
                },
                {
                    "engine_mode": "heuristic-only",
                    "board_type": "square8",
                    "num_players": 2,
                    "profile": "defensive",
                    "weight": 0.07,
                    "description": "GPU heuristic 2P square - defensive style",
                },
                {
                    "engine_mode": "heuristic-only",
                    "board_type": "hex8",
                    "num_players": 4,
                    "profile": "balanced",
                    "weight": 0.05,
                    "description": "GPU heuristic 4P hex8 - large multiplayer",
                },
                # Exploration profiles (10% of games)
                {
                    "engine_mode": "mixed",
                    "board_type": "square19",
                    "num_players": 2,
                    "profile": "balanced",
                    "weight": 0.04,
                    "description": "Mixed 2P large board - strategic depth",
                },
                {
                    "engine_mode": "nnue-guided",
                    "board_type": "hex8",
                    "num_players": 3,
                    "profile": "aggressive",
                    "weight": 0.04,
                    "description": "NNUE 3P hex8 - aggressive multiplayer",
                },
                {
                    "engine_mode": "policy-only",
                    "board_type": "square8",
                    "num_players": 4,
                    "profile": "territorial",
                    "weight": 0.05,
                    "description": "Policy 4P square - territory control",
                },
                # Dec 28, 2025: Added large board profiles (square19, hexagonal)
                # Uses lighter engines (heuristic, brs, maxn) for feasible throughput
                {
                    "engine_mode": "heuristic-only",
                    "board_type": "square19",
                    "num_players": 2,
                    "profile": "balanced",
                    "weight": 0.03,
                    "description": "Heuristic 2P square19 - fast large board",
                },
                {
                    "engine_mode": "heuristic-only",
                    "board_type": "hexagonal",
                    "num_players": 2,
                    "profile": "balanced",
                    "weight": 0.03,
                    "description": "Heuristic 2P hexagonal - fast large board",
                },
                {
                    "engine_mode": "brs",
                    "board_type": "square19",
                    "num_players": 3,
                    "profile": "balanced",
                    "weight": 0.02,
                    "description": "BRS 3P square19 - multiplayer large board",
                },
                {
                    "engine_mode": "brs",
                    "board_type": "hexagonal",
                    "num_players": 3,
                    "profile": "balanced",
                    "weight": 0.02,
                    "description": "BRS 3P hexagonal - multiplayer large board",
                },
                {
                    "engine_mode": "maxn",
                    "board_type": "square19",
                    "num_players": 4,
                    "profile": "balanced",
                    "weight": 0.02,
                    "description": "MaxN 4P square19 - high quality 4-player",
                },
                {
                    "engine_mode": "maxn",
                    "board_type": "hexagonal",
                    "num_players": 4,
                    "profile": "balanced",
                    "weight": 0.02,
                    "description": "MaxN 4P hexagonal - high quality 4-player",
                },
            ]

            # Select profiles based on weighted random sampling
            import random
            weights = [p["weight"] for p in DIVERSE_PROFILES]
            selected_profiles = random.choices(DIVERSE_PROFILES, weights=weights, k=num_processes)

            # Build job configs from selected profiles
            job_configs = []
            for i, profile in enumerate(selected_profiles):
                job_configs.append({
                    "board_type": profile["board_type"],
                    "num_players": profile["num_players"],
                    "num_games": games_per_process,
                    "engine_mode": profile["engine_mode"],
                    "heuristic_profile": profile["profile"],
                    "auto_assigned": True,
                    "reason": f"auto_idle_{profile['engine_mode']}_{profile['board_type']}_{profile['num_players']}p_{int(idle_duration)}s",
                })
                logger.debug(f"  Process {i}: {profile['description']}")

            # Dec 29, 2025: NAT-blocked node detection and work queue routing
            # NAT-blocked nodes can't receive inbound HTTP connections, so we
            # add work to the queue instead. WorkerPullLoop on the node will claim it.
            is_nat_blocked = getattr(peer, "nat_blocked", False)
            if is_nat_blocked:
                try:
                    from app.coordination.work_queue import WorkItem, WorkType, get_work_queue
                    wq = get_work_queue()
                    if wq is not None:
                        queued_count = 0
                        for cfg in job_configs:
                            work_item = WorkItem(
                                work_type=WorkType.SELFPLAY,
                                priority=50,  # Normal priority for auto-idle work
                                config={
                                    **cfg,
                                    "target_node": peer.node_id,  # Hint for WorkerPullLoop
                                    "nat_blocked_dispatch": True,  # Mark as NAT-blocked dispatch
                                },
                                timeout_seconds=3600.0,  # 1 hour timeout
                            )
                            wq.add_work(work_item)
                            queued_count += 1

                        from collections import Counter
                        engine_counts = Counter(cfg["engine_mode"] for cfg in job_configs)
                        board_counts = Counter(cfg["board_type"] for cfg in job_configs)
                        profile_summary = ", ".join(f"{k}:{v}" for k, v in engine_counts.items())
                        board_summary = ", ".join(f"{k}:{v}" for k, v in board_counts.items())

                        logger.info(
                            f"NAT-blocked node {peer.node_id}: Queued {queued_count} selfplay jobs "
                            f"[engines: {profile_summary}] [boards: {board_summary}] "
                            f"(idle for {idle_duration:.0f}s, WorkerPullLoop will claim)"
                        )
                        return  # Success via queue path
                    else:
                        logger.warning(f"NAT-blocked node {peer.node_id}: Work queue unavailable, cannot dispatch")
                        return
                except Exception as e:
                    logger.warning(f"NAT-blocked node {peer.node_id}: Failed to queue work: {e}")
                    return

            # Non-NAT-blocked nodes: Direct HTTP push to /selfplay/start endpoint
            async def send_selfplay_request(session, payload):
                """Send a single selfplay start request."""
                async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                    if resp.status == 200:
                        return True
                    else:
                        body = await resp.text()
                        logger.warning(f"Failed selfplay request on {peer.node_id}: {resp.status} {body[:100]}")
                        return False

            async with get_client_session(timeout) as session:
                tasks = [send_selfplay_request(session, cfg) for cfg in job_configs]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                started = 0
                failed = 0
                for r in results:
                    if isinstance(r, Exception):
                        failed += 1
                        logger.debug(f"Selfplay request failed with exception: {r}")
                    elif r is True:
                        started += 1
                if failed > 0:
                    logger.warning(f"Auto-start on {peer.node_id}: {failed}/{len(results)} requests failed")

                # Log profile distribution
                from collections import Counter
                engine_counts = Counter(cfg["engine_mode"] for cfg in job_configs)
                board_counts = Counter(cfg["board_type"] for cfg in job_configs)
                profile_summary = ", ".join(f"{k}:{v}" for k, v in engine_counts.items())
                board_summary = ", ".join(f"{k}:{v}" for k, v in board_counts.items())

                logger.info(f"Auto-started {started}/{num_processes} diverse selfplay on {peer.node_id} "
                           f"[engines: {profile_summary}] [boards: {board_summary}]")

        except Exception as e:  # noqa: BLE001
            logger.warning(f"Auto-start request failed for {peer.node_id}: {e}")

    # =========================================================================
    # AUTOMATION LOOPS (2024-12)
    # These loops enable hands-free cluster operation
    # =========================================================================

    # NOTE: _auto_scaling_loop() removed Dec 2025 (101 LOC).
    # Now runs via LoopManager as AutoScalingLoop.
    # See scripts/p2p/loops/coordination_loops.py for implementation.

    # NOTE: _predictive_monitoring_loop() removed Dec 2025 (~98 LOC).
    # Now runs via LoopManager as PredictiveMonitoringLoop.
    # See scripts/p2p/loops/resilience_loops.py for implementation.

    # NOTE: _self_healing_loop() removed Dec 2025 (~71 LOC).
    # Now runs via LoopManager as SelfHealingLoop.
    # See scripts/p2p/loops/resilience_loops.py for implementation.

    # NOTE: _job_reaper_loop() removed Dec 2025 (60 LOC).
    # Now runs via LoopManager as JobReaperLoop.
    # See scripts/p2p/loops/job_loops.py for implementation.

    # NOTE: _get_ssh_config_for_reaper() removed Dec 2025 (~23 LOC).
    # No callers existed - job reaper now uses cluster_config helpers.

    # NOTE: _validation_loop() removed Dec 2025 (92 LOC).
    # Now runs via LoopManager as ValidationLoop.
    # See scripts/p2p/loops/validation_loop.py for implementation.

    # NOTE: _queue_populator_loop() removed Dec 2025 (82 LOC).
    # Now runs via LoopManager as QueuePopulatorLoop.
    # See scripts/p2p/loops/queue_populator_loop.py for implementation.

    async def handle_games_analytics(self, request: web.Request) -> web.Response:
        """GET /games/analytics - Game statistics for dashboards.

        Returns aggregated game analytics including:
        - Average game length by config
        - Victory type distribution
        - Games per hour throughput
        - Opening move diversity
        """
        import json
        from collections import defaultdict

        try:
            # Skip JSONL scanning during startup grace period
            if self._is_in_startup_grace_period():
                return web.json_response({"configs": {}, "message": "Startup in progress"})

            hours = int(request.query.get("hours", "24"))
            cutoff = time.time() - (hours * 3600)

            ai_root = Path(self.ringrift_path) / "ai-service"
            data_dirs = [
                ai_root / "data" / "games" / "daemon_sync",
                ai_root / "data" / "selfplay",
            ]

            # Aggregation containers
            game_lengths: dict[str, list[int]] = defaultdict(list)
            victory_types: dict[str, dict[str, int]] = defaultdict(lambda: defaultdict(int))
            games_by_hour: dict[str, dict[int, int]] = defaultdict(lambda: defaultdict(int))
            opening_moves: dict[str, dict[str, int]] = defaultdict(lambda: defaultdict(int))
            total_games = 0

            for data_dir in data_dirs:
                if not data_dir.exists():
                    continue
                for jsonl_path in data_dir.rglob("*.jsonl"):
                    try:
                        if jsonl_path.stat().st_mtime < cutoff:
                            continue
                        with open_jsonl_file(jsonl_path) as f:
                            for line in f:
                                try:
                                    game = json.loads(line)
                                    board_type = game.get("board_type", "unknown")
                                    num_players = game.get("num_players", 0)
                                    config = f"{board_type}_{num_players}p"

                                    # Game length
                                    length = game.get("length", 0)
                                    if length > 0:
                                        game_lengths[config].append(length)

                                    # Victory type
                                    vt = game.get("victory_type", "unknown")
                                    if vt:
                                        victory_types[config][vt] += 1

                                    # Games by hour (for throughput)
                                    moves = game.get("moves", [])
                                    if moves and len(moves) > 0:
                                        # Use first move timestamp or file mtime
                                        hour_bucket = int(jsonl_path.stat().st_mtime // 3600)
                                        games_by_hour[config][hour_bucket] += 1

                                    # Opening moves (first 3 moves)
                                    if moves and len(moves) >= 1:
                                        first_move = str(moves[0].get("action", ""))[:20]
                                        if first_move:
                                            opening_moves[config][first_move] += 1

                                    total_games += 1
                                except json.JSONDecodeError:
                                    continue
                    except (OSError, ValueError, KeyError):
                        continue

            # Build response
            analytics = {
                "period_hours": hours,
                "total_games": total_games,
                "configs": {}
            }

            for config in set(list(game_lengths.keys()) + list(victory_types.keys())):
                lengths = game_lengths.get(config, [])
                vt = dict(victory_types.get(config, {}))
                openings = dict(opening_moves.get(config, {}))

                # Calculate throughput (games/hour)
                hourly = games_by_hour.get(config, {})
                throughput = sum(hourly.values()) / max(len(hourly), 1) if hourly else 0

                analytics["configs"][config] = {
                    "games": len(lengths),
                    "avg_length": round(sum(lengths) / len(lengths), 1) if lengths else 0,
                    "min_length": min(lengths) if lengths else 0,
                    "max_length": max(lengths) if lengths else 0,
                    "victory_types": vt,
                    "throughput_per_hour": round(throughput, 1),
                    "opening_diversity": len(openings),
                    "top_openings": dict(sorted(openings.items(), key=lambda x: -x[1])[:5]),
                }

            return web.json_response(analytics)

        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)})

    async def handle_training_metrics(self, request: web.Request) -> web.Response:
        """GET /training/metrics - Training loss and accuracy metrics.

        Returns recent training metrics from log files.
        """
        import re

        try:
            ai_root = Path(self.ringrift_path) / "ai-service"
            logs_dir = ai_root / "logs" / "training"

            metrics = {
                "configs": {},
                "latest_training": None,
            }

            if not logs_dir.exists():
                return web.json_response(metrics)

            # Find recent training logs
            log_files = sorted(logs_dir.glob("*.log"), key=lambda f: f.stat().st_mtime, reverse=True)[:10]

            for log_file in log_files:
                try:
                    content = log_file.read_text()

                    # Extract config from filename (e.g., train_square8_2p_20251214.log)
                    config_match = re.search(r"(square\d+|hexagonal|hex)_(\d+)p", log_file.name)
                    if not config_match:
                        continue
                    config = f"{config_match.group(1)}_{config_match.group(2)}p"

                    # Parse training metrics from log
                    # Look for patterns like: "Epoch 5: loss=0.423, policy_loss=0.312, value_loss=0.111"
                    loss_pattern = re.compile(
                        r"[Ee]poch\s+(\d+).*?loss[=:]\s*([\d.]+).*?"
                        r"(?:policy[_\s]?loss[=:]\s*([\d.]+))?.*?"
                        r"(?:value[_\s]?loss[=:]\s*([\d.]+))?"
                    )

                    epochs = []
                    for match in loss_pattern.finditer(content):
                        epoch = int(match.group(1))
                        total_loss = float(match.group(2))
                        policy_loss = float(match.group(3)) if match.group(3) else None
                        value_loss = float(match.group(4)) if match.group(4) else None
                        epochs.append({
                            "epoch": epoch,
                            "loss": total_loss,
                            "policy_loss": policy_loss,
                            "value_loss": value_loss,
                        })

                    if epochs:
                        metrics["configs"][config] = {
                            "log_file": log_file.name,
                            "epochs": epochs[-20:],  # Last 20 epochs
                            "latest_loss": epochs[-1]["loss"] if epochs else None,
                            "latest_epoch": epochs[-1]["epoch"] if epochs else None,
                        }
                        if not metrics["latest_training"]:
                            metrics["latest_training"] = {
                                "config": config,
                                "file": log_file.name,
                                "mtime": log_file.stat().st_mtime,
                            }

                except (OSError, ValueError, KeyError, json.JSONDecodeError):
                    continue

            return web.json_response(metrics)

        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)})

    async def handle_holdout_metrics(self, request: web.Request) -> web.Response:
        """GET /holdout/metrics - Holdout validation metrics.

        Returns holdout set statistics and evaluation results for overfitting detection.
        Supports optional query params:
            - config: Filter by config (e.g., square8_2p)
        """
        try:
            config_filter = request.query.get("config")
            metrics = await self._get_holdout_metrics_cached()

            if config_filter:
                # Filter to specific config
                filtered = {
                    "configs": {k: v for k, v in metrics.get("configs", {}).items() if k == config_filter},
                    "evaluations": [e for e in metrics.get("evaluations", []) if e.get("config") == config_filter],
                    "summary": metrics.get("summary", {}),
                }
                return web.json_response(filtered)

            return web.json_response(metrics)

        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)})

    # handle_holdout_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    async def handle_mcts_stats(self, request: web.Request) -> web.Response:
        """GET /mcts/stats - MCTS search statistics.

        Returns MCTS performance metrics including nodes/move, search depth, and timing.
        """
        try:
            stats = await self._get_mcts_stats_cached()
            return web.json_response(stats)

        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)})

    # handle_mcts_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    # =========================================================================
    # Feature Endpoints
    # =========================================================================

    async def handle_matchup_matrix(self, request: web.Request) -> web.Response:
        """GET /matchups/matrix - Head-to-head matchup statistics."""
        try:
            matrix = await self._get_matchup_matrix_cached()
            return web.json_response(matrix)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)})

    # handle_matchup_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    async def handle_model_lineage(self, request: web.Request) -> web.Response:
        """GET /models/lineage - Model ancestry and generation tracking."""
        try:
            lineage = await self._get_model_lineage_cached()
            return web.json_response(lineage)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)})

    # handle_model_lineage_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    async def handle_data_quality(self, request: web.Request) -> web.Response:
        """GET /data/quality - Data quality metrics and issue detection."""
        try:
            quality = await self._get_data_quality_cached()
            return web.json_response(quality)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)})

    # handle_data_quality_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)
    # handle_data_quality_issues() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    async def handle_training_efficiency(self, request: web.Request) -> web.Response:
        """GET /training/efficiency - Training efficiency and cost metrics."""
        try:
            efficiency = await self._get_training_efficiency_cached()
            return web.json_response(efficiency)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)})

    # handle_training_efficiency_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    async def handle_rollback_status(self, request: web.Request) -> web.Response:
        """GET /rollback/status - Model rollback status and recommendations."""
        try:
            status = await self._check_rollback_conditions()
            return web.json_response(status)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)})

    # handle_rollback_candidates() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    async def handle_rollback_execute(self, request: web.Request) -> web.Response:
        """POST /rollback/execute - Execute a model rollback.

        Query params:
            config: Config string like "square8_2p" (required)
            dry_run: If "true", only simulate the rollback (default: false)
        """
        try:
            config = request.query.get("config")
            if not config:
                return web.json_response({"error": "Missing required parameter: config"}, status=400)

            dry_run = request.query.get("dry_run", "").lower() in ("true", "1", "yes")

            result = await self._execute_rollback(config, dry_run=dry_run)
            status_code = 200 if result["success"] else 400
            return web.json_response(result, status=status_code)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=500)

    async def handle_rollback_auto(self, request: web.Request) -> web.Response:
        """POST /rollback/auto - Trigger automatic rollback check for all configs.

        This will check all configs for rollback conditions and execute rollbacks
        for any that meet the criteria.
        """
        try:
            # Temporarily enable auto-rollback for this request
            original_env = os.environ.get("RINGRIFT_AUTO_ROLLBACK", "")
            os.environ["RINGRIFT_AUTO_ROLLBACK"] = "true"

            executed = await self._auto_rollback_check()

            # Restore original env
            if original_env:
                os.environ["RINGRIFT_AUTO_ROLLBACK"] = original_env
            else:
                os.environ.pop("RINGRIFT_AUTO_ROLLBACK", None)

            return web.json_response({
                "executed_rollbacks": executed,
                "count": len(executed),
            })
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=500)

    async def handle_autoscale_metrics(self, request: web.Request) -> web.Response:
        """GET /autoscale/metrics - Autoscaling metrics and recommendations."""
        try:
            metrics = await self._get_autoscaling_metrics()
            return web.json_response(metrics)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)})

    # handle_autoscale_recommendations() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    async def handle_resource_optimizer(self, request: web.Request) -> web.Response:
        """GET /resource/optimizer - Resource optimizer state and recommendations.

        Returns cluster-wide utilization state, PID-controlled optimization
        recommendations, and target utilization ranges (60-80%).
        """
        try:
            if not HAS_NEW_COORDINATION:
                return web.json_response({
                    "error": "Resource optimizer not available",
                    "available": False,
                })

            optimizer = get_resource_optimizer()
            cluster_state = optimizer.get_cluster_state()
            recommendation = optimizer.get_optimization_recommendation()
            metrics = optimizer.get_metrics_dict()

            return web.json_response({
                "available": True,
                "cluster_state": {
                    "total_cpu_util": round(cluster_state.total_cpu_util, 1),
                    "total_gpu_util": round(cluster_state.total_gpu_util, 1),
                    "total_memory_util": round(cluster_state.total_memory_util, 1),
                    "gpu_node_count": cluster_state.gpu_node_count,
                    "cpu_node_count": cluster_state.cpu_node_count,
                    "total_jobs": cluster_state.total_jobs,
                    "nodes": [n.to_dict() for n in cluster_state.nodes],
                },
                "recommendation": recommendation.to_dict(),
                "targets": {
                    "min": TARGET_GPU_UTIL_MIN,
                    "max": TARGET_GPU_UTIL_MAX,
                    "optimal": (TARGET_GPU_UTIL_MIN + TARGET_GPU_UTIL_MAX) // 2,
                },
                "metrics": metrics,
                "in_target_range": {
                    "cpu": TARGET_GPU_UTIL_MIN <= cluster_state.total_cpu_util <= TARGET_GPU_UTIL_MAX,
                    "gpu": TARGET_GPU_UTIL_MIN <= cluster_state.total_gpu_util <= TARGET_GPU_UTIL_MAX
                           if cluster_state.gpu_node_count > 0 else True,
                },
            })
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e), "available": False}, status=500)

    async def handle_resource_utilization_history(self, request: web.Request) -> web.Response:
        """GET /resource/history - Resource utilization history for graphing.

        Query params:
            node_id: Specific node (optional, defaults to cluster average)
            hours: Hours of history (default: 1)
        """
        try:
            if not HAS_NEW_COORDINATION:
                return web.json_response([])

            node_id = request.query.get("node_id")
            hours = float(request.query.get("hours", "1"))

            optimizer = get_resource_optimizer()
            history = optimizer.get_utilization_history(node_id=node_id, hours=hours)
            return web.json_response(history)
        except (ValueError, AttributeError):
            return web.json_response([])

    async def handle_webhook_test(self, request: web.Request) -> web.Response:
        """POST /webhook/test - Test webhook notification.

        Query params:
            level: debug/info/warning/error (default: info)
            message: Custom message (default: "Test notification")
        """
        try:
            level = request.query.get("level", "info")
            message = request.query.get("message", "Test notification from RingRift AI orchestrator")

            has_slack = bool(self.notifier.slack_webhook)
            has_discord = bool(self.notifier.discord_webhook)

            if not has_slack and not has_discord:
                return web.json_response({
                    "success": False,
                    "message": "No webhooks configured. Set RINGRIFT_SLACK_WEBHOOK and/or RINGRIFT_DISCORD_WEBHOOK",
                })

            await self.notifier.send(
                title="Webhook Test",
                message=message,
                level=level,
                fields={
                    "Node": self.node_id,
                    "Timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "Level": level.upper(),
                },
                node_id=self.node_id,
            )

            return web.json_response({
                "success": True,
                "message": f"Test notification sent to {'Slack' if has_slack else ''}{' and ' if has_slack and has_discord else ''}{'Discord' if has_discord else ''}",
                "level": level,
            })
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=500)

    async def handle_trends_summary(self, request: web.Request) -> web.Response:
        """GET /trends/summary - Get summary of metrics over time period.

        Query params:
            hours: Time period in hours (default: 24)
        """
        try:
            hours = float(request.query.get("hours", "24"))
            summary = self.get_metrics_summary(hours)
            return web.json_response(summary)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=500)

    async def handle_trends_history(self, request: web.Request) -> web.Response:
        """GET /trends/history - Get historical metrics data.

        Query params:
            metric: Metric type (required) - e.g., "best_elo", "games_generated", "training_loss"
            hours: Time period in hours (default: 24)
            board: Board type filter (optional) - e.g., "square8"
            players: Number of players filter (optional) - e.g., 2
            limit: Max records to return (default: 1000)
        """
        try:
            metric_type = request.query.get("metric")
            if not metric_type:
                return web.json_response({"error": "Missing required parameter: metric"}, status=400)

            hours = float(request.query.get("hours", "24"))
            board_type = request.query.get("board")
            num_players = int(request.query.get("players")) if request.query.get("players") else None
            limit = int(request.query.get("limit", "1000"))

            history = self.get_metrics_history(
                metric_type=metric_type,
                board_type=board_type,
                num_players=num_players,
                hours=hours,
                limit=limit,
            )

            return web.json_response({
                "metric": metric_type,
                "period_hours": hours,
                "count": len(history),
                "data": history,
            })
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=500)

    # handle_trends_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    # ==================== A/B Testing Framework ====================

    def _calculate_ab_test_stats(self, test_id: str) -> dict[str, Any]:
        """Calculate statistical significance for an A/B test."""
        import math

        try:
            # Phase 3.4 Dec 29, 2025: Use context manager to prevent connection leaks
            with safe_db_connection(self.db_path) as conn:
                cursor = conn.cursor()

                # Get game results
                cursor.execute("""
                    SELECT model_a_result, model_a_score, model_b_score, game_length
                    FROM ab_test_games WHERE test_id = ?
                """, (test_id,))
                games = cursor.fetchall()

            if not games:
                return {
                    "games_played": 0,
                    "model_a_wins": 0,
                    "model_b_wins": 0,
                    "draws": 0,
                    "model_a_score": 0.0,
                    "model_b_score": 0.0,
                    "model_a_winrate": 0.0,
                    "model_b_winrate": 0.0,
                    "confidence": 0.0,
                    "likely_winner": None,
                    "statistically_significant": False,
                }

            # Count results
            model_a_wins = sum(1 for g in games if g[0] == "win")
            model_b_wins = sum(1 for g in games if g[0] == "loss")
            draws = sum(1 for g in games if g[0] == "draw")
            total = len(games)

            model_a_score = sum(g[1] for g in games)
            model_b_score = sum(g[2] for g in games)

            # Winrate (using score, e.g., 1 for win, 0.5 for draw, 0 for loss)
            model_a_winrate = model_a_score / total if total > 0 else 0.0
            model_b_winrate = model_b_score / total if total > 0 else 0.0

            # Wilson score confidence interval for statistical significance
            # Using normal approximation for simplicity
            def wilson_ci(wins: int, n: int, z: float = 1.96) -> tuple[float, float]:
                if n == 0:
                    return (0.0, 1.0)
                p = wins / n
                denominator = 1 + z * z / n
                center = (p + z * z / (2 * n)) / denominator
                spread = z * math.sqrt((p * (1 - p) + z * z / (4 * n)) / n) / denominator
                return (max(0, center - spread), min(1, center + spread))

            # Calculate confidence intervals
            a_lo, a_hi = wilson_ci(model_a_wins + draws // 2, total)
            b_lo, b_hi = wilson_ci(model_b_wins + draws // 2, total)

            # Determine if statistically significant (non-overlapping CIs)
            statistically_significant = a_hi < b_lo or b_hi < a_lo

            # Estimate confidence based on score difference and sample size
            if total > 0:
                score_diff = abs(model_a_winrate - model_b_winrate)
                # Rough confidence estimate (higher with more games and larger diff)
                confidence = min(0.99, 1 - math.exp(-total * score_diff * 2))
            else:
                confidence = 0.0

            # Determine likely winner
            likely_winner = None
            if model_a_winrate > model_b_winrate + 0.05:
                likely_winner = "model_a"
            elif model_b_winrate > model_a_winrate + 0.05:
                likely_winner = "model_b"

            avg_game_length = sum(g[3] for g in games if g[3]) / max(1, sum(1 for g in games if g[3]))

            return {
                "games_played": total,
                "model_a_wins": model_a_wins,
                "model_b_wins": model_b_wins,
                "draws": draws,
                "model_a_score": model_a_score,
                "model_b_score": model_b_score,
                "model_a_winrate": round(model_a_winrate, 4),
                "model_b_winrate": round(model_b_winrate, 4),
                "confidence": round(confidence, 4),
                "likely_winner": likely_winner,
                "statistically_significant": statistically_significant,
                "avg_game_length": round(avg_game_length, 1),
            }
        except Exception as e:  # noqa: BLE001
            return {"error": str(e)}

    # A/B Test handlers moved to scripts/p2p/handlers/abtest.py (Dec 28, 2025 - Phase 8)
    # Inherited from ABTestHandlersMixin:
    # - handle_abtest_create, handle_abtest_result, handle_abtest_status
    # - handle_abtest_list, handle_abtest_cancel, handle_abtest_run
    # Note: handle_abtest_table() was previously moved to TableHandlersMixin

    async def handle_api_training_status(self, request: web.Request) -> web.Response:
        """Get training pipeline status including NNUE, CMAES, and auto-promotion state.

        Returns daemon state for NNUE training, CMAES optimization, and model promotion.
        """
        try:
            from datetime import datetime

            ai_root = Path(self.ringrift_path) / "ai-service"

            # Load daemon state (from continuous_improvement_daemon.py)
            daemon_state_path = ai_root / "logs" / "improvement_daemon" / "state.json"
            daemon_state = {}
            daemon_running = False
            daemon_pid = None
            daemon_uptime = 0

            # Check if daemon is running
            pid_file = ai_root / "logs" / "improvement_daemon" / "daemon.pid"
            if pid_file.exists():
                try:
                    daemon_pid = int(pid_file.read_text().strip())
                    # Check if process is running
                    import os
                    os.kill(daemon_pid, 0)  # Doesn't kill, just checks
                    daemon_running = True
                except (ValueError, ProcessLookupError, PermissionError):
                    daemon_running = False

            if daemon_state_path.exists():
                try:
                    daemon_state = json.loads(daemon_state_path.read_text())
                    # Calculate uptime if daemon is running
                    if daemon_running and daemon_state.get("started_at"):
                        started = datetime.fromisoformat(daemon_state["started_at"])
                        daemon_uptime = (datetime.now() - started).total_seconds()
                except (json.JSONDecodeError, ValueError, OSError):
                    pass

            # Load runtime overrides (promoted models)
            overrides_path = ai_root / "data" / "ladder_runtime_overrides.json"
            runtime_overrides = {}
            if overrides_path.exists():
                with contextlib.suppress(json.JSONDecodeError, ValueError, OSError):
                    runtime_overrides = json.loads(overrides_path.read_text())

            # Load auto-promotion log
            promotion_log_path = (
                ai_root / "runs" / "promotion" / "model_promotion_history.json"
                if (ai_root / "runs" / "promotion" / "model_promotion_history.json").exists()
                else (ai_root / "data" / "auto_promotion_log.json")
            )
            promotion_log = []
            if promotion_log_path.exists():
                try:
                    promotion_log = json.loads(promotion_log_path.read_text())
                    if isinstance(promotion_log, list):
                        promotion_log = promotion_log[-10:]  # Last 10 entries
                except (json.JSONDecodeError, ValueError, OSError):
                    pass

            # Check NNUE model timestamps
            nnue_models = {}
            nnue_dir = ai_root / "models" / "nnue"
            if nnue_dir.exists():
                for model_file in nnue_dir.glob("*.pt"):
                    if "_prev" not in model_file.name:
                        stat = model_file.stat()
                        nnue_models[model_file.stem] = {
                            "path": str(model_file),
                            "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                            "size_mb": round(stat.st_size / 1024 / 1024, 2),
                        }

            # Check trained heuristic profiles
            profiles_path = ai_root / "data" / "trained_heuristic_profiles.json"
            heuristic_profiles = {}
            if profiles_path.exists():
                try:
                    profiles_data = json.loads(profiles_path.read_text())
                    heuristic_profiles = {
                        "count": len(profiles_data),
                        "profiles": list(profiles_data.keys())[:20],
                    }
                except (json.JSONDecodeError, ValueError, OSError):
                    pass

            return web.json_response({
                "success": True,
                "daemon": {
                    "running": daemon_running,
                    "pid": daemon_pid,
                    "uptime_seconds": daemon_uptime,
                    "current_cycle": daemon_state.get("total_cycles", 0),
                    "last_cycle_at": daemon_state.get("last_cycle_at", ""),
                    "total_games_generated": daemon_state.get("total_games_generated", 0),
                    "total_training_runs": daemon_state.get("total_training_runs", 0),
                    "total_tournaments": daemon_state.get("total_tournaments", 0),
                    "total_auto_promotions": daemon_state.get("total_auto_promotions", 0),
                    "last_auto_promote_time": daemon_state.get("last_auto_promote_time", 0),
                    "consecutive_failures": daemon_state.get("consecutive_failures", 0),
                },
                "nnue": {
                    "state": "idle" if not daemon_state.get("nnue_state") else "active",
                    "models": list(nnue_models.keys()),
                    "model_details": nnue_models,
                    "per_config_state": daemon_state.get("nnue_state", {}),
                    "last_gate_result": daemon_state.get("last_nnue_gate_result", None),
                },
                "cmaes": {
                    "state": "idle" if not daemon_state.get("cmaes_state") else "active",
                    "profiles": heuristic_profiles.get("profiles", []) if heuristic_profiles else [],
                    "profile_count": heuristic_profiles.get("count", 0) if heuristic_profiles else 0,
                    "per_config_state": daemon_state.get("cmaes_state", {}),
                    "generations": sum(s.get("generations", 0) for s in daemon_state.get("cmaes_state", {}).values()),
                },
                "promotion": {
                    "runtime_overrides": runtime_overrides,
                    "recent_promotions": promotion_log,
                },
                "timestamp": time.time(),
            })

        except Exception as e:  # noqa: BLE001
            return web.json_response({"success": False, "error": str(e)}, status=500)

    # =========================================================================
    # NOTE: Canonical gate handlers moved to scripts/p2p/handlers/canonical_gate.py (Dec 28, 2025 - Phase 8)
    # Inherited from CanonicalGateHandlersMixin:
    # - _canonical_slug_for_board, _canonical_gate_paths, _tail_text_file, _canonical_gate_log_dir
    # - _monitor_canonical_gate_job
    # - handle_api_canonical_health, handle_api_canonical_jobs_list, handle_api_canonical_job_get
    # - handle_api_canonical_job_log, handle_api_canonical_logs_list, handle_api_canonical_log_tail
    # - handle_api_canonical_generate, handle_api_canonical_job_cancel
    # =========================================================================

    # =========================================================================
    # NOTE: Jobs API handlers moved to scripts/p2p/handlers/jobs_api.py (Dec 28, 2025 - Phase 8)
    # Inherited from JobsApiHandlersMixin:
    # - handle_api_jobs_list, handle_api_jobs_submit, handle_api_job_get, handle_api_job_cancel
    # - _get_job_type_enum (helper for lazy JobType import)
    # =========================================================================

    async def handle_dashboard(self, request: web.Request) -> web.Response:
        """Serve the web dashboard HTML."""
        dashboard_path = Path(__file__).resolve().parent / "dashboard_assets" / "dashboard.html"
        try:
            html = dashboard_path.read_text(encoding="utf-8")
        except Exception as e:  # noqa: BLE001
            html = (
                "<!doctype html><html><body style='font-family:monospace'>"
                f"<h3>Dashboard asset unavailable</h3><pre>{e}</pre>"
                f"<pre>Expected: {dashboard_path}</pre>"
                "</body></html>"
            )
        headers = {
            # Avoid stale HTML across load balancers / browsers during rapid iteration.
            "Cache-Control": "no-store, max-age=0",
            "Pragma": "no-cache",
            "Expires": "0",
            # Simple diagnostics (no secrets).
            "X-RingRift-Node-Id": str(self.node_id or ""),
            "X-RingRift-Build-Version": str(getattr(self, "build_version", "") or ""),
        }
        return web.Response(text=html, content_type="text/html", headers=headers)

    async def handle_work_queue_dashboard(self, request: web.Request) -> web.Response:
        """Serve the work queue dashboard HTML."""
        dashboard_path = Path(__file__).resolve().parent / "dashboard_assets" / "work_queue_dashboard.html"
        try:
            html = dashboard_path.read_text(encoding="utf-8")
        except Exception as e:  # noqa: BLE001
            html = (
                "<!doctype html><html><body style='font-family:monospace'>"
                f"<h3>Work Queue Dashboard unavailable</h3><pre>{e}</pre>"
                f"<pre>Expected: {dashboard_path}</pre>"
                "</body></html>"
            )
        headers = {
            "Cache-Control": "no-store, max-age=0",
            "Pragma": "no-cache",
            "Expires": "0",
            "X-RingRift-Node-Id": str(self.node_id or ""),
        }
        return web.Response(text=html, content_type="text/html", headers=headers)

    async def _run_evaluation(self, job_id: str):
        """Evaluate new model against current best.

        Runs evaluation games between the candidate model and the best model.
        Reports win rate for the candidate.
        """
        import json as json_module
        import sys

        state = self.improvement_loop_state.get(job_id)
        if not state:
            return

        logger.info(f"Running evaluation for job {job_id}, iteration {state.current_iteration}")

        getattr(state, 'candidate_model_path', None)

        # Number of evaluation games
        eval_games = 100

        eval_script = f"""
import sys
sys.path.insert(0, '{self.ringrift_path}/ai-service')
from app.game_engine import GameEngine
from app.agents.heuristic_agent import HeuristicAgent
import json

# Run evaluation games
candidate_wins = 0
best_wins = 0
draws = 0

for game_idx in range({eval_games}):
    engine = GameEngine(board_type='{state.board_type}', num_players={state.num_players})

    # Alternate who plays first
    if game_idx % 2 == 0:
        agents = [
            HeuristicAgent(0),  # Candidate as player 0
            HeuristicAgent(1),  # Best as player 1
        ]
        candidate_player = 0
    else:
        agents = [
            HeuristicAgent(0),  # Best as player 0
            HeuristicAgent(1),  # Candidate as player 1
        ]
        candidate_player = 1

    # Play game
    max_moves = 10000
    move_count = 0
    while not engine.is_game_over() and move_count < max_moves:
        current_player = engine.current_player
        agent = agents[current_player]
        legal_moves = engine.get_legal_moves()
        if not legal_moves:
            break
        move = agent.select_move(engine.get_state(), legal_moves)
        engine.apply_move(move)
        move_count += 1

    outcome = engine.get_outcome()
    winner = outcome.get('winner')

    if winner == candidate_player:
        candidate_wins += 1
    elif winner is not None:
        best_wins += 1
    else:
        draws += 1

# Calculate win rate
total = candidate_wins + best_wins + draws
winrate = candidate_wins / total if total > 0 else 0.5

print(json.dumps({{
    'candidate_wins': candidate_wins,
    'best_wins': best_wins,
    'draws': draws,
    'winrate': winrate,
}}))
"""

        cmd = [sys.executable, "-c", eval_script]
        env = os.environ.copy()
        env["PYTHONPATH"] = os.path.join(self.ringrift_path, "ai-service")
        env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

        try:
            proc = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env,
            )

            stdout, stderr = await asyncio.wait_for(
                proc.communicate(),
                timeout=3600  # 1 hour max
            )

            if proc.returncode == 0:
                output_lines = stdout.decode().strip().split('\n')
                result_line = output_lines[-1] if output_lines else '{}'
                result = json_module.loads(result_line)

                state.evaluation_winrate = result.get('winrate', 0.5)
                logger.info(f"Evaluation result: winrate={state.evaluation_winrate:.2%}")
                logger.info("  Candidate")
            else:
                logger.info(f"Evaluation failed: {stderr.decode()[:500]}")
                state.evaluation_winrate = 0.5

        except asyncio.TimeoutError:
            logger.info("Evaluation timed out")
            state.evaluation_winrate = 0.5
        except Exception as e:  # noqa: BLE001
            logger.info(f"Evaluation error: {e}")
            state.evaluation_winrate = 0.5

    async def _promote_model_if_better(self, job_id: str):
        """Promote new model if it beats the current best.

        Promotion threshold: candidate must win >= 55% of evaluation games.
        """
        state = self.improvement_loop_state.get(job_id)
        if not state:
            return

        PROMOTION_THRESHOLD = 0.55  # 55% win rate required

        winrate = getattr(state, 'evaluation_winrate', 0.5)
        candidate_path = getattr(state, 'candidate_model_path', None)

        logger.info(f"Checking model promotion for job {job_id}")
        logger.info("  Current")
        logger.info("  Candidate")
        logger.info("  Threshold")

        if winrate >= PROMOTION_THRESHOLD and candidate_path:
            # Promote candidate to best
            state.best_model_path = candidate_path
            state.best_winrate = winrate

            # Save best model to well-known location
            best_model_dir = os.path.join(
                self.ringrift_path, "ai-service", "models", "best"
            )
            os.makedirs(best_model_dir, exist_ok=True)

            import shutil
            best_path = os.path.join(best_model_dir, f"{state.board_type}_{state.num_players}p.pt")
            if os.path.exists(candidate_path):
                shutil.copy2(candidate_path, best_path)
                logger.info(f"PROMOTED: New best model at {best_path}")
                logger.info(f"  Win rate: {winrate:.2%}")
            else:
                logger.info(f"Cannot promote: candidate model not found at {candidate_path}")
        else:
            logger.info(f"No promotion: candidate ({winrate:.2%}) below threshold ({PROMOTION_THRESHOLD:.0%})")

    # ============================================
    # Core Logic
    # ============================================

    def _update_self_info(self):
        """Update self info with current resource usage."""
        usage = self._get_resource_usage()
        selfplay, training = self._count_local_jobs()

        # NAT/relay detection: if we haven't received any inbound heartbeats for a
        # while (but we do know about other peers), assume we're not reachable
        # inbound and must poll a relay for commands.
        now = time.time()
        if self.known_peers or self.peers:
            last_inbound = self.last_inbound_heartbeat or self.start_time
            self.self_info.nat_blocked = (now - last_inbound) >= NAT_INBOUND_HEARTBEAT_STALE_SECONDS
        else:
            self.self_info.nat_blocked = False

        if not self.self_info.nat_blocked:
            self.self_info.relay_via = ""
        elif self.leader_id and self.leader_id != self.node_id:
            self.self_info.relay_via = self.leader_id

        self.self_info.cpu_percent = usage["cpu_percent"]
        self.self_info.memory_percent = usage["memory_percent"]
        self.self_info.disk_percent = usage["disk_percent"]
        self.self_info.gpu_percent = usage["gpu_percent"]
        self.self_info.gpu_memory_percent = usage["gpu_memory_percent"]
        self.self_info.selfplay_jobs = selfplay
        self.self_info.training_jobs = training
        self.self_info.role = self.role
        self.self_info.last_heartbeat = time.time()
        # Dec 2025: Propagate leader_id in heartbeats for cluster-wide leader discovery
        self.self_info.leader_id = self.leader_id or ""

        # Detect external work (running outside P2P orchestrator tracking)
        external = self._detect_local_external_work()
        self.self_info.cmaes_running = external.get('cmaes_running', False)
        self.self_info.gauntlet_running = external.get('gauntlet_running', False)
        self.self_info.tournament_running = external.get('tournament_running', False)
        self.self_info.data_merge_running = external.get('data_merge_running', False)

        # Phase 6: Health broadcasting - additional health metrics
        self.self_info.nfs_accessible = self._check_nfs_accessible()
        self.self_info.code_version = self.build_version
        self.self_info.errors_last_hour = getattr(self, '_error_count_last_hour', 0)
        self.self_info.disk_free_gb = usage.get("disk_free_gb", 0.0)
        self.self_info.active_job_count = (
            selfplay + training +
            (1 if self.self_info.cmaes_running else 0) +
            (1 if self.self_info.gauntlet_running else 0) +
            (1 if self.self_info.tournament_running else 0)
        )

        # Report to unified resource optimizer for cluster-wide coordination
        if HAS_NEW_COORDINATION:
            try:
                optimizer = get_resource_optimizer()
                node_resources = NodeResources(
                    node_id=self.node_id,
                    cpu_percent=usage["cpu_percent"],
                    gpu_percent=usage["gpu_percent"],
                    memory_percent=usage["memory_percent"],
                    disk_percent=usage["disk_percent"],
                    gpu_memory_percent=usage["gpu_memory_percent"],
                    cpu_count=int(getattr(self.self_info, "cpu_count", 0) or 0),
                    memory_gb=float(getattr(self.self_info, "memory_gb", 0) or 0),
                    has_gpu=bool(getattr(self.self_info, "has_gpu", False)),
                    gpu_name=str(getattr(self.self_info, "gpu_name", "") or ""),
                    active_jobs=selfplay + training,
                    selfplay_jobs=selfplay,
                    training_jobs=training,
                    orchestrator="p2p_orchestrator",
                )
                optimizer.report_node_resources(node_resources)
            except (ValueError, KeyError, IndexError, AttributeError):
                pass  # Don't fail heartbeat if optimizer unavailable

        # December 2025: Emit NODE_CAPACITY_UPDATED for backpressure detection
        # Throttled to every 30 seconds to avoid event spam
        now = time.time()
        last_emit = getattr(self, "_last_capacity_emit_time", 0)
        if now - last_emit >= 30:  # 30s throttle matches backpressure cooldown
            self._last_capacity_emit_time = now
            try:
                from app.coordination.event_router import get_event_bus
                from app.distributed.data_events import DataEventType, DataEvent

                bus = get_event_bus()
                if bus:
                    event = DataEvent(
                        event_type=DataEventType.NODE_CAPACITY_UPDATED,
                        payload={
                            "node_id": self.node_id,
                            "gpu_utilization": usage["gpu_percent"],
                            "cpu_utilization": usage["cpu_percent"],
                            "memory_used_percent": usage["memory_percent"],
                            "disk_used_percent": usage["disk_percent"],
                            "gpu_memory_percent": usage["gpu_memory_percent"],
                            "task_slots_available": max(0, self._get_max_selfplay_jobs() - selfplay - training),
                            "task_slots_total": self._get_max_selfplay_jobs(),
                        },
                    )
                    bus.publish_sync(event)
            except (ImportError, RuntimeError, AttributeError):
                pass  # Event system not available or no event loop

    async def _update_self_info_async(self):
        """Async version of _update_self_info() to avoid blocking event loop.

        Dec 30, 2025: Added to fix gossip latency issues on coordinator nodes.
        The sync version calls subprocess for resource detection which blocks
        the event loop. This async version uses asyncio.to_thread() for those
        blocking operations.
        """
        import asyncio

        # Run blocking operations in thread pool
        usage = await self._get_resource_usage_async()
        selfplay, training = await asyncio.to_thread(self._count_local_jobs)

        # NAT/relay detection (fast, no subprocess)
        now = time.time()
        if self.known_peers or self.peers:
            last_inbound = self.last_inbound_heartbeat or self.start_time
            self.self_info.nat_blocked = (now - last_inbound) >= NAT_INBOUND_HEARTBEAT_STALE_SECONDS
        else:
            self.self_info.nat_blocked = False

        if not self.self_info.nat_blocked:
            self.self_info.relay_via = ""
        elif self.leader_id and self.leader_id != self.node_id:
            self.self_info.relay_via = self.leader_id

        self.self_info.cpu_percent = usage["cpu_percent"]
        self.self_info.memory_percent = usage["memory_percent"]
        self.self_info.disk_percent = usage["disk_percent"]
        self.self_info.gpu_percent = usage["gpu_percent"]
        self.self_info.gpu_memory_percent = usage["gpu_memory_percent"]
        self.self_info.selfplay_jobs = selfplay
        self.self_info.training_jobs = training
        self.self_info.role = self.role
        self.self_info.last_heartbeat = time.time()
        self.self_info.leader_id = self.leader_id or ""

        # Run blocking external work detection in thread pool
        external = await asyncio.to_thread(self._detect_local_external_work)
        self.self_info.cmaes_running = external.get('cmaes_running', False)
        self.self_info.gauntlet_running = external.get('gauntlet_running', False)
        self.self_info.tournament_running = external.get('tournament_running', False)
        self.self_info.data_merge_running = external.get('data_merge_running', False)

        # Health metrics (NFS check in thread pool as it can block)
        self.self_info.nfs_accessible = await asyncio.to_thread(self._check_nfs_accessible)
        self.self_info.code_version = self.build_version
        self.self_info.errors_last_hour = getattr(self, '_error_count_last_hour', 0)
        self.self_info.disk_free_gb = usage.get("disk_free_gb", 0.0)
        self.self_info.active_job_count = (
            selfplay + training +
            (1 if self.self_info.cmaes_running else 0) +
            (1 if self.self_info.gauntlet_running else 0) +
            (1 if self.self_info.tournament_running else 0)
        )

        # Report to resource optimizer (fast, in-memory)
        if HAS_NEW_COORDINATION:
            try:
                optimizer = get_resource_optimizer()
                node_resources = NodeResources(
                    node_id=self.node_id,
                    cpu_percent=usage["cpu_percent"],
                    gpu_percent=usage["gpu_percent"],
                    memory_percent=usage["memory_percent"],
                    disk_percent=usage["disk_percent"],
                    gpu_memory_percent=usage["gpu_memory_percent"],
                    cpu_count=int(getattr(self.self_info, "cpu_count", 0) or 0),
                    memory_gb=float(getattr(self.self_info, "memory_gb", 0) or 0),
                    has_gpu=bool(getattr(self.self_info, "has_gpu", False)),
                    gpu_name=str(getattr(self.self_info, "gpu_name", "") or ""),
                    active_jobs=selfplay + training,
                    selfplay_jobs=selfplay,
                    training_jobs=training,
                    orchestrator="p2p_orchestrator",
                )
                optimizer.report_node_resources(node_resources)
            except (ValueError, KeyError, IndexError, AttributeError):
                pass

        # NODE_CAPACITY_UPDATED event (throttled, fast)
        last_emit = getattr(self, "_last_capacity_emit_time", 0)
        if now - last_emit >= 30:
            self._last_capacity_emit_time = now
            try:
                from app.coordination.event_router import get_event_bus
                from app.distributed.data_events import DataEventType, DataEvent

                bus = get_event_bus()
                if bus:
                    event = DataEvent(
                        event_type=DataEventType.NODE_CAPACITY_UPDATED,
                        payload={
                            "node_id": self.node_id,
                            "gpu_utilization": usage["gpu_percent"],
                            "cpu_utilization": usage["cpu_percent"],
                            "memory_used_percent": usage["memory_percent"],
                            "disk_used_percent": usage["disk_percent"],
                            "gpu_memory_percent": usage["gpu_memory_percent"],
                            "task_slots_available": max(0, self._get_max_selfplay_jobs() - selfplay - training),
                            "task_slots_total": self._get_max_selfplay_jobs(),
                        },
                    )
                    bus.publish_sync(event)
            except (ImportError, RuntimeError, AttributeError):
                pass

    async def _send_heartbeat_to_peer(self, peer_host: str, peer_port: int, scheme: str = "http", timeout: int = 10) -> NodeInfo | None:
        """Send heartbeat to a peer and return their info.

        Args:
            peer_host: Target peer hostname or IP
            peer_port: Target peer port
            scheme: HTTP or HTTPS scheme
            timeout: Request timeout in seconds (default 10, use smaller for voter heartbeats)

        Dec 2025: Added SSH fallback via HybridTransport when HTTP fails.
        """
        target = f"{peer_host}:{peer_port}"
        breaker = self._circuit_registry.get_breaker("p2p")

        # Check circuit breaker before attempting request
        if not breaker.can_execute(target):
            state = breaker.get_state(target)
            if state == CircuitState.OPEN:
                # Circuit is open - skip this peer temporarily
                return None

        http_failed = False
        # Prepare payload outside try block so it's available for SSH fallback
        self._update_self_info()
        payload = self.self_info.to_dict()
        voter_node_ids = list(getattr(self, "voter_node_ids", []) or [])
        if voter_node_ids:
            payload["voter_node_ids"] = voter_node_ids
            payload["voter_quorum_size"] = int(getattr(self, "voter_quorum_size", 0) or 0)
            payload["voter_config_source"] = str(getattr(self, "voter_config_source", "") or "")

        try:
            # Adjust timeout based on circuit state (shorter for half-open probing)
            effective_timeout = self._circuit_registry.get_timeout("p2p", target, float(timeout))
            client_timeout = ClientTimeout(total=effective_timeout)

            async with get_client_session(client_timeout) as session:
                scheme = (scheme or "http").lower()
                url = f"{scheme}://{peer_host}:{peer_port}/heartbeat"
                async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                    data, json_error = await safe_json_response(resp, default=None, log_errors=False)
                    if json_error or data is None:
                        # Record failure with circuit breaker for empty/invalid responses
                        breaker.record_failure(target)
                        http_failed = True
                    else:
                        incoming_voters = data.get("voter_node_ids") or data.get("voters") or None
                        if incoming_voters:
                            voters_list: list[str] = []
                            if isinstance(incoming_voters, list):
                                voters_list = [str(v).strip() for v in incoming_voters if str(v).strip()]
                            elif isinstance(incoming_voters, str):
                                voters_list = [t.strip() for t in incoming_voters.split(",") if t.strip()]
                            if voters_list:
                                self._maybe_adopt_voter_node_ids(voters_list, source="learned")
                        info = NodeInfo.from_dict(data)
                        if not info.reported_host:
                            info.reported_host = info.host
                        if not info.reported_port:
                            info.reported_port = info.port
                        # Use the address we successfully reached instead of any
                        # self-reported interface address.
                        info.scheme = scheme
                        info.host = peer_host
                        info.port = peer_port
                        # Record success with circuit breaker
                        breaker.record_success(target)

                        # Phase 27: Cache peer for persistence across restarts
                        self._save_peer_to_cache(
                            info.node_id,
                            peer_host,
                            peer_port,
                            str(getattr(info, "tailscale_ip", "") or "")
                        )
                        self._update_peer_reputation(info.node_id, success=True)

                        return info
        except (aiohttp.ClientError, asyncio.TimeoutError, OSError, ValueError, KeyError):
            # Record failure with circuit breaker
            breaker.record_failure(target)
            http_failed = True

        # Dec 2025: SSH fallback via HybridTransport when HTTP fails
        if http_failed and self.hybrid_transport:
            info = await self._send_heartbeat_via_ssh_fallback(peer_host, peer_port, payload)
            if info:
                breaker.record_success(target)
                return info

        return None

    async def _send_heartbeat_via_ssh_fallback(
        self, peer_host: str, peer_port: int, payload: dict[str, Any]
    ) -> NodeInfo | None:
        """Send heartbeat via SSH when HTTP fails.

        Dec 2025: Uses HybridTransport SSH fallback for nodes behind NAT or
        with unreachable HTTP endpoints.

        Args:
            peer_host: Target peer hostname or IP
            peer_port: Target peer port
            payload: Heartbeat payload dict

        Returns:
            NodeInfo if successful, None otherwise
        """
        if not self.hybrid_transport:
            return None

        # Try to find node_id for this host
        node_id = self._find_node_id_for_host(peer_host)
        if not node_id:
            return None

        try:
            success, response = await self.hybrid_transport.send_heartbeat(
                node_id=node_id,
                host=peer_host,
                port=peer_port,
                self_info=payload,
            )

            if success and response:
                info = NodeInfo.from_dict(response)
                if not info.reported_host:
                    info.reported_host = info.host
                if not info.reported_port:
                    info.reported_port = info.port
                info.scheme = "http"  # SSH proxied to local HTTP
                info.host = peer_host
                info.port = peer_port

                # Cache peer for persistence
                self._save_peer_to_cache(
                    info.node_id,
                    peer_host,
                    peer_port,
                    str(getattr(info, "tailscale_ip", "") or "")
                )
                self._update_peer_reputation(info.node_id, success=True)

                logger.debug(f"[P2P] SSH fallback heartbeat successful to {node_id}")
                return info

        except Exception as e:  # noqa: BLE001
            logger.debug(f"[P2P] SSH fallback heartbeat failed for {node_id}: {e}")

        return None

    def _find_node_id_for_host(self, host: str) -> str | None:
        """Find node_id for a given host address.

        Dec 2025: Looks up node_id from cluster_hosts.yaml or peer cache.

        Args:
            host: Hostname or IP address

        Returns:
            node_id if found, None otherwise
        """
        # Check peers first
        with self.peers_lock:
            for peer in self.peers.values():
                if peer.host == host or getattr(peer, "tailscale_ip", None) == host:
                    return peer.node_id

        # Check cluster_hosts.yaml
        try:
            from app.sync.cluster_hosts import get_cluster_nodes
            configured_hosts = get_cluster_nodes()
            for name, cfg in configured_hosts.items():
                if cfg.tailscale_ip == host or cfg.ssh_host == host or cfg.best_ip == host:
                    return name
        except (AttributeError, ImportError):
            pass

        return None

    async def _bootstrap_from_known_peers(self) -> bool:
        """Import cluster membership from seed peers via `/relay/peers`.

        Heartbeats intentionally return only a single peer's NodeInfo, which
        makes initial convergence slow when only one seed peer is configured
        (common for cloud nodes). `/relay/peers` returns a snapshot of the
        sender's full peer list, allowing new nodes to quickly learn about the
        leader and other cluster members.
        """
        # Seed peers are configured via `--peers`, but relying on a single
        # coordinator makes clusters brittle. Also bootstrap from any
        # previously-seen directly-reachable peers so nodes can re-join after
        # restarts even if the original seed goes offline.
        known_seed_peers: list[str] = [p for p in (self.known_peers or []) if p]
        discovered_seed_peers: list[str] = []

        with self.peers_lock:
            peers_snapshot = [p for p in self.peers.values() if p.node_id != self.node_id]
        peers_snapshot.sort(key=lambda p: str(getattr(p, "node_id", "") or ""))

        for peer in peers_snapshot:
            if getattr(peer, "nat_blocked", False):
                # NAT-blocked nodes cannot serve as inbound seeds.
                continue
            if not peer.should_retry():
                continue

            scheme = (getattr(peer, "scheme", "http") or "http").lower()
            host = str(getattr(peer, "host", "") or "").strip()
            try:
                port = int(getattr(peer, "port", DEFAULT_PORT) or DEFAULT_PORT)
            except (ValueError):
                port = DEFAULT_PORT
            if host:
                discovered_seed_peers.append(f"{scheme}://{host}:{port}")

            rh = str(getattr(peer, "reported_host", "") or "").strip()
            try:
                rp = int(getattr(peer, "reported_port", 0) or 0)
            except (ValueError):
                rp = 0
            if rh and rp:
                discovered_seed_peers.append(f"{scheme}://{rh}:{rp}")

        seen: set[str] = set()
        seed_peers: list[str] = []
        ki = 0
        di = 0
        while ki < len(known_seed_peers) or di < len(discovered_seed_peers):
            if ki < len(known_seed_peers):
                candidate = known_seed_peers[ki]
                ki += 1
                if candidate and candidate not in seen:
                    seen.add(candidate)
                    seed_peers.append(candidate)
            if di < len(discovered_seed_peers):
                candidate = discovered_seed_peers[di]
                di += 1
                if candidate and candidate not in seen:
                    seen.add(candidate)
                    seed_peers.append(candidate)
        if not seed_peers:
            return False

        now = time.time()
        if now - self.last_peer_bootstrap < PEER_BOOTSTRAP_INTERVAL:
            return False

        max_seeds = int(os.environ.get("RINGRIFT_P2P_BOOTSTRAP_MAX_SEEDS_PER_RUN", "8") or 8)
        max_seeds = max(1, min(max_seeds, 32))

        timeout = ClientTimeout(total=8)
        bootstrapped = False
        imported_any = False

        async with get_client_session(timeout) as session:
            for idx, peer_addr in enumerate(seed_peers):
                if idx >= max_seeds:
                    break
                try:
                    scheme, host, port = self._parse_peer_address(peer_addr)
                    scheme = (scheme or "http").lower()
                    url = f"{scheme}://{host}:{port}/relay/peers"
                    async with session.get(url, headers=self._auth_headers()) as resp:
                        if resp.status != 200:
                            continue
                        data = await resp.json()

                    if not isinstance(data, dict) or not data.get("success"):
                        continue

                    bootstrapped = True

                    incoming_voters = data.get("voter_node_ids") or data.get("voters") or None
                    if incoming_voters:
                        voters_list: list[str] = []
                        if isinstance(incoming_voters, list):
                            voters_list = [str(v).strip() for v in incoming_voters if str(v).strip()]
                        elif isinstance(incoming_voters, str):
                            voters_list = [t.strip() for t in incoming_voters.split(",") if t.strip()]
                        if voters_list:
                            self._maybe_adopt_voter_node_ids(voters_list, source="learned")

                    peers_data = data.get("peers") or {}
                    if not isinstance(peers_data, dict):
                        continue

                    with self.peers_lock:
                        before = len(self.peers)
                        for node_id, peer_dict in peers_data.items():
                            if not node_id or node_id == self.node_id:
                                continue
                            try:
                                info = NodeInfo.from_dict(peer_dict)
                            except (AttributeError):
                                continue
                            existing = self.peers.get(info.node_id)
                            if existing:
                                # Preserve relay/NAT routing and retirement state when merging peer snapshots.
                                if getattr(existing, "nat_blocked", False) and not getattr(info, "nat_blocked", False):
                                    info.nat_blocked = True
                                    info.nat_blocked_since = float(getattr(existing, "nat_blocked_since", 0.0) or 0.0) or time.time()
                                    info.last_nat_probe = float(getattr(existing, "last_nat_probe", 0.0) or 0.0)
                                if (getattr(existing, "relay_via", "") or "") and not (getattr(info, "relay_via", "") or ""):
                                    info.relay_via = str(getattr(existing, "relay_via", "") or "")
                                if getattr(existing, "retired", False):
                                    info.retired = True
                                    info.retired_at = float(getattr(existing, "retired_at", 0.0) or 0.0)
                                # Preserve local reachability diagnostics.
                                info.consecutive_failures = int(getattr(existing, "consecutive_failures", 0) or 0)
                                info.last_failure_time = float(getattr(existing, "last_failure_time", 0.0) or 0.0)

                            self.peers[info.node_id] = info
                        after = len(self.peers)

                    new = max(0, after - before)
                    if new:
                        imported_any = True
                        logger.info(f"Bootstrap: imported {new} new peers from {host}:{port}")

                    leader_id = str(data.get("leader_id") or "").strip()
                    if leader_id and leader_id != self.node_id:
                        # If we're currently leader but the seed reports a higher-priority
                        # leader, step down to converge quickly.
                        if self.role == NodeRole.LEADER and leader_id > self.node_id:
                            logger.info(f"Bootstrap: stepping down for leader {leader_id}")
                            self.role = NodeRole.FOLLOWER
                        self.leader_id = leader_id
                except (aiohttp.ClientError, asyncio.TimeoutError, json.JSONDecodeError, KeyError, ValueError):
                    continue

        self.last_peer_bootstrap = now
        if bootstrapped:
            self._maybe_adopt_leader_from_peers()
            self._save_state()
        return imported_any

    async def _continuous_bootstrap_loop(self) -> None:
        """Phase 26.3: Continuously attempt to join cluster when isolated.

        This loop runs on ALL nodes (not just leader) and ensures that
        isolated nodes can rejoin the cluster without manual intervention.

        Triggers when:
        - Less than MIN_CONNECTED_PEERS alive peers
        - No leader known

        Uses multi-seed bootstrap with:
        1. Cached peers (highest reputation first)
        2. CLI-provided peers
        3. Hardcoded BOOTSTRAP_SEEDS
        4. Tailscale network scan (fallback)
        """
        # Dec 30, 2025: Conditional startup grace period
        # - If --peers provided: Wait STARTUP_GRACE_PERIOD for other nodes to restart
        # - If no --peers: Start immediately (we're already isolated, no point waiting)
        # This enables fast mesh join when nodes start without explicit peer list.
        if self.known_peers:
            logger.debug(f"[ContinuousBootstrap] Waiting {STARTUP_GRACE_PERIOD}s grace period (have known peers)")
            await asyncio.sleep(STARTUP_GRACE_PERIOD)
        else:
            # No peers configured - skip grace period but wait briefly for HTTP server
            logger.info("[ContinuousBootstrap] No known peers, skipping startup grace period")
            await asyncio.sleep(5)

        while self.running:
            try:
                await asyncio.sleep(ISOLATED_BOOTSTRAP_INTERVAL)

                # Count alive peers
                with self.peers_lock:
                    peers_alive = sum(
                        1 for p in self.peers.values()
                        if p.node_id != self.node_id and p.is_alive()
                    )

                # Check if we're isolated (few peers or no leader)
                is_isolated = peers_alive < MIN_CONNECTED_PEERS
                no_leader = self.leader_id is None or (
                    self.leader_id != self.node_id and
                    self.leader_id not in self.peers
                )

                if is_isolated or no_leader:
                    if is_isolated:
                        logger.warning(
                            f"Isolated: only {peers_alive} alive peers "
                            f"(need {MIN_CONNECTED_PEERS}), attempting bootstrap..."
                        )
                    elif no_leader:
                        logger.warning(
                            f"No valid leader (current: {self.leader_id}), "
                            f"attempting bootstrap..."
                        )

                    # Try bootstrap from multiple sources
                    bootstrapped = await self._bootstrap_from_multiple_seeds()

                    if bootstrapped:
                        logger.info(
                            f"Bootstrap successful! "
                            f"Now have {len([p for p in self.peers.values() if p.is_alive()])} alive peers"
                        )
                        # Try to adopt leader from newly discovered peers
                        self._maybe_adopt_leader_from_peers()

                        # If still no leader, start election
                        if not self.leader_id:
                            # CRITICAL: Check quorum before starting election to prevent quorum bypass
                            if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                                logger.warning("Skipping election after bootstrap: no voter quorum available")
                            else:
                                await self._start_election()
                    else:
                        # Fallback: try Tailscale peer discovery
                        logger.info("Bootstrap from seeds failed, trying Tailscale discovery...")
                        await self._discover_tailscale_peers()

            except asyncio.CancelledError:
                break
            except Exception as e:  # noqa: BLE001
                logger.error(f"Error in continuous bootstrap loop: {e}")
                await asyncio.sleep(30)  # Back off on errors

    async def _bootstrap_from_multiple_seeds(self) -> bool:
        """Phase 26.3: Try multiple seeds until we join the cluster.

        Priority order:
        1. Cached peers with high reputation (from peer_cache table)
        2. CLI --peers (self.known_peers)
        3. Hardcoded BOOTSTRAP_SEEDS

        Returns True if we successfully connected to any peer.
        """
        # Build seed list with priority ordering
        all_seeds: list[str] = []
        seen: set[str] = set()

        # 1. First, try cached peers by reputation (if available)
        cached_peers = self._get_bootstrap_peers_by_reputation(limit=3)
        for seed in cached_peers:
            if seed and seed not in seen:
                seen.add(seed)
                all_seeds.append(seed)

        # 2. Then, CLI peers and hardcoded seeds (already merged in self.known_peers)
        for seed in self.known_peers:
            if seed and seed not in seen:
                seen.add(seed)
                all_seeds.append(seed)

        if not all_seeds:
            logger.warning("No bootstrap seeds available")
            return False

        # Limit attempts per cycle
        max_attempts = min(MIN_BOOTSTRAP_ATTEMPTS * 2, len(all_seeds))
        timeout = ClientTimeout(total=10)
        success = False

        async with get_client_session(timeout) as session:
            for idx, seed_addr in enumerate(all_seeds[:max_attempts]):
                try:
                    scheme, host, port = self._parse_peer_address(seed_addr)
                    scheme = (scheme or "http").lower()
                    url = f"{scheme}://{host}:{port}/relay/peers"

                    async with session.get(url, headers=self._auth_headers()) as resp:
                        if resp.status != 200:
                            self._update_peer_reputation(seed_addr, success=False)
                            continue

                        data = await resp.json()
                        if not isinstance(data, dict) or not data.get("success"):
                            self._update_peer_reputation(seed_addr, success=False)
                            continue

                    # Successfully got peer list
                    self._update_peer_reputation(seed_addr, success=True)
                    success = True

                    # Import peers
                    peers_data = data.get("peers") or {}
                    if isinstance(peers_data, dict):
                        with self.peers_lock:
                            for node_id, peer_dict in peers_data.items():
                                if node_id and node_id != self.node_id:
                                    try:
                                        info = NodeInfo.from_dict(peer_dict)
                                        self.peers[info.node_id] = info
                                        # Cache the peer for future restarts
                                        self._save_peer_to_cache(
                                            info.node_id,
                                            str(getattr(info, "host", "") or ""),
                                            int(getattr(info, "port", DEFAULT_PORT) or DEFAULT_PORT),
                                            str(getattr(info, "tailscale_ip", "") or "")
                                        )
                                    except (ValueError, KeyError, IndexError, AttributeError):
                                        continue

                    # Adopt leader if provided
                    leader_id = str(data.get("leader_id") or "").strip()
                    if leader_id and leader_id != self.node_id:
                        self.leader_id = leader_id
                        if self.role == NodeRole.LEADER:
                            logger.info(f"Stepping down for discovered leader: {leader_id}")
                            self.role = NodeRole.FOLLOWER

                    # Handle cluster epoch (Phase 29)
                    incoming_epoch = data.get("cluster_epoch")
                    if incoming_epoch is not None:
                        try:
                            epoch = int(incoming_epoch)
                            if epoch > self._cluster_epoch:
                                logger.info(f"Adopting higher cluster epoch: {epoch} (was {self._cluster_epoch})")
                                self._cluster_epoch = epoch
                                self._save_cluster_epoch()
                        except (ValueError, TypeError):
                            pass

                    # Import voter config if provided
                    incoming_voters = data.get("voter_node_ids") or data.get("voters")
                    if incoming_voters:
                        voters_list = []
                        if isinstance(incoming_voters, list):
                            voters_list = [str(v).strip() for v in incoming_voters if str(v).strip()]
                        elif isinstance(incoming_voters, str):
                            voters_list = [t.strip() for t in incoming_voters.split(",") if t.strip()]
                        if voters_list:
                            self._maybe_adopt_voter_node_ids(voters_list, source="learned")

                    self._save_state()
                    logger.info(f"Bootstrap from {host}:{port}: imported {len(peers_data)} peers")
                    break  # Success, no need to try more seeds

                except asyncio.TimeoutError:
                    self._update_peer_reputation(seed_addr, success=False)
                    continue
                except Exception as e:  # noqa: BLE001
                    self._update_peer_reputation(seed_addr, success=False)
                    if self.verbose:
                        logger.debug(f"Bootstrap seed {seed_addr} failed: {e}")
                    continue

        return success

    def _load_bootstrap_seeds_from_config(self) -> list[str]:
        """Load bootstrap seed peers from distributed_hosts.yaml.

        Selects stable coordinator and voter nodes as default seeds when no --peers provided.
        This enables automatic peer discovery via Tailscale even when CLI args are missing.

        Returns:
            List of seed peer URLs (e.g., ["http://100.x.x.x:8770", ...])

        December 30, 2025: Added for automatic P2P peer discovery.
        """
        try:
            from app.config.cluster_config import get_cluster_nodes, get_coordinator_node

            seeds: list[str] = []
            seen_ips: set[str] = set()

            # Primary: coordinator node (most stable)
            coord = get_coordinator_node()
            if coord and getattr(coord, "tailscale_ip", None):
                ip = str(coord.tailscale_ip)
                if ip and ip not in seen_ips:
                    seeds.append(f"http://{ip}:{DEFAULT_PORT}")
                    seen_ips.add(ip)

            # Secondary: voter nodes (stable, always online)
            try:
                nodes = get_cluster_nodes()
                for node in nodes.values():
                    if getattr(node, "role", "") == "voter" and getattr(node, "tailscale_ip", None):
                        ip = str(node.tailscale_ip)
                        if ip and ip not in seen_ips:
                            seeds.append(f"http://{ip}:{DEFAULT_PORT}")
                            seen_ips.add(ip)
                            if len(seeds) >= 5:
                                break
            except Exception:  # noqa: BLE001
                pass

            # Fallback: any active nodes with Tailscale IPs
            if len(seeds) < 3:
                try:
                    nodes = get_cluster_nodes()
                    for node in nodes.values():
                        if getattr(node, "tailscale_ip", None) and getattr(node, "is_active", True):
                            ip = str(node.tailscale_ip)
                            if ip and ip not in seen_ips:
                                seeds.append(f"http://{ip}:{DEFAULT_PORT}")
                                seen_ips.add(ip)
                                if len(seeds) >= 5:
                                    break
                except Exception:  # noqa: BLE001
                    pass

            if seeds:
                logger.debug(f"Loaded {len(seeds)} bootstrap seeds from config: {seeds[:3]}...")

            return seeds

        except ImportError:
            logger.debug("cluster_config not available for bootstrap seeds")
            return []
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Could not load bootstrap seeds from config: {e}")
            return []

    def _load_distributed_hosts(self) -> dict[str, Any]:
        """Load distributed hosts configuration for NetworkHealthMixin.

        Required by NetworkHealthMixin for cross-verifying P2P mesh health
        against Tailscale connectivity.

        Returns:
            Dict with structure: {"hosts": {node_name: {config...}}}
            Each host config includes: tailscale_ip, p2p_enabled, p2p_port, etc.

        December 30, 2025: Added to fix /network/health endpoint.
        """
        try:
            from app.config.cluster_config import load_cluster_config

            config = load_cluster_config()
            hosts_raw = getattr(config, "hosts_raw", {})

            # Convert to the format expected by NetworkHealthMixin
            # hosts_raw already has the right structure: {node_name: {config_dict}}
            return {"hosts": hosts_raw}

        except ImportError:
            logger.debug("cluster_config not available for distributed hosts")
            return {"hosts": {}}
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Could not load distributed hosts: {e}")
            return {"hosts": {}}

    # NOTE: _follower_discovery_loop() removed Dec 2025 (75 LOC).
    # Now runs via LoopManager as FollowerDiscoveryLoop.
    # See scripts/p2p/loops/discovery_loop.py for implementation.
    # The loop uses callbacks: get_known_peers, query_peer_list, add_peer, is_leader.

    async def _send_relay_heartbeat(self, relay_url: str) -> dict[str, Any]:
        """Send heartbeat via relay endpoint for NAT-blocked nodes.

        This is used when the peer URL is HTTPS (indicating a relay/proxy endpoint)
        or when direct heartbeats fail consistently.

        Returns dict with:
        - success: bool
        - peers: dict of all cluster peers
        - leader_id: current leader
        """
        try:
            self._update_self_info()

            timeout = ClientTimeout(total=15)
            async with get_client_session(timeout) as session:
                # Use /relay/heartbeat endpoint
                url = f"{relay_url.rstrip('/')}/relay/heartbeat"
                payload = self.self_info.to_dict()
                if self.pending_relay_acks:
                    payload["relay_ack"] = sorted(self.pending_relay_acks)
                if self.pending_relay_results:
                    payload["relay_results"] = list(self.pending_relay_results)
                async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                    if resp.status != 200:
                        return {"success": False, "error": f"HTTP {resp.status}"}

                    data = await resp.json()
                    if not data.get("success"):
                        return {"success": False, "error": data.get("error", "Unknown error")}

                    incoming_voters = data.get("voter_node_ids") or data.get("voters") or None
                    if incoming_voters:
                        voters_list: list[str] = []
                        if isinstance(incoming_voters, list):
                            voters_list = [str(v).strip() for v in incoming_voters if str(v).strip()]
                        elif isinstance(incoming_voters, str):
                            voters_list = [t.strip() for t in incoming_voters.split(",") if t.strip()]
                        if voters_list:
                            self._maybe_adopt_voter_node_ids(voters_list, source="learned")

                    # Clear pending acks/results only after a successful round-trip.
                    self.pending_relay_acks.clear()
                    self.pending_relay_results.clear()

                    # Update our peer list with all peers from relay
                    peers_data = data.get("peers", {})
                    with self.peers_lock:
                        for node_id, peer_dict in peers_data.items():
                            if node_id != self.node_id:
                                peer_info = NodeInfo.from_dict(peer_dict)
                                existing = self.peers.get(node_id)
                                if existing:
                                    if getattr(existing, "nat_blocked", False) and not getattr(peer_info, "nat_blocked", False):
                                        peer_info.nat_blocked = True
                                        peer_info.nat_blocked_since = float(getattr(existing, "nat_blocked_since", 0.0) or 0.0) or time.time()
                                        peer_info.last_nat_probe = float(getattr(existing, "last_nat_probe", 0.0) or 0.0)
                                    if (getattr(existing, "relay_via", "") or "") and not (getattr(peer_info, "relay_via", "") or ""):
                                        peer_info.relay_via = str(getattr(existing, "relay_via", "") or "")
                                    if getattr(existing, "retired", False):
                                        peer_info.retired = True
                                        peer_info.retired_at = float(getattr(existing, "retired_at", 0.0) or 0.0)
                                    peer_info.consecutive_failures = int(getattr(existing, "consecutive_failures", 0) or 0)
                                    peer_info.last_failure_time = float(getattr(existing, "last_failure_time", 0.0) or 0.0)
                                self.peers[node_id] = peer_info

                    # Execute any queued commands addressed to us.
                    commands = data.get("commands") or []
                    if isinstance(commands, list) and commands:
                        await self._execute_relay_commands(commands)

                    # Update leader if provided
                    leader_id = data.get("leader_id")
                    if leader_id and leader_id != self.node_id:
                        if self.leader_id != leader_id:
                            logger.info(f"Adopted leader from relay: {leader_id}")
                        self.leader_id = leader_id
                        self.role = NodeRole.FOLLOWER

                    return {
                        "success": True,
                        "peers_received": len(peers_data) if isinstance(peers_data, dict) else 0,
                        "leader_id": leader_id,
                        "commands_received": len(commands) if isinstance(commands, list) else 0,
                    }
        except Exception as e:  # noqa: BLE001
            return {"success": False, "error": str(e)}

    async def _execute_relay_commands(self, commands: list[dict[str, Any]]) -> None:
        """Execute relay commands (polling mode for NAT-blocked nodes)."""
        now = time.time()
        for cmd in commands:
            try:
                cmd_id = str(cmd.get("id") or "")
                cmd_type = str(cmd.get("type") or "")
                payload = cmd.get("payload") or {}
                if not cmd_id or not cmd_type:
                    continue

                # Check for stale commands (>5 min old indicates relay/polling issues)
                cmd_ts = cmd.get("ts") or cmd.get("timestamp") or now
                cmd_age_secs = now - float(cmd_ts)
                if cmd_age_secs > 300:
                    logger.info(f"WARNING: Relay command {cmd_id} ({cmd_type}) is {cmd_age_secs:.0f}s old - relay delivery may be delayed")

                attempts = int(self.relay_command_attempts.get(cmd_id, 0) or 0) + 1
                self.relay_command_attempts[cmd_id] = attempts

                ok = False
                err = ""
                if cmd_type == "start_job":
                    job_type = JobType(str(payload.get("job_type") or "selfplay"))
                    board_type = str(payload.get("board_type") or "square8")
                    num_players = int(payload.get("num_players") or 2)
                    engine_mode = str(payload.get("engine_mode") or "mixed")
                    job_id = str(payload.get("job_id") or "")

                    if job_id:
                        with self.jobs_lock:
                            existing = self.local_jobs.get(job_id)
                        if existing and existing.status == "running":
                            ok = True
                        else:
                            job = await self._start_local_job(
                                job_type,
                                board_type=board_type,
                                num_players=num_players,
                                engine_mode=engine_mode,
                                job_id=job_id,
                            )
                            ok = job is not None
                    else:
                        job = await self._start_local_job(
                            job_type,
                            board_type=board_type,
                            num_players=num_players,
                            engine_mode=engine_mode,
                        )
                        ok = job is not None
                elif cmd_type == "cleanup":
                    asyncio.create_task(self._cleanup_local_disk())
                    ok = True
                elif cmd_type == "restart_stuck_jobs":
                    asyncio.create_task(self._restart_local_stuck_jobs())
                    ok = True
                elif cmd_type == "reduce_selfplay":
                    target = payload.get("target_selfplay_jobs", payload.get("target", 0))
                    reason = str(payload.get("reason") or "relay")
                    try:
                        target_jobs = int(target)
                    except (ValueError):
                        target_jobs = 0
                    await self._reduce_local_selfplay_jobs(target_jobs, reason=reason)
                    ok = True
                elif cmd_type == "cleanup_files":
                    files = payload.get("files", []) or []
                    reason = str(payload.get("reason") or "relay")
                    if not isinstance(files, list) or not files:
                        ok = False
                        err = "no_files"
                    else:
                        data_dir = self.get_data_directory()
                        freed_bytes = 0
                        deleted_count = 0
                        data_root = data_dir.resolve()
                        for file_path in files:
                            full_path = data_dir / (str(file_path or "").lstrip("/"))
                            try:
                                resolved = full_path.resolve()
                                resolved.relative_to(data_root)
                            except (AttributeError):
                                continue
                            if not resolved.exists():
                                continue
                            try:
                                size = resolved.stat().st_size
                                resolved.unlink()
                                freed_bytes += size
                                deleted_count += 1
                            except (AttributeError):
                                continue
                        print(
                            f"[P2P] Relay cleanup_files: {deleted_count} files deleted, "
                            f"{freed_bytes / 1e6:.1f}MB freed (reason={reason})"
                        )
                        ok = True
                elif cmd_type == "canonical_selfplay":
                    job_id = str(payload.get("job_id") or "")
                    board_type = str(payload.get("board_type") or "square8")
                    num_players = int(payload.get("num_players") or 2)
                    num_games = int(payload.get("num_games") or payload.get("games_per_node") or 500)
                    seed = int(payload.get("seed") or 0)
                    if not job_id:
                        ok = False
                        err = "missing_job_id"
                    else:
                        asyncio.create_task(
                            self._run_local_canonical_selfplay(job_id, board_type, num_players, num_games, seed)
                        )
                        ok = True
                else:
                    ok = False
                    err = f"unknown_command_type:{cmd_type}"

                if ok:
                    self._add_pending_relay_ack(cmd_id)
                    self._add_pending_relay_result({"id": cmd_id, "ok": True})
                    self.relay_command_attempts.pop(cmd_id, None)
                else:
                    if not err:
                        err = "command_failed"
                    if attempts >= RELAY_COMMAND_MAX_ATTEMPTS:
                        self._add_pending_relay_ack(cmd_id)
                        self._add_pending_relay_result({"id": cmd_id, "ok": False, "error": err})
                        self.relay_command_attempts.pop(cmd_id, None)
            except Exception as exc:
                try:
                    cmd_id = str(cmd.get("id") or "")
                    if cmd_id:
                        attempts = int(self.relay_command_attempts.get(cmd_id, 0) or 0)
                        if attempts >= RELAY_COMMAND_MAX_ATTEMPTS:
                            self._add_pending_relay_ack(cmd_id)
                            self._add_pending_relay_result({"id": cmd_id, "ok": False, "error": str(exc)})
                            self.relay_command_attempts.pop(cmd_id, None)
                except (ValueError, AttributeError):
                    continue

    async def _heartbeat_loop(self):
        """Send heartbeats to all known peers."""
        while self.running:
            try:
                # Send to known peers from config
                for peer_addr in self.known_peers:
                    try:
                        scheme, host, port = self._parse_peer_address(peer_addr)
                    except (AttributeError):
                        continue

                    # Use relay heartbeat for HTTPS endpoints (they're proxies/relays)
                    # or for explicitly configured relay peers (--relay-peers flag)
                    use_relay = scheme == "https" or peer_addr in self.relay_peers
                    if use_relay:
                        # Relay/proxy endpoint, use relay heartbeat
                        relay_url = f"{scheme}://{host}" if port in (80, 443) else f"{scheme}://{host}:{port}"
                        result = await self._send_relay_heartbeat(relay_url)
                        if result.get("success"):
                            # Relay heartbeat already updates peers and leader
                            continue

                    info = await self._send_heartbeat_to_peer(host, port, scheme=scheme)
                    if info:
                        if info.node_id == self.node_id:
                            continue
                        # Dec 2025: Track first-contact for HOST_ONLINE emission
                        async with AsyncLockWrapper(self.peers_lock):
                            is_first_contact = info.node_id not in self.peers
                            info.last_heartbeat = time.time()
                            self.peers[info.node_id] = info
                        # Dec 2025: Emit HOST_ONLINE for newly discovered peers
                        if is_first_contact:
                            capabilities = []
                            if getattr(info, "has_gpu", False):
                                gpu_type = getattr(info, "gpu_type", "") or "gpu"
                                capabilities.append(gpu_type)
                            else:
                                capabilities.append("cpu")
                            await self._emit_host_online(info.node_id, capabilities)
                            logger.info(f"First-contact peer via heartbeat loop: {info.node_id}")
                        if info.role == NodeRole.LEADER and info.node_id != self.node_id:
                            async with AsyncLockWrapper(self.peers_lock):
                                peers_snapshot = list(self.peers.values())
                            conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])
                            if not self._is_leader_eligible(info, conflict_keys, require_alive=False):
                                continue
                            if self.role == NodeRole.LEADER and info.node_id <= self.node_id:
                                continue
                            if (
                                self.leader_id
                                and self.leader_id != info.node_id
                                and self._is_leader_lease_valid()
                                and info.node_id <= self.leader_id
                            ):
                                continue
                            if self.leader_id != info.node_id or self.role != NodeRole.FOLLOWER:
                                logger.info(f"Following configured leader from heartbeat: {info.node_id}")
                            prev_leader = self.leader_id
                            self.leader_id = info.node_id
                            # Provisional lease: allow time for the leader to send
                            # a /coordinator lease renewal after we discover it via
                            # heartbeat (prevents leaderless oscillation right after
                            # restarts/partitions).
                            if prev_leader != info.node_id or not self._is_leader_lease_valid():
                                self.leader_lease_id = ""
                                self.leader_lease_expires = time.time() + LEADER_LEASE_DURATION
                            self.role = NodeRole.FOLLOWER

                # Send to discovered peers (skip NAT-blocked peers and ambiguous endpoints).
                async with AsyncLockWrapper(self.peers_lock):
                    peers_snapshot = list(self.peers.values())
                conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])
                peer_list = [
                    p for p in peers_snapshot
                    if (
                        not p.nat_blocked
                        and self._endpoint_key(p) not in conflict_keys
                    )
                ]

                for peer in peer_list:
                    if peer.node_id != self.node_id:
                        if not peer.should_retry():
                            continue
                        peer_scheme = getattr(peer, "scheme", "http") or "http"
                        info = await self._send_heartbeat_to_peer(peer.host, peer.port, scheme=peer_scheme)
                        if not info and getattr(peer, "reported_host", "") and getattr(peer, "reported_port", 0):
                            # Multi-path retry: fall back to self-reported endpoint when the
                            # observed reachable endpoint fails (e.g., mixed overlays).
                            try:
                                rh = str(getattr(peer, "reported_host", "") or "").strip()
                                rp = int(getattr(peer, "reported_port", 0) or 0)
                            except (ValueError, AttributeError):
                                rh, rp = "", 0
                            if rh and rp and (rh != peer.host or rp != peer.port):
                                info = await self._send_heartbeat_to_peer(rh, rp, scheme=peer_scheme)
                        # Self-healing: Tailscale IP fallback when both primary and reported fail
                        if not info:
                            ts_ip = self._get_tailscale_ip_for_peer(peer.node_id)
                            if ts_ip and ts_ip != peer.host:
                                # Try Tailscale mesh IP (100.x.x.x)
                                info = await self._send_heartbeat_to_peer(ts_ip, peer.port, scheme=peer_scheme)
                                if info:
                                    logger.info(f"Reached {peer.node_id} via Tailscale ({ts_ip})")
                        if info:
                            info.consecutive_failures = 0
                            info.last_failure_time = 0.0
                            async with AsyncLockWrapper(self.peers_lock):
                                info.last_heartbeat = time.time()
                                self.peers[info.node_id] = info
                            if info.role == NodeRole.LEADER and self.role != NodeRole.LEADER:
                                if not self._is_leader_eligible(info, conflict_keys, require_alive=False):
                                    continue
                                if (
                                    self.leader_id
                                    and self.leader_id != info.node_id
                                    and self._is_leader_lease_valid()
                                    and info.node_id <= self.leader_id
                                ):
                                    continue
                                if self.leader_id != info.node_id:
                                    logger.info(f"Adopted leader from heartbeat: {info.node_id}")
                                prev_leader = self.leader_id
                                self.leader_id = info.node_id
                                if prev_leader != info.node_id or not self._is_leader_lease_valid():
                                    self.leader_lease_id = ""
                                    self.leader_lease_expires = time.time() + LEADER_LEASE_DURATION
                                self.role = NodeRole.FOLLOWER
                        else:
                            async with AsyncLockWrapper(self.peers_lock):
                                existing = self.peers.get(peer.node_id)
                                if existing:
                                    existing.consecutive_failures = int(getattr(existing, "consecutive_failures", 0) or 0) + 1
                                    existing.last_failure_time = time.time()

                # If we're only connected to a seed peer (or lost cluster membership),
                # pull a fresh peer snapshot so leader election converges quickly.
                await self._bootstrap_from_known_peers()

                # Get current time for all time-based checks in this cycle
                now = time.time()

                # NAT-blocked nodes: poll a relay endpoint for peer snapshots + commands.
                if getattr(self.self_info, "nat_blocked", False):
                    if now - self.last_relay_heartbeat >= RELAY_HEARTBEAT_INTERVAL:
                        relay_urls: list[str] = []
                        leader_peer = self._get_leader_peer()
                        if leader_peer and leader_peer.node_id != self.node_id:
                            relay_urls.append(f"{leader_peer.scheme}://{leader_peer.host}:{leader_peer.port}")
                        for peer_addr in self.known_peers:
                            try:
                                scheme, host, port = self._parse_peer_address(peer_addr)
                            except (AttributeError):
                                continue
                            relay_urls.append(f"{scheme}://{host}:{port}")
                        seen: set[str] = set()
                        relay_urls = [u for u in relay_urls if not (u in seen or seen.add(u))]

                        for relay_url in relay_urls:
                            result = await self._send_relay_heartbeat(relay_url)
                            if result.get("success"):
                                self.last_relay_heartbeat = now
                                break

                # Check for dead peers
                await self._check_dead_peers_async()

                # Dec 30, 2025: Probe retired peers periodically to detect recovery
                # This runs every PEER_RECOVERY_RETRY_INTERVAL (120s) to actively probe
                # retired nodes that may have come back online after cluster restart.
                last_probe = getattr(self, "_last_retired_probe", 0.0)
                if now - last_probe >= PEER_RECOVERY_RETRY_INTERVAL:
                    self._last_retired_probe = now
                    try:
                        await self._probe_retired_peers_async()
                    except Exception as e:
                        logger.warning(f"Error in retired peer probe: {e}")

                # Self-healing: detect network partition and trigger Tailscale-priority mode
                if self._detect_network_partition():
                    self._enable_tailscale_priority()
                    # Also enable partition-local election if no voters reachable
                    if not self._has_voter_quorum():
                        self._enable_partition_local_election()
                    # Force refresh all IP sources to discover alternative paths
                    last_refresh = getattr(self, "_last_partition_ip_refresh", 0)
                    if time.time() - last_refresh > 60:  # Refresh at most once per minute
                        self._last_partition_ip_refresh = time.time()
                        asyncio.create_task(self._force_ip_refresh_all_sources())
                elif getattr(self, "_tailscale_priority", False):
                    # Check if priority mode should expire
                    if time.time() > getattr(self, "_tailscale_priority_until", 0):
                        # Check if connectivity recovered
                        alive_count = sum(1 for p in self.peers.values() if p.is_alive() and p.node_id != self.node_id)
                        if alive_count > 0:
                            self._disable_tailscale_priority()

                # Self-healing: check if partition healed and restore original voters
                if hasattr(self, "_original_voters"):
                    self._restore_original_voters()

                # Dynamic voter management: promote/demote voters based on health
                # Only the leader manages voters to ensure consistency
                if self.role == NodeRole.LEADER:
                    self._manage_dynamic_voters()

                # Health-based leadership: step down if we can't reach enough peers
                if self.role == NodeRole.LEADER and not self._check_leader_health():
                    logger.info("Stepping down due to degraded health")
                    self.role = NodeRole.FOLLOWER
                    self.leader_id = None
                    self.leader_lease_id = ""
                    self.leader_lease_expires = 0.0
                    self._release_voter_grant_if_self()
                    self._save_state()
                    continue  # Skip leader duties this cycle

                # P0 Dec 2025: Monitor leader heartbeat for early warning
                # Emit LEADER_HEARTBEAT_MISSING if leader lease is approaching expiry
                if self.role == NodeRole.FOLLOWER and self.leader_id:
                    now = time.time()
                    # Warning threshold: 45 seconds (3x lease renewal interval)
                    heartbeat_warning_threshold = LEADER_LEASE_RENEW_INTERVAL * 3
                    time_until_expiry = self.leader_lease_expires - now
                    # Emit warning if lease will expire within warning threshold
                    if 0 < time_until_expiry < heartbeat_warning_threshold:
                        last_warning = getattr(self, "_last_heartbeat_missing_warning", 0.0)
                        # Only warn once per 30 seconds to avoid spam
                        if now - last_warning > 30:
                            self._last_heartbeat_missing_warning = now
                            delay_seconds = (LEADER_LEASE_DURATION - time_until_expiry)
                            try:
                                from app.distributed.data_events import emit_leader_heartbeat_missing
                                asyncio.create_task(emit_leader_heartbeat_missing(
                                    leader_id=self.leader_id,
                                    last_heartbeat=self.leader_lease_expires - LEADER_LEASE_DURATION,
                                    expected_interval=LEADER_LEASE_RENEW_INTERVAL,
                                    delay_seconds=delay_seconds,
                                    source=self.node_id,
                                ))
                            except ImportError:
                                pass  # Graceful degradation if event system not available

                # LEARNED LESSONS - Lease renewal to maintain leadership
                if self.role == NodeRole.LEADER:
                    await self._renew_leader_lease()

                # P2P monitoring: start/stop services based on leadership
                await self._stop_monitoring_if_not_leader()
                if self.role == NodeRole.LEADER:
                    await self._start_monitoring_if_leader()

                # P2P auto-deployer: start/stop based on leadership
                if self.role != NodeRole.LEADER and self._auto_deployer_task:
                    await self._stop_p2p_auto_deployer()
                elif self.role == NodeRole.LEADER and not self._auto_deployer_task:
                    await self._start_p2p_auto_deployer()

                # Report node resources to resource_optimizer for cluster-wide utilization tracking
                # This enables cooperative 60-80% utilization targeting across orchestrators
                if HAS_NEW_COORDINATION and get_resource_optimizer is not None:
                    try:
                        optimizer = get_resource_optimizer()
                        self._update_self_info()
                        node_resources = NodeResources(
                            node_id=self.node_id,
                            cpu_percent=self.self_info.cpu_percent,
                            memory_percent=self.self_info.memory_percent,
                            active_jobs=self.self_info.selfplay_jobs + self.self_info.training_jobs,
                            has_gpu=self.self_info.has_gpu,
                            gpu_name=self.self_info.gpu_type or "",
                        )
                        optimizer.report_node_resources(node_resources)
                    except (AttributeError):
                        pass  # Non-critical, don't disrupt heartbeat

                # Save state periodically
                self._save_state()

            except Exception as e:  # noqa: BLE001
                logger.info(f"Heartbeat error: {e}")

            # Notify systemd watchdog that we're still alive
            systemd_notify_watchdog()

            await asyncio.sleep(HEARTBEAT_INTERVAL)

    async def _voter_heartbeat_loop(self):
        """
        Dedicated high-frequency heartbeat loop for voter nodes.

        IMPROVEMENTS:
        - Faster heartbeat interval (10s vs 30s) for voter nodes
        - Aggressively clears NAT-blocked status on successful heartbeats
        - Maintains full mesh connectivity between all voters
        - Propagates voter list to ensure consistent quorum
        """
        # Only run if this node is a voter
        if self.node_id not in self.voter_node_ids:
            return

        logger.info(f"Starting voter heartbeat loop (interval={VOTER_HEARTBEAT_INTERVAL}s)")
        last_voter_mesh_refresh = 0.0

        while self.running:
            try:
                now = time.time()

                # Get all other voters
                other_voters = [v for v in self.voter_node_ids if v != self.node_id]

                for voter_id in other_voters:
                    # Find voter peer info
                    async with AsyncLockWrapper(self.peers_lock):
                        voter_peer = self.peers.get(voter_id)

                    if not voter_peer:
                        # Try to discover voter from known peers
                        await self._discover_voter_peer(voter_id)
                        continue

                    # Attempt heartbeat to voter
                    success = await self._send_voter_heartbeat(voter_peer)

                    if success:
                        # AGGRESSIVE NAT RECOVERY: Clear NAT-blocked immediately on success
                        if VOTER_NAT_RECOVERY_AGGRESSIVE and voter_peer.nat_blocked:
                            logger.info(f"Voter {voter_id} NAT-blocked status cleared (heartbeat succeeded)")
                            async with AsyncLockWrapper(self.peers_lock):
                                if voter_id in self.peers:
                                    self.peers[voter_id].nat_blocked = False
                                    self.peers[voter_id].nat_blocked_since = 0.0
                                    self.peers[voter_id].consecutive_failures = 0
                    else:
                        # Try alternative endpoints
                        success = await self._try_voter_alternative_endpoints(voter_peer)

                        if not success:
                            # Increment failure count but don't mark NAT-blocked yet
                            with self.peers_lock:
                                if voter_id in self.peers:
                                    self.peers[voter_id].consecutive_failures = \
                                        int(getattr(self.peers[voter_id], "consecutive_failures", 0) or 0) + 1

                # Periodic voter mesh refresh - ensure all voters know about each other
                if now - last_voter_mesh_refresh > VOTER_MESH_REFRESH_INTERVAL:
                    last_voter_mesh_refresh = now
                    await self._refresh_voter_mesh()

            except Exception as e:  # noqa: BLE001
                logger.info(f"Voter heartbeat error: {e}")

            await asyncio.sleep(VOTER_HEARTBEAT_INTERVAL)

    async def _send_voter_heartbeat(self, voter_peer) -> bool:
        """Send a heartbeat to a voter peer with shorter timeout."""
        try:
            peer_scheme = getattr(voter_peer, "scheme", "http") or "http"

            # Use Tailscale IP if available (more reliable for cross-provider)
            target_host = voter_peer.host
            ts_ip = self._get_tailscale_ip_for_peer(voter_peer.node_id)
            if ts_ip:
                target_host = ts_ip

            info = await self._send_heartbeat_to_peer(
                target_host,
                voter_peer.port,
                scheme=peer_scheme,
                timeout=VOTER_HEARTBEAT_TIMEOUT
            )

            if info:
                with self.peers_lock:
                    info.last_heartbeat = time.time()
                    info.consecutive_failures = 0
                    self.peers[info.node_id] = info

                # Update leader if this voter claims leadership
                if info.role == NodeRole.LEADER and info.node_id != self.node_id and self.leader_id != info.node_id:
                    logger.info(f"Discovered leader from voter heartbeat: {info.node_id}")
                    self.leader_id = info.node_id
                    self.role = NodeRole.FOLLOWER
                    self.leader_lease_expires = time.time() + LEADER_LEASE_DURATION

                return True
        except (aiohttp.ClientError, asyncio.TimeoutError, json.JSONDecodeError, KeyError, AttributeError) as e:
            # Log voter coordination failures for debugging cluster connectivity issues
            logger.debug(f"Voter heartbeat failed for {voter_peer.node_id if voter_peer else 'unknown'}: {type(e).__name__}: {e}")
        return False

    async def _try_voter_alternative_endpoints(self, voter_peer) -> bool:
        """Try alternative endpoints for a voter peer."""
        peer_scheme = getattr(voter_peer, "scheme", "http") or "http"

        # Try 1: Tailscale IP
        ts_ip = self._get_tailscale_ip_for_peer(voter_peer.node_id)
        if ts_ip and ts_ip != voter_peer.host:
            info = await self._send_heartbeat_to_peer(ts_ip, voter_peer.port, scheme=peer_scheme, timeout=VOTER_HEARTBEAT_TIMEOUT)
            if info:
                logger.info(f"Reached voter {voter_peer.node_id} via Tailscale ({ts_ip})")
                with self.peers_lock:
                    info.last_heartbeat = time.time()
                    info.consecutive_failures = 0
                    self.peers[info.node_id] = info
                return True

        # Try 2: Reported host/port
        rh = str(getattr(voter_peer, "reported_host", "") or "").strip()
        rp = int(getattr(voter_peer, "reported_port", 0) or 0)
        if rh and rp and (rh != voter_peer.host or rp != voter_peer.port):
            info = await self._send_heartbeat_to_peer(rh, rp, scheme=peer_scheme, timeout=VOTER_HEARTBEAT_TIMEOUT)
            if info:
                logger.info(f"Reached voter {voter_peer.node_id} via reported endpoint ({rh}:{rp})")
                with self.peers_lock:
                    info.last_heartbeat = time.time()
                    info.consecutive_failures = 0
                    self.peers[info.node_id] = info
                return True

        return False

    async def _discover_voter_peer(self, voter_id: str):
        """Discover a voter peer from known peers."""
        # Ask known peers for the voter's endpoint
        for peer_addr in self.known_peers:
            try:
                scheme, host, port = self._parse_peer_address(peer_addr)
                async with aiohttp.ClientSession() as session, session.get(
                    f"{scheme}://{host}:{port}/relay/peers",
                    timeout=aiohttp.ClientTimeout(total=5),
                    headers=self._auth_headers()
                ) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        peers_data = data.get("peers", {})
                        if voter_id in peers_data:
                            peer_info = NodeInfo.from_dict(peers_data[voter_id])
                            with self.peers_lock:
                                self.peers[voter_id] = peer_info
                            logger.info(f"Discovered voter {voter_id} from {host}")
                            return
            except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError, ImportError):
                continue

    async def _refresh_voter_mesh(self):
        """Ensure all voters have knowledge of each other."""
        if not self.voter_node_ids:
            return

        # Check how many voters we know about
        with self.peers_lock:
            known_voters = [v for v in self.voter_node_ids if v in self.peers or v == self.node_id]

        if len(known_voters) < len(self.voter_node_ids):
            missing_voters = [v for v in self.voter_node_ids if v not in known_voters]
            logger.info(f"Voter mesh incomplete, missing: {missing_voters}")

            # Try to discover missing voters
            for voter_id in missing_voters:
                await self._discover_voter_peer(voter_id)

    # NOTE: _nat_management_loop() removed Dec 2025 (32 LOC).
    # Now runs via LoopManager as NATManagementLoop.
    # See scripts/p2p/loops/network_loops.py for implementation.

    async def _detect_nat_type(self):
        """
        Detect NAT type using STUN-like probing.

        NAT Types:
        - Full Cone: Any external host can send packets to internal host
        - Restricted Cone: Only hosts that internal has contacted can respond
        - Port Restricted: Only hosts+ports that internal has contacted can respond
        - Symmetric: Different external IP:port for each destination (breaks P2P)
        """
        external_ips = set()

        # Probe multiple peers to detect if we get different external IPs
        with self.peers_lock:
            alive_peers = [p for p in self.peers.values() if p.is_alive() and p.node_id != self.node_id]

        for peer in alive_peers[:5]:  # Probe up to 5 peers
            try:
                peer_scheme = getattr(peer, "scheme", "http") or "http"
                async with aiohttp.ClientSession() as session, session.get(
                    f"{peer_scheme}://{peer.host}:{peer.port}/health",
                    timeout=aiohttp.ClientTimeout(total=5),
                    headers=self._auth_headers()
                ) as resp:
                    if resp.status == 200:
                        # The peer would report our external IP if we had an endpoint for it
                        # For now, just track connectivity
                        await resp.json()
                        external_ips.add(peer.host)  # Track which peers we can reach
            except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
                continue

        # If we see ourselves with different IPs from different vantage points,
        # we likely have symmetric NAT
        if len(external_ips) > 1:
            self._nat_type = "symmetric"
            logger.info("Detected symmetric NAT (multiple external IPs seen)")
        elif len(external_ips) == 1:
            self._nat_type = "cone"
        else:
            self._nat_type = "unknown"

    async def _probe_nat_blocked_peers(self):
        """Probe NAT-blocked peers to see if they've become reachable."""
        with self.peers_lock:
            nat_blocked_peers = [
                p for p in self.peers.values()
                if p.nat_blocked and p.node_id != self.node_id
            ]

        for peer in nat_blocked_peers:
            # Check if enough time has passed since blocking
            blocked_duration = time.time() - (peer.nat_blocked_since or 0)
            if blocked_duration < NAT_BLOCKED_RECOVERY_TIMEOUT:
                continue

            # Try to reach the peer
            peer_scheme = getattr(peer, "scheme", "http") or "http"

            # Try multiple endpoints
            endpoints_to_try = [(peer.host, peer.port)]

            # Add Tailscale IP
            ts_ip = self._get_tailscale_ip_for_peer(peer.node_id)
            if ts_ip and ts_ip != peer.host:
                endpoints_to_try.insert(0, (ts_ip, peer.port))  # Prefer Tailscale

            # Add reported endpoint
            rh = str(getattr(peer, "reported_host", "") or "").strip()
            rp = int(getattr(peer, "reported_port", 0) or 0)
            if rh and rp:
                endpoints_to_try.append((rh, rp))

            for host, port in endpoints_to_try:
                try:
                    async with aiohttp.ClientSession() as session, session.get(
                        f"{peer_scheme}://{host}:{port}/health",
                        timeout=aiohttp.ClientTimeout(total=NAT_BLOCKED_PROBE_TIMEOUT),
                        headers=self._auth_headers()
                    ) as resp:
                        if resp.status == 200:
                            # Peer is reachable! Clear NAT-blocked status
                            logger.info(f"NAT-blocked peer {peer.node_id} is now reachable at {host}:{port}")
                            with self.peers_lock:
                                if peer.node_id in self.peers:
                                    self.peers[peer.node_id].nat_blocked = False
                                    self.peers[peer.node_id].nat_blocked_since = 0.0
                                    self.peers[peer.node_id].host = host  # Update to working endpoint
                                    self.peers[peer.node_id].consecutive_failures = 0
                            break
                except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
                    continue

    async def _update_relay_preferences(self):
        """Update relay preferences based on connectivity patterns."""
        # Identify peers that consistently fail direct connections
        with self.peers_lock:
            peers_needing_relay = [
                p for p in self.peers.values()
                if (getattr(p, "consecutive_failures", 0) or 0) >= NAT_RELAY_PREFERENCE_THRESHOLD
                and not p.nat_blocked
                and p.node_id != self.node_id
            ]

        for peer in peers_needing_relay:
            # Mark as preferring relay
            if not peer.nat_blocked:
                logger.info(f"Peer {peer.node_id} has {peer.consecutive_failures} consecutive failures, marking as NAT-blocked")
                with self.peers_lock:
                    if peer.node_id in self.peers:
                        self.peers[peer.node_id].nat_blocked = True
                        self.peers[peer.node_id].nat_blocked_since = time.time()
                        # Set relay to best available relay node
                        relay_node = self._select_best_relay()
                        if relay_node:
                            self.peers[peer.node_id].relay_via = relay_node

    def _select_best_relay(self) -> str:
        """Select the best relay node based on connectivity and load."""
        with self.peers_lock:
            candidates = [
                p for p in self.peers.values()
                if p.is_alive()
                and not p.nat_blocked
                and p.node_id != self.node_id
                and (getattr(p, "consecutive_failures", 0) or 0) < 2
            ]

        if not candidates:
            return ""

        # Prefer leader, then voters, then lowest load
        leader_peer = next((p for p in candidates if p.node_id == self.leader_id), None)
        if leader_peer:
            return leader_peer.node_id

        voter_peer = next((p for p in candidates if p.node_id in self.voter_node_ids), None)
        if voter_peer:
            return voter_peer.node_id

        # Lowest load
        candidates.sort(key=lambda p: getattr(p, "load_score", 100))
        return candidates[0].node_id if candidates else ""

    # NOTE: _manifest_collection_loop removed Dec 27, 2025
    # Now handled by ManifestCollectionLoop via LoopManager
    # See scripts/p2p/loops/manifest_collection_loop.py

    def _record_selfplay_stats_sample(self, manifest: ClusterDataManifest) -> None:
        """Record a lightweight selfplay totals sample for dashboard charts."""
        try:
            sample = {
                "timestamp": time.time(),
                "manifest_collected_at": float(getattr(manifest, "collected_at", 0.0) or 0.0),
                "total_selfplay_games": int(getattr(manifest, "total_selfplay_games", 0) or 0),
                "by_board_type": manifest.by_board_type,
                "total_nodes": int(getattr(manifest, "total_nodes", 0) or 0),
            }
            self.selfplay_stats_history.append(sample)
            max_samples = int(getattr(self, "selfplay_stats_history_max_samples", 288) or 288)
            if max_samples > 0 and len(self.selfplay_stats_history) > max_samples:
                self.selfplay_stats_history = self.selfplay_stats_history[-max_samples:]
        except (ValueError, KeyError, IndexError, AttributeError):
            # Never let dashboard bookkeeping break manifest collection.
            return

    def _endpoint_key(self, info: NodeInfo) -> tuple[str, str, int] | None:
        """Return the normalized reachable endpoint key for a peer (scheme, host, port)."""
        host = str(getattr(info, "host", "") or "").strip()
        if not host:
            return None
        scheme = str(getattr(info, "scheme", "http") or "http").lower()
        try:
            port = int(getattr(info, "port", DEFAULT_PORT) or DEFAULT_PORT)
        except (ValueError):
            port = DEFAULT_PORT
        reported_host = str(getattr(info, "reported_host", "") or "").strip()
        try:
            reported_port = int(getattr(info, "reported_port", 0) or 0)
        except (ValueError):
            reported_port = 0

        if reported_host and reported_port > 0:
            # Reverse proxies / relays can cause inbound peer requests to appear as loopback.
            # Prefer the peer's self-reported advertised endpoint in that case so:
            # - endpoint conflict detection remains meaningful, and
            # - eligible leaders don't get filtered out as "conflicted".
            if host in {"127.0.0.1", "localhost", "0.0.0.0", "::1"} or self._is_tailscale_host(reported_host):
                host, port = reported_host, reported_port
        return (scheme, host, port)

    def _endpoint_conflict_keys(self, peers: list[NodeInfo]) -> set[tuple[str, str, int]]:
        """Compute endpoint keys that are shared by >1 node (NAT/port collisions)."""
        counts: dict[tuple[str, str, int], int] = {}
        for p in peers:
            # Ignore dead peers: stale node_ids can linger after restarts and would
            # otherwise permanently mark the live node as "conflicted".
            if not p.is_alive():
                continue
            key = self._endpoint_key(p)
            if not key:
                continue
            counts[key] = counts.get(key, 0) + 1
        return {k for k, v in counts.items() if v > 1}

    async def _probe_nat_blocked_peer(self, peer: NodeInfo) -> bool:
        """Probe a NAT-blocked peer to check if it's now directly reachable.

        Returns True if peer is reachable and NAT-blocked status was cleared.
        """
        if not peer.nat_blocked:
            return False

        now = time.time()
        nat_blocked_since = float(getattr(peer, "nat_blocked_since", 0.0) or 0.0)
        last_probe = float(getattr(peer, "last_nat_probe", 0.0) or 0.0)

        # Don't probe too frequently
        if now - last_probe < NAT_BLOCKED_PROBE_INTERVAL:
            return False

        # Don't probe if not blocked long enough
        if nat_blocked_since > 0 and (now - nat_blocked_since) < NAT_BLOCKED_RECOVERY_TIMEOUT:
            return False

        # Update last probe time
        with self.peers_lock:
            existing = self.peers.get(peer.node_id)
            if existing:
                existing.last_nat_probe = now

        try:
            url = self._url_for_peer(peer, "/status")
            timeout = ClientTimeout(total=NAT_BLOCKED_PROBE_TIMEOUT)
            async with get_client_session(timeout) as session:
                async with session.get(url, headers=self._auth_headers()) as resp:
                    if resp.status == 200:
                        # Peer is reachable! Clear NAT-blocked status
                        with self.peers_lock:
                            existing = self.peers.get(peer.node_id)
                            if existing and existing.nat_blocked:
                                existing.nat_blocked = False
                                existing.nat_blocked_since = 0.0
                                existing.relay_via = ""
                                logger.info(f"NAT recovery: {peer.node_id} is now directly reachable")
                                return True
        except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError) as e:
            # Probe failed - peer still not reachable (expected for NAT-blocked nodes)
            logger.debug(f"NAT recovery probe failed for {peer.node_id}: {type(e).__name__}")

        return False

    async def _sweep_nat_recovery(self) -> int:
        """Periodically probe NAT-blocked peers to check if they've become reachable.

        Returns the number of peers that recovered from NAT-blocked state.
        """
        recovered = 0
        with self.peers_lock:
            nat_blocked_peers = [
                p for p in self.peers.values()
                if p.nat_blocked and p.is_alive()
            ]

        if not nat_blocked_peers:
            return 0

        # Probe in parallel but limit concurrency
        tasks = [self._probe_nat_blocked_peer(p) for p in nat_blocked_peers[:10]]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for i, result in enumerate(results):
            if isinstance(result, Exception):
                peer_id = nat_blocked_peers[i].node_id if i < len(nat_blocked_peers) else "unknown"
                logger.debug(f"NAT probe failed for {peer_id}: {result}")
            elif result is True:
                recovered += 1

        if recovered > 0:
            logger.info(f"NAT recovery sweep: {recovered} peer(s) recovered")

        return recovered

    def _is_leader_eligible(
        self,
        peer: NodeInfo,
        conflict_keys: set[tuple[str, str, int]],
        *,
        require_alive: bool = True,
    ) -> bool:
        """Heuristic: leaders must be directly reachable and uniquely addressable."""
        if require_alive and not peer.is_alive():
            return False
        voters = list(getattr(self, "voter_node_ids", []) or [])
        if voters and peer.node_id not in voters:
            return False
        if int(getattr(peer, "consecutive_failures", 0) or 0) >= MAX_CONSECUTIVE_FAILURES:
            return False
        if getattr(peer, "nat_blocked", False):
            return False
        key = self._endpoint_key(peer)
        return not (key and key in conflict_keys)

    def _register_peer_with_dedup(self, info: NodeInfo) -> bool:
        """Register or update a peer with deduplication support.

        Dec 29, 2025: Implements peer deduplication by node_id. When the same
        node is discovered via multiple IPs (Tailscale, public, etc.), this
        method merges them into a single canonical entry instead of creating
        duplicate entries.

        Args:
            info: NodeInfo to register

        Returns:
            True if this was a new peer, False if updating existing
        """
        if not info or not info.node_id:
            return False

        with self.peers_lock:
            existing = self.peers.get(info.node_id)
            if existing is not None:
                # Merge new info into existing entry
                existing.merge_from(info)
                return False
            else:
                # New peer - initialize alternate_ips from host if available
                if info.host and not info.alternate_ips:
                    info.alternate_ips = set()
                self.peers[info.node_id] = info
                return True

    def _deduplicate_peers(self) -> int:
        """Periodic deduplication of peers dict.

        Dec 29, 2025: Scans for any duplicate entries that may have been
        created before deduplication was implemented, and merges them.

        Returns:
            Number of duplicates removed
        """
        removed = 0
        # This method is called periodically to clean up any legacy duplicates
        # Since we now key by node_id, duplicates shouldn't occur, but this
        # handles edge cases from state file loading
        with self.peers_lock:
            # Build IP -> node_id mapping to detect duplicates
            ip_to_nodes: dict[str, list[str]] = {}
            for node_id, peer in self.peers.items():
                if peer.host:
                    ip_to_nodes.setdefault(peer.host, []).append(node_id)
                if peer.reported_host:
                    ip_to_nodes.setdefault(peer.reported_host, []).append(node_id)
                for alt_ip in (peer.alternate_ips or set()):
                    ip_to_nodes.setdefault(alt_ip, []).append(node_id)

            # Find IPs shared by multiple node_ids (potential duplicates)
            # This can happen if the same physical node registers with different
            # node_ids (rare, but possible after hostname changes)
            for ip, node_ids in ip_to_nodes.items():
                if len(node_ids) > 1 and ip:
                    # Multiple nodes claim the same IP - log for investigation
                    logger.warning(
                        f"Dedup: IP {ip} claimed by multiple nodes: {node_ids}. "
                        f"Consider manual cleanup if these are the same physical host."
                    )

        return removed

    def _maybe_adopt_leader_from_peers(self) -> bool:
        """If we can already see a healthy leader, adopt it and avoid elections."""
        if self.role == NodeRole.LEADER:
            return False

        with self.peers_lock:
            peers = [p for p in self.peers.values() if p.node_id != self.node_id]

        conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers])
        leaders = [
            p for p in peers
            if p.role == NodeRole.LEADER and self._is_leader_eligible(p, conflict_keys)
        ]

        voter_ids = list(getattr(self, "voter_node_ids", []) or [])
        if voter_ids:
            leaders = [p for p in leaders if p.node_id in voter_ids]

        if not leaders:
            return False

        # If multiple leaders exist (split brain), pick the lexicographically highest
        # ID (matches bully ordering) to converge.
        leader = sorted(leaders, key=lambda p: p.node_id)[-1]

        if self.leader_id != leader.node_id:
            logger.info(f"Adopted existing leader from peers: {leader.node_id}")
        self.leader_id = leader.node_id
        self.last_leader_seen = time.time()  # Track when we last had a functioning leader
        self.role = NodeRole.FOLLOWER
        self._save_state()
        return True

    async def _check_dead_peers_async(self):
        """Check for peers that have stopped responding (async version).

        This version uses AsyncLockWrapper to avoid blocking the event loop
        when acquiring the peers_lock.
        """
        now = time.time()
        dead_peers = []
        peers_to_purge = []

        async with AsyncLockWrapper(self.peers_lock):
            for node_id, info in self.peers.items():
                if not info.is_alive() and node_id != self.node_id:
                    dead_peers.append(node_id)
                    # Retire long-dead peers so they don't pollute active scheduling.
                    try:
                        dead_for = now - float(getattr(info, "last_heartbeat", 0.0) or 0.0)
                    except (ValueError, AttributeError):
                        dead_for = float("inf")
                    if not getattr(info, "retired", False) and dead_for >= PEER_RETIRE_AFTER_SECONDS:
                        info.retired = True
                        info.retired_at = now
                        logger.info(f"Retiring peer {node_id} (offline for {int(dead_for)}s)")
                        # CRITICAL: Emit HOST_OFFLINE event (Dec 2025 fix)
                        last_hb = getattr(info, "last_heartbeat", None)
                        asyncio.create_task(self._emit_host_offline(node_id, "retired", last_hb))
                        # CRITICAL: Emit P2P_NODE_DEAD for work reassignment (Dec 2025)
                        asyncio.create_task(self._emit_node_dead(
                            node_id, "retired", last_hb, dead_for
                        ))
                        # Emit CLUSTER_CAPACITY_CHANGED (Dec 2025 - enables SyncRouter reaction)
                        alive_count = sum(1 for p in self.peers.values() if p.is_alive() and not getattr(p, "retired", False))
                        gpu_count = sum(1 for p in self.peers.values() if p.is_alive() and not getattr(p, "retired", False) and getattr(p, "gpu_type", None))
                        asyncio.create_task(self._emit_cluster_capacity_changed(
                            change_type="node_removed",
                            node_id=node_id,
                            total_nodes=alive_count,
                            gpu_nodes=gpu_count,
                            reason="peer_timeout",
                        ))
                elif info.is_alive() and getattr(info, "retired", False):
                    # Peer came back: clear retirement.
                    info.retired = False
                    info.retired_at = 0.0
                    # CRITICAL: Emit HOST_ONLINE event (Dec 2025 fix)
                    caps = []
                    if hasattr(info, "gpu_type") and info.gpu_type:
                        caps.append(f"gpu:{info.gpu_type}")
                    asyncio.create_task(self._emit_host_online(node_id, caps))
                    # Emit CLUSTER_CAPACITY_CHANGED (Dec 2025 - enables SyncRouter reaction)
                    alive_count = sum(1 for p in self.peers.values() if p.is_alive() and not getattr(p, "retired", False))
                    gpu_count = sum(1 for p in self.peers.values() if p.is_alive() and not getattr(p, "retired", False) and getattr(p, "gpu_type", None))
                    asyncio.create_task(self._emit_cluster_capacity_changed(
                        change_type="node_added",
                        node_id=node_id,
                        total_nodes=alive_count,
                        gpu_nodes=gpu_count,
                        reason="peer_recovered",
                    ))
                    logger.info(f"Peer {node_id} recovered from retirement")

            for node_id in dead_peers:
                info = self.peers.get(node_id)
                if info and getattr(info, "retired", False):
                    continue
                logger.info(f"Peer {node_id} is dead (no heartbeat for {PEER_TIMEOUT}s)")

            # Auto-purge very old retired peers
            for node_id, info in self.peers.items():
                if getattr(info, "retired", False):
                    retired_at = getattr(info, "retired_at", 0.0) or 0.0
                    if retired_at > 0 and (now - retired_at) >= PEER_PURGE_AFTER_SECONDS:
                        peers_to_purge.append(node_id)

            for node_id in peers_to_purge:
                del self.peers[node_id]
                logger.info(f"Auto-purged stale peer: {node_id} (retired for >{PEER_PURGE_AFTER_SECONDS}s)")

            # December 2025: Emit cluster health event if state changed
            # This enables pipeline coordination to pause/resume based on cluster health
            final_alive_count = sum(
                1 for p in self.peers.values()
                if p.is_alive() and not getattr(p, "retired", False)
            )
            final_node_count = len([
                p for p in self.peers.values()
                if not getattr(p, "retired", False)
            ])
            # Add self if not in peers
            if self.node_id not in self.peers:
                final_alive_count += 1
                final_node_count += 1
            # Dec 28, 2025: Fixed signature mismatch - pass correct parameters
            is_healthy = final_alive_count > 0
            quorum_met = self._has_voter_quorum() if hasattr(self, '_has_voter_quorum') else True
            self._emit_cluster_health_event_sync(is_healthy, final_alive_count, quorum_met)

        # Clear stale leader IDs after restarts/partitions
        if self.leader_id and not self._is_leader_lease_valid():
            logger.info(f"Clearing stale/expired leader lease: leader_id={self.leader_id}")
            old_leader_id = self.leader_id
            self.leader_id = None
            self.leader_lease_id = ""
            self.leader_lease_expires = 0.0
            self.last_lease_renewal = 0.0
            self.role = NodeRole.FOLLOWER
            # Emit LEADER_LOST before starting election (Dec 2025 fix)
            asyncio.create_task(self._emit_leader_lost(old_leader_id, "lease_expired"))
            # CRITICAL: Check quorum before starting election to prevent quorum bypass
            if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                logger.warning("Skipping election after stale lease clear: no voter quorum available")
            else:
                asyncio.create_task(self._start_election())

        # If leader is dead, start election
        if self.leader_id and self.leader_id != self.node_id:
            async with AsyncLockWrapper(self.peers_lock):
                leader = self.peers.get(self.leader_id)
                peers_snapshot = [p for p in self.peers.values() if p.node_id != self.node_id]
            if leader:
                conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])
                if not self._is_leader_eligible(leader, conflict_keys):
                    reason = "dead" if not leader.is_alive() else "ineligible"
                    logger.info(f"Leader {self.leader_id} is {reason}, starting election")
                    # Dec 2025: Emit LEADER_LOST before clearing leader_id
                    old_leader_id = self.leader_id
                    self.leader_id = None
                    self.leader_lease_id = ""
                    self.leader_lease_expires = 0.0
                    self.last_lease_renewal = 0.0
                    self.role = NodeRole.FOLLOWER
                    asyncio.create_task(self._emit_leader_lost(old_leader_id, reason))
                    # CRITICAL: Check quorum before starting election to prevent quorum bypass
                    if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                        logger.warning(f"Skipping election after leader {reason}: no voter quorum available")
                    else:
                        asyncio.create_task(self._start_election())

        # If we're leaderless, periodically retry elections with adaptive backoff
        # December 29, 2025: Improved backoff to start faster then slow down
        if not self.leader_id and not self.election_in_progress:
            now = time.time()
            retry_count = int(getattr(self, "_election_retry_count", 0) or 0)
            # Adaptive backoff: 15s, 30s, 60s, 90s (capped)
            backoff_intervals = [15, 30, 60, 90]
            backoff_seconds = backoff_intervals[min(retry_count, len(backoff_intervals) - 1)]
            last_attempt = float(getattr(self, "last_election_attempt", 0.0) or 0.0)
            if now - last_attempt >= backoff_seconds:
                self.last_election_attempt = now
                self._election_retry_count = retry_count + 1
                # CRITICAL: Check quorum before starting election to prevent quorum bypass
                if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                    logger.warning(f"Skipping periodic election retry {retry_count + 1}: no voter quorum available")
                else:
                    logger.info(f"Triggering election retry {retry_count + 1} after {backoff_seconds}s leaderless")
                    asyncio.create_task(self._start_election())
        elif self.leader_id:
            # Reset retry count when we have a leader
            self._election_retry_count = 0

    async def _probe_retired_peers_async(self) -> None:
        """Actively probe retired peers to detect recovery.

        Dec 30, 2025: Added to fix cluster connectivity after restart.
        Retired nodes don't send heartbeats, so we must probe them.
        This runs periodically (every PEER_RECOVERY_RETRY_INTERVAL) to
        detect nodes that have come back online after being retired.
        """
        # Collect retired peers (excluding self)
        async with AsyncLockWrapper(self.peers_lock):
            retired = [
                p for p in self.peers.values()
                if getattr(p, "retired", False) and p.node_id != self.node_id
            ]

        if not retired:
            return

        logger.debug(f"Probing {len(retired)} retired peers for recovery")

        # Use a short timeout for health checks
        timeout = aiohttp.ClientTimeout(total=5)

        for peer in retired:
            try:
                # Try to reach the peer's health endpoint
                url = f"http://{peer.host}:{peer.port}/health"
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    async with session.get(url) as resp:
                        if resp.status == 200:
                            # Node is alive - un-retire it
                            async with AsyncLockWrapper(self.peers_lock):
                                if peer.node_id in self.peers:
                                    self.peers[peer.node_id].retired = False
                                    self.peers[peer.node_id].retired_at = 0.0
                                    self.peers[peer.node_id].last_heartbeat = time.time()

                            logger.info(f"Recovered retired peer via probe: {peer.node_id}")

                            # Emit HOST_ONLINE event
                            caps = []
                            if hasattr(peer, "gpu_type") and peer.gpu_type:
                                caps.append(f"gpu:{peer.gpu_type}")
                            await self._emit_host_online(peer.node_id, caps)

                            # Emit CLUSTER_CAPACITY_CHANGED
                            async with AsyncLockWrapper(self.peers_lock):
                                alive_count = sum(
                                    1 for p in self.peers.values()
                                    if p.is_alive() and not getattr(p, "retired", False)
                                )
                                gpu_count = sum(
                                    1 for p in self.peers.values()
                                    if p.is_alive() and not getattr(p, "retired", False)
                                    and getattr(p, "gpu_type", None)
                                )
                            asyncio.create_task(self._emit_cluster_capacity_changed(
                                change_type="node_added",
                                node_id=peer.node_id,
                                total_nodes=alive_count,
                                gpu_nodes=gpu_count,
                                reason="peer_recovered_via_probe",
                            ))

            except (aiohttp.ClientError, asyncio.TimeoutError, OSError) as e:
                # Still unreachable - remain retired
                logger.debug(f"Retired peer {peer.node_id} still unreachable: {e}")
                continue
            except Exception as e:
                # Unexpected error - log but don't crash
                logger.warning(f"Error probing retired peer {peer.node_id}: {e}")
                continue

    def _check_dead_peers(self):
        """Check for peers that have stopped responding."""
        now = time.time()
        with self.peers_lock:
            dead_peers = []
            for node_id, info in self.peers.items():
                if not info.is_alive() and node_id != self.node_id:
                    dead_peers.append(node_id)
                    # Retire long-dead peers so they don't pollute active scheduling.
                    try:
                        dead_for = now - float(getattr(info, "last_heartbeat", 0.0) or 0.0)
                    except (ValueError, AttributeError):
                        dead_for = float("inf")
                    if not getattr(info, "retired", False) and dead_for >= PEER_RETIRE_AFTER_SECONDS:
                        info.retired = True
                        info.retired_at = now
                        logger.info(f"Retiring peer {node_id} (offline for {int(dead_for)}s)")
                        # CRITICAL: Emit HOST_OFFLINE event (Dec 2025 fix) - sync version
                        last_hb = getattr(info, "last_heartbeat", None)
                        self._emit_host_offline_sync(node_id, "retired", last_hb)
                        # CRITICAL: Emit P2P_NODE_DEAD for work reassignment (Dec 2025) - sync version
                        self._emit_node_dead_sync(node_id, "retired", last_hb, dead_for)
                        # Emit CLUSTER_CAPACITY_CHANGED (Dec 2025 - enables SyncRouter reaction)
                        alive_count = sum(1 for p in self.peers.values() if p.is_alive() and not getattr(p, "retired", False))
                        gpu_count = sum(1 for p in self.peers.values() if p.is_alive() and not getattr(p, "retired", False) and getattr(p, "gpu_type", None))
                        self._emit_cluster_capacity_changed_sync(
                            change_type="node_removed",
                            node_id=node_id,
                            total_nodes=alive_count,
                            gpu_nodes=gpu_count,
                            reason="peer_timeout",
                        )
                elif info.is_alive() and getattr(info, "retired", False):
                    # Peer came back: clear retirement.
                    info.retired = False
                    info.retired_at = 0.0
                    # CRITICAL: Emit HOST_ONLINE event (Dec 2025 fix) - sync version
                    caps = []
                    if hasattr(info, "gpu_type") and info.gpu_type:
                        caps.append(f"gpu:{info.gpu_type}")
                    self._emit_host_online_sync(node_id, caps)
                    # Emit CLUSTER_CAPACITY_CHANGED (Dec 2025 - enables SyncRouter reaction)
                    alive_count = sum(1 for p in self.peers.values() if p.is_alive() and not getattr(p, "retired", False))
                    gpu_count = sum(1 for p in self.peers.values() if p.is_alive() and not getattr(p, "retired", False) and getattr(p, "gpu_type", None))
                    self._emit_cluster_capacity_changed_sync(
                        change_type="node_added",
                        node_id=node_id,
                        total_nodes=alive_count,
                        gpu_nodes=gpu_count,
                        reason="peer_recovered",
                    )
                    logger.info(f"Peer {node_id} recovered from retirement")

            for node_id in dead_peers:
                info = self.peers.get(node_id)
                if info and getattr(info, "retired", False):
                    continue
                logger.info(f"Peer {node_id} is dead (no heartbeat for {PEER_TIMEOUT}s)")
                # Don't remove immediately, just mark as dead for historical tracking

            # STABILITY FIX: Auto-purge very old retired peers to prevent unbounded list growth
            # This removes stale entries that would otherwise accumulate and cause confusion
            # (e.g., old leader role claims that don't match current elected leader)
            peers_to_purge = []
            for node_id, info in self.peers.items():
                if getattr(info, "retired", False):
                    retired_at = getattr(info, "retired_at", 0.0) or 0.0
                    if retired_at > 0 and (now - retired_at) >= PEER_PURGE_AFTER_SECONDS:
                        peers_to_purge.append(node_id)

            for node_id in peers_to_purge:
                del self.peers[node_id]
                logger.info(f"Auto-purged stale peer: {node_id} (retired for >{PEER_PURGE_AFTER_SECONDS}s)")

            # December 2025: Emit cluster health event if state changed
            # This enables pipeline coordination to pause/resume based on cluster health
            final_alive_count = sum(
                1 for p in self.peers.values()
                if p.is_alive() and not getattr(p, "retired", False)
            )
            final_node_count = len([
                p for p in self.peers.values()
                if not getattr(p, "retired", False)
            ])
            # Add self if not in peers
            if self.node_id not in self.peers:
                final_alive_count += 1
                final_node_count += 1
            # Dec 28, 2025: Fixed signature mismatch - pass correct parameters
            is_healthy = final_alive_count > 0
            quorum_met = self._has_voter_quorum() if hasattr(self, '_has_voter_quorum') else True
            self._emit_cluster_health_event_sync(is_healthy, final_alive_count, quorum_met)

        # LEARNED LESSONS - Clear stale leader IDs after restarts/partitions.
        #
        # Nodes persist `leader_id` but not lease metadata. After a restart, it's
        # possible to have `leader_id` point at an alive peer that is no longer a
        # leader (or to a leader whose lease is expired). Without an explicit lease
        # validity check, the cluster can get stuck leaderless and stop dispatching
        # jobs (while still "thinking" it has a leader).
        if self.leader_id and not self._is_leader_lease_valid():
            logger.info(f"Clearing stale/expired leader lease: leader_id={self.leader_id}")
            self.leader_id = None
            self.leader_lease_id = ""
            self.leader_lease_expires = 0.0
            self.last_lease_renewal = 0.0
            self.role = NodeRole.FOLLOWER
            # CRITICAL: Check quorum before starting election to prevent quorum bypass
            if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                logger.warning("Skipping election after stale lease clear (sync): no voter quorum available")
            else:
                asyncio.create_task(self._start_election())

        # If leader is dead, start election
        if self.leader_id and self.leader_id != self.node_id:
            with self.peers_lock:
                leader = self.peers.get(self.leader_id)
                peers_snapshot = [p for p in self.peers.values() if p.node_id != self.node_id]
            if leader:
                conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])
                if not self._is_leader_eligible(leader, conflict_keys):
                    reason = "dead" if not leader.is_alive() else "ineligible"
                    logger.info(f"Leader {self.leader_id} is {reason}, starting election")
                    # Dec 2025: Emit LEADER_LOST before clearing stale/ineligible leader
                    old_leader_id = self.leader_id
                    self.leader_id = None
                    self.leader_lease_id = ""
                    self.leader_lease_expires = 0.0
                    self.last_lease_renewal = 0.0
                    self.role = NodeRole.FOLLOWER
                    asyncio.create_task(self._emit_leader_lost(old_leader_id, reason))
                    # CRITICAL: Check quorum before starting election to prevent quorum bypass
                    if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                        logger.warning(f"Skipping election after leader {reason} (sync): no voter quorum available")
                    else:
                        asyncio.create_task(self._start_election())

        # If we're leaderless, periodically retry elections with adaptive backoff
        # December 29, 2025: Same adaptive backoff as async version
        if not self.leader_id and not self.election_in_progress:
            now = time.time()
            retry_count = int(getattr(self, "_election_retry_count", 0) or 0)
            # Adaptive backoff: 15s, 30s, 60s, 90s (capped)
            backoff_intervals = [15, 30, 60, 90]
            backoff_seconds = backoff_intervals[min(retry_count, len(backoff_intervals) - 1)]
            last_attempt = float(getattr(self, "last_election_attempt", 0.0) or 0.0)
            if now - last_attempt >= backoff_seconds:
                self.last_election_attempt = now
                self._election_retry_count = retry_count + 1
                # CRITICAL: Check quorum before starting election to prevent quorum bypass
                if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                    logger.warning(f"Skipping periodic election retry {retry_count + 1} (sync): no voter quorum available")
                else:
                    logger.info(f"Triggering election retry {retry_count + 1} after {backoff_seconds}s leaderless (sync)")
                    asyncio.create_task(self._start_election())
        elif self.leader_id:
            # Reset retry count when we have a leader
            self._election_retry_count = 0

    async def _start_election(self):
        """Start leader election using Bully algorithm."""
        self._update_self_info()

        # NAT-blocked nodes cannot act as a leader because peers can't reach them.
        if getattr(self.self_info, "nat_blocked", False):
            return
        # Optional quorum gating: only configured voters may lead, and only when
        # a majority of voters are currently visible.
        voter_node_ids = list(getattr(self, "voter_node_ids", []) or [])
        if voter_node_ids:
            if self.node_id not in voter_node_ids:
                # December 29, 2025: Non-voters can request elections from voters
                # instead of just returning silently
                await self._request_election_from_voters("non_voter_detected_leaderless")
                return
            if not self._has_voter_quorum():
                return

        with self.peers_lock:
            peers_snapshot = [p for p in self.peers.values() if p.node_id != self.node_id]

        conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])

        if self.leader_id and self.leader_id != self.node_id:
            with self.peers_lock:
                leader = self.peers.get(self.leader_id)
            leader_ok = (
                leader is not None
                and leader.is_alive()
                and leader.role == NodeRole.LEADER
                and self._is_leader_eligible(leader, conflict_keys)
                and self._is_leader_lease_valid()
            )
            if leader_ok:
                return
            # Drop stale/ineligible leader so we don't keep advertising it.
            self.leader_id = None
            self.leader_lease_id = ""
            self.leader_lease_expires = 0.0
        if self._maybe_adopt_leader_from_peers():
            return

        if self.election_in_progress:
            return

        self.election_in_progress = True
        self.role = NodeRole.CANDIDATE
        logger.info(f"Starting election, my ID: {self.node_id}")

        try:
            # Send election message to all nodes with higher IDs
            with self.peers_lock:
                higher_nodes = [
                    p for p in self.peers.values()
                    if (
                        p.node_id > self.node_id
                        and self._is_leader_eligible(p, conflict_keys)
                    )
                ]
                voter_node_ids = list(getattr(self, "voter_node_ids", []) or [])
                if voter_node_ids:
                    higher_nodes = [p for p in higher_nodes if p.node_id in voter_node_ids]

            got_response = False

            timeout = ClientTimeout(total=ELECTION_TIMEOUT)
            async with get_client_session(timeout) as session:
                for peer in higher_nodes:
                    try:
                        url = self._url_for_peer(peer, "/election")
                        async with session.post(url, json={"candidate_id": self.node_id}, headers=self._auth_headers()) as resp:
                            if resp.status == 200:
                                data = await resp.json()
                                if data.get("response") == "ALIVE":
                                    got_response = True
                                    logger.info(f"Higher node {peer.node_id} responded")
                    except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
                        pass  # Network errors expected during elections

            # If no higher node responded, we become leader
            if not got_response:
                # Only become leader if we're eligible (unique + directly reachable).
                if self._is_leader_eligible(self.self_info, conflict_keys):
                    await self._become_leader()
            else:
                # Wait for coordinator message
                await asyncio.sleep(ELECTION_TIMEOUT * 2)
                # If no coordinator arrives, fall back to adopting any eligible leader we can see.
                self._maybe_adopt_leader_from_peers()

        finally:
            self.election_in_progress = False
            if self.role == NodeRole.CANDIDATE:
                self.role = NodeRole.FOLLOWER

    async def _become_leader(self):
        """Become the cluster leader with lease-based leadership."""
        self._update_self_info()
        if getattr(self.self_info, "nat_blocked", False):
            logger.info(f"Refusing leadership while NAT-blocked: {self.node_id}")
            return
        if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
            logger.info(f"Refusing leadership without voter quorum: {self.node_id}")
            return
        import uuid
        lease_id = f"{self.node_id}_{int(time.time())}_{uuid.uuid4().hex[:8]}"
        lease_expires = await self._acquire_voter_lease_quorum(lease_id, int(LEADER_LEASE_DURATION))
        if getattr(self, "voter_node_ids", []) and not lease_expires:
            logger.error(f"Failed to obtain voter lease quorum; refusing leadership: {self.node_id}")
            self.role = NodeRole.FOLLOWER
            self.leader_id = None
            self.leader_lease_id = ""
            self.leader_lease_expires = 0.0
            self.last_lease_renewal = 0.0
            self._release_voter_grant_if_self()
            self._save_state()
            return

        logger.info(f"I am now the leader: {self.node_id}")
        self.role = NodeRole.LEADER
        self.leader_id = self.node_id
        self.last_leader_seen = time.time()  # Track when we last had a functioning leader

        # Phase 29: Increment cluster epoch on leadership change
        # This helps resolve split-brain when partitions merge
        self._increment_cluster_epoch()

        # Phase 15.1.1: Increment lease epoch and create fence token
        # This provides split-brain protection by ensuring each leadership
        # term has a unique, monotonically increasing epoch
        self._lease_epoch += 1
        self._fence_token = f"{self.node_id}:{self._lease_epoch}:{time.time()}"
        logger.info(f"Leader lease fencing: epoch={self._lease_epoch}, token={self._fence_token}")

        # CRITICAL: Emit LEADER_ELECTED event (Dec 2025 fix)
        # This enables LeadershipCoordinator and other components to track leadership changes
        asyncio.create_task(self._emit_leader_elected(self.node_id, getattr(self, "cluster_epoch", 0)))

        # Lease-based leadership (voter-backed when enabled).
        self.leader_lease_id = lease_id
        self.leader_lease_expires = float(lease_expires or (time.time() + LEADER_LEASE_DURATION))
        self.last_lease_renewal = time.time()

        # Announce to all peers with lease information
        with self.peers_lock:
            peers = list(self.peers.values())

        timeout = ClientTimeout(total=5)
        async with get_client_session(timeout) as session:
            for peer in peers:
                if peer.node_id != self.node_id:
                    try:
                        url = self._url_for_peer(peer, "/coordinator")
                        await session.post(url, json={
                            "leader_id": self.node_id,
                            "lease_id": self.leader_lease_id,
                            "lease_expires": self.leader_lease_expires,
                            "voter_node_ids": list(getattr(self, "voter_node_ids", []) or []),
                            # Phase 15.1.1: Include epoch and fence token for split-brain protection
                            "lease_epoch": self._lease_epoch,
                            "fence_token": self._fence_token,
                        }, headers=self._auth_headers())
                    except (aiohttp.ClientError, asyncio.TimeoutError, KeyError, IndexError, AttributeError):
                        pass  # Network errors expected during leader announcements

        self._save_state()

        # Start monitoring services when becoming leader
        await self._start_monitoring_if_leader()

        # Start P2P auto-deployer when becoming leader
        await self._start_p2p_auto_deployer()

    async def _request_election_from_voters(self, reason: str = "non_voter_request") -> bool:
        """December 29, 2025: Non-voters can request that voters start an election.

        Instead of silently returning when a non-voter tries to start an election,
        this method sends requests to known voters to have them start one.

        Args:
            reason: Why the election is being requested

        Returns:
            True if at least one voter accepted the request
        """
        voter_node_ids = list(getattr(self, "voter_node_ids", []) or [])
        if not voter_node_ids:
            return False

        logger.info(f"Non-voter {self.node_id} requesting election from voters: {reason}")

        # Rate limit election requests to avoid spamming
        now = time.time()
        last_request = getattr(self, "_last_election_request", 0.0)
        if now - last_request < 30:  # At most once per 30 seconds
            logger.debug("Skipping election request: rate limited")
            return False
        self._last_election_request = now

        accepted = False
        async with aiohttp.ClientSession() as session:
            for voter_id in voter_node_ids[:3]:  # Limit to 3 voters
                with self.peers_lock:
                    voter = self.peers.get(voter_id)
                if not voter or not voter.is_alive():
                    continue

                try:
                    url = self._url_for_peer(voter, "/election/request")
                    async with session.post(
                        url,
                        json={"requester_id": self.node_id, "reason": reason},
                        headers=self._auth_headers(),
                        timeout=aiohttp.ClientTimeout(total=5.0),
                    ) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            if data.get("accepted"):
                                logger.info(
                                    f"Voter {voter_id} accepted election request: {data.get('action')}"
                                )
                                accepted = True
                                break  # One voter accepting is enough
                except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                    logger.debug(f"Failed to request election from {voter_id}: {e}")
                    continue

        if not accepted:
            logger.warning(f"No voters accepted election request from {self.node_id}")
        return accepted

    async def _check_emergency_coordinator_fallback(self):
        """DECENTRALIZED: When voter quorum is unreachable for >5 min, any GPU node can coordinate.

        EMERGENCY COORDINATOR: This is a last-resort fallback when the normal voter-based
        leadership cannot be established due to:
        - Too many voters being offline
        - Network partition isolating voters
        - Cluster-wide issues

        In this mode, the node acts as a temporary coordinator WITHOUT voter consensus.
        It will relinquish control once voter quorum is restored.
        """
        now = time.time()

        # Only check every 60 seconds
        last_check = getattr(self, "_last_emergency_coord_check", 0)
        if now - last_check < 60:
            return
        self._last_emergency_coord_check = now

        # Skip if we already are a leader
        if self.role == NodeRole.LEADER:
            return

        # Skip if we have a known leader
        if self.leader_id:
            self._emergency_coordinator_since = 0
            return

        # Check if we have voter quorum
        if self._has_voter_quorum():
            self._emergency_coordinator_since = 0
            return  # Normal election should work

        # Track how long we've been without voter quorum
        quorum_missing_since = getattr(self, "_quorum_missing_since", 0)
        if quorum_missing_since == 0:
            self._quorum_missing_since = now
            return

        EMERGENCY_THRESHOLD = 300  # 5 minutes without quorum triggers emergency
        quorum_missing_duration = now - quorum_missing_since

        if quorum_missing_duration < EMERGENCY_THRESHOLD:
            return

        # Check if we're eligible (must be GPU node, not NAT-blocked)
        self._update_self_info()
        if not getattr(self.self_info, "has_gpu", False):
            return
        if getattr(self.self_info, "nat_blocked", False):
            return

        # Use consistent hashing to determine which node should be emergency coordinator
        # This prevents multiple nodes from declaring themselves coordinator
        with self.peers_lock:
            candidates = [self.node_id]
            for peer in self.peers.values():
                if not peer.is_alive():
                    continue
                if not getattr(peer, "has_gpu", False):
                    continue
                if getattr(peer, "nat_blocked", False):
                    continue
                candidates.append(peer.node_id)

        if not candidates:
            return

        # Deterministic selection: highest node_id wins (simple, consistent)
        candidates.sort(reverse=True)
        designated_coordinator = candidates[0]

        if designated_coordinator != self.node_id:
            return  # Another node should be coordinator

        # Become emergency coordinator (without voter lease)
        logger.info(f"EMERGENCY COORDINATOR: Taking leadership without voter quorum "
              f"(quorum missing for {int(quorum_missing_duration)}s, {len(candidates)} candidates)")

        self.role = NodeRole.LEADER
        self.leader_id = self.node_id
        self.last_leader_seen = now
        self._emergency_coordinator_since = now

        # Use a special lease ID to mark emergency mode
        import uuid
        self.leader_lease_id = f"EMERGENCY_{self.node_id}_{uuid.uuid4().hex[:8]}"
        self.leader_lease_expires = now + 120  # Short lease - needs frequent renewal
        self.last_lease_renewal = now

        # Announce emergency leadership
        with self.peers_lock:
            peers = list(self.peers.values())

        timeout = ClientTimeout(total=5)
        async with get_client_session(timeout) as session:
            for peer in peers:
                if peer.node_id != self.node_id:
                    try:
                        url = self._url_for_peer(peer, "/coordinator")
                        await session.post(url, json={
                            "leader_id": self.node_id,
                            "lease_id": self.leader_lease_id,
                            "lease_expires": self.leader_lease_expires,
                            "emergency": True,
                        }, headers=self._auth_headers())
                    except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
                        pass  # Network errors expected during emergency coordination

        self._save_state()
        logger.info(f"EMERGENCY COORDINATOR: {self.node_id} is now emergency leader")

    def _get_peer_health_score(self, peer_id: str) -> float:
        """Calculate health score for a peer (0-100, higher is healthier).

        HEALTH-BASED PEER SELECTION: Considers multiple factors to pick
        the best peer for data sync, avoiding overloaded or unreliable nodes.
        """
        with self.peers_lock:
            peer = self.peers.get(peer_id)
        if not peer or not peer.is_alive():
            return 0.0

        score = 100.0

        # Penalize high resource usage
        cpu = float(getattr(peer, "cpu_percent", 0) or 0)
        memory = float(getattr(peer, "memory_percent", 0) or 0)
        disk = float(getattr(peer, "disk_percent", 0) or 0)

        score -= cpu * 0.3  # CPU impact
        score -= memory * 0.2  # Memory impact
        score -= max(0, disk - 50) * 0.5  # Disk penalty above 50%

        # Penalize consecutive failures (circuit breaker input)
        failures = int(getattr(peer, "consecutive_failures", 0) or 0)
        score -= failures * 10

        # Penalize NAT-blocked peers (slower sync)
        if getattr(peer, "nat_blocked", False):
            score -= 20

        # Bonus for GPU nodes (typically more powerful)
        if getattr(peer, "has_gpu", False):
            score += 10

        # Check circuit breaker
        circuit_breaker = getattr(self, "_p2p_circuit_breaker", {})
        breaker_info = circuit_breaker.get(peer_id, {})
        if breaker_info.get("open_until", 0) > time.time():
            score = 0  # Circuit is open, don't use this peer

        return max(0.0, min(100.0, score))

    def _record_p2p_sync_result(self, peer_id: str, success: bool):
        """Record P2P sync result for circuit breaker, metrics, and reputation.

        CIRCUIT BREAKER: After 3 consecutive failures, open circuit for 5 minutes.
        This prevents wasting time on unreliable peers.

        PEER REPUTATION: Also records sync result for reputation tracking.
        """
        if not hasattr(self, "_p2p_circuit_breaker"):
            self._p2p_circuit_breaker = {}
        if not hasattr(self, "_p2p_sync_metrics"):
            self._p2p_sync_metrics = {"success": 0, "failure": 0, "bytes": 0}

        breaker = self._p2p_circuit_breaker.get(peer_id, {"failures": 0, "open_until": 0})

        # Record for reputation tracking
        self._record_peer_interaction(peer_id, success, "sync")

        if success:
            breaker["failures"] = 0
            breaker["open_until"] = 0
            self._p2p_sync_metrics["success"] += 1
        else:
            breaker["failures"] = breaker.get("failures", 0) + 1
            self._p2p_sync_metrics["failure"] += 1

            # Open circuit after 3 failures
            if breaker["failures"] >= 3:
                breaker["open_until"] = time.time() + 300  # 5 minute cooldown
                logger.info(f"CIRCUIT BREAKER: Opening circuit for {peer_id} (3 failures)")

        self._p2p_circuit_breaker[peer_id] = breaker

    async def _p2p_data_sync(self):
        """DECENTRALIZED: Nodes sync data directly with peers without leader coordination.

        P2P DATA SYNC with enhancements:
        - Health-based peer selection (avoids overloaded nodes)
        - Circuit breaker (skips unreliable peers)
        - Delta sync (only syncs files newer than last sync)
        - Model file prioritization (syncs models first)
        - ADAPTIVE INTERVALS: adjusts based on cluster activity and success rate
        """
        now = time.time()

        # ADAPTIVE INTERVAL: Uses activity-aware interval instead of fixed 5 min
        interval = self._get_adaptive_sync_interval("data")
        last_check = getattr(self, "_last_p2p_sync_check", 0)
        if now - last_check < interval:
            return
        self._last_p2p_sync_check = now

        # Skip if leader is actively managing sync (avoid conflicts)
        if self.role == NodeRole.LEADER:
            return  # Leader uses centralized sync

        # Skip if a sync is already in progress
        if getattr(self, "sync_in_progress", False):
            return

        # Skip if under disk pressure
        if getattr(self.self_info, "disk_percent", 0) > 85:
            return

        # Get our local manifest (use cache for speed)
        local_manifest = getattr(self, "local_data_manifest", None)
        if not local_manifest:
            try:
                local_manifest = self.sync_planner.collect_local_manifest_cached(max_cache_age=600)
                with self.manifest_lock:
                    self.local_data_manifest = local_manifest
            except (AttributeError):
                return

        # Get local file set with timestamps for delta sync
        local_files = {}
        for file_info in getattr(local_manifest, "files", []) or []:
            rel_path = getattr(file_info, "relative_path", "")
            if rel_path:
                local_files[rel_path] = getattr(file_info, "modified_at", 0)

        # Check peer manifests from gossip cache
        peer_manifests = getattr(self, "_gossip_peer_manifests", {})
        if not peer_manifests:
            return

        # Find files we're missing that peers have (with prioritization)
        files_to_sync: dict[str, list[tuple]] = {}  # peer_id -> [(file, priority)]
        file_hashes: dict[str, str] = {}  # file_path -> hash (for dedup tracking)
        last_sync_time = getattr(self, "_last_successful_p2p_sync", 0)

        for peer_id, peer_manifest in peer_manifests.items():
            if peer_id == self.node_id:
                continue

            # Check circuit breaker
            health = self._get_peer_health_score(peer_id)
            if health <= 0:
                continue

            peer_files = getattr(peer_manifest, "files", []) or []
            for file_info in peer_files:
                rel_path = getattr(file_info, "relative_path", "")
                modified_at = getattr(file_info, "modified_at", 0)
                file_hash = getattr(file_info, "file_hash", "")
                file_size = getattr(file_info, "size_bytes", 0)

                if not rel_path:
                    continue

                # Skip if we have this file with same or newer timestamp
                if rel_path in local_files and local_files[rel_path] >= modified_at:
                    continue

                # Skip if file is older than last sync (delta optimization)
                if modified_at < last_sync_time and rel_path in local_files:
                    continue

                # DATA DEDUPLICATION: Skip if we already synced this file (by hash)
                if file_hash and self._is_file_already_synced(file_hash):
                    self._record_dedup_skip(file_count=1, bytes_saved=file_size)
                    continue

                # Calculate priority (models > ELO/training DBs > training data > selfplay)
                priority = 0
                if "models/" in rel_path or rel_path.endswith(".pt") or rel_path.endswith(".onnx"):
                    priority = 100  # Highest priority for models
                elif rel_path.endswith(".db") and ("unified_elo" in rel_path or "elo_ratings" in rel_path):
                    priority = 90  # Very high priority for ELO database
                elif rel_path.endswith(".db") and ("canonical_" in rel_path or "consolidated_training" in rel_path or "training_pool" in rel_path):
                    priority = 80  # High priority for training databases
                elif "training/" in rel_path:
                    priority = 50
                elif rel_path.endswith(".db"):
                    priority = 30  # Medium priority for other databases
                else:
                    priority = 10

                if peer_id not in files_to_sync:
                    files_to_sync[peer_id] = []
                files_to_sync[peer_id].append((rel_path, priority, health))

                # Track hash for dedup recording after sync
                if file_hash:
                    file_hashes[rel_path] = file_hash

        if not files_to_sync:
            return

        # Select best peer using health score AND file count
        def peer_score(peer_id):
            files = files_to_sync[peer_id]
            health = self._get_peer_health_score(peer_id)
            file_score = sum(f[1] for f in files)  # Sum of priorities
            return health * 0.4 + file_score * 0.6

        best_peer = max(files_to_sync.keys(), key=peer_score)
        files_with_priority = files_to_sync[best_peer]

        # Sort by priority (highest first) and take top 10
        files_with_priority.sort(key=lambda x: x[1], reverse=True)
        files_to_request = [f[0] for f in files_with_priority[:10]]

        # Check if peer is alive
        with self.peers_lock:
            peer = self.peers.get(best_peer)

        if not peer or not peer.is_alive():
            return

        # Log and initiate sync
        total_missing = sum(len(f) for f in files_to_sync.values())
        model_files = sum(1 for f in files_to_request if "models/" in f or f.endswith(".pt"))
        logger.info(f"P2P SYNC: Missing {total_missing} files, requesting {len(files_to_request)} "
              f"({model_files} models) from {best_peer} (health={self._get_peer_health_score(best_peer):.0f})")

        try:
            import uuid
            job = DataSyncJob(
                job_id=f"p2p_{uuid.uuid4().hex[:8]}",
                source_node=best_peer,
                target_node=self.node_id,
                files=files_to_request,
            )

            self.sync_in_progress = True
            try:
                success = await self._request_node_sync(job)
                self._record_p2p_sync_result(best_peer, success)
                self._record_sync_result_for_adaptive("data", success)  # ADAPTIVE INTERVAL

                if success:
                    logger.info(f"P2P SYNC: Completed {len(files_to_request)} files from {best_peer}")
                    self._last_successful_p2p_sync = now
                    # Invalidate manifest cache
                    cache_path = self.sync_planner.get_manifest_cache_path()
                    if cache_path.exists():
                        cache_path.unlink()
                    # Update metrics
                    if hasattr(self, "_p2p_sync_metrics"):
                        self._p2p_sync_metrics["bytes"] += job.bytes_transferred
                    # DATA DEDUPLICATION: Record synced file hashes
                    for fpath in files_to_request:
                        if fpath in file_hashes:
                            self._record_synced_file(file_hashes[fpath], 0)
                else:
                    logger.info(f"P2P SYNC: Failed from {best_peer}: {job.error_message}")
            finally:
                self.sync_in_progress = False

        except Exception as e:  # noqa: BLE001
            logger.info(f"P2P SYNC: Error: {e}")
            self._record_sync_result_for_adaptive("data", False)  # ADAPTIVE: record failure
            self._record_p2p_sync_result(best_peer, False)
            self.sync_in_progress = False

    async def _p2p_model_sync(self):
        """DECENTRALIZED: Sync model files via P2P for faster model distribution.

        MODEL P2P SYNC: Ensures all nodes have access to latest trained models
        without relying on leader-coordinated sync. Prioritizes:
        - Newer models (by timestamp)
        - Models for active board configurations
        - NNUE models (smaller, faster to sync)
        - ADAPTIVE INTERVALS: faster during training, slower when idle
        """
        now = time.time()

        # ADAPTIVE INTERVAL: Uses activity-aware interval (faster during training)
        interval = self._get_adaptive_sync_interval("model")
        last_check = getattr(self, "_last_p2p_model_sync", 0)
        if now - last_check < interval:
            return
        self._last_p2p_model_sync = now

        # Skip if sync in progress
        if getattr(self, "sync_in_progress", False):
            return

        # Get model files from local manifest
        local_manifest = getattr(self, "local_data_manifest", None)
        if not local_manifest:
            return

        local_models = set()
        for file_info in getattr(local_manifest, "files", []) or []:
            rel_path = getattr(file_info, "relative_path", "")
            if rel_path and ("models/" in rel_path or rel_path.endswith((".pt", ".onnx", ".bin"))):
                local_models.add(rel_path)

        # Check peer manifests for models we're missing
        peer_manifests = getattr(self, "_gossip_peer_manifests", {})
        missing_models: dict[str, list[str]] = {}

        for peer_id, peer_manifest in peer_manifests.items():
            if peer_id == self.node_id:
                continue

            health = self._get_peer_health_score(peer_id)
            if health <= 0:
                continue

            peer_files = getattr(peer_manifest, "files", []) or []
            for file_info in peer_files:
                rel_path = getattr(file_info, "relative_path", "")
                if not rel_path:
                    continue
                if not ("models/" in rel_path or rel_path.endswith((".pt", ".onnx", ".bin"))):
                    continue
                if rel_path in local_models:
                    continue

                if peer_id not in missing_models:
                    missing_models[peer_id] = []
                missing_models[peer_id].append(rel_path)

        if not missing_models:
            return

        # Pick healthiest peer with models
        best_peer = max(missing_models.keys(), key=lambda p: self._get_peer_health_score(p))
        models_to_sync = missing_models[best_peer][:5]  # Max 5 models per cycle

        with self.peers_lock:
            peer = self.peers.get(best_peer)
        if not peer or not peer.is_alive():
            return

        logger.info(f"MODEL SYNC: Requesting {len(models_to_sync)} models from {best_peer}")

        try:
            import uuid
            job = DataSyncJob(
                job_id=f"model_{uuid.uuid4().hex[:8]}",
                source_node=best_peer,
                target_node=self.node_id,
                files=models_to_sync,
            )

            self.sync_in_progress = True
            try:
                success = await self._request_node_sync(job)
                self._record_p2p_sync_result(best_peer, success)
                self._record_sync_result_for_adaptive("model", success)  # ADAPTIVE INTERVAL
                if success:
                    logger.info(f"MODEL SYNC: Got {len(models_to_sync)} models from {best_peer}")
            finally:
                self.sync_in_progress = False
        except Exception as e:  # noqa: BLE001
            logger.info(f"MODEL SYNC: Error: {e}")
            self._record_sync_result_for_adaptive("model", False)  # ADAPTIVE: record failure
            self.sync_in_progress = False

    async def _p2p_training_db_sync(self):
        """DECENTRALIZED: Sync training databases via P2P for improved training diversity.

        TRAINING DB P2P SYNC: Ensures all nodes have access to consolidated training
        data without relying on leader-coordinated sync. Prioritizes:
        - canonical_*.db (canonical training data)
        - consolidated_training*.db (merged training data)
        - training_pool*.db (training pool databases)

        This improves training diversity by ensuring all nodes can train on
        cluster-wide data, not just their local selfplay games.

        ADAPTIVE INTERVALS: faster during training, slower when idle.
        """
        now = time.time()

        # ADAPTIVE INTERVAL: Uses activity-aware interval (faster during training)
        interval = self._get_adaptive_sync_interval("training_db")
        last_check = getattr(self, "_last_p2p_training_db_sync", 0)
        if now - last_check < interval:
            return
        self._last_p2p_training_db_sync = now

        # Skip if sync is in progress
        if getattr(self, "sync_in_progress", False):
            return

        # Skip if under disk pressure
        if getattr(self.self_info, "disk_percent", 0) > 80:
            return

        # Get our local files
        local_manifest = getattr(self, "local_data_manifest", None)
        if not local_manifest:
            return

        local_dbs = set()
        local_db_sizes = {}
        for file_info in getattr(local_manifest, "files", []) or []:
            rel_path = getattr(file_info, "relative_path", "")
            if rel_path.endswith(".db"):
                local_dbs.add(rel_path)
                local_db_sizes[rel_path] = getattr(file_info, "size_bytes", 0)

        # Check peer manifests for training databases
        peer_manifests = getattr(self, "_gossip_peer_manifests", {})
        if not peer_manifests:
            return

        # Find training databases we're missing or have smaller versions of
        missing_dbs: dict[str, list[tuple]] = {}  # peer_id -> [(db_path, size)]

        for peer_id, peer_manifest in peer_manifests.items():
            if peer_id == self.node_id:
                continue

            # Check circuit breaker
            health = self._get_peer_health_score(peer_id)
            if health <= 0:
                continue

            peer_files = getattr(peer_manifest, "files", []) or []
            for file_info in peer_files:
                rel_path = getattr(file_info, "relative_path", "")
                size = getattr(file_info, "size_bytes", 0)

                # Only sync training-related databases and ELO database
                if not rel_path.endswith(".db"):
                    continue
                if not ("canonical_" in rel_path or "consolidated_training" in rel_path or
                        "training_pool" in rel_path or "unified_elo" in rel_path or
                        "elo_ratings" in rel_path):
                    continue

                # Skip empty databases
                if size < 1024:
                    continue

                # Check if we don't have it or have a smaller version
                local_size = local_db_sizes.get(rel_path, 0)
                if local_size >= size:
                    continue

                if peer_id not in missing_dbs:
                    missing_dbs[peer_id] = []
                missing_dbs[peer_id].append((rel_path, size, health))

        if not missing_dbs:
            return

        # Pick healthiest peer with training DBs
        best_peer = max(missing_dbs.keys(), key=lambda p: self._get_peer_health_score(p))
        dbs_to_sync = [db[0] for db in missing_dbs[best_peer][:3]]  # Max 3 DBs per cycle

        # Check if peer is alive
        with self.peers_lock:
            peer = self.peers.get(best_peer)
        if not peer or not peer.is_alive():
            return

        logger.info(f"TRAINING DB SYNC: Requesting {len(dbs_to_sync)} training DBs from {best_peer}")

        try:
            import uuid
            job = DataSyncJob(
                job_id=f"traindb_{uuid.uuid4().hex[:8]}",
                source_node=best_peer,
                target_node=self.node_id,
                files=dbs_to_sync,
            )

            self.sync_in_progress = True
            try:
                success = await self._request_node_sync(job)
                self._record_p2p_sync_result(best_peer, success)
                self._record_sync_result_for_adaptive("training_db", success)  # ADAPTIVE INTERVAL
                if success:
                    logger.info(f"TRAINING DB SYNC: Got {len(dbs_to_sync)} training DBs from {best_peer}")
            finally:
                self.sync_in_progress = False
        except Exception as e:  # noqa: BLE001
            logger.info(f"TRAINING DB SYNC: Error: {e}")
            self._record_sync_result_for_adaptive("training_db", False)  # ADAPTIVE: record failure
            self.sync_in_progress = False

    async def _gossip_state_to_peers(self):
        """DECENTRALIZED: Share node state with random peers using gossip protocol.

        GOSSIP PROTOCOL: Instead of relying solely on leader to collect state,
        nodes share information with neighbors, and it propagates through the cluster.

        Benefits:
        - Faster state propagation (O(log N) instead of O(N))
        - Works without a leader
        - Resilient to network partitions (state eventually converges)
        - Reduces load on leader

        Implementation:
        1. Each node maintains local state (jobs, resources, health)
        2. Periodically send state to K random peers (fanout)
        3. Receive state from peers and update local view
        4. Include version/timestamp to handle conflicts (last-write-wins)
        """
        now = time.time()

        # Rate limit: gossip every 30 seconds
        last_gossip = getattr(self, "_last_gossip_time", 0)
        if now - last_gossip < 30:
            return
        self._last_gossip_time = now

        # Prepare our state to share
        self._update_self_info()
        local_state = {
            "node_id": self.node_id,
            "timestamp": now,
            "version": int(now * 1000),  # Millisecond version for conflict resolution
            "role": self.role.value if hasattr(self.role, "value") else str(self.role),
            "leader_id": self.leader_id,
            "leader_lease_expires": getattr(self, "leader_lease_expires", 0),
            "selfplay_jobs": getattr(self.self_info, "selfplay_jobs", 0),
            "training_jobs": getattr(self.self_info, "training_jobs", 0),
            "gpu_percent": getattr(self.self_info, "gpu_percent", 0),
            "cpu_percent": getattr(self.self_info, "cpu_percent", 0),
            "memory_percent": getattr(self.self_info, "memory_percent", 0),
            "disk_percent": getattr(self.self_info, "disk_percent", 0),
            "has_gpu": getattr(self.self_info, "has_gpu", False),
            "gpu_name": getattr(self.self_info, "gpu_name", ""),
            "voter_quorum_ok": self._has_voter_quorum(),
        }

        # DISTRIBUTED TRAINING COORDINATION: Include active training configs
        # This allows nodes to coordinate training without a leader
        local_state["active_training_configs"] = self._get_local_active_training_configs()

        # DISTRIBUTED ELO: Include ELO summary for cluster-wide visibility
        local_state["elo_summary"] = self._get_local_elo_summary()

        # GOSSIP-BASED LEADER HINTS: Share leader preference for faster elections
        local_state["leader_hint"] = self._get_leader_hint()

        # PEER REPUTATION: Share peer reliability scores
        local_state["peer_reputation"] = self._get_peer_reputation_summary()

        # DISTRIBUTED TOURNAMENT: Share tournament proposals and active tournaments
        local_state["tournament"] = self._get_tournament_gossip_state()

        # Include manifest summary if available
        local_manifest = getattr(self, "local_data_manifest", None)
        if local_manifest:
            local_state["manifest_summary"] = {
                "total_files": getattr(local_manifest, "total_files", 0),
                "selfplay_games": getattr(local_manifest, "selfplay_games", 0),
                "collected_at": getattr(local_manifest, "collected_at", 0),
            }

        # Select K random peers to gossip with (fanout = 3)
        GOSSIP_FANOUT = 3
        with self.peers_lock:
            alive_peers = [
                p for p in self.peers.values()
                if p.is_alive() and not getattr(p, "retired", False)
            ]

        if not alive_peers:
            return

        import random
        peers_to_gossip = random.sample(alive_peers, min(GOSSIP_FANOUT, len(alive_peers)))

        # Send gossip to selected peers
        timeout = ClientTimeout(total=5)
        async with get_client_session(timeout) as session:
            for peer in peers_to_gossip:
                try:
                    # Include known state about other nodes (propagation)
                    gossip_payload = {
                        "sender": self.node_id,
                        "sender_state": local_state,
                        "known_states": self._get_gossip_known_states(),
                        # Phase 28: Peer-of-peer discovery - share peer endpoints
                        "peer_endpoints": self._get_peer_endpoints_for_gossip(),
                        # Phase 29: Cluster epoch for split-brain resolution
                        "cluster_epoch": self._cluster_epoch,
                    }

                    # GOSSIP COMPRESSION: Compress payload with gzip to reduce network transfer
                    json_bytes = json.dumps(gossip_payload).encode("utf-8")
                    original_size = len(json_bytes)
                    compressed_bytes = gzip.compress(json_bytes, compresslevel=6)
                    compressed_size = len(compressed_bytes)

                    # Track compression metrics
                    self._record_gossip_compression(original_size, compressed_size)

                    start_time = time.time()
                    for url in self._urls_for_peer(peer, "/gossip"):
                        try:
                            headers = self._auth_headers()
                            headers["Content-Encoding"] = "gzip"
                            headers["Content-Type"] = "application/json"
                            async with session.post(url, data=compressed_bytes, headers=headers) as resp:
                                if resp.status == 200:
                                    # Process response (peer shares their state back)
                                    # Check if response is compressed
                                    content_encoding = resp.headers.get("Content-Encoding", "")
                                    if content_encoding == "gzip":
                                        response_bytes = await resp.read()
                                        try:
                                            decompressed = gzip.decompress(response_bytes)
                                            payload_bytes = decompressed
                                        except OSError as e:
                                            logger.debug(
                                                f"[Gossip] Gzip decode failed from {peer.node_id}: {e}. "
                                                "Falling back to plain JSON."
                                            )
                                            payload_bytes = response_bytes
                                        response_data = json.loads(payload_bytes.decode("utf-8"))
                                    else:
                                        response_data = await resp.json()
                                    self._process_gossip_response(response_data)
                                    # Record metrics
                                    latency_ms = (time.time() - start_time) * 1000
                                    self._record_gossip_metrics("sent", peer.node_id)
                                    self._record_gossip_metrics("latency", peer.node_id, latency_ms)
                                    break
                        except (aiohttp.ClientError, asyncio.TimeoutError, json.JSONDecodeError, AttributeError):
                            continue
                except (aiohttp.ClientError, asyncio.TimeoutError, json.JSONDecodeError, AttributeError):
                    pass

    def _get_gossip_known_states(self) -> dict[str, dict]:
        """Get known states about other nodes to propagate via gossip."""
        known = {}
        gossip_states = getattr(self, "_gossip_peer_states", {})
        # Only share recent states (last 5 minutes)
        cutoff = time.time() - 300
        for node_id, state in gossip_states.items():
            if state.get("timestamp", 0) > cutoff:
                known[node_id] = state
        return known

    def _get_peer_endpoints_for_gossip(self) -> list[dict[str, Any]]:
        """Phase 28: Get peer endpoints to share via gossip for peer-of-peer discovery.

        Returns a list of alive peer endpoints with connection info.
        This enables nodes to discover peers they can't reach directly.
        """
        endpoints = []
        with self.peers_lock:
            # Get alive, non-retired peers
            alive_peers = [
                p for p in self.peers.values()
                if p.node_id != self.node_id and p.is_alive() and not getattr(p, "retired", False)
            ]

        # Limit to top N peers to avoid payload bloat
        for peer in alive_peers[:GOSSIP_MAX_PEER_ENDPOINTS]:
            endpoint = {
                "node_id": peer.node_id,
                "host": str(getattr(peer, "host", "") or ""),
                "port": int(getattr(peer, "port", DEFAULT_PORT) or DEFAULT_PORT),
                "tailscale_ip": str(getattr(peer, "tailscale_ip", "") or ""),
                "is_alive": True,
                "last_heartbeat": float(getattr(peer, "last_heartbeat", 0) or 0),
            }
            endpoints.append(endpoint)

        return endpoints

    def _process_gossip_response(self, response: dict):
        """Process gossip response from a peer, updating our view of the cluster."""
        if not response:
            return

        # Initialize gossip state storage if needed
        if not hasattr(self, "_gossip_peer_states"):
            self._gossip_peer_states = {}
        if not hasattr(self, "_gossip_peer_manifests"):
            self._gossip_peer_manifests = {}

        # Process sender's state
        sender_state = response.get("sender_state", {})
        if sender_state:
            sender_id = sender_state.get("node_id")
            if sender_id and sender_id != self.node_id:
                existing = self._gossip_peer_states.get(sender_id, {})
                # Last-write-wins conflict resolution
                if sender_state.get("version", 0) > existing.get("version", 0):
                    self._gossip_peer_states[sender_id] = sender_state

                    # Update leader info if sender claims to know a leader
                    if sender_state.get("leader_id") and not self.leader_id:
                        claimed_leader = sender_state.get("leader_id")
                        lease_expires = sender_state.get("leader_lease_expires", 0)
                        if lease_expires > time.time():
                            self.leader_id = claimed_leader
                            self.last_leader_seen = time.time()

        # Process known states (propagation)
        known_states = response.get("known_states", {})
        for node_id, state in known_states.items():
            if node_id == self.node_id:
                continue
            existing = self._gossip_peer_states.get(node_id, {})
            if state.get("version", 0) > existing.get("version", 0):
                self._gossip_peer_states[node_id] = state

        # Process manifest info for P2P sync
        peer_manifests = response.get("peer_manifests", {})
        for node_id, manifest_data in peer_manifests.items():
            if node_id != self.node_id:
                with contextlib.suppress(Exception):
                    self._gossip_peer_manifests[node_id] = NodeDataManifest.from_dict(manifest_data)

        # Process tournament gossip for distributed scheduling
        for node_id, state in known_states.items():
            if node_id == self.node_id:
                continue
            tournament_state = state.get("tournament")
            if tournament_state:
                with contextlib.suppress(Exception):
                    self._process_tournament_gossip(node_id, tournament_state)

        # Check for tournament consensus after processing gossip
        with contextlib.suppress(Exception):
            self._check_tournament_consensus()

        # Phase 28: Process peer endpoints for peer-of-peer discovery
        peer_endpoints = response.get("peer_endpoints") or []
        if peer_endpoints:
            self._process_gossip_peer_endpoints(peer_endpoints)

        # Phase 29: Process cluster epoch for split-brain resolution
        incoming_epoch = response.get("cluster_epoch")
        if incoming_epoch is not None:
            self._handle_incoming_cluster_epoch(incoming_epoch, response)

    def _process_gossip_peer_endpoints(self, peer_endpoints: list[dict]) -> None:
        """Phase 28: Process peer endpoints learned via gossip.

        Enables discovery of peers we can't reach directly through intermediaries.
        """
        for endpoint in peer_endpoints:
            node_id = endpoint.get("node_id")
            if not node_id or node_id == self.node_id:
                continue

            # Store in gossip-learned endpoints for later connection attempts
            host = endpoint.get("tailscale_ip") or endpoint.get("host")
            port = endpoint.get("port", DEFAULT_PORT)

            if host and port:
                self._gossip_learned_endpoints[node_id] = {
                    "host": host,
                    "port": port,
                    "tailscale_ip": endpoint.get("tailscale_ip", ""),
                    "last_heartbeat": endpoint.get("last_heartbeat", 0),
                    "learned_at": time.time(),
                }

                # If this is an unknown peer, try to connect
                if node_id not in self.peers:
                    # Queue for async connection attempt
                    asyncio.create_task(self._try_connect_gossip_peer(node_id, host, port))

    async def _try_connect_gossip_peer(self, node_id: str, host: str, port: int) -> None:
        """Phase 28: Attempt to connect to a peer learned via gossip."""
        try:
            # Check if already connected
            if node_id in self.peers and self.peers[node_id].is_alive():
                return

            logger.info(f"Attempting connection to gossip-learned peer: {node_id} at {host}:{port}")

            # Try to send heartbeat
            info = await self._send_heartbeat_to_peer(host, port)
            if info:
                with self.peers_lock:
                    self.peers[info.node_id] = info
                logger.info(f"Successfully connected to gossip-learned peer: {info.node_id}")

                # Save to cache for future restarts
                self._save_peer_to_cache(
                    info.node_id, host, port,
                    str(getattr(info, "tailscale_ip", "") or "")
                )
        except Exception as e:  # noqa: BLE001
            if self.verbose:
                logger.debug(f"Failed to connect to gossip-learned peer {node_id}: {e}")

    def _handle_incoming_cluster_epoch(self, incoming_epoch: Any, response: dict) -> None:
        """Phase 29: Handle incoming cluster epoch for split-brain resolution."""
        try:
            epoch = int(incoming_epoch)
        except (ValueError, TypeError):
            return

        if epoch > self._cluster_epoch:
            # Accept higher epoch - this cluster partition is more authoritative
            logger.info(f"Adopting higher cluster epoch: {epoch} (was {self._cluster_epoch})")
            self._cluster_epoch = epoch
            self._save_cluster_epoch()

            # If response includes a leader, adopt it
            sender_state = response.get("sender_state", {})
            incoming_leader = sender_state.get("leader_id")
            if incoming_leader and incoming_leader != self.node_id:
                if self.role == NodeRole.LEADER:
                    logger.info(f"Stepping down: higher epoch cluster has leader {incoming_leader}")
                    self.role = NodeRole.FOLLOWER
                self.leader_id = incoming_leader

    # Gossip metrics methods (provided by GossipProtocolMixin after Dec 28, 2025 merge):
    # _record_gossip_metrics, _record_gossip_compression, _reset_gossip_metrics_hourly
    # _get_gossip_metrics_summary, _get_gossip_health_status

    async def _gossip_anti_entropy_repair(self):
        """DECENTRALIZED: Periodic full state reconciliation with random peer.

        ANTI-ENTROPY REPAIR: Gossip protocols can miss updates due to:
        - Network partitions
        - Message loss
        - Node restarts

        Solution: Periodically do full state exchange with a random peer to
        ensure eventual consistency. This catches any missed updates.

        Frequency: Every 5 minutes with a random healthy peer
        """
        now = time.time()

        # Rate limit: anti-entropy every 5 minutes
        last_repair = getattr(self, "_last_anti_entropy_repair", 0)
        if now - last_repair < 300:
            return
        self._last_anti_entropy_repair = now

        # Select a random healthy peer for full state exchange
        with self.peers_lock:
            alive_peers = [
                p for p in self.peers.values()
                if p.is_alive() and not getattr(p, "retired", False)
            ]

        if not alive_peers:
            return

        import random
        peer = random.choice(alive_peers)

        # Prepare full state dump (not just recent states)
        full_state = {
            "anti_entropy": True,  # Flag for full state exchange
            "sender": self.node_id,
            "timestamp": now,
            "all_known_states": {},
        }

        # Include all known peer states (not just recent)
        gossip_states = getattr(self, "_gossip_peer_states", {})
        for node_id, state in gossip_states.items():
            full_state["all_known_states"][node_id] = state

        # Include our own state
        self._update_self_info()
        full_state["all_known_states"][self.node_id] = {
            "node_id": self.node_id,
            "timestamp": now,
            "version": int(now * 1000),
            "role": self.role.value if hasattr(self.role, "value") else str(self.role),
            "leader_id": self.leader_id,
            "selfplay_jobs": getattr(self.self_info, "selfplay_jobs", 0),
            "training_jobs": getattr(self.self_info, "training_jobs", 0),
        }

        # Send anti-entropy request
        start_time = time.time()
        timeout = ClientTimeout(total=10)  # Longer timeout for full exchange
        try:
            async with get_client_session(timeout) as session:
                for url in self._urls_for_peer(peer, "/gossip/anti-entropy"):
                    try:
                        async with session.post(url, json=full_state, headers=self._auth_headers()) as resp:
                            if resp.status == 200:
                                response_data = await resp.json()
                                latency = (time.time() - start_time) * 1000
                                self._record_gossip_metrics("latency", peer.node_id, latency)

                                # Process peer's full state
                                peer_states = response_data.get("all_known_states", {})
                                updates = 0
                                for node_id, state in peer_states.items():
                                    if node_id == self.node_id:
                                        continue
                                    existing = self._gossip_peer_states.get(node_id, {})
                                    if state.get("version", 0) > existing.get("version", 0):
                                        self._gossip_peer_states[node_id] = state
                                        updates += 1
                                        self._record_gossip_metrics("update", node_id)

                                # Check for stale states we have that peer doesn't know
                                our_nodes = set(self._gossip_peer_states.keys())
                                peer_nodes = set(peer_states.keys())
                                stale_candidates = our_nodes - peer_nodes - {self.node_id}

                                for stale_node in stale_candidates:
                                    stale_state = self._gossip_peer_states.get(stale_node, {})
                                    # If state is older than 10 minutes and peer doesn't know it,
                                    # the node might be offline - mark as stale
                                    if stale_state.get("timestamp", 0) < now - 600:
                                        self._record_gossip_metrics("stale", stale_node)

                                if updates > 0:
                                    self._record_gossip_metrics("anti_entropy")
                                    logger.debug(f"[GOSSIP] Anti-entropy repair: {updates} state updates from {peer.node_id}")

                                return
                    except (aiohttp.ClientError, asyncio.TimeoutError, json.JSONDecodeError, KeyError, ValueError):
                        continue
        except (AttributeError, KeyError, ValueError, TypeError):
            pass  # Silent failure, will retry next cycle

    # =========================================================================
    # DISTRIBUTED TRAINING COORDINATION
    # =========================================================================
    # These functions enable nodes to coordinate training decisions without
    # relying on a leader, using gossip to share training state cluster-wide.
    # =========================================================================

    def _get_local_active_training_configs(self) -> list[dict]:
        """Get list of training configs currently running on this node.

        DISTRIBUTED TRAINING: Share what training this node is doing so other
        nodes can avoid duplicate training for the same configuration.

        Returns list of dicts with:
        - config_key: e.g. "square8_2p"
        - job_type: "nnue", "cmaes", etc.
        - started_at: timestamp when training started
        """
        active_configs = []
        with self.jobs_lock:
            for _job_id, job in self.local_jobs.items():
                job_type = getattr(job, "job_type", "")
                # Only include training-type jobs
                if job_type in ("nnue", "nnue_training", "training", "cmaes"):
                    board_type = getattr(job, "board_type", "")
                    num_players = getattr(job, "num_players", 2)
                    if board_type:
                        config_key = f"{board_type}_{num_players}p"
                        started_at = getattr(job, "started_at", time.time())
                        active_configs.append({
                            "config_key": config_key,
                            "job_type": job_type,
                            "started_at": started_at,
                        })
        return active_configs

    def _get_cluster_active_training_configs(self) -> dict[str, list[str]]:
        """Get all active training configs across the cluster via gossip.

        DISTRIBUTED TRAINING COORDINATION: Query gossip state to see what
        training is running cluster-wide. This enables nodes to avoid
        duplicate training without leader coordination.

        Returns: { config_key -> [list of node_ids training that config] }
        """
        cluster_configs: dict[str, list[str]] = {}

        # Include our own training
        for config in self._get_local_active_training_configs():
            config_key = config["config_key"]
            if config_key not in cluster_configs:
                cluster_configs[config_key] = []
            cluster_configs[config_key].append(self.node_id)

        # Include training from gossip state
        gossip_states = getattr(self, "_gossip_peer_states", {})
        now = time.time()
        for node_id, state in gossip_states.items():
            # Skip stale states (older than 2 minutes)
            if state.get("timestamp", 0) < now - 120:
                continue
            # Skip our own state
            if node_id == self.node_id:
                continue

            active_training = state.get("active_training_configs", [])
            for config in active_training:
                config_key = config.get("config_key", "")
                if config_key:
                    if config_key not in cluster_configs:
                        cluster_configs[config_key] = []
                    if node_id not in cluster_configs[config_key]:
                        cluster_configs[config_key].append(node_id)

        return cluster_configs

    def _is_config_being_trained_cluster_wide(self, config_key: str) -> tuple[bool, list[str]]:
        """Check if a config is already being trained somewhere in the cluster.

        DISTRIBUTED TRAINING: Before starting training for a config, check if
        another node is already training it. This avoids wasted resources.

        Returns: (is_being_trained, list_of_nodes_training_it)
        """
        cluster_configs = self._get_cluster_active_training_configs()
        training_nodes = cluster_configs.get(config_key, [])
        return (len(training_nodes) > 0, training_nodes)

    def _should_claim_training_slot(self, config_key: str) -> bool:
        """Decide if this node should claim a training slot for a config.

        DISTRIBUTED TRAINING COORDINATION: Use a deterministic algorithm to
        decide which node gets to train a config when multiple nodes want to.

        Algorithm:
        - If no one is training this config, the node with lowest ID claims it
        - If already training, don't start a duplicate
        - Include jitter to handle race conditions
        """
        is_training, _training_nodes = self._is_config_being_trained_cluster_wide(config_key)

        if is_training:
            # Config is already being trained somewhere
            return False

        # Get all nodes that might want to train (GPU nodes with data)
        candidate_nodes = [self.node_id]
        gossip_states = getattr(self, "_gossip_peer_states", {})
        now = time.time()
        for node_id, state in gossip_states.items():
            if state.get("timestamp", 0) < now - 120:
                continue
            if state.get("has_gpu", False):
                training_jobs = state.get("training_jobs", 0)
                # Only consider nodes with capacity (< 3 training jobs)
                if training_jobs < 3:
                    candidate_nodes.append(node_id)

        # Sort deterministically
        candidate_nodes = sorted(set(candidate_nodes))

        # The node with lowest ID that has capacity claims the slot
        # Add position-based jitter: higher position = less likely to claim
        import random
        my_position = candidate_nodes.index(self.node_id) if self.node_id in candidate_nodes else len(candidate_nodes)

        # First candidate always claims, others have decreasing probability
        claim_probability = max(0.1, 1.0 - (my_position * 0.3))

        return random.random() < claim_probability

    # =========================================================================
    # TRAINING TRIGGER IDEMPOTENCY (Phase 4 - Dec 2025)
    # =========================================================================
    # Hash-based deduplication to prevent duplicate training during leader
    # transitions. Each training trigger is hashed and stored; subsequent
    # triggers with the same hash within the TTL are rejected.
    # =========================================================================

    def _compute_training_trigger_hash(self, config_key: str, game_count: int) -> str:
        """Compute a hash for training trigger deduplication.

        IDEMPOTENCY: Hash is based on:
        - config_key (board_type + num_players)
        - game_count bucket (rounded to 1000 to allow minor variations)
        - time bucket (15-minute windows)

        This allows the same trigger to be rejected if attempted multiple times
        within a 15-minute window for the same approximate data state.
        """
        import hashlib

        # Round game count to nearest 1000 to tolerate minor variations
        game_bucket = (game_count // 1000) * 1000

        # Use 15-minute time buckets
        time_bucket = int(time.time() // 900) * 900

        hash_input = f"{config_key}:{game_bucket}:{time_bucket}"
        return hashlib.sha256(hash_input.encode()).hexdigest()[:16]

    def _is_training_trigger_duplicate(self, trigger_hash: str) -> bool:
        """Check if a training trigger is a duplicate.

        IDEMPOTENCY: Returns True if this trigger hash was seen recently.
        """
        if not hasattr(self, "_training_trigger_cache"):
            self._training_trigger_cache: dict[str, float] = {}

        now = time.time()
        ttl = 900  # 15-minute TTL for trigger cache

        # Cleanup old entries
        expired = [h for h, ts in self._training_trigger_cache.items() if now - ts > ttl]
        for h in expired:
            del self._training_trigger_cache[h]

        # Check if duplicate
        return trigger_hash in self._training_trigger_cache

    def _record_training_trigger(self, trigger_hash: str) -> None:
        """Record a training trigger for deduplication."""
        if not hasattr(self, "_training_trigger_cache"):
            self._training_trigger_cache = {}

        self._training_trigger_cache[trigger_hash] = time.time()

    def _check_training_idempotency(self, config_key: str, game_count: int) -> tuple[bool, str]:
        """Check if training can proceed (idempotency check).

        Returns:
            (can_proceed, trigger_hash) - can_proceed is False if duplicate
        """
        trigger_hash = self._compute_training_trigger_hash(config_key, game_count)

        if self._is_training_trigger_duplicate(trigger_hash):
            logger.info(f"IDEMPOTENT: Training trigger {trigger_hash[:8]} for {config_key} is duplicate, skipping")
            return False, trigger_hash

        return True, trigger_hash

    def _get_distributed_training_summary(self) -> dict:
        """Get summary of distributed training state for /status endpoint."""
        cluster_configs = self._get_cluster_active_training_configs()
        return {
            "active_configs": list(cluster_configs.keys()),
            "total_training_jobs": sum(len(nodes) for nodes in cluster_configs.values()),
            "configs_by_node_count": {k: len(v) for k, v in cluster_configs.items()},
        }

    # =========================================================================
    # DISTRIBUTED ELO
    # =========================================================================
    # Share ELO ratings via gossip for cluster-wide visibility without
    # requiring every node to query the ELO database directly.
    # =========================================================================

    def _get_local_elo_summary(self) -> dict:
        """Get summary of local ELO ratings for gossip propagation.

        DISTRIBUTED ELO: Share top models and their ratings via gossip so all
        nodes have visibility into model performance without querying the DB.

        LAZY LOADING: Defers ELO query until after startup (60s) to avoid
        slowing node initialization. Uses 10-minute cache to reduce DB load.

        Returns dict with:
        - top_models: List of top 5 models with ratings
        - total_models: Total number of rated models
        - last_update: Timestamp of last ELO update
        """
        now = time.time()
        cache_key = "_elo_summary_cache"
        cache_time_key = "_elo_summary_cache_time"
        startup_key = "_elo_startup_time"
        cached = getattr(self, cache_key, None)
        cached_time = getattr(self, cache_time_key, 0)

        # Track startup time for lazy loading
        if not hasattr(self, startup_key):
            setattr(self, startup_key, now)

        startup_time = getattr(self, startup_key, now)

        # LAZY LOADING: Don't query ELO during first 60s of startup
        if now - startup_time < 60:
            return {"top_models": [], "total_models": 0, "last_update": 0, "deferred": True}

        # Use 10-minute cache (was 5 min) to reduce DB load
        if cached and now - cached_time < 600:
            return cached

        summary = {
            "top_models": [],
            "total_models": 0,
            "last_update": 0,
        }

        try:
            from app.tournament import get_elo_database
            db = get_elo_database()

            # Get top 5 models by ELO (single optimized query)
            with db._get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT participant_id, rating, games_played, last_update,
                           (SELECT COUNT(*) FROM elo_ratings) as total,
                           (SELECT MAX(last_update) FROM elo_ratings) as max_updated
                    FROM elo_ratings
                    ORDER BY rating DESC
                    LIMIT 5
                """)
                rows = cursor.fetchall()

                if rows:
                    summary["total_models"] = rows[0][4] if rows[0][4] else 0
                    summary["last_update"] = rows[0][5] if rows[0][5] else 0

                for row in rows:
                    summary["top_models"].append({
                        "model": row[0],
                        "elo": round(row[1]),
                        "games": row[2],
                    })

        except (KeyError, IndexError, AttributeError, ImportError):
            # Silently fail - ELO summary is optional
            pass

        # Cache the result
        setattr(self, cache_key, summary)
        setattr(self, cache_time_key, now)

        return summary

    def _get_cluster_elo_summary(self) -> dict:
        """Get cluster-wide ELO summary from gossip state.

        DISTRIBUTED ELO: Aggregate ELO info from all nodes via gossip to get
        a cluster-wide view of model performance.
        """
        all_models = {}
        gossip_states = getattr(self, "_gossip_peer_states", {})
        now = time.time()

        # Include our own ELO summary
        local_summary = self._get_local_elo_summary()
        for model_info in local_summary.get("top_models", []):
            model_name = model_info.get("model", "")
            if model_name:
                all_models[model_name] = model_info

        # Include ELO summaries from gossip
        for _node_id, state in gossip_states.items():
            if state.get("timestamp", 0) < now - 300:  # Skip stale states
                continue

            elo_summary = state.get("elo_summary", {})
            for model_info in elo_summary.get("top_models", []):
                model_name = model_info.get("model", "")
                if model_name:
                    # Keep highest ELO seen for each model
                    existing = all_models.get(model_name, {})
                    if model_info.get("elo", 0) > existing.get("elo", 0):
                        all_models[model_name] = model_info

        # Sort by ELO and return top 10
        sorted_models = sorted(all_models.values(), key=lambda x: x.get("elo", 0), reverse=True)
        return {
            "top_models": sorted_models[:10],
            "total_unique_models": len(all_models),
        }

    def _load_curriculum_weights(self) -> dict[str, float]:
        """Load curriculum weights for selfplay prioritization."""
        if not HAS_CURRICULUM_WEIGHTS or load_curriculum_weights is None:
            return {}
        try:
            return load_curriculum_weights()
        except Exception as e:  # noqa: BLE001
            logger.debug(f"[P2P] Failed to load curriculum weights: {e}")
            return {}

    # =========================================================================
    # AUTOMATIC NODE RECOVERY
    # =========================================================================
    # Detect stuck/unhealthy nodes via gossip and trigger automatic recovery
    # (service restart) to maintain cluster health without manual intervention.
    # =========================================================================

    async def _check_node_recovery(self):
        """DECENTRALIZED: Detect and recover stuck nodes via gossip.

        AUTOMATIC NODE RECOVERY: Uses gossip to detect nodes that are:
        - Unresponsive (stale gossip timestamp)
        - Stuck (high failure count, no job progress)
        - Resource-exhausted (high disk/memory)

        Recovery actions:
        - SSH to node and restart the ringrift-p2p service
        - Only leader attempts recovery to avoid duplicate restarts
        - Rate limit recovery attempts (one per node per 10 minutes)
        """
        # Only leader performs recovery to avoid duplicate restarts
        if self.role != NodeRole.LEADER:
            return

        now = time.time()

        # Rate limit: check every 2 minutes
        last_check = getattr(self, "_last_node_recovery_check", 0)
        if now - last_check < 120:
            return
        self._last_node_recovery_check = now

        # Initialize recovery tracking
        if not hasattr(self, "_node_recovery_attempts"):
            self._node_recovery_attempts = {}  # node_id -> last_attempt_time
        if not hasattr(self, "_node_recovery_metrics"):
            self._node_recovery_metrics = {"attempts": 0, "successes": 0, "failures": 0}

        # Check each peer for health issues
        gossip_states = getattr(self, "_gossip_peer_states", {})
        nodes_to_recover = []

        with self.peers_lock:
            for node_id, peer in self.peers.items():
                if node_id == self.node_id:
                    continue

                # Skip recently recovered nodes (10 minute cooldown)
                last_attempt = self._node_recovery_attempts.get(node_id, 0)
                if now - last_attempt < 600:
                    continue

                # Check for unhealthy indicators
                needs_recovery = False
                reason = ""

                # 1. Peer not alive (no recent heartbeat)
                if not peer.is_alive():
                    needs_recovery = True
                    reason = "not responding to heartbeat"

                # 2. Stale gossip state (no updates in 5 minutes)
                elif node_id in gossip_states:
                    state = gossip_states[node_id]
                    state_age = now - state.get("timestamp", 0)
                    if state_age > 300:
                        needs_recovery = True
                        reason = f"stale gossip ({int(state_age)}s old)"

                # 3. High consecutive failures
                elif getattr(peer, "consecutive_failures", 0) >= 5:
                    needs_recovery = True
                    reason = f"high failure count ({peer.consecutive_failures})"

                # 4. Disk nearly full (>95%)
                elif getattr(peer, "disk_percent", 0) > 95:
                    needs_recovery = True
                    reason = f"disk full ({peer.disk_percent}%)"

                if needs_recovery:
                    nodes_to_recover.append((node_id, peer, reason))

        # Attempt recovery for identified nodes (max 2 per cycle)
        for node_id, peer, reason in nodes_to_recover[:2]:
            logger.info(f"NODE RECOVERY: Attempting to recover {node_id} ({reason})")
            self._node_recovery_attempts[node_id] = now
            self._node_recovery_metrics["attempts"] += 1

            # ALERTING: Notify on node recovery attempt
            asyncio.create_task(self.notifier.send(
                title="Node Recovery Initiated",
                message=f"Attempting to recover node {node_id}: {reason}",
                level="warning",
                fields={
                    "Node": node_id,
                    "Reason": reason,
                    "Host": getattr(peer, "host", "unknown"),
                },
                node_id=self.node_id,
            ))

            success = await self._attempt_node_recovery(node_id, peer)
            if success:
                self._node_recovery_metrics["successes"] += 1
                logger.info(f"NODE RECOVERY: Successfully restarted {node_id}")
                # ALERTING: Notify on successful recovery
                asyncio.create_task(self.notifier.send(
                    title="Node Recovery Success",
                    message=f"Successfully recovered node {node_id}",
                    level="info",
                    fields={"Node": node_id, "Reason": reason},
                    node_id=self.node_id,
                ))
            else:
                self._node_recovery_metrics["failures"] += 1
                logger.info(f"NODE RECOVERY: Failed to restart {node_id}")
                # ALERTING: Notify on failed recovery
                asyncio.create_task(self.notifier.send(
                    title="Node Recovery Failed",
                    message=f"Failed to recover node {node_id} ({reason})",
                    level="error",
                    fields={
                        "Node": node_id,
                        "Reason": reason,
                        "Action": "Manual intervention may be required",
                    },
                    node_id=self.node_id,
                ))

    async def _attempt_node_recovery(self, node_id: str, peer) -> bool:
        """Attempt to recover a node by restarting its service via SSH.

        Returns True if recovery command succeeded, False otherwise.
        """
        host = getattr(peer, "host", None)
        if not host:
            return False

        try:
            pass

            # Try to restart the service via SSH
            cmd = f"timeout 30 ssh -o ConnectTimeout=10 -o StrictHostKeyChecking=no {host} 'sudo systemctl restart ringrift-p2p'"
            proc = await asyncio.create_subprocess_shell(
                cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            _stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=45)

            if proc.returncode == 0:
                return True
            else:
                logger.info(f"NODE RECOVERY: SSH failed for {node_id}: {stderr.decode()[:100]}")
                return False

        except asyncio.TimeoutError:
            logger.info(f"NODE RECOVERY: SSH timeout for {node_id}")
            return False
        except Exception as e:  # noqa: BLE001
            logger.info(f"NODE RECOVERY: Error recovering {node_id}: {e}")
            return False

    def _get_node_recovery_metrics(self) -> dict:
        """Get node recovery metrics for /status endpoint."""
        metrics = getattr(self, "_node_recovery_metrics", {"attempts": 0, "successes": 0, "failures": 0})
        attempts = getattr(self, "_node_recovery_attempts", {})
        now = time.time()

        # Count nodes in recovery cooldown
        in_cooldown = sum(1 for t in attempts.values() if now - t < 600)

        return {
            "total_attempts": metrics.get("attempts", 0),
            "successes": metrics.get("successes", 0),
            "failures": metrics.get("failures", 0),
            "nodes_in_cooldown": in_cooldown,
        }

    # =========================================================================
    # GOSSIP-BASED LEADER HINTS
    # =========================================================================
    # Share leader preferences via gossip to enable faster leader elections.
    # When current leader fails, nodes can quickly converge on a new leader
    # based on hints from peers rather than running full election.
    # =========================================================================

    def _get_leader_hint(self) -> dict:
        """Get this node's leader hint for gossip propagation.

        LEADER HINTS: Share information about preferred leader candidates to
        enable faster convergence during elections. Hints include:
        - Current known leader and lease expiry
        - Preferred successor (highest-priority eligible node)
        - This node's priority rank
        """
        hint = {
            "current_leader": self.leader_id,
            "lease_expires": getattr(self, "leader_lease_expires", 0),
            "preferred_successor": None,
            "my_priority": 0,
        }

        # Calculate this node's priority (lower is better for Bully algorithm)
        # But we want to express it as a score (higher is better)
        with self.peers_lock:
            all_nodes = [self.node_id] + [p.node_id for p in self.peers.values() if p.is_alive()]

        all_nodes_sorted = sorted(all_nodes, reverse=True)  # Bully: higher ID wins
        if self.node_id in all_nodes_sorted:
            hint["my_priority"] = len(all_nodes_sorted) - all_nodes_sorted.index(self.node_id)

        # Find preferred successor (highest priority eligible node that's not current leader)
        voter_ids = list(getattr(self, "voter_node_ids", []) or [])
        for node_id in all_nodes_sorted:
            if node_id == self.leader_id:
                continue
            if voter_ids and node_id not in voter_ids:
                continue
            hint["preferred_successor"] = node_id
            break

        return hint

    def _get_cluster_leader_consensus(self) -> dict:
        """Get cluster consensus on leader from gossip hints.

        LEADER CONSENSUS: Aggregate leader hints from all nodes to determine
        if there's agreement on who the leader is/should be.
        """
        leader_votes = {}
        successor_votes = {}
        gossip_states = getattr(self, "_gossip_peer_states", {})
        now = time.time()

        # Count our vote
        our_hint = self._get_leader_hint()
        if our_hint["current_leader"]:
            leader_votes[our_hint["current_leader"]] = leader_votes.get(our_hint["current_leader"], 0) + 1
        if our_hint["preferred_successor"]:
            successor_votes[our_hint["preferred_successor"]] = successor_votes.get(our_hint["preferred_successor"], 0) + 1

        # Count votes from gossip
        for _node_id, state in gossip_states.items():
            if state.get("timestamp", 0) < now - 120:  # Skip stale states
                continue

            hint = state.get("leader_hint", {})
            leader = hint.get("current_leader")
            successor = hint.get("preferred_successor")

            if leader:
                leader_votes[leader] = leader_votes.get(leader, 0) + 1
            if successor:
                successor_votes[successor] = successor_votes.get(successor, 0) + 1

        # Find consensus leader and successor
        consensus_leader = max(leader_votes.items(), key=lambda x: x[1])[0] if leader_votes else None
        consensus_successor = max(successor_votes.items(), key=lambda x: x[1])[0] if successor_votes else None

        result = {
            "consensus_leader": consensus_leader,
            "leader_agreement": leader_votes.get(consensus_leader, 0) if consensus_leader else 0,
            "consensus_successor": consensus_successor,
            "successor_agreement": successor_votes.get(consensus_successor, 0) if consensus_successor else 0,
            "total_voters": len(gossip_states) + 1,
        }

        # ALERTING: Check for low leader consensus (only leader alerts, rate limited)
        if self.role == NodeRole.LEADER and result["total_voters"] >= 3:
            agreement_ratio = result["leader_agreement"] / result["total_voters"]
            last_low_consensus_alert = getattr(self, "_last_low_consensus_alert", 0)
            if agreement_ratio < 0.5 and now - last_low_consensus_alert > 3600:  # Alert once per hour max
                self._last_low_consensus_alert = now
                asyncio.create_task(self.notifier.send(
                    title="Low Leader Consensus",
                    message=f"Only {result['leader_agreement']}/{result['total_voters']} nodes agree on leader",
                    level="warning",
                    fields={
                        "Agreement": f"{agreement_ratio*100:.0f}%",
                        "Consensus Leader": str(consensus_leader),
                        "Total Voters": str(result["total_voters"]),
                        "Action": "Check for network partitions or stale nodes",
                    },
                    node_id=self.node_id,
                ))

        return result

    # =========================================================================
    # PEER REPUTATION TRACKING
    # =========================================================================
    # Track peer reliability over time for better peer selection in P2P sync,
    # gossip, and other distributed operations.
    # =========================================================================

    def _record_peer_interaction(self, peer_id: str, success: bool, interaction_type: str = "general"):
        """Record a peer interaction for reputation tracking.

        PEER REPUTATION: Track success/failure rates for different interaction types:
        - sync: File sync operations
        - gossip: Gossip message exchanges
        - heartbeat: Heartbeat responses
        - command: Remote command executions
        """
        if not hasattr(self, "_peer_reputation"):
            self._peer_reputation = {}

        if peer_id not in self._peer_reputation:
            self._peer_reputation[peer_id] = {
                "total_success": 0,
                "total_failure": 0,
                "recent_success": 0,
                "recent_failure": 0,
                "last_success": 0,
                "last_failure": 0,
                "last_reset": time.time(),
                "by_type": {},
            }

        rep = self._peer_reputation[peer_id]
        now = time.time()

        # Reset recent counters every hour
        if now - rep["last_reset"] > 3600:
            rep["recent_success"] = 0
            rep["recent_failure"] = 0
            rep["last_reset"] = now

        if success:
            rep["total_success"] += 1
            rep["recent_success"] += 1
            rep["last_success"] = now
        else:
            rep["total_failure"] += 1
            rep["recent_failure"] += 1
            rep["last_failure"] = now

        # Track by type
        if interaction_type not in rep["by_type"]:
            rep["by_type"][interaction_type] = {"success": 0, "failure": 0}
        if success:
            rep["by_type"][interaction_type]["success"] += 1
        else:
            rep["by_type"][interaction_type]["failure"] += 1

    def _get_peer_reputation_score(self, peer_id: str) -> float:
        """Get reputation score for a peer (0-100, higher is better).

        PEER REPUTATION SCORE: Combines multiple factors:
        - Recent success rate (70% weight) - last hour
        - Historical success rate (20% weight) - all time
        - Recency bonus (10% weight) - recent activity
        """
        if not hasattr(self, "_peer_reputation"):
            return 50.0  # Default neutral score

        rep = self._peer_reputation.get(peer_id)
        if not rep:
            return 50.0

        now = time.time()

        # Recent success rate (last hour)
        recent_total = rep["recent_success"] + rep["recent_failure"]
        recent_rate = rep["recent_success"] / max(1, recent_total)

        # Historical success rate
        total = rep["total_success"] + rep["total_failure"]
        historical_rate = rep["total_success"] / max(1, total)

        # Recency bonus (active peers get a boost)
        last_interaction = max(rep["last_success"], rep["last_failure"])
        recency_hours = (now - last_interaction) / 3600 if last_interaction > 0 else 24
        recency_score = max(0, 1.0 - (recency_hours / 24))  # Decays over 24 hours

        # Weighted score
        score = (recent_rate * 70) + (historical_rate * 20) + (recency_score * 10)

        return min(100.0, max(0.0, score))

    def _get_peer_reputation_summary(self) -> dict:
        """Get summary of peer reputation for gossip propagation.

        Share top/bottom peers by reputation to help cluster converge on
        reliable peer selection.
        """
        if not hasattr(self, "_peer_reputation"):
            return {"reliable_peers": [], "unreliable_peers": []}

        scores = []
        for peer_id in self._peer_reputation:
            score = self._get_peer_reputation_score(peer_id)
            scores.append((peer_id, score))

        scores.sort(key=lambda x: x[1], reverse=True)

        return {
            "reliable_peers": [{"peer": p, "score": round(s)} for p, s in scores[:5] if s >= 70],
            "unreliable_peers": [{"peer": p, "score": round(s)} for p, s in scores[-3:] if s < 30],
        }

    def _get_cluster_peer_reputation(self) -> dict:
        """Aggregate peer reputation from gossip for cluster-wide view."""
        all_scores = {}
        gossip_states = getattr(self, "_gossip_peer_states", {})
        now = time.time()

        # Include our own reputation data
        local_summary = self._get_peer_reputation_summary()
        for peer_info in local_summary.get("reliable_peers", []):
            peer_id = peer_info["peer"]
            if peer_id not in all_scores:
                all_scores[peer_id] = []
            all_scores[peer_id].append(peer_info["score"])

        # Include reputation from gossip
        for _node_id, state in gossip_states.items():
            if state.get("timestamp", 0) < now - 300:
                continue

            rep_summary = state.get("peer_reputation", {})
            for peer_info in rep_summary.get("reliable_peers", []):
                peer_id = peer_info["peer"]
                if peer_id not in all_scores:
                    all_scores[peer_id] = []
                all_scores[peer_id].append(peer_info["score"])

        # Calculate average scores
        avg_scores = {peer: sum(scores) / len(scores) for peer, scores in all_scores.items() if scores}
        sorted_peers = sorted(avg_scores.items(), key=lambda x: x[1], reverse=True)

        return {
            "most_reliable": [{"peer": p, "avg_score": round(s)} for p, s in sorted_peers[:10]],
            "peers_tracked": len(all_scores),
        }

    # ============================================================================
    # ADAPTIVE SYNC INTERVAL MANAGEMENT
    # ============================================================================
    # Dynamically adjusts P2P sync intervals based on:
    # - Cluster activity (training happening = more frequent sync)
    # - Success rate (failures = back off, successes = speed up)
    # - Data freshness (new data in cluster = more frequent sync)
    # ============================================================================

    def _init_adaptive_sync_intervals(self):
        """Initialize adaptive sync interval tracking."""
        self._adaptive_intervals = {
            "data": P2P_DATA_SYNC_BASE,
            "model": P2P_MODEL_SYNC_BASE,
            "training_db": P2P_TRAINING_DB_SYNC_BASE,
        }
        self._sync_success_streak = {
            "data": 0,
            "model": 0,
            "training_db": 0,
        }
        self._sync_failure_streak = {
            "data": 0,
            "model": 0,
            "training_db": 0,
        }
        self._last_interval_adjustment = 0

    def _get_adaptive_sync_interval(self, sync_type: str) -> float:
        """Get the current adaptive interval for a sync type.

        ADAPTIVE SYNC INTERVALS: Intervals adjust based on:
        1. Cluster activity (training = faster sync for models)
        2. Success rate (failures = back off)
        3. Base/min/max bounds per sync type

        Args:
            sync_type: One of "data", "model", "training_db"

        Returns:
            Current interval in seconds
        """
        if not hasattr(self, "_adaptive_intervals"):
            self._init_adaptive_sync_intervals()

        # Get current interval
        current = self._adaptive_intervals.get(sync_type, P2P_DATA_SYNC_BASE)

        # Apply activity-based adjustment
        activity_factor = self._calculate_cluster_activity_factor()

        # Get bounds for this sync type
        if sync_type == "data":
            min_interval = P2P_DATA_SYNC_MIN
            max_interval = P2P_DATA_SYNC_MAX
        elif sync_type == "model":
            min_interval = P2P_MODEL_SYNC_MIN
            max_interval = P2P_MODEL_SYNC_MAX
        elif sync_type == "training_db":
            min_interval = P2P_TRAINING_DB_SYNC_MIN
            max_interval = P2P_TRAINING_DB_SYNC_MAX
        else:
            min_interval = 120
            max_interval = 600

        # Apply activity factor (0.5-1.0 = active cluster, 1.0-2.0 = idle cluster)
        adjusted = current * activity_factor

        # Clamp to bounds
        return max(min_interval, min(max_interval, adjusted))

    def _calculate_cluster_activity_factor(self) -> float:
        """Calculate cluster activity factor for sync interval adjustment.

        CLUSTER ACTIVITY FACTOR:
        - < 1.0: Active cluster (training, selfplay) = faster sync
        - 1.0: Normal activity
        - > 1.0: Idle cluster = slower sync

        Returns:
            Activity factor (0.5 to 2.0)
        """
        now = time.time()

        # Check training activity (with defensive checks)
        training_active = False
        training_lock = getattr(self, "training_lock", None)
        training_jobs = getattr(self, "training_jobs", {})
        if training_lock and training_jobs:
            try:
                with training_lock:
                    for job in training_jobs.values():
                        if getattr(job, "status", None) == "running":
                            training_active = True
                            break
            except (AttributeError):
                pass

        # Check selfplay activity (count active jobs)
        selfplay_count = 0
        jobs_lock = getattr(self, "jobs_lock", None)
        selfplay_jobs = getattr(self, "selfplay_jobs", {})
        if jobs_lock and selfplay_jobs:
            try:
                with jobs_lock:
                    for job in selfplay_jobs.values():
                        if getattr(job, "status", None) == "running":
                            selfplay_count += 1
            except (AttributeError):
                pass

        # Check recent data generation from gossip
        recent_data = False
        gossip_states = getattr(self, "_gossip_node_states", {}) or {}
        for _node_id, state in gossip_states.items():
            if not isinstance(state, dict):
                continue
            last_game = state.get("last_game_time", 0)
            if now - last_game < 300:  # Game in last 5 min
                recent_data = True
                break

        # Calculate factor
        factor = 1.0

        if training_active:
            factor *= 0.5  # Much faster sync during training
        elif selfplay_count >= 5:
            factor *= 0.7  # Faster sync with active selfplay
        elif selfplay_count > 0:
            factor *= 0.85  # Slightly faster with some activity

        if recent_data:
            factor *= 0.9  # Faster sync when new data available

        # If completely idle (no jobs, stale data), slow down
        if selfplay_count == 0 and not training_active and not recent_data:
            factor *= 1.5

        return max(0.5, min(2.0, factor))

    def _record_sync_result_for_adaptive(self, sync_type: str, success: bool):
        """Record sync result to adjust adaptive intervals.

        ADAPTIVE INTERVAL ADJUSTMENT:
        - On success: reduce interval (speed up) up to min
        - On failure: increase interval (back off) up to max

        Args:
            sync_type: One of "data", "model", "training_db"
            success: Whether sync succeeded
        """
        if not hasattr(self, "_adaptive_intervals"):
            self._init_adaptive_sync_intervals()

        if success:
            self._sync_success_streak[sync_type] = self._sync_success_streak.get(sync_type, 0) + 1
            self._sync_failure_streak[sync_type] = 0

            # After 3 consecutive successes, speed up
            if self._sync_success_streak[sync_type] >= 3:
                current = self._adaptive_intervals[sync_type]
                new_interval = current * P2P_SYNC_SPEEDUP_FACTOR

                # Get min bound
                if sync_type == "data":
                    min_interval = P2P_DATA_SYNC_MIN
                elif sync_type == "model":
                    min_interval = P2P_MODEL_SYNC_MIN
                else:
                    min_interval = P2P_TRAINING_DB_SYNC_MIN

                self._adaptive_intervals[sync_type] = max(min_interval, new_interval)
                self._sync_success_streak[sync_type] = 0  # Reset streak
        else:
            self._sync_failure_streak[sync_type] = self._sync_failure_streak.get(sync_type, 0) + 1
            self._sync_success_streak[sync_type] = 0

            # On any failure, back off
            current = self._adaptive_intervals[sync_type]
            new_interval = current * P2P_SYNC_BACKOFF_FACTOR

            # Get max bound
            if sync_type == "data":
                max_interval = P2P_DATA_SYNC_MAX
            elif sync_type == "model":
                max_interval = P2P_MODEL_SYNC_MAX
            else:
                max_interval = P2P_TRAINING_DB_SYNC_MAX

            self._adaptive_intervals[sync_type] = min(max_interval, new_interval)

    def _get_sync_interval_summary(self) -> dict:
        """Get summary of current adaptive sync intervals for monitoring."""
        if not hasattr(self, "_adaptive_intervals"):
            self._init_adaptive_sync_intervals()

        return {
            "data_interval": round(self._get_adaptive_sync_interval("data")),
            "model_interval": round(self._get_adaptive_sync_interval("model")),
            "training_db_interval": round(self._get_adaptive_sync_interval("training_db")),
            "activity_factor": round(self._calculate_cluster_activity_factor(), 2),
            "data_streak": {
                "success": self._sync_success_streak.get("data", 0),
                "failure": self._sync_failure_streak.get("data", 0),
            },
            "model_streak": {
                "success": self._sync_success_streak.get("model", 0),
                "failure": self._sync_failure_streak.get("model", 0),
            },
        }

    # ============================================================================
    # SELFPLAY DATA DEDUPLICATION
    # ============================================================================
    # Tracks synced files and game IDs to avoid redundant transfers during P2P sync.
    # Uses bloom filter for efficient game ID tracking and file hash caching.
    # ============================================================================

    def _init_data_deduplication(self):
        """Initialize data deduplication tracking."""
        self._synced_file_hashes: set[str] = set()  # Hash -> synced
        self._known_game_ids: set[str] = set()  # Game IDs we have
        self._dedup_stats = {
            "files_skipped": 0,
            "games_skipped": 0,
            "bytes_saved": 0,
            "last_cleanup": time.time(),
        }
        self._dedup_lock = threading.Lock()

    def _record_synced_file(self, file_hash: str, file_size: int):
        """Record a file as synced for deduplication.

        DATA DEDUPLICATION: Track file hashes we've synced to avoid
        re-syncing the same file from different peers.

        Args:
            file_hash: Hash of the synced file
            file_size: Size in bytes (for metrics)
        """
        if not hasattr(self, "_synced_file_hashes"):
            self._init_data_deduplication()

        with self._dedup_lock:
            self._synced_file_hashes.add(file_hash)

    def _is_file_already_synced(self, file_hash: str) -> bool:
        """Check if file was already synced based on hash.

        Args:
            file_hash: Hash to check

        Returns:
            True if file was already synced
        """
        if not hasattr(self, "_synced_file_hashes"):
            self._init_data_deduplication()

        if not file_hash:
            return False

        with self._dedup_lock:
            return file_hash in self._synced_file_hashes

    def _record_game_ids(self, game_ids: list[str]):
        """Record game IDs as known for deduplication.

        GAME ID TRACKING: Track game IDs we have to avoid syncing
        duplicate games from different DB files.

        Args:
            game_ids: List of game IDs to record
        """
        if not hasattr(self, "_known_game_ids"):
            self._init_data_deduplication()

        with self._dedup_lock:
            self._known_game_ids.update(game_ids)

    # NOTE: _filter_unknown_games removed Dec 27, 2025 (dead code, never called)

    def _record_dedup_skip(self, file_count: int = 0, game_count: int = 0, bytes_saved: int = 0):
        """Record deduplication skip for metrics.

        Args:
            file_count: Number of files skipped
            game_count: Number of games skipped
            bytes_saved: Bytes saved by skipping
        """
        if not hasattr(self, "_dedup_stats"):
            self._init_data_deduplication()

        with self._dedup_lock:
            self._dedup_stats["files_skipped"] += file_count
            self._dedup_stats["games_skipped"] += game_count
            self._dedup_stats["bytes_saved"] += bytes_saved

    # NOTE: _cleanup_dedup_cache removed Dec 27, 2025 (dead code, never called)

    def _get_dedup_summary(self) -> dict:
        """Get deduplication metrics summary."""
        if not hasattr(self, "_dedup_stats"):
            self._init_data_deduplication()

        with self._dedup_lock:
            return {
                "files_skipped": self._dedup_stats.get("files_skipped", 0),
                "games_skipped": self._dedup_stats.get("games_skipped", 0),
                "bytes_saved_mb": round(self._dedup_stats.get("bytes_saved", 0) / (1024 * 1024), 2),
                "known_file_hashes": len(self._synced_file_hashes),
                "known_game_ids": len(self._known_game_ids),
            }

    def _get_swim_raft_status(self) -> dict[str, Any]:
        """Get SWIM/Raft protocol status summary.

        Returns status of the new P2P protocols (Phase 5, Dec 26, 2025):
        - SWIM: Gossip-based membership with 5s failure detection
        - Raft: Replicated work queue with sub-second failover

        These protocols are enabled via feature flags and provide
        gradual migration from HTTP/Bully to SWIM/Raft.
        """
        try:
            from scripts.p2p.constants import (  # noqa: I001
                SWIM_ENABLED, RAFT_ENABLED, MEMBERSHIP_MODE, CONSENSUS_MODE
            )
        except ImportError:
            SWIM_ENABLED = False
            RAFT_ENABLED = False
            MEMBERSHIP_MODE = "http"
            CONSENSUS_MODE = "bully"

        # Check SWIM status
        swim_status = {
            "enabled": SWIM_ENABLED,
            "available": False,
            "started": getattr(self, "_swim_started", False),
            "alive_count": 0,
        }
        try:
            from app.p2p.swim_adapter import SWIM_AVAILABLE
            swim_status["available"] = SWIM_AVAILABLE

            # Get membership summary if SWIM is active
            if hasattr(self, "get_swim_membership_summary"):
                summary = self.get_swim_membership_summary()
                swim_status.update({
                    "started": summary.get("swim_started", False),
                    "alive_count": summary.get("swim", {}).get("alive", 0),
                    "suspected_count": summary.get("swim", {}).get("suspected", 0),
                    "failed_count": summary.get("swim", {}).get("failed", 0),
                })
        except ImportError:
            pass
        except Exception as e:  # noqa: BLE001
            swim_status["error"] = str(e)

        # Check Raft status
        raft_status = {
            "enabled": RAFT_ENABLED,
            "available": False,
            "initialized": getattr(self, "_raft_initialized", False),
            "is_leader": False,
        }
        try:
            from scripts.p2p.consensus_mixin import PYSYNCOBJ_AVAILABLE
            raft_status["available"] = PYSYNCOBJ_AVAILABLE

            # Get Raft consensus status if available
            if hasattr(self, "get_raft_status"):
                raft_info = self.get_raft_status()
                raft_status.update({
                    "initialized": raft_info.get("raft_initialized", False),
                    "is_leader": raft_info.get("is_raft_leader", False),
                    "leader_address": raft_info.get("raft_leader", ""),
                    "should_use_raft": raft_info.get("should_use_raft", False),
                })
                if "work_queue_status" in raft_info:
                    wq = raft_info["work_queue_status"]
                    raft_status["work_queue"] = {
                        "pending": wq.get("by_status", {}).get("pending", 0),
                        "claimed": wq.get("by_status", {}).get("claimed", 0),
                        "completed": wq.get("by_status", {}).get("completed", 0),
                    }
        except ImportError:
            pass
        except Exception as e:  # noqa: BLE001
            raft_status["error"] = str(e)

        return {
            "membership_mode": MEMBERSHIP_MODE,
            "consensus_mode": CONSENSUS_MODE,
            "swim": swim_status,
            "raft": raft_status,
            "hybrid_status": {
                "swim_fallback_active": not swim_status.get("started", False) and MEMBERSHIP_MODE != "http",
                "raft_fallback_active": not raft_status.get("initialized", False) and CONSENSUS_MODE != "bully",
            },
        }

    # ============================================================================
    # DISTRIBUTED TOURNAMENT SCHEDULING
    # ============================================================================
    # Allows tournaments to be scheduled and coordinated via gossip protocol
    # without requiring a leader. Uses consensus to elect tournament coordinator.
    # ============================================================================

    def _init_distributed_tournament_scheduling(self):
        """Initialize distributed tournament scheduling state."""
        self._tournament_proposals: dict[str, dict] = {}  # proposal_id -> proposal
        self._tournament_votes: dict[str, dict[str, str]] = {}  # proposal_id -> {node_id: vote}
        self._active_tournaments_gossip: dict[str, dict] = {}  # tournament_id -> state
        self._last_tournament_check = 0
        self._tournament_coordination_lock = threading.Lock()

    # NOTE: _propose_tournament, _vote_on_tournament_proposal removed Dec 27, 2025 (dead code, never called)

    def _get_tournament_gossip_state(self) -> dict:
        """Get tournament state for gossip propagation.

        TOURNAMENT GOSSIP: Share active tournament info via gossip so nodes
        can coordinate without leader.

        Returns:
            Dict with proposals and active tournaments
        """
        if not hasattr(self, "_tournament_proposals"):
            self._init_distributed_tournament_scheduling()

        now = time.time()

        with self._tournament_coordination_lock:
            # Only share recent proposals (last 10 min)
            active_proposals = {
                pid: p for pid, p in self._tournament_proposals.items()
                if now - p.get("proposed_at", 0) < 600 and p.get("status") == "proposed"
            }

        # Get active distributed tournaments
        active_tournaments = {}
        for tid, state in getattr(self, "distributed_tournament_state", {}).items():
            if hasattr(state, "status") and state.status == "running":
                active_tournaments[tid] = {
                    "job_id": tid,
                    "coordinator": self.node_id,  # We're coordinating if we have it
                    "progress": state.completed_matches / max(1, state.total_matches),
                    "status": state.status,
                }

        return {
            "proposals": list(active_proposals.values()),
            "active": active_tournaments,
            "last_update": now,
        }

    def _process_tournament_gossip(self, node_id: str, tournament_state: dict):
        """Process tournament info received via gossip.

        GOSSIP PROCESSING: When receiving tournament state from peers,
        - Record their proposals and votes
        - Check if any proposals reached consensus
        - Start tournaments that we're elected to coordinate

        Args:
            node_id: ID of node that sent this state
            tournament_state: Tournament state from gossip
        """
        if not hasattr(self, "_tournament_proposals"):
            self._init_distributed_tournament_scheduling()

        if not tournament_state or not isinstance(tournament_state, dict):
            return

        time.time()

        # Process proposals from gossip
        for proposal in tournament_state.get("proposals", []):
            if not isinstance(proposal, dict):
                continue

            proposal_id = proposal.get("proposal_id")
            if not proposal_id:
                continue

            with self._tournament_coordination_lock:
                if proposal_id not in self._tournament_proposals:
                    # New proposal from peer - add it and auto-approve
                    self._tournament_proposals[proposal_id] = proposal.copy()
                    self._tournament_proposals[proposal_id]["votes"][self.node_id] = "approve"
                else:
                    # Merge votes
                    existing = self._tournament_proposals[proposal_id]
                    for voter, vote in proposal.get("votes", {}).items():
                        if voter not in existing["votes"]:
                            existing["votes"][voter] = vote

    def _check_tournament_consensus(self):
        """Check if any tournament proposals have reached consensus.

        CONSENSUS CHECK: A proposal is approved when majority of alive peers approve.
        The coordinator is elected as the highest-ID approving voter node.
        """
        if not hasattr(self, "_tournament_proposals"):
            return

        now = time.time()

        # Get alive peer count for quorum
        with self.peers_lock:
            alive_peers = [p for p in self.peers.values() if p.is_alive()]
        alive_count = len(alive_peers) + 1  # +1 for self

        quorum = (alive_count // 2) + 1

        with self._tournament_coordination_lock:
            for proposal_id, proposal in list(self._tournament_proposals.items()):
                if proposal.get("status") != "proposed":
                    continue

                # Count votes
                approve_votes = [
                    voter for voter, vote in proposal.get("votes", {}).items()
                    if vote == "approve"
                ]

                if len(approve_votes) >= quorum:
                    # Consensus reached! Elect coordinator (highest ID)
                    coordinator = max(approve_votes)
                    proposal["status"] = "approved"
                    proposal["coordinator"] = coordinator

                    logger.info(f"TOURNAMENT: Proposal {proposal_id} approved! "
                          f"Coordinator: {coordinator} ({len(approve_votes)}/{alive_count} votes)")

                    # If we're the coordinator, start the tournament
                    if coordinator == self.node_id:
                        asyncio.create_task(self._start_tournament_from_proposal(proposal))

                # Expire old proposals
                elif now - proposal.get("proposed_at", 0) > 600:
                    proposal["status"] = "expired"

    async def _start_tournament_from_proposal(self, proposal: dict):
        """Start a tournament from an approved proposal.

        COORDINATOR DUTIES: When elected as coordinator, start the tournament
        and manage match distribution to workers.

        Args:
            proposal: Approved proposal dict
        """
        import uuid

        job_id = f"tournament_{uuid.uuid4().hex[:8]}"
        agent_ids = proposal.get("agent_ids", [])

        if len(agent_ids) < 2:
            logger.info("TOURNAMENT: Cannot start - need at least 2 agents")
            return

        # Create round-robin pairings
        pairings = []
        for i, a1 in enumerate(agent_ids):
            for a2 in agent_ids[i+1:]:
                for game_num in range(proposal.get("games_per_pairing", 2)):
                    pairings.append({
                        "agent1": a1,
                        "agent2": a2,
                        "game_num": game_num,
                        "status": "pending",
                    })

        state = DistributedTournamentState(
            job_id=job_id,
            board_type=proposal.get("board_type", "square8"),
            num_players=proposal.get("num_players", 2),
            agent_ids=agent_ids,
            games_per_pairing=proposal.get("games_per_pairing", 2),
            total_matches=len(pairings),
            pending_matches=pairings,
            status="running",
            started_at=time.time(),
            last_update=time.time(),
        )

        # Find workers
        with self.peers_lock:
            workers = [p.node_id for p in self.peers.values() if p.is_healthy()]
        state.worker_nodes = workers

        if not state.worker_nodes:
            logger.info(f"TOURNAMENT: No workers available for {job_id}")
            return

        self.distributed_tournament_state[job_id] = state

        logger.info(f"TOURNAMENT: Started {job_id} from proposal {proposal.get('proposal_id')}: "
              f"{len(agent_ids)} agents, {len(pairings)} matches, {len(workers)} workers")

        # Launch coordinator task
        asyncio.create_task(self.job_manager.run_distributed_tournament(job_id))

    def _get_distributed_tournament_summary(self) -> dict:
        """Get summary of distributed tournament scheduling for status endpoint."""
        if not hasattr(self, "_tournament_proposals"):
            self._init_distributed_tournament_scheduling()

        with self._tournament_coordination_lock:
            pending_proposals = sum(
                1 for p in self._tournament_proposals.values()
                if p.get("status") == "proposed"
            )
            approved_proposals = sum(
                1 for p in self._tournament_proposals.values()
                if p.get("status") == "approved"
            )

        active_tournaments = sum(
            1 for s in getattr(self, "distributed_tournament_state", {}).values()
            if hasattr(s, "status") and s.status == "running"
        )

        return {
            "pending_proposals": pending_proposals,
            "approved_proposals": approved_proposals,
            "active_tournaments": active_tournaments,
            "enabled": True,
        }

    async def _start_monitoring_if_leader(self):
        """Start Prometheus/Grafana when we become leader (P2P monitoring resilience)."""
        if not self.monitoring_manager:
            return
        if self.role != NodeRole.LEADER:
            return
        if self._monitoring_was_leader:
            return  # Already started

        try:
            # Update peer list for Prometheus config
            with self.peers_lock:
                peer_list = [
                    {"node_id": p.node_id, "host": p.host, "port": getattr(p, "metrics_port", 9091)}
                    for p in self.peers.values()
                    if p.node_id != self.node_id and p.is_healthy()
                ]
            self.monitoring_manager.update_peers(peer_list)

            # Start monitoring services
            success = await self.monitoring_manager.start_as_leader()
            if success:
                logger.info("Monitoring services started on leader node")
                self._monitoring_was_leader = True
            else:
                logger.error("Failed to start monitoring services")
        except Exception as e:  # noqa: BLE001
            logger.error(f"starting monitoring services: {e}")

    async def _stop_monitoring_if_not_leader(self):
        """Stop Prometheus/Grafana when we step down from leadership."""
        if not self.monitoring_manager:
            return
        if not self._monitoring_was_leader:
            return  # Never started

        if self.role != NodeRole.LEADER:
            try:
                await self.monitoring_manager.stop()
                logger.info("Monitoring services stopped (no longer leader)")
                self._monitoring_was_leader = False
            except Exception as e:  # noqa: BLE001
                logger.error(f"stopping monitoring services: {e}")

    async def _start_p2p_auto_deployer(self):
        """Start P2P auto-deployer when we become leader.

        The auto-deployer ensures P2P orchestrator is running on all cluster nodes.
        This solves the fundamental gap where P2P deployment was manual-only.
        """
        if self.role != NodeRole.LEADER:
            return
        if self._auto_deployer_task is not None:
            return  # Already running

        try:
            from app.coordination.p2p_auto_deployer import P2PAutoDeployer, P2PDeploymentConfig

            config = P2PDeploymentConfig(
                check_interval_seconds=300.0,  # Check every 5 minutes
                min_coverage_percent=90.0,
            )
            self.p2p_auto_deployer = P2PAutoDeployer(config=config)

            # Run as background task
            self._auto_deployer_task = asyncio.create_task(
                self.p2p_auto_deployer.run_daemon(),
                name="p2p_auto_deployer"
            )
            logger.info("P2P Auto-Deployer started (leader responsibility)")
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to start P2P auto-deployer: {e}")

    async def _stop_p2p_auto_deployer(self):
        """Stop P2P auto-deployer when we step down from leadership."""
        if self.p2p_auto_deployer:
            self.p2p_auto_deployer.stop()
        if self._auto_deployer_task:
            self._auto_deployer_task.cancel()
            with contextlib.suppress(asyncio.CancelledError):
                await self._auto_deployer_task
            self._auto_deployer_task = None
        self.p2p_auto_deployer = None
        logger.info("P2P Auto-Deployer stopped")

    async def _update_monitoring_peers(self):
        """Update Prometheus config with current peer list."""
        if not self.monitoring_manager or not self._monitoring_was_leader:
            return
        if self.role != NodeRole.LEADER:
            return

        try:
            with self.peers_lock:
                peer_list = [
                    {"node_id": p.node_id, "host": p.host, "port": getattr(p, "metrics_port", 9091)}
                    for p in self.peers.values()
                    if p.node_id != self.node_id and p.is_healthy()
                ]
            self.monitoring_manager.update_peers(peer_list)
            await self.monitoring_manager.reload_config()
        except Exception as e:  # noqa: BLE001
            logger.error(f"updating monitoring peers: {e}")

    async def _renew_leader_lease(self):
        """Renew our leadership lease and broadcast to peers."""
        if self.role != NodeRole.LEADER:
            return
        if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
            logger.info(f"Lost voter quorum; stepping down: {self.node_id}")
            self.role = NodeRole.FOLLOWER
            self.leader_id = None
            self.leader_lease_id = ""
            self.leader_lease_expires = 0.0
            self.last_lease_renewal = 0.0
            self._release_voter_grant_if_self()
            self._save_state()
            return

        now = time.time()
        if now - self.last_lease_renewal < LEADER_LEASE_RENEW_INTERVAL:
            return  # Too soon to renew

        lease_id = str(self.leader_lease_id or "")
        if not lease_id:
            lease_id = f"{self.node_id}_{int(time.time())}_{uuid.uuid4().hex[:8]}"
        lease_expires = await self._acquire_voter_lease_quorum(lease_id, int(LEADER_LEASE_DURATION))
        if getattr(self, "voter_node_ids", []) and not lease_expires:
            # Voter quorum failed - try arbiter fallback before stepping down
            logger.info("Voter lease quorum failed; checking arbiter...")
            arbiter_leader = await self._query_arbiter_for_leader()
            if arbiter_leader == self.node_id:
                # Arbiter still recognizes us as leader - extend lease provisionally
                logger.info("Arbiter confirms us as leader despite quorum failure; continuing with provisional lease")
                lease_expires = now + LEADER_LEASE_DURATION / 2  # Shorter lease until quorum recovers
            elif arbiter_leader:
                # Arbiter says someone else is leader - defer to arbiter
                logger.info(f"Arbiter reports different leader ({arbiter_leader}); stepping down")
                self.role = NodeRole.FOLLOWER
                self.leader_id = arbiter_leader
                self.leader_lease_id = ""
                self.leader_lease_expires = 0.0
                self.last_lease_renewal = 0.0
                self._release_voter_grant_if_self()
                self._save_state()
                return
            else:
                # Arbiter also unreachable - step down to be safe
                logger.error(f"Failed to renew voter lease quorum and arbiter unreachable; stepping down: {self.node_id}")
                self.role = NodeRole.FOLLOWER
                self.leader_id = None
                self.leader_lease_id = ""
                self.leader_lease_expires = 0.0
                self.last_lease_renewal = 0.0
                self._release_voter_grant_if_self()
                self._save_state()
                return

        self.leader_lease_id = lease_id
        self.leader_lease_expires = float(lease_expires or (now + LEADER_LEASE_DURATION))
        self.last_lease_renewal = now

        # Broadcast lease renewal to all peers
        with self.peers_lock:
            peers = list(self.peers.values())

        timeout = ClientTimeout(total=3)
        try:
            async with get_client_session(timeout) as session:
                for peer in peers:
                    if peer.node_id != self.node_id and peer.is_alive():
                        try:
                            url = self._url_for_peer(peer, "/coordinator")
                            await session.post(url, json={
                                "leader_id": self.node_id,
                                "lease_id": self.leader_lease_id,
                                "lease_expires": self.leader_lease_expires,
                                "lease_renewal": True,
                                "voter_node_ids": list(getattr(self, "voter_node_ids", []) or []),
                            }, headers=self._auth_headers())
                        except (aiohttp.ClientError, asyncio.TimeoutError, KeyError, IndexError, AttributeError):
                            pass  # Network errors expected during lease renewal
        except Exception as e:  # noqa: BLE001
            logger.info(f"Lease renewal error: {e}")

    def _is_leader_lease_valid(self) -> bool:
        """Check if the current leader's lease is still valid."""
        if not self.leader_id:
            return False
        if self.leader_id == self.node_id:
            # We are leader - check our own lease
            return time.time() < self.leader_lease_expires
        else:
            # Another node is leader - check if we've received recent lease renewal
            # Allow some grace period (2x lease duration) for network delays
            return time.time() < self.leader_lease_expires + LEADER_LEASE_DURATION

    async def _check_and_resolve_split_brain(self) -> bool:
        """Check for split-brain (multiple leaders) and resolve by stepping down if needed.

        LEARNED LESSONS - This addresses the cluster status showing multiple leaders.
        Uses Bully algorithm: highest node_id wins.

        Returns True if we stepped down (caller should skip leadership duties).
        """
        if self.role != NodeRole.LEADER:
            return False

        with self.peers_lock:
            peers_snapshot = [p for p in self.peers.values() if p.node_id != self.node_id]

        conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])

        # Gather all peers claiming to be leader.
        other_leaders = [peer for peer in peers_snapshot if peer.role == NodeRole.LEADER and peer.is_alive()]

        voter_ids = list(getattr(self, "voter_node_ids", []) or [])

        # PROACTIVE VOTER ACK VERIFICATION: Even when no other leaders are visible,
        # periodically verify that voters still acknowledge us as leader.
        # This catches split-brain where we can't see the other partition's leader.
        if not other_leaders and voter_ids:
            now = time.time()
            last_voter_check = float(getattr(self, "_last_voter_ack_check", 0) or 0)
            # Check every 30 seconds (more frequent than lease renewal)
            if now - last_voter_check >= 30:
                self._last_voter_ack_check = now
                leased_leader = await self._determine_leased_leader_from_voters()
                if leased_leader and leased_leader != self.node_id:
                    logger.info(f"VOTER ACK CHECK: Voters grant to {leased_leader}, not us; stepping down")
                    self.role = NodeRole.FOLLOWER
                    self.leader_id = leased_leader
                    self.leader_lease_id = ""
                    self.leader_lease_expires = 0.0
                    self._release_voter_grant_if_self()
                    self._save_state()
                    return True
            return False  # No split-brain detected

        if not other_leaders:
            return False  # No split-brain
        if voter_ids:
            # In quorum-gated clusters, only voters may safely lead.
            if self.node_id not in voter_ids:
                print(
                    f"[P2P] SPLIT-BRAIN detected, but {self.node_id} is not a voter; stepping down."
                )
                # December 2025: Emit SPLIT_BRAIN_DETECTED event
                leaders_detected = [p.node_id for p in other_leaders] + [self.node_id]
                await self._emit_split_brain_detected(
                    detected_leaders=leaders_detected,
                    resolution_action="step_down_non_voter",
                )
                self.role = NodeRole.FOLLOWER
                self.leader_id = None
                self.leader_lease_id = ""
                self.leader_lease_expires = 0.0
                self._release_voter_grant_if_self()
                self._save_state()
                return True

            leased_leader = await self._determine_leased_leader_from_voters()
            if leased_leader and leased_leader != self.node_id:
                print(
                    f"[P2P] SPLIT-BRAIN resolved by voter quorum: stepping down for lease-holder {leased_leader}"
                )
                self.role = NodeRole.FOLLOWER
                self.leader_id = leased_leader
                self.leader_lease_id = ""
                self.leader_lease_expires = 0.0
                self._release_voter_grant_if_self()
                self._save_state()
                return True

        # Find the highest-priority *eligible* leader (including ourselves).
        considered_leaders = other_leaders
        if voter_ids:
            # Prefer voter leaders when resolving conflicts; non-voter leaders
            # are treated as noise from older configs/versions.
            voter_leaders = [p for p in other_leaders if p.node_id in voter_ids]
            if voter_leaders:
                considered_leaders = voter_leaders

        eligible_leaders = [p for p in considered_leaders if self._is_leader_eligible(p, conflict_keys)]
        if self._is_leader_eligible(self.self_info, conflict_keys):
            eligible_leaders.append(self.self_info)

        # If none are eligible, fall back to bully ordering (best-effort).
        candidates = eligible_leaders or ([*considered_leaders, self.self_info])
        highest_leader = max(candidates, key=lambda p: p.node_id)

        if highest_leader.node_id != self.node_id:
            # We're not the highest-priority leader - step down
            logger.info(f"SPLIT-BRAIN detected! Found leaders: {[p.node_id for p in other_leaders]}")
            logger.info(f"Stepping down in favor of higher-priority leader: {highest_leader.node_id}")
            # December 2025: Emit SPLIT_BRAIN_DETECTED event
            leaders_detected = [p.node_id for p in other_leaders] + [self.node_id]
            await self._emit_split_brain_detected(
                detected_leaders=leaders_detected,
                resolution_action="step_down_bully",
            )
            self.role = NodeRole.FOLLOWER
            self.leader_id = highest_leader.node_id
            self.leader_lease_id = ""
            self.leader_lease_expires = 0.0
            self._release_voter_grant_if_self()
            self._save_state()
            return True

        # We are the highest - other leaders should step down
        # Send coordinator message to assert our leadership
        logger.info(f"SPLIT-BRAIN detected! Asserting leadership over: {[p.node_id for p in other_leaders]}")

        # Emit SPLIT_BRAIN_DETECTED event for this case (asserting leadership)
        leaders_detected = [p.node_id for p in other_leaders] + [self.node_id]
        await self._emit_split_brain_detected(
            detected_leaders=leaders_detected,
            resolution_action="assert_leadership",
        )

        timeout = ClientTimeout(total=5)
        async with get_client_session(timeout) as session:
            for peer in other_leaders:
                try:
                    url = self._url_for_peer(peer, "/coordinator")
                    await session.post(
                        url,
                        json={
                            "leader_id": self.node_id,
                            "lease_id": self.leader_lease_id,
                            "lease_expires": self.leader_lease_expires,
                            "voter_node_ids": list(getattr(self, "voter_node_ids", []) or []),
                        },
                        headers=self._auth_headers(),
                    )
                except (aiohttp.ClientError, asyncio.TimeoutError, KeyError, IndexError, AttributeError):
                    pass  # Network errors expected during step-down notifications

        return False  # We remain leader

    async def _job_management_loop(self):
        """Manage jobs - leader coordinates cluster, all nodes handle local operations."""
        while self.running:
            try:
                # ==== DECENTRALIZED OPERATIONS (all nodes) ====
                # These run on every node to ensure cluster health even without a leader
                # Makes the cluster resilient to leader instability

                # Local data consolidation: merge siloed job DBs to main selfplay.db
                await self._consolidate_selfplay_data()

                # Local stuck job detection: each node monitors its own processes
                await self._check_local_stuck_jobs()

                # Local resource cleanup: handle disk/memory pressure independently
                await self._local_resource_cleanup()

                # Local job management: start/stop jobs based on node capacity
                await self._manage_local_jobs_decentralized()

                # Local GPU auto-scaling: optimize GPU utilization independently
                await self._local_gpu_auto_scale()

                # Leaderless training fallback: trigger local training if no leader for too long
                await self._check_local_training_fallback()

                # Emergency coordinator: if voter quorum unavailable for >5min, take leadership
                await self._check_emergency_coordinator_fallback()

                # P2P data sync: nodes can sync data directly without leader
                await self._p2p_data_sync()

                # P2P model sync: dedicated model distribution (more frequent)
                await self._p2p_model_sync()

                # P2P training DB sync: sync training databases for diversity
                await self._p2p_training_db_sync()

                # Gossip protocol: share state with random peers
                await self._gossip_state_to_peers()

                # Anti-entropy repair: periodic full state reconciliation
                await self._gossip_anti_entropy_repair()

                # ==== LEADER-ONLY OPERATIONS ====
                if self.role == NodeRole.LEADER:
                    # LEARNED LESSONS - Check for split-brain before acting as leader
                    if await self._check_and_resolve_split_brain():
                        # We stepped down, skip this cycle
                        await asyncio.sleep(JOB_CHECK_INTERVAL)
                        continue

                    await self._manage_cluster_jobs()
                    # Cluster rebalancing: migrate jobs from weak to powerful nodes
                    await self._check_cluster_balance()
                    # Phase 3: Check if training should be triggered automatically
                    await self._check_and_trigger_training()
                    # Phase 5: Check improvement cycles for automated training
                    await self._check_improvement_cycles()
                    # Cluster-wide stuck job detection (remote nodes)
                    await self._check_and_kill_stuck_jobs()
                    # Work queue rebalancing: assign queued work to idle nodes
                    await self._auto_rebalance_from_work_queue()
                    # Self-healing: auto-scale GPU utilization toward 60-80% target
                    await self._auto_scale_gpu_utilization()
                    # Self-healing: probe NAT-blocked peers to check if they've become reachable
                    await self._sweep_nat_recovery()
                    # Self-healing: detect and recover stuck nodes via SSH restart
                    await self._check_node_recovery()
            except Exception as e:  # noqa: BLE001
                logger.info(f"Job management error: {e}")

            await asyncio.sleep(JOB_CHECK_INTERVAL)

    async def _check_and_kill_stuck_jobs(self) -> int:
        """Detect and terminate stuck training/selfplay jobs.

        A job is considered stuck if:
        - Training: No log output for 10+ minutes while process still running
        - Selfplay: No new games generated for 15+ minutes

        Returns:
            Number of stuck jobs terminated
        """
        killed = 0
        now = time.time()
        TRAINING_STUCK_THRESHOLD = 600  # 10 minutes
        SELFPLAY_STUCK_THRESHOLD = 900  # 15 minutes

        # Check training jobs
        with self.training_lock:
            training_snapshot = list(self.training_jobs.values())

        for job in training_snapshot:
            if job.status != "running":
                continue
            started = getattr(job, "started_at", 0) or 0
            last_progress = getattr(job, "last_progress_time", started) or started
            if now - last_progress > TRAINING_STUCK_THRESHOLD and now - started > TRAINING_STUCK_THRESHOLD:
                logger.info(f"STUCK DETECTED: Training job {job.job_id} on {job.target_node} - no progress for {int((now - last_progress)/60)}min")
                # Try to kill the process on the target node
                target_node = job.target_node
                if target_node and target_node != self.node_id:
                    await self._remote_kill_stuck_job(target_node, job.job_id, "training")
                else:
                    # Local kill
                    try:
                        import subprocess
                        subprocess.run(["pkill", "-9", "-f", f"train.*{job.job_id}"], timeout=5, capture_output=True)
                    except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError, KeyError, IndexError, AttributeError, ImportError):
                        pass
                job.status = "failed"
                job.error_message = "Killed: no progress detected"
                job.completed_at = now
                killed += 1
                logger.info(f"Killed stuck training job {job.job_id}")
                # ALERTING: Notify when stuck job is killed
                asyncio.create_task(self.notifier.send(
                    title="Stuck Job Killed",
                    message=f"Training job {job.job_id} killed after no progress for {int((now - last_progress)/60)}min",
                    level="warning",
                    fields={
                        "Job ID": job.job_id,
                        "Type": job.job_type,
                        "Node": job.target_node or "local",
                        "Config": f"{job.board_type}_{job.num_players}p",
                        "Stuck For": f"{int((now - last_progress)/60)} minutes",
                    },
                    node_id=self.node_id,
                ))

        # Check for GPU nodes with 0% GPU but running GPU jobs
        with self.peers_lock:
            peers_snapshot = list(self.peers.values())

        for peer in peers_snapshot:
            if not peer.is_alive():
                continue
            gpu_percent = float(getattr(peer, "gpu_percent", 0) or 0)
            selfplay_jobs = int(getattr(peer, "selfplay_jobs", 0) or 0)
            has_gpu = bool(getattr(peer, "has_gpu", False))

            # Check for stuck GPU selfplay (has GPU, jobs running, but 0% GPU util)
            if has_gpu and selfplay_jobs > 0 and gpu_percent == 0:
                last_gpu_active = getattr(peer, "_last_gpu_active_time", 0)
                if last_gpu_active == 0:
                    peer._last_gpu_active_time = now
                elif now - last_gpu_active > SELFPLAY_STUCK_THRESHOLD:
                    logger.info(f"STUCK DETECTED: {peer.node_id} has {selfplay_jobs} jobs but 0% GPU for {int((now - last_gpu_active)/60)}min")
                    # Don't auto-kill selfplay, just log - might be CPU selfplay
            elif has_gpu and gpu_percent > 5:
                peer._last_gpu_active_time = now

        if killed > 0:
            logger.info(f"Self-healing: killed {killed} stuck job(s)")
        return killed

    async def _check_local_stuck_jobs(self) -> int:
        """DECENTRALIZED: Detect and kill stuck processes on THIS node only.

        Runs on ALL nodes (not just leader) to ensure each node can self-heal
        even when there's no functioning leader in the cluster.

        Detects:
        - GPU selfplay processes with 0% GPU utilization for too long
        - Training processes that haven't made progress
        - Orphaned processes that aren't tracked in local_jobs

        Returns:
            Number of stuck processes terminated
        """
        killed = 0
        now = time.time()
        STUCK_THRESHOLD = 900  # 15 minutes

        # Only check periodically to avoid excessive process scanning
        last_check = getattr(self, "_last_local_stuck_check", 0)
        if now - last_check < 300:  # Check every 5 minutes
            return 0
        self._last_local_stuck_check = now

        # Check if local GPU is at 0% but we have running GPU selfplay jobs
        gpu_percent = float(getattr(self.self_info, "gpu_percent", 0) or 0)
        selfplay_jobs = int(getattr(self.self_info, "selfplay_jobs", 0) or 0)
        has_gpu = bool(getattr(self.self_info, "has_gpu", False))

        if has_gpu and selfplay_jobs > 0 and gpu_percent < 5:
            last_gpu_active = getattr(self, "_local_last_gpu_active", 0)
            if last_gpu_active == 0:
                self._local_last_gpu_active = now
            elif now - last_gpu_active > STUCK_THRESHOLD:
                logger.info(f"LOCAL STUCK: {selfplay_jobs} selfplay jobs but {gpu_percent:.0f}% GPU for {int((now - last_gpu_active)/60)}min")
                # Kill all local GPU selfplay processes and let them restart
                try:
                    import subprocess
                    # Kill gpu_selfplay processes specifically
                    result = subprocess.run(
                        ["pkill", "-9", "-f", "gpu_selfplay"],
                        timeout=10, capture_output=True
                    )
                    if result.returncode == 0:
                        killed += 1
                        logger.info("LOCAL: Killed stuck GPU selfplay processes")
                        # Clear job tracking so they restart
                        with self.jobs_lock:
                            gpu_jobs = [jid for jid, job in self.local_jobs.items()
                                       if "gpu" in str(getattr(job, "job_type", "")).lower()]
                            for jid in gpu_jobs:
                                del self.local_jobs[jid]
                        self._local_last_gpu_active = now
                except Exception as e:  # noqa: BLE001
                    logger.info(f"LOCAL: Failed to kill stuck processes: {e}")
        elif has_gpu and gpu_percent >= 5:
            self._local_last_gpu_active = now

        # Check for orphaned selfplay processes (processes running but not in job tracking)
        try:
            import subprocess
            # Count actual selfplay processes
            result = subprocess.run(
                ["pgrep", "-fc", "selfplay|gpu_selfplay"],
                timeout=5, capture_output=True, text=True
            )
            actual_processes = int(result.stdout.strip() or "0") if result.returncode == 0 else 0

            with self.jobs_lock:
                tracked_jobs = len(self.local_jobs)

            # If we have way more processes than tracked jobs, we have orphans
            if actual_processes > tracked_jobs + 10:
                last_orphan_check = getattr(self, "_last_orphan_kill", 0)
                if now - last_orphan_check > 3600:  # Max once per hour
                    logger.info(f"LOCAL: Orphan detection: {actual_processes} processes vs {tracked_jobs} tracked")
                    # Don't auto-kill orphans yet, just warn
                    # Could add aggressive cleanup here if needed
                    self._last_orphan_kill = now
        except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError, ValueError, KeyError, IndexError, AttributeError, ImportError):
            pass

        if killed > 0:
            logger.info(f"LOCAL self-healing: terminated {killed} stuck process(es)")
        return killed

    async def _remote_kill_stuck_job(self, target_node: str, job_id: str, job_type: str) -> bool:
        """Send kill command to remote node for stuck job."""
        with self.peers_lock:
            peer = self.peers.get(target_node)
        if not peer or not peer.is_alive():
            return False

        try:
            timeout = ClientTimeout(total=10)
            async with get_client_session(timeout) as session:
                url = self._url_for_peer(peer, "/job/kill")
                payload = {"job_id": job_id, "job_type": job_type, "reason": "stuck"}
                async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                    return resp.status == 200
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to kill stuck job on {target_node}: {e}")
            return False

    async def _manage_local_jobs_decentralized(self) -> int:
        """DECENTRALIZED: Each node manages its own job count based on gossip state.

        Runs on ALL nodes to ensure selfplay continues even during leader elections.
        Each node autonomously:
        1. Checks its own resource pressure (disk, memory, CPU)
        2. Uses gossip state to calculate proportional job count
        3. Starts or stops local jobs as needed

        PHASE 3 DECENTRALIZATION (Dec 2025):
        - With Serf providing reliable failure detection, we can act quickly
        - Proportional allocation based on gossip cluster capacity
        - 30-second timeout for faster leader-failure recovery

        Returns:
            Number of jobs started/stopped
        """
        changes = 0
        now = time.time()

        # Rate limit: check every 30 seconds (reduced from 60s for faster response)
        last_check = getattr(self, "_last_local_job_manage", 0)
        if now - last_check < 30:
            return 0
        self._last_local_job_manage = now

        # Skip if leader is managing (avoid conflicts)
        # But continue if leaderless for > 30 seconds (reduced from 60s for Serf reliability)
        if self.role == NodeRole.LEADER:
            return 0  # Leader uses centralized management
        if self.leader_id:
            leaderless_duration = now - getattr(self, "last_leader_seen", now)
            if leaderless_duration < LEADERLESS_TRAINING_TIMEOUT:
                return 0  # Have a leader, let them manage

        # Update self info
        self._update_self_info()
        node = self.self_info

        # Check resource pressure - don't start jobs if under pressure
        if node.disk_percent >= DISK_WARNING_THRESHOLD:
            logger.info(f"LOCAL: Disk at {node.disk_percent:.0f}% - skipping job starts")
            await self._cleanup_local_disk()
            return 0

        if node.memory_percent >= MEMORY_WARNING_THRESHOLD:
            logger.info(f"LOCAL: Memory at {node.memory_percent:.0f}% - skipping job starts")
            return 0

        # Calculate target jobs for this node (delegated to SelfplayScheduler Dec 2025)
        target_selfplay = self.selfplay_scheduler.get_target_jobs_for_node(node)
        current_jobs = int(getattr(node, "selfplay_jobs", 0) or 0)

        # Start jobs if below target
        if current_jobs < target_selfplay:
            needed = min(target_selfplay - current_jobs, 3)  # Max 3 per cycle
            logger.info(f"LOCAL: Starting {needed} selfplay job(s) ({current_jobs}/{target_selfplay})")

            # Dec 27, 2025: Generate batch ID and emit BATCH_SCHEDULED
            batch_id = f"selfplay_{self.node_id}_{int(time.time())}"
            first_config = self.selfplay_scheduler.pick_weighted_config(node)
            config_key = f"{first_config['board_type']}_{first_config['num_players']}p" if first_config else "mixed"
            await self._emit_batch_scheduled(
                batch_id=batch_id,
                batch_type="selfplay",
                config_key=config_key,
                job_count=needed,
                target_nodes=[self.node_id],
                reason="local_job_management",
            )

            jobs_dispatched = 0
            jobs_failed = 0
            for _ in range(needed):
                try:
                    # Pick a config weighted by priority (using SelfplayScheduler manager)
                    config = self.selfplay_scheduler.pick_weighted_config(node)
                    if config:
                        job = await self._start_local_job(
                            JobType.HYBRID_SELFPLAY,
                            board_type=config["board_type"],
                            num_players=config["num_players"],
                            engine_mode=config.get("engine_mode", "gumbel-mcts"),  # GPU-accelerated
                        )
                        if job:
                            changes += 1
                            jobs_dispatched += 1
                        else:
                            jobs_failed += 1
                except Exception as e:  # noqa: BLE001
                    logger.info(f"LOCAL: Failed to start selfplay: {e}")
                    jobs_failed += 1
                    break

            # Dec 27, 2025: Emit BATCH_DISPATCHED after loop completes
            await self._emit_batch_dispatched(
                batch_id=batch_id,
                batch_type="selfplay",
                config_key=config_key,
                jobs_dispatched=jobs_dispatched,
                jobs_failed=jobs_failed,
                target_nodes=[self.node_id],
            )

        # Stop jobs if way over target (2x or more)
        elif current_jobs > target_selfplay * 2:
            excess = current_jobs - target_selfplay
            logger.info(f"LOCAL: Reducing selfplay jobs by {excess} ({current_jobs}/{target_selfplay})")
            await self._reduce_local_selfplay_jobs(target_selfplay, reason="over_target")
            changes += excess

        if changes > 0:
            logger.info(f"LOCAL job management: {changes} change(s)")
        return changes

    async def _local_gpu_auto_scale(self) -> int:
        """DECENTRALIZED: Each GPU node manages its own GPU utilization.

        Runs on ALL GPU nodes to ensure optimal GPU usage without leader.
        Targets 60-80% GPU utilization by starting/stopping GPU selfplay jobs.

        Returns:
            Number of GPU jobs started
        """
        started = 0
        now = time.time()

        # Rate limit: check every 2 minutes
        last_check = getattr(self, "_last_local_gpu_scale", 0)
        if now - last_check < 120:
            return 0
        self._last_local_gpu_scale = now

        # Skip if not a GPU node
        if not getattr(self.self_info, "has_gpu", False):
            return 0

        # Skip if training is running (training uses GPU)
        training_jobs = int(getattr(self.self_info, "training_jobs", 0) or 0)
        if training_jobs > 0:
            return 0

        # DECENTRALIZED: Always allow local GPU scaling
        # Leader manages cluster-wide coordination, but each node optimizes its own GPU
        # Use smaller batches when leader is present to avoid conflicts
        has_leader = bool(self.leader_id or self.role == NodeRole.LEADER)
        max_jobs_per_cycle = 1 if has_leader else 3

        TARGET_GPU_MIN = 60.0
        TARGET_GPU_MAX = 80.0
        MIN_IDLE_TIME = 120 if has_leader else 60  # Faster response when leaderless

        gpu_percent = float(getattr(self.self_info, "gpu_percent", 0) or 0)
        int(getattr(self.self_info, "selfplay_jobs", 0) or 0)
        gpu_name = (getattr(self.self_info, "gpu_name", "") or "").lower()

        # Track GPU idle time
        if gpu_percent < TARGET_GPU_MIN:
            idle_since = getattr(self, "_local_gpu_idle_since", 0)
            if idle_since == 0:
                self._local_gpu_idle_since = now
            elif now - idle_since > MIN_IDLE_TIME:
                # Calculate new jobs to add
                gpu_headroom = TARGET_GPU_MAX - gpu_percent
                if any(tag in gpu_name for tag in ("h100", "h200", "gh200", "5090")):
                    jobs_per_10_percent = 2
                elif any(tag in gpu_name for tag in ("a100", "4090", "3090")):
                    jobs_per_10_percent = 1.5
                else:
                    jobs_per_10_percent = 1

                new_jobs = max(1, int(gpu_headroom / 10 * jobs_per_10_percent))
                new_jobs = min(new_jobs, max_jobs_per_cycle)  # Cap based on leader presence

                logger.info(f"LOCAL: {gpu_percent:.0f}% GPU util, starting {new_jobs} diverse/hybrid selfplay job(s)")

                # Dec 27, 2025: Generate batch ID and emit BATCH_SCHEDULED
                batch_id = f"gpu_selfplay_{self.node_id}_{int(time.time())}"
                first_config = self.selfplay_scheduler.pick_weighted_config(self.self_info)
                config_key = f"{first_config['board_type']}_{first_config['num_players']}p" if first_config else "mixed"
                await self._emit_batch_scheduled(
                    batch_id=batch_id,
                    batch_type="selfplay",
                    config_key=config_key,
                    job_count=new_jobs,
                    target_nodes=[self.node_id],
                    reason="gpu_auto_scale",
                )

                gpu_jobs_dispatched = 0
                gpu_jobs_failed = 0
                for _ in range(new_jobs):
                    try:
                        config = self.selfplay_scheduler.pick_weighted_config(self.self_info)
                        if config:
                            # Use hybrid mode (CPU rules + GPU eval) for quality + speed
                            job = await self._start_local_job(
                                JobType.HYBRID_SELFPLAY,
                                board_type=config["board_type"],
                                num_players=config["num_players"],
                                engine_mode="mixed",  # Diverse AI matchups for better training data
                            )
                            if job:
                                started += 1
                                gpu_jobs_dispatched += 1
                            else:
                                gpu_jobs_failed += 1
                    except Exception as e:  # noqa: BLE001
                        logger.info(f"LOCAL: Failed to start diverse selfplay: {e}")
                        gpu_jobs_failed += 1
                        break

                # Dec 27, 2025: Emit BATCH_DISPATCHED after loop completes
                await self._emit_batch_dispatched(
                    batch_id=batch_id,
                    batch_type="selfplay",
                    config_key=config_key,
                    jobs_dispatched=gpu_jobs_dispatched,
                    jobs_failed=gpu_jobs_failed,
                    target_nodes=[self.node_id],
                )

                self._local_gpu_idle_since = now  # Reset after action
        else:
            self._local_gpu_idle_since = 0  # GPU is busy, reset

        if started > 0:
            logger.info(f"LOCAL GPU auto-scale: started {started} job(s)")
        return started

    async def _local_resource_cleanup(self):
        """DECENTRALIZED: Each node handles its own resource pressure.

        Runs on ALL nodes to ensure resource cleanup without leader coordination.
        Handles disk cleanup, memory pressure, and log rotation.
        """
        now = time.time()

        # Rate limit: check every 5 minutes
        last_check = getattr(self, "_last_local_resource_check", 0)
        if now - last_check < 300:
            return
        self._last_local_resource_check = now

        self._update_self_info()
        node = self.self_info

        # Disk cleanup
        if node.disk_percent >= DISK_CLEANUP_THRESHOLD:
            logger.info(f"LOCAL: Disk at {node.disk_percent:.0f}% - triggering cleanup")
            await self._cleanup_local_disk()

        # Memory pressure - reduce jobs
        if node.memory_percent >= MEMORY_CRITICAL_THRESHOLD:
            logger.info(f"LOCAL: Memory CRITICAL at {node.memory_percent:.0f}%")
            await self._reduce_local_selfplay_jobs(0, reason="memory_critical")
        elif node.memory_percent >= MEMORY_WARNING_THRESHOLD:
            current = int(getattr(node, "selfplay_jobs", 0) or 0)
            target = max(1, current // 2)
            logger.info(f"LOCAL: Memory warning at {node.memory_percent:.0f}% - reducing jobs to {target}")
            await self._reduce_local_selfplay_jobs(target, reason="memory_warning")

        # Clean up old completed/failed jobs to prevent memory leak
        self.job_manager.cleanup_completed_jobs()

    # NOTE: _cleanup_old_completed_jobs() removed Dec 2025 (31 LOC).
    # Use self.job_manager.cleanup_completed_jobs() directly.

    # NOTE: _get_elo_based_priority_boost() removed Dec 2025 (~45 LOC).
    # Use self.selfplay_scheduler.get_elo_based_priority_boost() instead.

    # NOTE: _pick_weighted_selfplay_config() removed Dec 2025 (95 LOC).
    # Use self.selfplay_scheduler.pick_weighted_config() instead.
    # See scripts/p2p/managers/selfplay_scheduler.py for implementation.

    async def _auto_scale_gpu_utilization(self) -> int:
        """Auto-scale diverse/hybrid selfplay jobs to reach 60-80% GPU utilization.

        Detects underutilized GPU nodes and starts HYBRID selfplay jobs to improve
        cluster throughput while maintaining game quality and rule fidelity.

        NOTE: GPU-only selfplay is DISABLED. All auto-scaled jobs use hybrid mode
        which provides 100% rule fidelity (CPU rules) + GPU-accelerated evaluation.
        This produces higher quality training data than pure GPU selfplay.

        Returns:
            Number of new diverse selfplay jobs started
        """
        TARGET_GPU_MIN = 60.0  # Target minimum GPU utilization
        TARGET_GPU_MAX = 80.0  # Target maximum GPU utilization
        MIN_IDLE_TIME = 120    # Seconds of low GPU before scaling up

        started = 0
        now = time.time()

        # Rate limit auto-scaling (once per 2 minutes)
        last_scale = getattr(self, "_last_gpu_auto_scale", 0)
        if now - last_scale < 120:
            return 0

        with self.peers_lock:
            peers_snapshot = list(self.peers.values())

        underutilized_gpu_nodes = []

        # Load policy manager for filtering
        policy_manager = None
        try:
            from app.coordination.node_policies import get_policy_manager
            policy_manager = get_policy_manager()
        except ImportError:
            pass

        for peer in peers_snapshot:
            if not peer.is_alive():
                continue
            has_gpu = bool(getattr(peer, "has_gpu", False))
            if not has_gpu:
                continue

            # Policy check: skip nodes that don't allow selfplay
            if policy_manager and not policy_manager.is_work_allowed(peer.node_id, "selfplay"):
                continue

            gpu_percent = float(getattr(peer, "gpu_percent", 0) or 0)
            gpu_name = (getattr(peer, "gpu_name", "") or "").lower()
            selfplay_jobs = int(getattr(peer, "selfplay_jobs", 0) or 0)
            training_jobs = int(getattr(peer, "training_jobs", 0) or 0)

            # Skip if already training
            if training_jobs > 0:
                continue

            # Check if underutilized
            if gpu_percent < TARGET_GPU_MIN:
                # Track how long it's been underutilized
                idle_key = f"_gpu_idle_since_{peer.node_id}"
                idle_since = getattr(self, idle_key, 0)
                if idle_since == 0:
                    setattr(self, idle_key, now)
                elif now - idle_since > MIN_IDLE_TIME:
                    # Calculate how many more jobs to add
                    gpu_headroom = TARGET_GPU_MAX - gpu_percent
                    # Estimate jobs based on GPU tier
                    if any(tag in gpu_name for tag in ("h100", "h200", "gh200", "5090")):
                        jobs_per_10_percent = 2
                    elif any(tag in gpu_name for tag in ("a100", "4090", "3090")):
                        jobs_per_10_percent = 1.5
                    else:
                        jobs_per_10_percent = 1

                    new_jobs = max(1, int(gpu_headroom / 10 * jobs_per_10_percent))
                    new_jobs = min(new_jobs, 4)  # Cap at 4 new jobs per cycle

                    underutilized_gpu_nodes.append({
                        "node_id": peer.node_id,
                        "gpu_percent": gpu_percent,
                        "gpu_name": gpu_name,
                        "current_jobs": selfplay_jobs,
                        "new_jobs": new_jobs,
                    })
            else:
                # GPU is utilized, reset idle timer
                idle_key = f"_gpu_idle_since_{peer.node_id}"
                setattr(self, idle_key, 0)

        # Start GPU selfplay on underutilized nodes
        for node_info in underutilized_gpu_nodes[:3]:  # Max 3 nodes per cycle
            node_id = node_info["node_id"]
            new_jobs = node_info["new_jobs"]

            print(
                f"[P2P] Auto-scale: {node_id} at {node_info['gpu_percent']:.0f}% GPU, "
                f"starting {new_jobs} diverse/hybrid selfplay job(s)"
            )

            for _ in range(new_jobs):
                try:
                    # Schedule diverse/hybrid selfplay job (GPU-only selfplay disabled)
                    job = await self._schedule_diverse_selfplay_on_node(node_id)
                    if job:
                        started += 1
                except Exception as e:  # noqa: BLE001
                    logger.error(f"Failed to start diverse selfplay on {node_id}: {e}")
                    break

        if started > 0:
            self._last_gpu_auto_scale = now
            logger.info(f"Auto-scale: started {started} new diverse/hybrid selfplay job(s)")

        return started

    async def _auto_rebalance_from_work_queue(self) -> int:
        """Auto-rebalance: assign queued work to idle GPU nodes.

        When idle GPU-heavy nodes are detected, check the work queue for pending
        high-priority work and dispatch it. This ensures queued work gets done
        before falling back to selfplay auto-scaling.

        Returns:
            Number of work items dispatched
        """
        GPU_IDLE_THRESHOLD = 10.0  # Node is idle if GPU < 10%
        MIN_IDLE_TIME = 60  # Seconds of idle before assigning work
        GPU_HEAVY_TAGS = ['gh200', 'h100', 'h200', 'a100', '4090', '5090']

        dispatched = 0
        now = time.time()

        # Rate limit rebalancing (once per minute)
        last_rebalance = getattr(self, "_last_work_queue_rebalance", 0)
        if now - last_rebalance < 60:
            return 0

        # Check if work queue is available
        wq = get_work_queue()
        if wq is None:
            return 0

        # Get queue status
        queue_status = wq.get_queue_status()
        pending_count = queue_status.get("by_status", {}).get("pending", 0)
        if pending_count == 0:
            return 0  # No work to dispatch

        # Find idle GPU-heavy nodes
        idle_nodes = []

        with self.peers_lock:
            peers_snapshot = list(self.peers.values())

        for peer in peers_snapshot:
            if not peer.is_alive() or peer.retired:
                continue

            has_gpu = bool(getattr(peer, "has_gpu", False))
            if not has_gpu:
                continue

            gpu_name = (getattr(peer, "gpu_name", "") or "").upper()
            is_gpu_heavy = any(tag.upper() in gpu_name for tag in GPU_HEAVY_TAGS)
            if not is_gpu_heavy:
                continue

            gpu_percent = float(getattr(peer, "gpu_percent", 0) or 0)
            training_jobs = int(getattr(peer, "training_jobs", 0) or 0)
            selfplay_jobs = int(getattr(peer, "selfplay_jobs", 0) or 0)

            # Skip if already busy
            if training_jobs > 0:
                continue

            # Check if truly idle
            if gpu_percent < GPU_IDLE_THRESHOLD:
                # Track how long it's been idle
                idle_key = f"_wq_idle_since_{peer.node_id}"
                idle_since = getattr(self, idle_key, 0)
                if idle_since == 0:
                    setattr(self, idle_key, now)
                elif now - idle_since > MIN_IDLE_TIME:
                    # Get allowed work types for this node
                    try:
                        from app.coordination.node_policies import get_policy_manager
                        pm = get_policy_manager()
                        allowed = list(pm.get_allowed_work_types(peer.node_id))
                    except ImportError:
                        allowed = ["training", "gpu_cmaes", "tournament", "selfplay"]

                    idle_nodes.append({
                        "node_id": peer.node_id,
                        "peer": peer,
                        "gpu_percent": gpu_percent,
                        "gpu_name": gpu_name,
                        "allowed": allowed,
                        "selfplay_jobs": selfplay_jobs,
                    })
            else:
                # Not idle, reset timer
                idle_key = f"_wq_idle_since_{peer.node_id}"
                setattr(self, idle_key, 0)

        # Dispatch work to idle nodes
        for node_info in idle_nodes[:5]:  # Max 5 nodes per cycle
            node_id = node_info["node_id"]
            allowed = node_info["allowed"]

            # Try to claim work for this node using Raft or SQLite based on consensus mode
            # claim_work_distributed() returns a dict with work_id, work_type, config, etc.
            work_item = self.claim_work_distributed(node_id, allowed)
            if work_item is None:
                continue

            # Get work_type - may be string or WorkType enum
            work_type_str = work_item.get("work_type", "unknown")
            if hasattr(work_type_str, "value"):
                work_type_str = work_type_str.value
            work_id = work_item.get("work_id", "unknown")

            print(
                f"[P2P] Work queue rebalance: {node_id} idle at {node_info['gpu_percent']:.0f}% GPU, "
                f"assigning {work_type_str} work ({work_id})"
            )

            # Dispatch work to the node
            success = await self._dispatch_queued_work(node_info["peer"], work_item)
            if success:
                # Mark work as started using distributed method for Raft consistency
                self.start_work_distributed(work_id)
                dispatched += 1
                # Reset idle timer since we assigned work
                idle_key = f"_wq_idle_since_{node_id}"
                setattr(self, idle_key, 0)
            else:
                # Failed to dispatch, reset work status for retry
                self.fail_work_distributed(work_id, "dispatch_failed")

        if dispatched > 0:
            self._last_work_queue_rebalance = now
            logger.info(f"Work queue rebalance: dispatched {dispatched} work item(s) to idle nodes")

        return dispatched

    async def _dispatch_queued_work(self, peer: NodeInfo, work_item: dict) -> bool:
        """Dispatch a work queue item to a specific node.

        Routes different work types to appropriate endpoints.

        Args:
            peer: Target node info
            work_item: Work item dict with work_id, work_type, config, etc.
                       (from claim_work_distributed)
        """
        from app.coordination.work_queue import WorkType

        try:
            timeout = ClientTimeout(total=30)
            async with get_client_session(timeout) as session:
                # Handle both dict and object formats for backward compatibility
                if isinstance(work_item, dict):
                    work_type = work_item.get("work_type")
                    config = work_item.get("config", {})
                    work_id = work_item.get("work_id")
                else:
                    work_type = work_item.work_type
                    config = work_item.config
                    work_id = work_item.work_id

                # Convert string work_type to enum if needed
                if isinstance(work_type, str):
                    try:
                        work_type = WorkType(work_type)
                    except ValueError:
                        logger.warning(f"Unknown work type string: {work_type}")
                        return False

                if work_type == WorkType.TRAINING:
                    # Route to training endpoint
                    url = self._url_for_peer(peer, "/start_job")
                    payload = {
                        "job_type": "training",
                        "board_type": config.get("board_type", "square8"),
                        "num_players": config.get("num_players", 2),
                        "work_id": work_id,
                    }
                elif work_type == WorkType.GPU_CMAES:
                    # Route to CMA-ES endpoint
                    url = self._url_for_peer(peer, "/cmaes/start")
                    payload = {
                        "board_type": config.get("board_type", "square8"),
                        "num_players": config.get("num_players", 2),
                        "work_id": work_id,
                    }
                elif work_type == WorkType.TOURNAMENT:
                    # Route to tournament endpoint
                    url = self._url_for_peer(peer, "/tournament/start")
                    payload = {
                        "board_type": config.get("board_type", "square8"),
                        "num_players": config.get("num_players", 2),
                        "work_id": work_id,
                    }
                elif work_type == WorkType.SELFPLAY:
                    # Route to selfplay endpoint
                    url = self._url_for_peer(peer, "/selfplay/start")
                    payload = {
                        "board_type": config.get("board_type", "square8"),
                        "num_players": config.get("num_players", 2),
                        "num_games": config.get("num_games", 500),
                        "work_id": work_id,
                    }
                else:
                    logger.warning(f"Unknown work type: {work_type}")
                    return False

                async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                    if resp.status == 200:
                        logger.info(f"Dispatched {work_type.value} work to {peer.node_id}")
                        return True
                    else:
                        error = await resp.text()
                        logger.warning(f"Failed to dispatch work to {peer.node_id}: {error}")
                        return False

        except Exception as e:  # noqa: BLE001
            logger.error(f"Error dispatching work to {peer.node_id}: {e}")
            return False

    async def _schedule_diverse_selfplay_on_node(self, node_id: str) -> dict | None:
        """Schedule a diverse/hybrid selfplay job on a specific node.

        Uses HYBRID mode (CPU rules + GPU eval) for 100% rule fidelity.
        Rotates through all board/player configurations for diversity.
        """
        with self.peers_lock:
            peer = self.peers.get(node_id)
        if not peer or not peer.is_alive():
            return None

        # Policy check: ensure selfplay is allowed on this node
        try:
            from app.coordination.node_policies import is_work_allowed
            if not is_work_allowed(node_id, "selfplay"):
                logger.debug(f"Selfplay not allowed on {node_id} by policy")
                return None
        except ImportError:
            pass

        # Rotate through diverse configurations with priority-based weighting
        # Uses board_priority_overrides from config (0=CRITICAL, 1=HIGH, 2=MEDIUM, 3=LOW)
        board_priority_overrides = get_board_priority_overrides()

        diverse_configs = [
            ("hexagonal", 2), ("hexagonal", 3), ("hexagonal", 4),
            ("square19", 3), ("square19", 4), ("square19", 2),
            ("hex8", 2), ("hex8", 3), ("hex8", 4),
            ("square8", 3), ("square8", 4), ("square8", 2),
        ]

        # Build weighted list based on priority overrides
        # Lower priority value = more weight (0=CRITICAL gets 4x, 3=LOW gets 1x)
        weighted_configs = []
        for board_type, num_players in diverse_configs:
            config_key = f"{board_type}_{num_players}p"
            priority = board_priority_overrides.get(config_key, 3)  # Default LOW
            weight = 4 - priority  # 0->4, 1->3, 2->2, 3->1
            weighted_configs.extend([(board_type, num_players)] * weight)

        # Round-robin through weighted list based on node-specific counter
        counter_key = f"_diverse_config_counter_{node_id}"
        counter = getattr(self, counter_key, 0)
        setattr(self, counter_key, counter + 1)
        board_type, num_players = weighted_configs[counter % len(weighted_configs)]

        try:
            timeout = ClientTimeout(total=30)
            async with get_client_session(timeout) as session:
                url = self._url_for_peer(peer, "/selfplay/start")
                payload = {
                    "board_type": board_type,
                    "num_players": num_players,
                    "num_games": 200,  # Smaller batches for diversity
                    "engine_mode": "mixed",  # HYBRID mode: CPU rules + GPU eval
                    "auto_scaled": True,
                    "job_type": "hybrid_selfplay",  # Explicitly request hybrid
                }
                async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        logger.info(f"Started diverse selfplay on {node_id}: {board_type} {num_players}p")
                        return data
                    else:
                        error = await resp.text()
                        logger.info(f"Diverse selfplay start failed on {node_id}: {error}")
                        return None
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to schedule diverse selfplay on {node_id}: {e}")
            return None

    # Backward compatibility alias (GPU selfplay now redirects to diverse/hybrid)
    _schedule_gpu_selfplay_on_node = _schedule_diverse_selfplay_on_node

    # NOTE: _target_selfplay_jobs_for_node() removed Dec 2025 (160 LOC).
    # Use self.selfplay_scheduler.get_target_jobs_for_node() instead.

    # NOTE: _get_hybrid_job_targets() removed Dec 2025 (38 LOC).
    # Use self.selfplay_scheduler.get_hybrid_job_targets() instead.

    # NOTE: _should_spawn_cpu_only_jobs() removed Dec 2025 (33 LOC).
    # Use self.selfplay_scheduler.should_spawn_cpu_only_jobs() instead.


    async def _check_cluster_balance(self) -> dict[str, Any]:
        """Check and rebalance jobs across the cluster.

        This method identifies:
        1. Powerful nodes that are underutilized (high capacity, low jobs)
        2. Weak nodes that are overloaded (low capacity, high jobs)

        When imbalance is detected, it reduces jobs on weak nodes so the
        scheduler can assign them to more powerful nodes.

        Returns dict with rebalancing actions taken.
        """
        try:
            with self.peers_lock:
                alive_peers = [p for p in self.peers.values() if p.is_alive()]

            all_nodes = [*alive_peers, self.self_info]
            healthy_nodes = [n for n in all_nodes if n.is_healthy()]

            if len(healthy_nodes) < 2:
                return {"action": "none", "reason": "insufficient_nodes"}

            # Calculate capacity and utilization for each node
            node_stats = []
            for node in healthy_nodes:
                target = self.selfplay_scheduler.get_target_jobs_for_node(node)
                current = int(getattr(node, "selfplay_jobs", 0) or 0)
                utilization = current / max(1, target)  # How full is this node
                capacity_score = target  # Higher = more powerful

                node_stats.append({
                    "node": node,
                    "target": target,
                    "current": current,
                    "utilization": utilization,
                    "capacity": capacity_score,
                    "load_score": node.get_load_score(),
                })

            # Find underutilized powerful nodes (capacity > median, utilization < 50%)
            sorted_by_capacity = sorted(node_stats, key=lambda x: x["capacity"], reverse=True)
            median_capacity = sorted_by_capacity[len(sorted_by_capacity) // 2]["capacity"]

            underutilized_powerful = [
                n for n in node_stats
                if n["capacity"] > median_capacity and n["utilization"] < 0.5
            ]

            # Find overloaded weak nodes (capacity < median, utilization > 100%)
            overloaded_weak = [
                n for n in node_stats
                if n["capacity"] < median_capacity and n["utilization"] > 1.0
            ]

            if not underutilized_powerful or not overloaded_weak:
                return {"action": "none", "reason": "balanced"}

            # Calculate rebalancing opportunity
            spare_capacity = sum(
                max(0, n["target"] - n["current"]) for n in underutilized_powerful
            )
            excess_load = sum(
                max(0, n["current"] - n["target"]) for n in overloaded_weak
            )

            if spare_capacity < 2 or excess_load < 2:
                return {"action": "none", "reason": "minimal_imbalance"}

            # Migrate: reduce jobs on weak nodes
            rebalance_actions = []
            jobs_to_migrate = min(spare_capacity, excess_load)

            for weak_node in sorted(overloaded_weak, key=lambda x: x["utilization"], reverse=True):
                if jobs_to_migrate <= 0:
                    break

                node = weak_node["node"]
                reduce_by = min(
                    weak_node["current"] - weak_node["target"],
                    jobs_to_migrate
                )
                new_target = weak_node["current"] - reduce_by

                if reduce_by > 0:
                    print(
                        f"[P2P] Cluster rebalance: {node.node_id} overloaded "
                        f"({weak_node['current']}/{weak_node['target']} jobs, "
                        f"{weak_node['utilization']*100:.0f}% util) - reducing by {reduce_by}"
                    )

                    if node.node_id == self.node_id:
                        await self._reduce_local_selfplay_jobs(new_target, reason="cluster_rebalance")
                    else:
                        await self._request_reduce_selfplay(node, new_target, reason="cluster_rebalance")

                    rebalance_actions.append({
                        "node": node.node_id,
                        "reduced_by": reduce_by,
                        "new_target": new_target,
                    })
                    jobs_to_migrate -= reduce_by

            # Record rebalancing metric
            if rebalance_actions:
                self.record_metric(
                    "cluster_rebalance",
                    len(rebalance_actions),
                    metadata={
                        "spare_capacity": spare_capacity,
                        "excess_load": excess_load,
                        "actions": rebalance_actions,
                    },
                )

            return {
                "action": "rebalanced",
                "spare_capacity": spare_capacity,
                "excess_load": excess_load,
                "actions": rebalance_actions,
            }

        except Exception as e:  # noqa: BLE001
            logger.info(f"Cluster balance check error: {e}")
            return {"action": "error", "error": str(e)}

    async def _manage_cluster_jobs(self):
        """Manage jobs across the cluster (leader only).

        LEARNED LESSONS incorporated:
        - Check disk space BEFORE starting jobs (Vast.ai 91-93% disk issue)
        - Check memory to prevent OOM (AWS instance crashed at 31GB+)
        - Trigger cleanup when approaching limits
        - Use is_healthy() not just is_alive()
        """
        logger.info("Leader: Managing cluster jobs...")

        # Gather cluster state
        with self.peers_lock:
            alive_peers = [p for p in self.peers.values() if p.is_alive()]

        # Add self
        self._update_self_info()
        all_nodes = [*alive_peers, self.self_info]

        # Phase 1: Handle resource warnings and cleanup
        for node in all_nodes:
            # LEARNED LESSONS - Proactive disk cleanup before hitting critical
            if node.disk_percent >= DISK_CLEANUP_THRESHOLD:
                logger.info(f"{node.node_id}: Disk at {node.disk_percent:.0f}% - triggering cleanup")
                if node.node_id == self.node_id:
                    await self._cleanup_local_disk()
                else:
                    await self._request_remote_cleanup(node)
                continue  # Skip job creation this cycle

            # Load shedding: when a node is under memory/disk pressure, ask it to
            # stop excess selfplay jobs so it can recover (prevents OOM + disk-full).
            pressure_reasons: list[str] = []
            if node.memory_percent >= MEMORY_WARNING_THRESHOLD:
                pressure_reasons.append("memory")
            if node.disk_percent >= DISK_WARNING_THRESHOLD:
                pressure_reasons.append("disk")

            if pressure_reasons:
                desired = self.selfplay_scheduler.get_target_jobs_for_node(node)
                if node.memory_percent >= MEMORY_CRITICAL_THRESHOLD or node.disk_percent >= DISK_CRITICAL_THRESHOLD:
                    desired = 0

                if node.selfplay_jobs > desired:
                    reason = "+".join(pressure_reasons)
                    print(
                        f"[P2P] {node.node_id}: Load shedding (reason={reason}) "
                        f"{node.selfplay_jobs}->{desired} selfplay jobs"
                    )
                    if node.node_id == self.node_id:
                        await self._reduce_local_selfplay_jobs(desired, reason=reason)
                    else:
                        await self._request_reduce_selfplay(node, desired, reason=reason)

        # Phase 1.5: LEARNED LESSONS - Detect stuck jobs (GPU idle with running processes)
        # This addresses the vast-5090-quad issue where 582 processes ran at 0% GPU
        for node in all_nodes:
            if not node.has_gpu or node.selfplay_jobs <= 0:
                # No GPU or no jobs running - not stuck
                if node.node_id in self.gpu_idle_since:
                    del self.gpu_idle_since[node.node_id]
                continue

            # Check if GPU is idle (< threshold) with jobs running
            gpu_name = (node.gpu_name or "").upper()
            is_cuda_gpu = "MPS" not in gpu_name and "APPLE" not in gpu_name
            if not is_cuda_gpu:
                continue  # Skip Apple Silicon, doesn't have nvidia-smi

            if node.gpu_percent < GPU_IDLE_THRESHOLD:
                # GPU idle with jobs running - track or take action
                if node.node_id not in self.gpu_idle_since:
                    self.gpu_idle_since[node.node_id] = time.time()
                    logger.info(f"{node.node_id}: GPU idle ({node.gpu_percent:.0f}%) with {node.selfplay_jobs} jobs - monitoring")
                else:
                    idle_duration = time.time() - self.gpu_idle_since[node.node_id]
                    if idle_duration >= GPU_IDLE_RESTART_TIMEOUT:
                        logger.info(f"{node.node_id}: STUCK! GPU idle for {idle_duration:.0f}s with {node.selfplay_jobs} jobs")
                        logger.info(f"{node.node_id}: Requesting job restart...")
                        if node.node_id == self.node_id:
                            await self._restart_local_stuck_jobs()
                        else:
                            await self._request_job_restart(node)
                        del self.gpu_idle_since[node.node_id]
            else:
                # GPU is working - clear idle tracking
                if node.node_id in self.gpu_idle_since:
                    del self.gpu_idle_since[node.node_id]

        # Phase 1.6: Detect runaway selfplay processes (lost tracking / manual runs).
        # If a node reports an absurd number of selfplay processes, request a
        # restart sweep to kill untracked jobs and recover capacity.
        for node in all_nodes:
            try:
                target_selfplay = self.selfplay_scheduler.get_target_jobs_for_node(node)
                dynamic_threshold = max(16, target_selfplay * 3)
                runaway_threshold = (
                    int(RUNAWAY_SELFPLAY_PROCESS_THRESHOLD)
                    if int(RUNAWAY_SELFPLAY_PROCESS_THRESHOLD) > 0
                    else int(dynamic_threshold)
                )
                if int(getattr(node, "selfplay_jobs", 0) or 0) < runaway_threshold:
                    continue
            except (ValueError, AttributeError):
                continue

            print(
                f"[P2P] {node.node_id}: RUNAWAY selfplay count ({node.selfplay_jobs}) "
                f">= {runaway_threshold}  requesting restart sweep"
            )
            if node.node_id == self.node_id:
                await self._restart_local_stuck_jobs()
            else:
                await self._request_job_restart(node)

        # Phase 2: Calculate desired job distribution for healthy nodes
        # LEARNED LESSONS - Sort nodes by load score for load balancing
        # Least-loaded nodes get jobs first to ensure even distribution
        healthy_nodes = [n for n in all_nodes if n.is_healthy()]
        healthy_nodes.sort(key=lambda n: n.get_load_score())

        if healthy_nodes:
            load_summary = ", ".join(
                f"{n.node_id[:12]}={n.get_load_score():.0f}%"
                for n in healthy_nodes[:5]
            )
            logger.info(f"Load balancing: {load_summary}")

        for node in healthy_nodes:
            load_score = node.get_load_score()
            if load_score >= LOAD_MAX_FOR_NEW_JOBS:
                logger.info(f"{node.node_id}: Load {load_score:.0f}% - skipping new job starts")
                continue

            # LEARNED LESSONS - Reduce target when approaching limits
            # Base targets:
            # - GPU nodes: fixed concurrency tuned for GPU throughput.
            # - CPU-only nodes: scale with CPU cores (and cap by memory).
            # HYBRID MODE: Get separate GPU and CPU-only job targets (delegated)
            hybrid_targets = self.selfplay_scheduler.get_hybrid_job_targets(node)
            gpu_job_target = hybrid_targets.get("gpu_jobs", 0)
            cpu_only_target = hybrid_targets.get("cpu_only_jobs", 0)
            total_target = hybrid_targets.get("total_jobs", gpu_job_target)

            # Backward compat: use total_target like the old target_selfplay
            target_selfplay = total_target

            # Check if node needs more jobs
            if node.selfplay_jobs < target_selfplay:
                needed = target_selfplay - node.selfplay_jobs
                logger.info(f"{node.node_id} needs {needed} more selfplay jobs")

                # Job configuration diversity - cycle through different AI methods
                # LEARNED LESSONS - Prioritize varied AI methods for better training:
                # - nn-only: Neural network evaluation (NNUE + Descent)
                # - best-vs-pool: Tournament-style asymmetric play (best model vs varied pool)
                # - nn-vs-mcts: NN player vs MCTS player (asymmetric tournament)
                # - nn-vs-minimax: NN player vs Minimax player (asymmetric tournament)
                # - nn-vs-descent: NN player vs heuristic Descent (asymmetric tournament)
                # - tournament-varied: Each player gets different AI type (max variety)
                # - mcts-only: Pure Monte Carlo Tree Search
                # - descent-only: Gradient descent based evaluation (no NN)
                # - minimax-only: Classic minimax with alpha-beta pruning
                # NOTE: Heuristic-only modes removed to ensure NN/strong AI in every game
                selfplay_configs = [
                    # ================================================================
                    # GUMBEL MCTS - HIGHEST PRIORITY (70% of jobs should use Gumbel)
                    # GPU-accelerated Gumbel Top-K MCTS for high-quality training data
                    # 3p/4p get priority 12 (underrepresented), 2p get priority 10
                    # ================================================================
                    {"board_type": "hex8", "num_players": 2, "engine_mode": "gumbel-mcts", "priority": 10},
                    {"board_type": "hex8", "num_players": 3, "engine_mode": "gumbel-mcts", "priority": 12},  # Dec 27: +20% for underrepresented
                    {"board_type": "hex8", "num_players": 4, "engine_mode": "gumbel-mcts", "priority": 12},  # Dec 27: +20% for underrepresented
                    {"board_type": "square8", "num_players": 2, "engine_mode": "gumbel-mcts", "priority": 10},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "gumbel-mcts", "priority": 12},  # Dec 27: +20% for underrepresented
                    {"board_type": "square8", "num_players": 4, "engine_mode": "gumbel-mcts", "priority": 10},  # Already has most data
                    {"board_type": "square19", "num_players": 2, "engine_mode": "gumbel-mcts", "priority": 12},  # Dec 27: underrepresented
                    {"board_type": "square19", "num_players": 3, "engine_mode": "gumbel-mcts", "priority": 12},  # Dec 27: underrepresented
                    {"board_type": "square19", "num_players": 4, "engine_mode": "gumbel-mcts", "priority": 12},  # Dec 27: underrepresented
                    {"board_type": "hexagonal", "num_players": 2, "engine_mode": "gumbel-mcts", "priority": 12},  # Dec 27: underrepresented
                    {"board_type": "hexagonal", "num_players": 3, "engine_mode": "gumbel-mcts", "priority": 12},  # Dec 27: underrepresented
                    {"board_type": "hexagonal", "num_players": 4, "engine_mode": "gumbel-mcts", "priority": 12},  # Dec 27: underrepresented
                    # ================================================================
                    # UNDERREPRESENTED COMBINATIONS - MIXED MODE (PRIORITY 8)
                    # "mixed" mode provides varied AI matchups (NNUE, MCTS, heuristic combos)
                    # for maximum training data diversity (~30% of jobs)
                    # ================================================================
                    {"board_type": "hexagonal", "num_players": 3, "engine_mode": "mixed", "priority": 8},
                    {"board_type": "hexagonal", "num_players": 2, "engine_mode": "mixed", "priority": 8},
                    {"board_type": "hexagonal", "num_players": 4, "engine_mode": "mixed", "priority": 8},
                    {"board_type": "hex8", "num_players": 2, "engine_mode": "mixed", "priority": 8},
                    {"board_type": "hex8", "num_players": 3, "engine_mode": "mixed", "priority": 8},
                    {"board_type": "hex8", "num_players": 4, "engine_mode": "mixed", "priority": 8},
                    {"board_type": "square19", "num_players": 3, "engine_mode": "mixed", "priority": 8},
                    {"board_type": "square19", "num_players": 2, "engine_mode": "mixed", "priority": 8},
                    {"board_type": "square19", "num_players": 4, "engine_mode": "mixed", "priority": 8},
                    # ================================================================
                    # UNDERREPRESENTED COMBINATIONS (PRIORITY 7)
                    # Specific AI matchup modes for variety
                    # ================================================================
                    {"board_type": "square19", "num_players": 2, "engine_mode": "nn-only", "priority": 7},
                    {"board_type": "square19", "num_players": 2, "engine_mode": "nn-vs-mcts", "priority": 7},
                    {"board_type": "hexagonal", "num_players": 2, "engine_mode": "nn-only", "priority": 7},
                    {"board_type": "hexagonal", "num_players": 2, "engine_mode": "nn-vs-mcts", "priority": 7},
                    {"board_type": "hexagonal", "num_players": 3, "engine_mode": "nn-only", "priority": 7},
                    {"board_type": "hexagonal", "num_players": 3, "engine_mode": "nn-vs-mcts", "priority": 7},
                    # ================================================================
                    # SQUARE8 MULTI-PLAYER WITH MIXED MODE (PRIORITY 6.5)
                    # ================================================================
                    {"board_type": "square8", "num_players": 3, "engine_mode": "mixed", "priority": 7},
                    {"board_type": "square8", "num_players": 4, "engine_mode": "mixed", "priority": 7},
                    {"board_type": "square8", "num_players": 2, "engine_mode": "mixed", "priority": 6},
                    # ================================================================
                    # CROSS-AI MATCHES (PRIORITY 6) - Variety via asymmetric opponents
                    # heuristic/random vs strong AI (MCTS, Minimax, Descent, NN)
                    # ================================================================
                    {"board_type": "square8", "num_players": 2, "engine_mode": "heuristic-vs-nn", "priority": 6},
                    {"board_type": "square8", "num_players": 2, "engine_mode": "heuristic-vs-mcts", "priority": 6},
                    {"board_type": "square8", "num_players": 2, "engine_mode": "random-vs-mcts", "priority": 6},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "heuristic-vs-nn", "priority": 6},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "heuristic-vs-mcts", "priority": 6},
                    {"board_type": "square19", "num_players": 2, "engine_mode": "heuristic-vs-mcts", "priority": 6},
                    {"board_type": "hexagonal", "num_players": 2, "engine_mode": "heuristic-vs-mcts", "priority": 6},
                    {"board_type": "hexagonal", "num_players": 3, "engine_mode": "heuristic-vs-mcts", "priority": 6},
                    # ================================================================
                    # NEURAL NETWORK MODES (PRIORITY 5)
                    # ================================================================
                    {"board_type": "square8", "num_players": 2, "engine_mode": "nn-only", "priority": 5},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "nn-only", "priority": 5},
                    {"board_type": "square8", "num_players": 4, "engine_mode": "nn-only", "priority": 5},
                    {"board_type": "square8", "num_players": 2, "engine_mode": "best-vs-pool", "priority": 5},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "best-vs-pool", "priority": 5},
                    {"board_type": "square19", "num_players": 3, "engine_mode": "nn-only", "priority": 5},
                    {"board_type": "square19", "num_players": 4, "engine_mode": "nn-only", "priority": 5},
                    {"board_type": "hexagonal", "num_players": 4, "engine_mode": "nn-only", "priority": 5},
                    # ================================================================
                    # ASYMMETRIC TOURNAMENT MODES (PRIORITY 5) - NN vs other AI types
                    # ================================================================
                    {"board_type": "square8", "num_players": 2, "engine_mode": "nn-vs-mcts", "priority": 5},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "nn-vs-mcts", "priority": 5},
                    {"board_type": "square8", "num_players": 4, "engine_mode": "nn-vs-mcts", "priority": 5},
                    {"board_type": "square8", "num_players": 2, "engine_mode": "nn-vs-minimax", "priority": 5},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "nn-vs-minimax", "priority": 5},
                    {"board_type": "square8", "num_players": 2, "engine_mode": "nn-vs-descent", "priority": 5},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "nn-vs-descent", "priority": 5},
                    {"board_type": "square19", "num_players": 3, "engine_mode": "nn-vs-mcts", "priority": 5},
                    {"board_type": "square19", "num_players": 4, "engine_mode": "nn-vs-mcts", "priority": 5},
                    {"board_type": "hexagonal", "num_players": 4, "engine_mode": "nn-vs-mcts", "priority": 5},
                    # ================================================================
                    # TOURNAMENT-VARIED (PRIORITY 4) - Max diversity, always includes NN
                    # ================================================================
                    {"board_type": "square8", "num_players": 2, "engine_mode": "tournament-varied", "priority": 4},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "tournament-varied", "priority": 4},
                    {"board_type": "square8", "num_players": 4, "engine_mode": "tournament-varied", "priority": 4},
                    {"board_type": "square19", "num_players": 2, "engine_mode": "tournament-varied", "priority": 4},
                    {"board_type": "square19", "num_players": 3, "engine_mode": "tournament-varied", "priority": 4},
                    {"board_type": "hexagonal", "num_players": 2, "engine_mode": "tournament-varied", "priority": 4},
                    {"board_type": "hexagonal", "num_players": 3, "engine_mode": "tournament-varied", "priority": 4},
                    # ================================================================
                    # CPU-BOUND AI METHODS (PRIORITY 3) - MCTS, Descent, Minimax
                    # For CPU-only instances and variety
                    # ================================================================
                    {"board_type": "hexagonal", "num_players": 4, "engine_mode": "mcts-only", "priority": 3},
                    {"board_type": "hexagonal", "num_players": 3, "engine_mode": "descent-only", "priority": 3},
                    {"board_type": "square19", "num_players": 4, "engine_mode": "mcts-only", "priority": 3},
                    {"board_type": "square19", "num_players": 3, "engine_mode": "mcts-only", "priority": 3},
                    {"board_type": "square19", "num_players": 3, "engine_mode": "descent-only", "priority": 3},
                    {"board_type": "square8", "num_players": 2, "engine_mode": "mcts-only", "priority": 3},
                    {"board_type": "square8", "num_players": 2, "engine_mode": "descent-only", "priority": 3},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "descent-only", "priority": 3},
                    {"board_type": "square8", "num_players": 4, "engine_mode": "mcts-only", "priority": 3},
                    # ================================================================
                    # MINIMAX (PRIORITY 2) - Classical approach, good for variety
                    # ================================================================
                    {"board_type": "square8", "num_players": 2, "engine_mode": "minimax-only", "priority": 2},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "minimax-only", "priority": 2},
                    {"board_type": "square19", "num_players": 2, "engine_mode": "descent-only", "priority": 2},
                    {"board_type": "square19", "num_players": 2, "engine_mode": "minimax-only", "priority": 2},
                    {"board_type": "hexagonal", "num_players": 2, "engine_mode": "descent-only", "priority": 2},
                    {"board_type": "hexagonal", "num_players": 2, "engine_mode": "minimax-only", "priority": 2},
                    # NO PURE HEURISTIC-ONLY MODES - all modes include at least one
                    # strong AI (NN/MCTS/Descent/Minimax) for quality training data
                ]

                # LEARNED LESSONS - Weighted selection favoring high priority configs
                # Expand configs by priority for weighted random selection
                node_mem = int(getattr(node, "memory_gb", 0) or 0)
                filtered_configs = selfplay_configs
                if node_mem and node_mem < 48:
                    # Smaller CPU nodes should avoid square19/hexagonal to reduce
                    # OOM risk and thrash. Keep them productive with square8.
                    filtered_configs = [cfg for cfg in selfplay_configs if cfg.get("board_type") == "square8"]

                weighted_configs = []
                for cfg in filtered_configs:
                    weighted_configs.extend([cfg] * cfg.get("priority", 1))

                # FIXED: Start all needed jobs with fair config distribution
                # Instead of max 2, start up to 10 at a time to quickly fill all configs
                # Use round-robin across unique configs to ensure coverage
                unique_configs = list({(c["board_type"], c["num_players"]): c for c in filtered_configs}.values())
                jobs_to_start = min(needed, 10)  # Start up to 10 jobs per iteration

                # HYBRID MODE: Calculate how many GPU vs CPU-only jobs to spawn
                # If node already has gpu_job_target GPU jobs, spawn CPU-only jobs instead
                current_gpu_jobs = min(node.selfplay_jobs, gpu_job_target)
                remaining_gpu_slots = max(0, gpu_job_target - current_gpu_jobs)
                remaining_cpu_slots = max(0, cpu_only_target)  # Can always spawn CPU-only if capacity
                should_use_cpu_only = self.selfplay_scheduler.should_spawn_cpu_only_jobs(node) and cpu_only_target > 0

                for i in range(jobs_to_start):
                    # LEARNED LESSONS - Smart CPU/GPU task routing:
                    # - High-end GPUs (H100/H200/A100/5090/4090) get GPU_SELFPLAY for max throughput
                    #   with automatic CPU validation to ensure data quality
                    # - Mid-tier GPUs get HYBRID_SELFPLAY (CPU rules + GPU eval)
                    # - CPU-only nodes or GPU-saturated nodes get CPU_SELFPLAY
                    # This ensures expensive GPU resources are utilized properly
                    # while CPU instances handle CPU-bound tasks efficiently
                    gpu_name = (node.gpu_name or "").upper()
                    is_high_end_gpu = any(tag in gpu_name for tag in ("H100", "H200", "GH200", "A100", "5090", "4090"))
                    is_apple_gpu = "MPS" in gpu_name or "APPLE" in gpu_name

                    # GPU utilization check: if GPU is at 0% with jobs running, those jobs
                    # are probably CPU-only. This is an opportunity to START GPU work, not avoid it.
                    # Only consider GPU "unavailable" if there are existing GPU jobs AND utilization is 0%
                    # (which would indicate driver/container issues)
                    gpu_percent = getattr(node, "gpu_percent", 0) or 0
                    gpu_jobs_count = getattr(node, "gpu_selfplay_jobs", 0) or 0
                    gpu_seems_unavailable = (
                        node.has_gpu
                        and not is_apple_gpu
                        and gpu_jobs_count > 2  # Only flag if GPU JOBS (not CPU jobs) are running
                        and gpu_percent < 1
                    )
                    if gpu_seems_unavailable:
                        logger.info(f"WARNING: {node.node_id} has GPU but 0% utilization with {gpu_jobs_count} GPU jobs - possible driver issue")
                    elif node.has_gpu and gpu_percent < 10 and node.selfplay_jobs > 0:
                        # GPU idle but has CPU jobs - this is normal, will prioritize GPU work
                        logger.debug(f"Node {node.node_id} has {node.selfplay_jobs} CPU jobs but GPU idle ({gpu_percent:.0f}%) - will add GPU work")

                    # HYBRID MODE: Decide between GPU and CPU-only based on capacity
                    spawn_cpu_only = False
                    if remaining_gpu_slots > 0 and not gpu_seems_unavailable:
                        remaining_gpu_slots -= 1
                    elif should_use_cpu_only and remaining_cpu_slots > 0:
                        spawn_cpu_only = True
                        remaining_cpu_slots -= 1
                    elif gpu_seems_unavailable:
                        spawn_cpu_only = True

                    if spawn_cpu_only:
                        # Pure CPU selfplay to utilize excess CPU capacity
                        job_type = JobType.CPU_SELFPLAY
                        task_type_str = "CPU-only (hybrid mode)"
                    elif node.has_gpu and is_high_end_gpu and not is_apple_gpu and not gpu_seems_unavailable:
                        # High-end CUDA GPUs: Mix of GPU_SELFPLAY (volume) and GUMBEL_SELFPLAY (quality)
                        # GPU selfplay now has high parity with CPU rules (2025-12 upgrade)
                        # Use Gumbel MCTS ~50% of time for high-quality training data (self-improvement loop)
                        # Increased from 20% to close the training loop - AlphaZero needs NN+MCTS data
                        import random
                        if random.random() < 0.5:  # 50% chance for Gumbel MCTS (quality) - was 20%
                            job_type = JobType.GUMBEL_SELFPLAY
                            task_type_str = "GUMBEL (high-quality)"
                        else:  # 50% for GPU selfplay (volume)
                            job_type = JobType.GPU_SELFPLAY
                            task_type_str = "GPU (high-parity)"
                    elif node.has_gpu and not is_apple_gpu and not gpu_seems_unavailable:
                        # Mid-tier GPUs: Use hybrid (CPU rules + GPU eval)
                        job_type = JobType.HYBRID_SELFPLAY
                        task_type_str = "HYBRID (accel)"
                    else:
                        job_type = JobType.SELFPLAY
                        task_type_str = "CPU-only"

                    gpu_info = f"gpu={node.gpu_name or 'none'}, gpu%={getattr(node, 'gpu_percent', 0):.0f}" if node.has_gpu else "no-gpu"
                    logger.info(f"Assigning {task_type_str} task to {node.node_id} ({gpu_info}, load={node.get_load_score():.0f}%)")

                    # FIXED: Round-robin config selection to ensure all configs get coverage
                    # Use unique_configs list for fair distribution across all 9 board/player combos
                    if self.improvement_cycle_manager and hasattr(self.improvement_cycle_manager, 'get_next_selfplay_config_for_node'):
                        # Node-aware dynamic selection: routes hex/sq19/3p/4p to powerful nodes
                        node_gpu_power = node.gpu_power_score() if hasattr(node, 'gpu_power_score') else 0
                        node_memory = int(getattr(node, 'memory_gb', 0) or 0)
                        config = self.improvement_cycle_manager.get_next_selfplay_config_for_node(
                            node_gpu_power=node_gpu_power,
                            node_memory_gb=node_memory,
                            cluster_data=self.cluster_data_manifest
                        )
                    else:
                        # Round-robin across unique configs for fair coverage
                        # Each iteration picks the next config in the list
                        config_idx = i % len(unique_configs)
                        config = unique_configs[config_idx]

                    # Track diversity metrics for monitoring (delegated to SelfplayScheduler)
                    self.selfplay_scheduler.track_diversity(config)

                    if node.node_id == self.node_id:
                        await self._start_local_job(
                            job_type,
                            board_type=config["board_type"],
                            num_players=config["num_players"],
                            engine_mode=config["engine_mode"],
                        )
                    else:
                        await self._request_remote_job(
                            node, job_type,
                            board_type=config["board_type"],
                            num_players=config["num_players"],
                            engine_mode=config["engine_mode"],
                        )

    async def _cleanup_local_disk(self):
        """Clean up disk space on local node.

        LEARNED LESSONS - Automatically archive old data:
        - Remove deprecated selfplay databases
        - Compress and archive old logs
        - Clear /tmp files older than 24h
        """
        logger.info("Running local disk cleanup...")
        try:
            # Prefer the shared disk monitor (used by cron/resilience) for consistent cleanup policy.
            disk_monitor = Path(self.ringrift_path) / "ai-service" / "scripts" / "disk_monitor.py"
            if disk_monitor.exists():
                usage = self._get_resource_usage()
                disk_percent = float(usage.get("disk_percent", 0.0) or 0.0)
                cmd = [
                    sys.executable,  # Use venv Python
                    str(disk_monitor),
                    "--threshold",
                    str(DISK_CLEANUP_THRESHOLD),
                    "--ringrift-path",
                    str(self.ringrift_path),
                    "--aggressive",
                ]
                if disk_percent >= DISK_CRITICAL_THRESHOLD:
                    cmd.append("--force")

                out = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    timeout=300,
                    cwd=str(Path(self.ringrift_path) / "ai-service"),
                )
                if out.returncode == 0:
                    logger.info("Disk monitor cleanup completed")
                else:
                    logger.info(f"Disk monitor cleanup failed: {out.stderr[:200]}")
            else:
                # Minimal fallback: clear old logs if disk monitor isn't available.
                log_dir = Path(self.ringrift_path) / "ai-service" / "logs"
                if log_dir.exists():
                    for logfile in log_dir.rglob("*.log"):
                        if time.time() - logfile.stat().st_mtime > 7 * 86400:  # 7 days
                            logfile.unlink()
                            logger.info(f"Cleaned old log: {logfile}")

        except Exception as e:  # noqa: BLE001
            logger.info(f"Disk cleanup error: {e}")

    async def _request_remote_cleanup(self, node: NodeInfo):
        """Request a remote node to clean up disk space."""
        try:
            if getattr(node, "nat_blocked", False):
                cmd_id = await self._enqueue_relay_command_for_peer(node, "cleanup", {})
                if cmd_id:
                    logger.info(f"Enqueued relay cleanup for {node.node_id}")
                else:
                    logger.info(f"Relay queue full; skipping cleanup enqueue for {node.node_id}")
                return
            timeout = ClientTimeout(total=HTTP_TOTAL_TIMEOUT)
            async with get_client_session(timeout) as session:
                last_err: str | None = None
                for url in self._urls_for_peer(node, "/cleanup"):
                    try:
                        async with session.post(url, json={}, headers=self._auth_headers()) as resp:
                            if resp.status == 200:
                                logger.info(f"Cleanup requested on {node.node_id}")
                                return
                            last_err = f"http_{resp.status}"
                    except Exception as e:  # noqa: BLE001
                        last_err = str(e)
                        continue
                if last_err:
                    logger.info(f"Cleanup request failed on {node.node_id}: {last_err}")
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to request cleanup from {node.node_id}: {e}")

    async def _reduce_local_selfplay_jobs(self, target_selfplay_jobs: int, *, reason: str) -> dict[str, Any]:
        """Best-effort: stop excess selfplay jobs on this node (load shedding).

        Used when disk/memory pressure is high: we want the node to recover and
        avoid OOM/disk-full scenarios, even if it means reducing throughput.
        """
        try:
            target = max(0, int(target_selfplay_jobs))
        except (ValueError):
            target = 0

        # First, get an overall count using the same mechanism used for cluster
        # reporting (includes untracked processes).
        try:
            selfplay_before, _training_before = self._count_local_jobs()
        except (AttributeError):
            selfplay_before = 0

        # Hard shedding (target=0): reuse the existing restart sweep, which
        # kills both tracked and untracked selfplay processes.
        if target <= 0:
            await self._restart_local_stuck_jobs()
            try:
                selfplay_after, _training_after = self._count_local_jobs()
            except (AttributeError):
                selfplay_after = 0
            return {
                "running_before": int(selfplay_before),
                "running_after": int(selfplay_after),
                "stopped": max(0, int(selfplay_before) - int(selfplay_after)),
                "target": 0,
                "reason": reason,
            }

        with self.jobs_lock:
            running: list[tuple[str, ClusterJob]] = [
                (job_id, job)
                for job_id, job in self.local_jobs.items()
                if job.status == "running"
                and job.job_type in (JobType.SELFPLAY, JobType.GPU_SELFPLAY, JobType.HYBRID_SELFPLAY, JobType.CPU_SELFPLAY, JobType.GUMBEL_SELFPLAY)
            ]

        if selfplay_before <= target and len(running) <= target:
            return {
                "running_before": int(selfplay_before),
                "running_after": int(selfplay_before),
                "stopped": 0,
                "target": target,
                "reason": reason,
            }

        # Stop newest-first to avoid killing long-running jobs near completion.
        running.sort(key=lambda pair: float(getattr(pair[1], "started_at", 0.0) or 0.0), reverse=True)
        to_stop = running[target:]

        stopped = 0
        with self.jobs_lock:
            for _job_id, job in to_stop:
                try:
                    if job.pid:
                        os.kill(int(job.pid), signal.SIGTERM)
                    job.status = "stopped"
                    stopped += 1
                except (ValueError, AttributeError):
                    continue

        # If job tracking was lost, we may still have a large number of
        # untracked selfplay processes. Best-effort kill enough to hit target.
        try:
            selfplay_mid, _training_mid = self._count_local_jobs()
        except (AttributeError):
            selfplay_mid = max(0, int(selfplay_before) - stopped)

        if selfplay_mid > target:
            try:
                import shutil

                if shutil.which("pgrep"):
                    pids: list[int] = []
                    # December 2025: Added selfplay.py - unified entry point
                    for pattern in (
                        "selfplay.py",
                        "run_self_play_soak.py",
                        "run_gpu_selfplay.py",
                        "run_hybrid_selfplay.py",
                        "run_random_selfplay.py",
                    ):
                        out = subprocess.run(
                            ["pgrep", "-f", pattern],
                            capture_output=True,
                            text=True,
                            timeout=5,
                        )
                        if out.returncode == 0 and out.stdout.strip():
                            for token in out.stdout.strip().split():
                                try:
                                    pids.append(int(token))
                                except (ValueError, AttributeError):
                                    continue

                    # Kill newest-ish (highest PID) first.
                    pids = sorted(set(pids), reverse=True)
                    excess = int(selfplay_mid) - int(target)
                    killed = 0
                    for pid in pids:
                        if killed >= excess:
                            break
                        try:
                            os.kill(pid, signal.SIGTERM)
                            killed += 1
                        except (AttributeError):
                            continue
                    stopped += killed
            except (AttributeError):
                pass

        if stopped:
            self._save_state()

        try:
            selfplay_after, _training_after = self._count_local_jobs()
        except (AttributeError):
            selfplay_after = max(0, int(selfplay_before) - stopped)

        return {
            "running_before": int(selfplay_before),
            "running_after": int(selfplay_after),
            "stopped": int(max(0, int(selfplay_before) - int(selfplay_after))),
            "target": target,
            "reason": reason,
        }

    async def _request_reduce_selfplay(self, node: NodeInfo, target_selfplay_jobs: int, *, reason: str) -> None:
        """Ask a node to shed excess selfplay (used for memory/disk pressure)."""
        try:
            target = max(0, int(target_selfplay_jobs))
        except (ValueError):
            target = 0

        if getattr(node, "nat_blocked", False):
            payload = {"target_selfplay_jobs": target, "reason": reason}
            cmd_id = await self._enqueue_relay_command_for_peer(node, "reduce_selfplay", payload)
            if cmd_id:
                logger.info(f"Enqueued relay reduce_selfplay for {node.node_id} (target={target}, reason={reason})")
            else:
                logger.info(f"Relay queue full for {node.node_id}; skipping reduce_selfplay enqueue")
            return

        timeout = ClientTimeout(total=HTTP_TOTAL_TIMEOUT)
        async with get_client_session(timeout) as session:
            last_err: str | None = None
            payload = {"target_selfplay_jobs": target, "reason": reason}
            for url in self._urls_for_peer(node, "/reduce_selfplay"):
                try:
                    async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                        if resp.status == 200:
                            logger.info(f"Requested load shedding on {node.node_id} (target={target}, reason={reason})")
                            return
                        last_err = f"http_{resp.status}"
                except Exception as e:  # noqa: BLE001
                    last_err = str(e)
                    continue
            if last_err:
                logger.info(f"reduce_selfplay request failed on {node.node_id}: {last_err}")

    async def _restart_local_stuck_jobs(self):
        """Kill stuck selfplay processes and let job management restart them.

        LEARNED LESSONS - Addresses the issue where processes accumulate but GPU stays at 0%.
        """
        logger.info("Restarting stuck local selfplay jobs...")
        try:
            # Kill tracked selfplay jobs (avoid broad pkill patterns).
            jobs_to_clear: list[str] = []
            pids_to_kill: set[int] = set()
            with self.jobs_lock:
                for job_id, job in self.local_jobs.items():
                    if job.job_type not in (JobType.SELFPLAY, JobType.GPU_SELFPLAY, JobType.HYBRID_SELFPLAY, JobType.CPU_SELFPLAY, JobType.GUMBEL_SELFPLAY):
                        continue
                    jobs_to_clear.append(job_id)
                    if job.pid:
                        try:
                            pids_to_kill.add(int(job.pid))
                        except (ValueError, AttributeError):
                            continue

            # Sweep for untracked selfplay processes (e.g. lost local_jobs state) and kill them too.
            try:
                import shutil

                if shutil.which("pgrep"):
                    # December 2025: Added selfplay.py - unified entry point
                    for pattern in (
                        "selfplay.py",
                        "run_self_play_soak.py",
                        "run_gpu_selfplay.py",
                        "run_hybrid_selfplay.py",
                        "run_random_selfplay.py",
                    ):
                        out = subprocess.run(
                            ["pgrep", "-f", pattern],
                            capture_output=True,
                            text=True,
                            timeout=5,
                        )
                        if out.returncode == 0 and out.stdout.strip():
                            for token in out.stdout.strip().split():
                                try:
                                    pids_to_kill.add(int(token))
                                except (ValueError, AttributeError):
                                    continue
            except (ValueError, AttributeError):
                pass

            pids_to_kill.discard(int(os.getpid()))

            killed = 0
            for pid in sorted(pids_to_kill):
                try:
                    os.kill(pid, signal.SIGKILL)
                    killed += 1
                except (AttributeError):
                    continue

            # Clear our job tracking - they'll be restarted next cycle.
            with self.jobs_lock:
                for job_id in jobs_to_clear:
                    self.local_jobs.pop(job_id, None)

            logger.info(f"Killed {killed} processes, cleared {len(jobs_to_clear)} job records")
        except Exception as e:  # noqa: BLE001
            logger.error(f"killing stuck processes: {e}")

    async def _request_job_restart(self, node: NodeInfo):
        """Request a remote node to restart its stuck selfplay jobs."""
        try:
            if getattr(node, "nat_blocked", False):
                cmd_id = await self._enqueue_relay_command_for_peer(node, "restart_stuck_jobs", {})
                if cmd_id:
                    logger.info(f"Enqueued relay restart_stuck_jobs for {node.node_id}")
                else:
                    logger.info(f"Relay queue full for {node.node_id}; skipping restart enqueue")
                return
            timeout = ClientTimeout(total=HTTP_TOTAL_TIMEOUT)
            async with get_client_session(timeout) as session:
                last_err: str | None = None
                for url in self._urls_for_peer(node, "/restart_stuck_jobs"):
                    try:
                        async with session.post(url, json={}, headers=self._auth_headers()) as resp:
                            if resp.status != 200:
                                last_err = f"http_{resp.status}"
                                continue
                            data = await resp.json()
                            if data.get("success"):
                                logger.info(f"Job restart requested on {node.node_id}")
                                return
                            last_err = str(data.get("error") or "restart_failed")
                    except Exception as e:  # noqa: BLE001
                        last_err = str(e)
                        continue
                if last_err:
                    logger.info(f"Job restart request failed on {node.node_id}: {last_err}")
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to request job restart from {node.node_id}: {e}")

    async def _start_local_job(
        self,
        job_type: JobType,
        board_type: str = "square8",
        num_players: int = 2,
        engine_mode: str = "gumbel-mcts",  # GPU-accelerated Gumbel MCTS
        job_id: str | None = None,
        cuda_visible_devices: str | None = None,
        export_params: dict[str, Any] | None = None,
        simulation_budget: int | None = None,  # Gumbel MCTS budget (None = use tier default)
    ) -> ClusterJob | None:
        """Start a job on the local node.

        SAFEGUARD: Checks coordination safeguards before spawning.
        """
        try:
            # SAFEGUARD: Check safeguards before spawning
            if HAS_SAFEGUARDS and _safeguards:
                task_type_str = job_type.value if hasattr(job_type, 'value') else str(job_type)
                allowed, reason = check_before_spawn(task_type_str, self.node_id)
                if not allowed:
                    logger.info(f"SAFEGUARD blocked {task_type_str} on {self.node_id}: {reason}")
                    return None

                # Apply backpressure delay
                delay = _safeguards.get_delay()
                if delay > 0:
                    logger.info(f"SAFEGUARD applying {delay:.1f}s backpressure delay")
                    await asyncio.sleep(delay)

            if job_id:
                job_id = str(job_id)
                with self.jobs_lock:
                    existing = self.local_jobs.get(job_id)
                if existing and existing.status == "running":
                    return existing
            else:
                job_id = str(uuid.uuid4())[:8]

            if job_type == JobType.SELFPLAY:
                # Normalize engine_mode to what run_self_play_soak.py supports.
                # LEARNED LESSONS - Variety of AI methods for better training:
                # - nn-only: Uses NNUE/neural network evaluation
                # - best-vs-pool: Tournament-style with varied opponents
                # - mcts-only/descent-only/minimax-only: Single AI method
                supported_engine_modes = {
                    "descent-only",
                    "mixed",
                    "random-only",
                    "heuristic-only",
                    "minimax-only",
                    "mcts-only",
                    "nn-only",
                    "best-vs-pool",
                    # GPU-accelerated Gumbel MCTS modes
                    "gumbel",
                    "gumbel-mcts",
                    "gumbel-mcts-only",
                    # Cross-AI asymmetric matches for variety
                    "nn-vs-mcts",
                    "nn-vs-minimax",
                    "nn-vs-descent",
                    "tournament-varied",
                    "heuristic-vs-nn",
                    "heuristic-vs-mcts",
                    "random-vs-mcts",
                }
                # Normalize engine mode - map aliases to what run_self_play_soak.py expects
                gumbel_aliases = {"gumbel", "gumbel-mcts"}
                if engine_mode in gumbel_aliases:
                    engine_mode_norm = "gumbel-mcts-only"  # Actual mode name in run_self_play_soak.py
                elif engine_mode in supported_engine_modes:
                    engine_mode_norm = engine_mode
                else:
                    engine_mode_norm = "nn-only"

                # Memory-safety defaults for large boards.
                num_games = 1000
                extra_args: list[str] = []
                if board_type in ("square19", "hexagonal"):
                    num_games = 200 if board_type == "square19" else 100
                    extra_args.extend(["--memory-constrained"])

                output_dir = Path(
                    self.ringrift_path,
                    "ai-service",
                    "data",
                    "selfplay",
                    "p2p",
                    f"{board_type}_{num_players}p",
                    job_id,
                )
                output_dir.mkdir(parents=True, exist_ok=True)

                # Use venv python if available, otherwise fall back to system python3
                venv_python = Path(self.ringrift_path, "ai-service", "venv", "bin", "python")
                python_exec = str(venv_python) if venv_python.exists() else "python3"

                cmd = [
                    python_exec,
                    f"{self.ringrift_path}/ai-service/scripts/run_self_play_soak.py",
                    "--num-games", str(num_games),
                    "--board-type", board_type,
                    "--num-players", str(num_players),
                    "--engine-mode", engine_mode_norm,
                    "--max-moves", "10000",  # LEARNED LESSONS - Avoid draws due to move limit
                    "--log-jsonl", str(output_dir / "games.jsonl"),
                    "--summary-json", str(output_dir / "summary.json"),
                    "--record-db", str(output_dir / "games.db"),
                    "--lean-db",
                    "--verbose", "0",
                    *extra_args,
                ]

                # Start process
                env = os.environ.copy()
                env["PYTHONPATH"] = f"{self.ringrift_path}/ai-service"
                env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"
                env["RINGRIFT_JOB_ORIGIN"] = "p2p_orchestrator"

                # SAFEGUARD: Final check before spawning (load + rate limit)
                can_spawn, spawn_reason = self._can_spawn_process(f"selfplay-{board_type}-{num_players}p")
                if not can_spawn:
                    logger.info(f"BLOCKED selfplay spawn: {spawn_reason}")
                    return None

                log_handle = open(output_dir / "run.log", "a")  # noqa: SIM115
                try:
                    proc = subprocess.Popen(
                        cmd,
                        stdout=log_handle,
                        stderr=subprocess.STDOUT,
                        env=env,
                        cwd=self.ringrift_path,
                    )
                    self._record_spawn()  # Track spawn for rate limiting
                finally:
                    log_handle.close()

                job = ClusterJob(
                    job_id=job_id,
                    job_type=job_type,
                    node_id=self.node_id,
                    board_type=board_type,
                    num_players=num_players,
                    engine_mode=engine_mode_norm,
                    pid=proc.pid,
                    started_at=time.time(),
                    status="running",
                )

                with self.jobs_lock:
                    self.local_jobs[job_id] = job

                logger.info(f"Started {job_type.value} job {job_id} (PID {proc.pid})")
                self._save_state()
                return job

            elif job_type == JobType.CPU_SELFPLAY:
                # Pure CPU selfplay for high-CPU nodes with limited GPU VRAM
                # Uses CPU-efficient engine modes
                # This enables utilizing excess CPU capacity on Vast.ai hosts etc.

                # CPU-friendly engine modes - include descent, mcts, and nn-only (work on CPU)
                # run_self_play_soak.py supports all these modes
                cpu_engine_modes = {
                    "descent-only", "minimax-only", "mcts-only", "heuristic-only",
                    "random-only", "mixed", "nn-only", "best-vs-pool",
                    # Cross-AI asymmetric matches
                    "nn-vs-mcts", "nn-vs-minimax", "nn-vs-descent", "tournament-varied",
                    "heuristic-vs-nn", "heuristic-vs-mcts", "random-vs-mcts",
                }
                engine_mode_norm = engine_mode if engine_mode in cpu_engine_modes else "nn-only"

                # CPU-only jobs can handle more games per batch
                num_games = 2000
                extra_args: list[str] = []
                if board_type in ("square19", "hexagonal"):
                    num_games = 400 if board_type == "square19" else 200
                    extra_args.extend(["--memory-constrained"])

                output_dir = Path(
                    self.ringrift_path,
                    "ai-service",
                    "data",
                    "selfplay",
                    "p2p",
                    f"{board_type}_{num_players}p_cpu",  # Separate subdir for CPU-only
                    job_id,
                )
                output_dir.mkdir(parents=True, exist_ok=True)

                venv_python = Path(self.ringrift_path, "ai-service", "venv", "bin", "python")
                python_exec = str(venv_python) if venv_python.exists() else "python3"

                cmd = [
                    python_exec,
                    f"{self.ringrift_path}/ai-service/scripts/run_self_play_soak.py",
                    "--num-games", str(num_games),
                    "--board-type", board_type,
                    "--num-players", str(num_players),
                    "--engine-mode", engine_mode_norm,
                    "--max-moves", "10000",
                    "--log-jsonl", str(output_dir / "games.jsonl"),
                    "--summary-json", str(output_dir / "summary.json"),
                    "--record-db", str(output_dir / "games.db"),
                    "--lean-db",
                    "--verbose", "0",
                    *extra_args,
                ]

                env = os.environ.copy()
                env["PYTHONPATH"] = f"{self.ringrift_path}/ai-service"
                env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"
                env["RINGRIFT_JOB_ORIGIN"] = "p2p_orchestrator"
                env["CUDA_VISIBLE_DEVICES"] = ""  # Disable GPU for CPU-only jobs

                can_spawn, spawn_reason = self._can_spawn_process(f"cpu-selfplay-{board_type}-{num_players}p")
                if not can_spawn:
                    logger.info(f"BLOCKED CPU selfplay spawn: {spawn_reason}")
                    return None

                log_handle = open(output_dir / "run.log", "a")  # noqa: SIM115
                try:
                    proc = subprocess.Popen(
                        cmd,
                        stdout=log_handle,
                        stderr=subprocess.STDOUT,
                        env=env,
                        cwd=self.ringrift_path,
                    )
                    self._record_spawn()
                finally:
                    log_handle.close()

                job = ClusterJob(
                    job_id=job_id,
                    job_type=job_type,
                    node_id=self.node_id,
                    board_type=board_type,
                    num_players=num_players,
                    engine_mode=engine_mode_norm,
                    pid=proc.pid,
                    started_at=time.time(),
                    status="running",
                )

                with self.jobs_lock:
                    self.local_jobs[job_id] = job

                logger.info(f"Started {job_type.value} job {job_id} (PID {proc.pid}) [CPU-only hybrid mode]")
                self._save_state()
                return job

            elif job_type == JobType.GPU_SELFPLAY:
                # DIVERSE selfplay using run_diverse_selfplay.py for high-quality training data
                # Uses varied AI matchups (NNUE, NN-MCTS, NN-Minimax, heuristic)
                # NOTE: Renamed from GPU_SELFPLAY but job type kept for backwards compatibility

                # run_diverse_selfplay expects --board (square8/square19/hexagonal).
                board_arg = {
                    "square8": "square8",
                    "square19": "square19",
                    "hexagonal": "hexagonal",
                    "hex": "hexagonal",
                }.get(board_type, "square8")

                # Games per matchup - diverse selfplay generates multiple matchup types
                games_per_matchup = 100
                if board_arg == "square19":
                    games_per_matchup = 50  # Square19 games are longer
                elif board_arg == "hexagonal":
                    games_per_matchup = 100

                output_dir = Path(
                    self.ringrift_path,
                    "ai-service",
                    "data",
                    "games",
                )
                output_dir.mkdir(parents=True, exist_ok=True)

                # Use venv python if available, otherwise fall back to system python3
                venv_python = Path(self.ringrift_path, "ai-service", "venv", "bin", "python")
                python_exec = str(venv_python) if venv_python.exists() else "python3"

                cmd = [
                    python_exec,
                    f"{self.ringrift_path}/ai-service/scripts/run_diverse_selfplay.py",
                    "--board", board_arg,
                    "--players", str(num_players),
                    "--games-per-matchup", str(games_per_matchup),
                    "--output-dir", str(output_dir),
                ]

                # Start process with GPU environment
                env = os.environ.copy()
                env["PYTHONPATH"] = f"{self.ringrift_path}/ai-service"
                env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"
                env["RINGRIFT_JOB_ORIGIN"] = "p2p_orchestrator"

                if cuda_visible_devices is not None and str(cuda_visible_devices).strip():
                    env["CUDA_VISIBLE_DEVICES"] = str(cuda_visible_devices).strip()
                # Choose a GPU automatically if not explicitly pinned.
                elif "CUDA_VISIBLE_DEVICES" not in env:
                    gpu_count = 0
                    try:
                        out = subprocess.run(
                            ["nvidia-smi", "--query-gpu=name", "--format=csv,noheader"],
                            capture_output=True, text=True, timeout=5
                        )
                        if out.returncode == 0 and out.stdout.strip():
                            gpu_count = len([line for line in out.stdout.splitlines() if line.strip()])
                    except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError, KeyError, IndexError, AttributeError):
                        gpu_count = 0

                    if gpu_count > 0:
                        with self.jobs_lock:
                            running_gpu_jobs = sum(
                                1 for j in self.local_jobs.values()
                                if j.job_type == JobType.GPU_SELFPLAY and j.status == "running"
                            )
                        env["CUDA_VISIBLE_DEVICES"] = str(running_gpu_jobs % gpu_count)
                    else:
                        env["CUDA_VISIBLE_DEVICES"] = "0"

                # SAFEGUARD: Final check before spawning (load + rate limit)
                can_spawn, spawn_reason = self._can_spawn_process(f"gpu-selfplay-{board_type}-{num_players}p")
                if not can_spawn:
                    logger.info(f"BLOCKED GPU selfplay spawn: {spawn_reason}")
                    return None

                log_handle = open(output_dir / "gpu_run.log", "a")  # noqa: SIM115
                try:
                    proc = subprocess.Popen(
                        cmd,
                        stdout=log_handle,
                        stderr=subprocess.STDOUT,
                        env=env,
                        cwd=self.ringrift_path,
                    )
                    self._record_spawn()  # Track spawn for rate limiting
                finally:
                    log_handle.close()

                # Use gumbel-mcts for GPU selfplay (177x speedup with GPU tree)
                gpu_engine_mode = "gumbel-mcts"
                batch_size = games_per_matchup

                job = ClusterJob(
                    job_id=job_id,
                    job_type=job_type,
                    node_id=self.node_id,
                    board_type=board_type,
                    num_players=num_players,
                    engine_mode=gpu_engine_mode,
                    pid=proc.pid,
                    started_at=time.time(),
                    status="running",
                )

                with self.jobs_lock:
                    self.local_jobs[job_id] = job

                logger.info(f"Started GPU selfplay job {job_id} (PID {proc.pid}, batch={batch_size})")

                # Track diversity metrics for monitoring (Phase 2B, Dec 2025)
                self.selfplay_scheduler.track_diversity({
                    "board_type": board_type,
                    "num_players": num_players,
                    "engine_mode": gpu_engine_mode,
                })

                self._save_state()

                # Monitor GPU selfplay and trigger CPU validation when complete
                asyncio.create_task(self._monitor_gpu_selfplay_and_validate(
                    job_id, proc, output_dir, board_type, num_players
                ))

                return job

            elif job_type == JobType.HYBRID_SELFPLAY:
                # Hybrid CPU/GPU selfplay using run_self_play_soak.py
                # Uses CPU for game rules (100% canonical) but GPU for heuristic evaluation
                # This is the recommended default for GPU nodes
                # NOTE: run_hybrid_selfplay.py doesn't exist, use run_self_play_soak.py instead

                # Normalize engine_mode
                # run_self_play_soak.py supports: random-only, heuristic-only, mixed, nnue-guided, mcts, gumbel-mcts-only
                # Map NN-based modes to nnue-guided for neural network evaluation
                hybrid_engine_modes = {"random-only", "heuristic-only", "mixed", "nnue-guided", "mcts"}
                nn_modes = {"nn-only", "best-vs-pool", "nn-vs-mcts", "nn-vs-minimax", "nn-vs-descent", "tournament-varied"}
                if engine_mode in hybrid_engine_modes:
                    engine_mode_norm = engine_mode
                elif engine_mode in nn_modes:
                    engine_mode_norm = "nnue-guided"  # Use neural network
                elif engine_mode in ("mcts-only", "descent-only"):
                    engine_mode_norm = "mcts"  # Use MCTS
                elif engine_mode == "minimax-only":
                    engine_mode_norm = "mixed"  # Minimax included in mixed
                else:
                    engine_mode_norm = "heuristic-only"

                # Game counts based on board type
                num_games = 1000
                if board_type == "square19":
                    num_games = 500
                elif board_type in ("hex", "hexagonal"):
                    num_games = 300

                output_dir = Path(
                    self.ringrift_path,
                    "ai-service",
                    "data",
                    "selfplay",
                    "p2p_hybrid",
                    f"{board_type}_{num_players}p",
                    job_id,
                )
                output_dir.mkdir(parents=True, exist_ok=True)

                # Normalize board type for hybrid script (uses 'hex' not 'hexagonal')
                board_arg = "hex" if board_type == "hexagonal" else board_type

                # Use venv python if available, otherwise fall back to system python3
                venv_python = Path(self.ringrift_path, "ai-service", "venv", "bin", "python")
                python_exec = str(venv_python) if venv_python.exists() else "python3"

                cmd = [
                    python_exec,
                    f"{self.ringrift_path}/ai-service/scripts/run_self_play_soak.py",
                    "--board-type", board_arg,
                    "--num-players", str(num_players),
                    "--num-games", str(num_games),
                    "--log-jsonl", str(output_dir / "games.jsonl"),
                    "--summary-json", str(output_dir / "summary.json"),
                    "--record-db", str(output_dir / "games.db"),
                    "--lean-db",
                    "--engine-mode", engine_mode_norm,
                    "--max-moves", "10000",
                    "--verbose", "0",
                ]

                # Start process with GPU environment
                env = os.environ.copy()
                env["PYTHONPATH"] = f"{self.ringrift_path}/ai-service"
                env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"
                env["RINGRIFT_JOB_ORIGIN"] = "p2p_orchestrator"

                if cuda_visible_devices is not None and str(cuda_visible_devices).strip():
                    env["CUDA_VISIBLE_DEVICES"] = str(cuda_visible_devices).strip()
                elif "CUDA_VISIBLE_DEVICES" not in env:
                    gpu_count = 0
                    try:
                        out = subprocess.run(
                            ["nvidia-smi", "--query-gpu=name", "--format=csv,noheader"],
                            capture_output=True,
                            text=True,
                            timeout=5,
                        )
                        if out.returncode == 0 and out.stdout.strip():
                            gpu_count = len([line for line in out.stdout.splitlines() if line.strip()])
                    except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError, KeyError, IndexError, AttributeError):
                        gpu_count = 0

                    if gpu_count > 0:
                        with self.jobs_lock:
                            running_hybrid_jobs = sum(
                                1
                                for j in self.local_jobs.values()
                                if j.job_type == JobType.HYBRID_SELFPLAY and j.status == "running"
                            )
                        env["CUDA_VISIBLE_DEVICES"] = str(running_hybrid_jobs % gpu_count)
                    else:
                        env["CUDA_VISIBLE_DEVICES"] = "0"

                # SAFEGUARD: Final check before spawning (load + rate limit)
                can_spawn, spawn_reason = self._can_spawn_process(f"hybrid-selfplay-{board_type}-{num_players}p")
                if not can_spawn:
                    logger.info(f"BLOCKED hybrid selfplay spawn: {spawn_reason}")
                    return None

                log_handle = open(output_dir / "hybrid_run.log", "a")  # noqa: SIM115
                try:
                    proc = subprocess.Popen(
                        cmd,
                        stdout=log_handle,
                        stderr=subprocess.STDOUT,
                        env=env,
                        cwd=self.ringrift_path,
                    )
                    self._record_spawn()  # Track spawn for rate limiting
                finally:
                    log_handle.close()

                job = ClusterJob(
                    job_id=job_id,
                    job_type=job_type,
                    node_id=self.node_id,
                    board_type=board_type,
                    num_players=num_players,
                    engine_mode=engine_mode_norm,
                    pid=proc.pid,
                    started_at=time.time(),
                    status="running",
                )

                with self.jobs_lock:
                    self.local_jobs[job_id] = job

                logger.info(f"Started HYBRID selfplay job {job_id} (PID {proc.pid})")

                # Track diversity metrics for monitoring (Phase 2B, Dec 2025)
                self.selfplay_scheduler.track_diversity({
                    "board_type": board_type,
                    "num_players": num_players,
                    "engine_mode": engine_mode_norm,
                })

                self._save_state()
                return job

            elif job_type == JobType.GUMBEL_SELFPLAY:
                # High-quality Gumbel MCTS selfplay with NN policy for self-improvement training
                # Uses generate_gumbel_selfplay.py with proper MCTS simulation budget
                # Budget tiers: THROUGHPUT(64), STANDARD(800), QUALITY(800), ULTIMATE(1600), MASTER(3200)

                # Use passed budget if specified, otherwise default to STANDARD (800)
                # This allows callers to request faster THROUGHPUT (64) for bootstrap phases
                effective_budget = simulation_budget if simulation_budget is not None else 800

                # Games based on board type and budget
                # Lower budget = can run more games in same time
                num_games = 20 if effective_budget >= 800 else 100  # More games for lower budget
                if board_type == "square19":
                    num_games = 10 if effective_budget >= 800 else 50  # Large board
                elif board_type in ("hex", "hexagonal", "hex8"):
                    num_games = 20 if effective_budget >= 800 else 100

                output_dir = Path(
                    self.ringrift_path,
                    "ai-service",
                    "data",
                    "selfplay",
                    "gumbel",
                    f"{board_type}_{num_players}p",
                    job_id,
                )
                output_dir.mkdir(parents=True, exist_ok=True)

                # Normalize board type for gumbel script
                board_arg = {
                    "hex": "hexagonal",
                    "hex8": "hex8",
                }.get(board_type, board_type)

                # Use venv python if available
                venv_python = Path(self.ringrift_path, "ai-service", "venv", "bin", "python")
                python_exec = str(venv_python) if venv_python.exists() else "python3"

                cmd = [
                    python_exec,
                    f"{self.ringrift_path}/ai-service/scripts/generate_gumbel_selfplay.py",
                    "--board", board_arg,
                    "--num-players", str(num_players),
                    "--num-games", str(num_games),
                    "--simulation-budget", str(effective_budget),
                    "--output-dir", str(output_dir),
                    "--db", str(output_dir / "games.db"),
                    "--seed", str(int(time.time() * 1000) % 2**31),
                    "--allow-fresh-weights",  # Allow running even without trained model
                    "--use-gpu-tree",  # 170x speedup with GPU tensor tree MCTS (RR-GPU-TREE-001 fixed)
                ]

                # Start process with GPU environment
                env = os.environ.copy()
                env["PYTHONPATH"] = f"{self.ringrift_path}/ai-service"
                env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"
                env["RINGRIFT_JOB_ORIGIN"] = "p2p_orchestrator"

                if cuda_visible_devices is not None and str(cuda_visible_devices).strip():
                    env["CUDA_VISIBLE_DEVICES"] = str(cuda_visible_devices).strip()
                elif "CUDA_VISIBLE_DEVICES" not in env:
                    gpu_count = 0
                    try:
                        out = subprocess.run(
                            ["nvidia-smi", "--query-gpu=name", "--format=csv,noheader"],
                            capture_output=True,
                            text=True,
                            timeout=5,
                        )
                        if out.returncode == 0 and out.stdout.strip():
                            gpu_count = len([line for line in out.stdout.splitlines() if line.strip()])
                    except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError, KeyError, IndexError, AttributeError):
                        gpu_count = 0

                    if gpu_count > 0:
                        with self.jobs_lock:
                            running_gumbel_jobs = sum(
                                1
                                for j in self.local_jobs.values()
                                if j.job_type == JobType.GUMBEL_SELFPLAY and j.status == "running"
                            )
                        env["CUDA_VISIBLE_DEVICES"] = str(running_gumbel_jobs % gpu_count)
                    else:
                        env["CUDA_VISIBLE_DEVICES"] = "0"

                # SAFEGUARD: Final check before spawning
                can_spawn, spawn_reason = self._can_spawn_process(f"gumbel-selfplay-{board_type}-{num_players}p")
                if not can_spawn:
                    logger.info(f"BLOCKED gumbel selfplay spawn: {spawn_reason}")
                    return None

                log_handle = open(output_dir / "gumbel_run.log", "a")  # noqa: SIM115
                try:
                    proc = subprocess.Popen(
                        cmd,
                        stdout=log_handle,
                        stderr=subprocess.STDOUT,
                        env=env,
                        cwd=self.ringrift_path,
                    )
                    self._record_spawn()
                finally:
                    log_handle.close()

                job = ClusterJob(
                    job_id=job_id,
                    job_type=job_type,
                    node_id=self.node_id,
                    board_type=board_type,
                    num_players=num_players,
                    engine_mode="gumbel-mcts",
                    pid=proc.pid,
                    started_at=time.time(),
                    status="running",
                )

                with self.jobs_lock:
                    self.local_jobs[job_id] = job

                logger.info(f"Started GUMBEL selfplay job {job_id} (PID {proc.pid}, sims={effective_budget})")

                # Track diversity metrics for monitoring (Phase 2B, Dec 2025)
                self.selfplay_scheduler.track_diversity({
                    "board_type": board_type,
                    "num_players": num_players,
                    "engine_mode": "gumbel-mcts",
                })

                self._save_state()
                return job

            elif job_type == JobType.DATA_EXPORT:
                # CPU-intensive data export job (NPZ creation)
                # These jobs should be routed to high-CPU nodes (vast nodes preferred)
                if not export_params:
                    logger.info("DATA_EXPORT job requires export_params")
                    return None

                input_path = export_params.get("input_path")
                output_path = export_params.get("output_path")
                encoder_version = export_params.get("encoder_version", "v3")
                max_games = export_params.get("max_games", 5000)
                is_jsonl = export_params.get("is_jsonl", False)

                if not input_path or not output_path:
                    logger.info("DATA_EXPORT requires input_path and output_path")
                    return None

                # Ensure output directory exists
                output_dir = Path(output_path).parent
                output_dir.mkdir(parents=True, exist_ok=True)

                # Use venv python if available
                venv_python = Path(self.ringrift_path, "ai-service", "venv", "bin", "python")
                python_exec = str(venv_python) if venv_python.exists() else "python3"

                if is_jsonl:
                    # Use jsonl_to_npz.py for JSONL input (GPU selfplay data)
                    export_script = f"{self.ringrift_path}/ai-service/scripts/jsonl_to_npz.py"
                    cmd = [
                        python_exec,
                        export_script,
                        "--input", str(input_path),
                        "--output", str(output_path),
                        "--board-type", board_type,
                        "--num-players", str(num_players),
                        "--gpu-selfplay",
                        "--max-games", str(max_games),
                    ]
                    if encoder_version and encoder_version != "default":
                        cmd.extend(["--encoder-version", encoder_version])
                else:
                    # Use export_replay_dataset.py for DB input
                    export_script = f"{self.ringrift_path}/ai-service/scripts/export_replay_dataset.py"
                    cmd = [
                        python_exec,
                        export_script,
                        "--db", str(input_path),
                        "--output", str(output_path),
                        "--board-type", board_type,
                        "--num-players", str(num_players),
                        "--max-games", str(max_games),
                        "--require-completed",
                        "--min-moves", "10",
                    ]
                    if encoder_version and encoder_version != "default":
                        cmd.extend(["--encoder-version", encoder_version])

                # Start export process
                env = os.environ.copy()
                env["PYTHONPATH"] = f"{self.ringrift_path}/ai-service"
                env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

                log_path = output_dir / f"export_{job_id}.log"
                log_handle = open(log_path, "w")  # noqa: SIM115
                try:
                    proc = subprocess.Popen(
                        cmd,
                        stdout=log_handle,
                        stderr=subprocess.STDOUT,
                        env=env,
                        cwd=f"{self.ringrift_path}/ai-service",
                    )
                finally:
                    log_handle.close()

                job = ClusterJob(
                    job_id=job_id,
                    job_type=job_type,
                    node_id=self.node_id,
                    board_type=board_type,
                    num_players=num_players,
                    engine_mode="export",
                    pid=proc.pid,
                    started_at=time.time(),
                    status="running",
                )

                with self.jobs_lock:
                    self.local_jobs[job_id] = job

                logger.info(f"Started DATA_EXPORT job {job_id} (PID {proc.pid}): {input_path} -> {output_path}")
                self._save_state()
                return job

        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to start job: {e}")
        return None

    async def _dispatch_export_job(
        self,
        node: NodeInfo,
        input_path: str,
        output_path: str,
        board_type: str,
        num_players: int,
        encoder_version: str = "v3",
        max_games: int = 5000,
        is_jsonl: bool = False,
    ):
        """Dispatch a CPU-intensive export job to a high-CPU node.

        CPU-intensive jobs like NPZ export should run on vast nodes
        (256-512 CPUs) rather than lambda nodes (64 CPUs) to free
        GPU resources for training/selfplay.
        """
        try:
            job_id = f"export_{board_type}_{num_players}p_{int(time.time())}_{uuid.uuid4().hex[:6]}"

            payload = {
                "job_id": job_id,
                "job_type": JobType.DATA_EXPORT.value,
                "board_type": board_type,
                "num_players": num_players,
                "input_path": input_path,
                "output_path": output_path,
                "encoder_version": encoder_version,
                "max_games": max_games,
                "is_jsonl": is_jsonl,
            }

            # NAT-blocked nodes need relay command
            if getattr(node, "nat_blocked", False):
                cmd_id = await self._enqueue_relay_command_for_peer(node, "start_job", payload)
                if cmd_id:
                    logger.info(f"Enqueued relay export job for {node.node_id}: {job_id}")
                else:
                    logger.info(f"Relay queue full for {node.node_id}; export not dispatched")
                return

            timeout = ClientTimeout(total=30)
            async with get_client_session(timeout) as session:
                last_err: str | None = None
                for url in self._urls_for_peer(node, "/start_job"):
                    try:
                        async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                            if resp.status == 200:
                                result = await resp.json()
                                if result.get("success"):
                                    logger.info(f"Export job dispatched to {node.node_id}: {job_id}")
                                    return
                                last_err = result.get("error", "unknown")
                            else:
                                last_err = f"http_{resp.status}"
                    except Exception as e:  # noqa: BLE001
                        last_err = str(e)

                if last_err:
                    logger.info(f"Export job dispatch failed to {node.node_id}: {last_err}")

        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to dispatch export job to {node.node_id}: {e}")

    async def _request_remote_job(
        self,
        node: NodeInfo,
        job_type: JobType,
        board_type: str = "square8",
        num_players: int = 2,
        engine_mode: str = "hybrid",
    ):
        """Request a remote node to start a job with specific configuration.

        SAFEGUARD: Checks coordination safeguards before requesting remote spawn.
        """
        try:
            # SAFEGUARD: Check safeguards before requesting remote spawn
            if HAS_SAFEGUARDS and _safeguards:
                task_type_str = job_type.value if hasattr(job_type, 'value') else str(job_type)
                allowed, reason = check_before_spawn(task_type_str, node.node_id)
                if not allowed:
                    logger.info(f"SAFEGUARD blocked remote {task_type_str} on {node.node_id}: {reason}")
                    return

            job_id = f"{job_type.value}_{board_type}_{num_players}p_{int(time.time())}_{uuid.uuid4().hex[:6]}"

            # NAT-blocked nodes can't accept inbound /start_job; enqueue a relay command instead.
            if getattr(node, "nat_blocked", False):
                payload = {
                    "job_id": job_id,
                    "job_type": job_type.value,
                    "board_type": board_type,
                    "num_players": num_players,
                    "engine_mode": engine_mode,
                }
                cmd_id = await self._enqueue_relay_command_for_peer(node, "start_job", payload)
                if cmd_id:
                    print(
                        f"[P2P] Enqueued relay job for {node.node_id}: "
                        f"{job_type.value} {board_type} {num_players}p ({job_id})"
                    )
                else:
                    logger.info(f"Relay queue full for {node.node_id}; skipping enqueue")
                return

            timeout = ClientTimeout(total=10)
            async with get_client_session(timeout) as session:
                payload = {
                    "job_id": job_id,
                    "job_type": job_type.value,
                    "board_type": board_type,
                    "num_players": num_players,
                    "engine_mode": engine_mode,
                }
                last_err: str | None = None
                for url in self._urls_for_peer(node, "/start_job"):
                    try:
                        async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                            if resp.status != 200:
                                last_err = f"http_{resp.status}"
                                continue
                            data = await resp.json()
                            if data.get("success"):
                                logger.info(f"Started remote {board_type} {num_players}p job on {node.node_id}")
                                return
                            last_err = str(data.get("error") or "start_failed")
                    except Exception as e:  # noqa: BLE001
                        last_err = str(e)
                        continue
                if last_err:
                    logger.error(f"Failed to start remote job on {node.node_id}: {last_err}")
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to request remote job from {node.node_id}: {e}")

    def _enqueue_relay_command(self, node_id: str, cmd_type: str, payload: dict[str, Any]) -> str | None:
        """Leader-side: enqueue a command for a NAT-blocked node to pull."""
        now = time.time()
        cmd_type = str(cmd_type)
        payload = dict(payload or {})

        with self.relay_lock:
            queue = list(self.relay_command_queue.get(node_id, []))
            queue = [
                cmd for cmd in queue
                if float(cmd.get("expires_at", 0.0) or 0.0) > now
            ]

            if cmd_type == "start_job":
                pending = sum(1 for c in queue if str(c.get("type") or "") == "start_job")
                if pending >= RELAY_MAX_PENDING_START_JOBS:
                    self.relay_command_queue[node_id] = queue
                    return None

                job_id = str(payload.get("job_id") or "")
                if job_id:
                    for c in queue:
                        if str(c.get("payload", {}).get("job_id") or "") == job_id:
                            self.relay_command_queue[node_id] = queue
                            return str(c.get("id") or "")

            cmd_id = uuid.uuid4().hex
            queue.append(
                {
                    "id": cmd_id,
                    "type": cmd_type,
                    "payload": payload,
                    "created_at": now,
                    "expires_at": now + RELAY_COMMAND_TTL_SECONDS,
                }
            )
            self.relay_command_queue[node_id] = queue
            return cmd_id

    async def _enqueue_relay_command_for_peer(
        self,
        peer: NodeInfo,
        cmd_type: str,
        payload: dict[str, Any],
    ) -> str | None:
        """Enqueue a relay command for `peer`, forwarding via its relay hub when needed.

        Default behavior: NAT-blocked nodes poll the leader's `/relay/heartbeat`
        endpoint and the leader stores commands in-memory.

        Some nodes (notably certain containerized GPU providers) may be unable to
        reach the leader over the mesh network (e.g. TUN-less Tailscale) and also
        cannot accept inbound connections. Those nodes will instead send relay
        heartbeats to an internet-reachable hub (e.g. `aws-staging`). When
        `peer.relay_via` points to such a hub, the leader must enqueue the relay
        command on that hub so the node can pull and execute it.
        """
        if not peer or not getattr(peer, "node_id", ""):
            return None

        peer_id = str(getattr(peer, "node_id", "") or "").strip()
        if not peer_id:
            return None

        relay_node_id = str(getattr(peer, "relay_via", "") or "").strip()
        if relay_node_id and relay_node_id != self.node_id:
            with self.peers_lock:
                relay_peer = self.peers.get(relay_node_id)
            if relay_peer:
                timeout = ClientTimeout(total=10)
                async with get_client_session(timeout) as session:
                    last_err: str | None = None
                    for url in self._urls_for_peer(relay_peer, "/relay/enqueue"):
                        try:
                            async with session.post(
                                url,
                                json={
                                    "target_node_id": peer_id,
                                    "type": cmd_type,
                                    "payload": payload or {},
                                },
                                headers=self._auth_headers(),
                            ) as resp:
                                if resp.status != 200:
                                    last_err = f"http_{resp.status}"
                                    continue
                                data = await resp.json()
                                if data.get("success"):
                                    return str(data.get("id") or "")
                                last_err = str(data.get("error") or "enqueue_failed")
                        except Exception as e:  # noqa: BLE001
                            last_err = str(e)
                            continue
                    if last_err:
                        logger.info(f"Relay enqueue via {relay_node_id} failed for {peer_id}: {last_err}")

        # Fallback: enqueue locally (works when peer polls the leader directly).
        return self._enqueue_relay_command(peer_id, cmd_type, payload)

    async def _discovery_loop(self):
        """Broadcast UDP discovery messages to find peers on local network."""
        # Phase 3.1 Dec 29, 2025: Add max iterations to prevent infinite loop
        MAX_RECEIVE_ITERATIONS = 100

        while self.running:
            try:
                # Create UDP socket
                sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
                sock.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)
                sock.settimeout(1.0)

                # Broadcast our presence
                message = json.dumps({
                    "type": "p2p_discovery",
                    "node_id": self.node_id,
                    "host": self.self_info.host,
                    "port": self.port,
                }).encode()

                with contextlib.suppress(OSError):
                    sock.sendto(message, ("<broadcast>", DISCOVERY_PORT))

                # Listen for responses
                # Phase 3.1 Dec 29, 2025: Add max iterations to prevent infinite loop
                # if recvfrom keeps returning data (e.g., broadcast storms)
                receive_count = 0
                try:
                    while receive_count < MAX_RECEIVE_ITERATIONS:
                        data, _addr = sock.recvfrom(1024)
                        receive_count += 1
                        msg = json.loads(data.decode())
                        if msg.get("type") == "p2p_discovery" and msg.get("node_id") != self.node_id:
                            # Found a peer!
                            peer_addr = f"{msg.get('host')}:{msg.get('port')}"
                            if peer_addr not in self.known_peers:
                                self.known_peers.append(peer_addr)
                                logger.info(f"Discovered peer: {msg.get('node_id')} at {peer_addr}")
                    # Warn if we hit the limit
                    if receive_count >= MAX_RECEIVE_ITERATIONS:
                        logger.warning(f"[UdpDiscovery] Hit max receive limit ({MAX_RECEIVE_ITERATIONS})")
                except TimeoutError:
                    pass

                sock.close()

            except (json.JSONDecodeError, AttributeError):
                pass

            await asyncio.sleep(DISCOVERY_INTERVAL)

    def _validate_critical_subsystems(self) -> dict:
        """Validate critical subsystems at startup.

        Returns a status dict with protocol and manager availability.
        Logs clear messages about which protocols are active.

        December 2025: Added to address silent fallback behavior
        where operators couldn't tell if SWIM/Raft was running.
        """
        from app.p2p.constants import (
            SWIM_ENABLED, RAFT_ENABLED, MEMBERSHIP_MODE, CONSENSUS_MODE
        )

        status = {
            "protocols": {
                "membership_mode": MEMBERSHIP_MODE,
                "consensus_mode": CONSENSUS_MODE,
                "swim_enabled": SWIM_ENABLED,
                "raft_enabled": RAFT_ENABLED,
            },
            "managers": {},
            "warnings": [],
            "errors": [],
        }

        # Check SWIM availability
        try:
            from app.p2p.swim_adapter import SWIM_AVAILABLE
            status["protocols"]["swim_available"] = SWIM_AVAILABLE
            if SWIM_ENABLED and not SWIM_AVAILABLE:
                msg = "SWIM_ENABLED=true but swim-p2p not installed. Install: pip install swim-p2p>=1.2.0"
                status["warnings"].append(msg)
                logger.warning(f"[Startup Validation] {msg}")
            elif SWIM_AVAILABLE:
                logger.info(f"[Startup Validation] SWIM protocol available (membership_mode={MEMBERSHIP_MODE})")
        except ImportError:
            status["protocols"]["swim_available"] = False
            if SWIM_ENABLED:
                status["warnings"].append("swim_adapter import failed")

        # Check Raft availability
        try:
            from app.p2p.raft_state import PYSYNCOBJ_AVAILABLE
            status["protocols"]["raft_available"] = PYSYNCOBJ_AVAILABLE
            if RAFT_ENABLED and not PYSYNCOBJ_AVAILABLE:
                msg = "RAFT_ENABLED=true but pysyncobj not installed. Install: pip install pysyncobj>=0.3.14"
                status["warnings"].append(msg)
                logger.warning(f"[Startup Validation] {msg}")
            elif PYSYNCOBJ_AVAILABLE:
                logger.info(f"[Startup Validation] Raft protocol available (consensus_mode={CONSENSUS_MODE})")
        except ImportError:
            status["protocols"]["raft_available"] = False
            if RAFT_ENABLED:
                status["warnings"].append("raft_state import failed")

        # Log active protocol configuration
        logger.info(
            f"[Startup Validation] Protocol config: membership={MEMBERSHIP_MODE}, consensus={CONSENSUS_MODE}"
        )

        # Check critical managers (lazy load check - don't fail, just report)
        manager_checks = [
            ("work_queue", "app.coordination.work_queue", "get_work_queue"),
            ("health_manager", "app.coordination.unified_health_manager", "get_unified_health_manager"),
            ("sync_router", "app.coordination.sync_router", "get_sync_router"),
        ]

        for name, module_path, getter_name in manager_checks:
            try:
                module = importlib.import_module(module_path)
                getter = getattr(module, getter_name, None)
                status["managers"][name] = getter is not None
                if getter:
                    logger.debug(f"[Startup Validation] Manager {name} available")
            except ImportError as e:
                status["managers"][name] = False
                status["warnings"].append(f"{name} import failed: {e}")
                logger.warning(f"[Startup Validation] Manager {name} unavailable: {e}")

        # December 2025: P2P voter connectivity validation
        # Check that at least quorum voters are reachable before starting
        status["voters"] = {
            "configured": len(self.voter_node_ids),
            "quorum": self.voter_quorum_size,
            "reachable": 0,
            "unreachable": [],
        }

        if self.voter_node_ids:
            import socket
            import contextlib

            reachable_count = 0
            for voter_id in self.voter_node_ids:
                # Try to resolve voter IP from config
                try:
                    from app.config.cluster_config import get_cluster_nodes
                    nodes = get_cluster_nodes()
                    node = nodes.get(voter_id)
                    if node:
                        voter_ip = node.best_ip
                        if voter_ip:
                            # Quick TCP connect test to P2P port
                            with contextlib.suppress(Exception):
                                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                                sock.settimeout(2.0)
                                result = sock.connect_ex((voter_ip, self.port))
                                sock.close()
                                if result == 0:
                                    reachable_count += 1
                                    continue
                    status["voters"]["unreachable"].append(voter_id)
                except (socket.error, socket.timeout, OSError, TimeoutError, ConnectionRefusedError):
                    status["voters"]["unreachable"].append(voter_id)

            status["voters"]["reachable"] = reachable_count

            # Log voter connectivity status
            if reachable_count < self.voter_quorum_size:
                msg = (
                    f"Only {reachable_count}/{len(self.voter_node_ids)} voters reachable, "
                    f"need {self.voter_quorum_size} for quorum. Unreachable: {status['voters']['unreachable']}"
                )
                status["warnings"].append(msg)
                logger.warning(f"[Startup Validation] {msg}")
            else:
                logger.info(
                    f"[Startup Validation] Voter quorum OK: "
                    f"{reachable_count}/{len(self.voter_node_ids)} voters reachable"
                )
        else:
            logger.info("[Startup Validation] No voters configured - quorum checks disabled")

        # Summary log
        available_count = sum(1 for v in status["managers"].values() if v)
        total_count = len(status["managers"])
        if status["warnings"]:
            logger.warning(
                f"[Startup Validation] Completed with {len(status['warnings'])} warnings. "
                f"Managers: {available_count}/{total_count} available"
            )
        else:
            logger.info(
                f"[Startup Validation] All checks passed. "
                f"Managers: {available_count}/{total_count} available"
            )

        return status

    async def run(self):
        """Main entry point - start the orchestrator."""
        if not HAS_AIOHTTP:
            logger.error("aiohttp is required. Install with: pip install aiohttp")
            raise RuntimeError("aiohttp is required but not available - install with: pip install aiohttp")

        # Validate critical subsystems before starting (December 2025)
        self._startup_validation = self._validate_critical_subsystems()

        # Set up HTTP server
        @web.middleware
        async def auth_middleware(request: web.Request, handler):
            if self.auth_token and request.method not in ("GET", "HEAD", "OPTIONS") and not self._is_request_authorized(request):
                return web.json_response({"error": "unauthorized"}, status=401)
            return await handler(request)

        # Increase max body size for large file uploads (100MB)
        # Fixes "Request Entity Too Large" for Elo DB and other file uploads
        app = web.Application(
            middlewares=[auth_middleware],
            client_max_size=100 * 1024 * 1024,  # 100 MB
        )

        # Register all routes from centralized route registry (December 2025)
        # Replaces 200+ individual route registrations with declarative registry
        _routes_registered = False
        try:
            from scripts.p2p.routes import register_all_routes
            route_count = register_all_routes(app, self)
            logger.info(f"Registered {route_count} HTTP routes from route registry")
            _routes_registered = True
        except ImportError as e:
            logger.warning(f"Route registry not available, using inline routes: {e}")
            _routes_registered = False

        # Register file download routes (December 2025)
        # HTTP-based file sync for nodes with unreliable SSH
        try:
            from scripts.p2p.handlers.file_download import register_file_download_routes
            file_routes = register_file_download_routes(app, self)
            logger.info(f"Registered {file_routes} file download routes for HTTP-based sync")
        except ImportError as e:
            logger.debug(f"File download handler not available: {e}")

        # Register network health routes (December 30, 2025)
        # Cross-verification between P2P mesh and Tailscale connectivity
        try:
            setup_network_health_routes(app, self)
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Network health routes not registered: {e}")

        # Skip legacy route registrations if registry succeeded
        # These are kept as fallback and will be removed in a future cleanup
        if not _routes_registered:
            app.router.add_post('/heartbeat', self.handle_heartbeat)
            app.router.add_get('/status', self.handle_status)
            app.router.add_get('/external_work', self.handle_external_work)
                # Work queue routes (centralized work distribution)
            app.router.add_post('/work/add', self.handle_work_add)
            app.router.add_post('/work/add_batch', self.handle_work_add_batch)
            app.router.add_get('/work/claim', self.handle_work_claim)
            app.router.add_post('/work/start', self.handle_work_start)
            app.router.add_post('/work/complete', self.handle_work_complete)
            app.router.add_post('/work/fail', self.handle_work_fail)
            app.router.add_get('/work/status', self.handle_work_status)
            app.router.add_get('/work/populator', self.handle_populator_status)
            app.router.add_get('/work/node/{node_id}', self.handle_work_for_node)
            app.router.add_post('/work/cancel', self.handle_work_cancel)
            app.router.add_get('/work/history', self.handle_work_history)

            app.router.add_post('/election', self.handle_election)
            app.router.add_post('/election/lease', self.handle_lease_request)
            app.router.add_get('/election/grant', self.handle_voter_grant_status)
            app.router.add_post('/election/reset', self.handle_election_reset)
            app.router.add_post('/election/force_leader', self.handle_election_force_leader)
            # December 29, 2025: Allow non-voters to request elections via voters
            app.router.add_post('/election/request', self.handle_election_request)

            # Serf integration routes (battle-tested SWIM gossip)
            app.router.add_post('/serf/event', self.handle_serf_event)

            # Native SWIM integration (swim-p2p library) - Phase 5 Dec 26, 2025
            app.router.add_get('/swim/status', self.handle_swim_status)
            app.router.add_get('/swim/members', self.handle_swim_members)

            # Raft consensus integration (PySyncObj) - Phase 5 Dec 26, 2025
            app.router.add_get('/raft/status', self.handle_raft_status)
            app.router.add_get('/raft/work', self.handle_raft_work_queue)
            app.router.add_get('/raft/jobs', self.handle_raft_jobs)
            app.router.add_post('/raft/lock/{name}', self.handle_raft_lock)
            app.router.add_delete('/raft/lock/{name}', self.handle_raft_unlock)

            app.router.add_post('/coordinator', self.handle_coordinator)
            app.router.add_post('/start_job', self.handle_start_job)
            app.router.add_post('/stop_job', self.handle_stop_job)
            app.router.add_post('/job/kill', self.handle_job_kill)
            app.router.add_post('/cleanup', self.handle_cleanup)
            app.router.add_post('/restart_stuck_jobs', self.handle_restart_stuck_jobs)
            app.router.add_post('/reduce_selfplay', self.handle_reduce_selfplay)
            app.router.add_post('/selfplay/start', self.handle_selfplay_start)  # GPU selfplay dispatch endpoint
            app.router.add_post('/dispatch_selfplay', self.handle_dispatch_selfplay)  # Coordinator-facing selfplay request endpoint (Dec 29, 2025)
            app.router.add_get('/health', self.handle_health)
            app.router.add_get('/cluster/health', self.handle_cluster_health)
            app.router.add_get('/git/status', self.handle_git_status)
            app.router.add_post('/git/update', self.handle_git_update)

            # Dynamic host registry routes (for IP auto-updates)
            app.router.add_post('/register', self.handle_register)
            app.router.add_get('/registry/status', self.handle_registry_status)
            app.router.add_post('/registry/update_vast', self.handle_registry_update_vast)
            app.router.add_post('/registry/update_aws', self.handle_registry_update_aws)
            app.router.add_post('/registry/update_tailscale', self.handle_registry_update_tailscale)
            app.router.add_post('/registry/save_yaml', self.handle_registry_save_yaml)

            # Connectivity diagnosis routes (SSH/HTTP fallback)
            app.router.add_get('/connectivity/diagnose/{node_id}', self.handle_connectivity_diagnose)
            app.router.add_get('/connectivity/transport_stats', self.handle_transport_stats)
            app.router.add_post('/connectivity/probe_vast', self.handle_probe_vast_nodes)

            # Gauntlet evaluation routes
            app.router.add_post('/gauntlet/execute', self.handle_gauntlet_execute)
            app.router.add_get('/gauntlet/status', self.handle_gauntlet_status)
            app.router.add_post('/gauntlet/quick-eval', self.handle_gauntlet_quick_eval)

            # Relay/Hub routes for NAT-blocked nodes
            app.router.add_post('/relay/heartbeat', self.handle_relay_heartbeat)
            app.router.add_get('/relay/peers', self.handle_relay_peers)
            app.router.add_get('/relay/status', self.handle_relay_status)
            app.router.add_post('/relay/enqueue', self.handle_relay_enqueue)

            # Gossip protocol for decentralized state sharing
            app.router.add_post('/gossip', self.handle_gossip)
            app.router.add_post('/gossip/anti-entropy', self.handle_gossip_anti_entropy)

            # Phase 2: Distributed data manifest routes
            app.router.add_get('/data_manifest', self.handle_data_manifest)
            app.router.add_get('/cluster_data_manifest', self.handle_cluster_data_manifest)
            app.router.add_post('/refresh_manifest', self.handle_refresh_manifest)
            app.router.add_get('/data-inventory', self.handle_data_inventory)  # Dec 30, 2025: Quick game counts

            # Phase 3: Delivery verification routes (Dec 27, 2025)
            app.router.add_post('/delivery/verify', self.handle_delivery_verify)
            app.router.add_get('/delivery/status/{node_id}', self.handle_delivery_status)

            # Distributed CMA-ES routes
            app.router.add_post('/cmaes/start', self.handle_cmaes_start)
            app.router.add_post('/cmaes/evaluate', self.handle_cmaes_evaluate)
            app.router.add_get('/cmaes/status', self.handle_cmaes_status)
            app.router.add_post('/cmaes/result', self.handle_cmaes_result)

            # Distributed tournament routes
            app.router.add_post('/tournament/start', self.handle_tournament_start)
            app.router.add_post('/tournament/match', self.handle_tournament_match)
            app.router.add_post('/tournament/play_elo_match', self.handle_play_elo_match)
            app.router.add_get('/tournament/status', self.handle_tournament_status)
            app.router.add_post('/tournament/result', self.handle_tournament_result)
            app.router.add_post('/tournament/ssh_start', self.handle_ssh_tournament_start)
            app.router.add_get('/tournament/ssh_status', self.handle_ssh_tournament_status)
            app.router.add_post('/tournament/ssh_cancel', self.handle_ssh_tournament_cancel)

            # Improvement loop routes
            app.router.add_post('/improvement/start', self.handle_improvement_start)
            app.router.add_get('/improvement/status', self.handle_improvement_status)
            app.router.add_post('/improvement/phase_complete', self.handle_improvement_phase_complete)

            # Phase 2: P2P data sync routes
            app.router.add_post('/sync/start', self.handle_sync_start)
            app.router.add_get('/sync/status', self.handle_sync_status)
            app.router.add_post('/sync/pull', self.handle_sync_pull)
            app.router.add_get('/sync/file', self.handle_sync_file)
            app.router.add_post('/sync/job_update', self.handle_sync_job_update)
            app.router.add_post('/sync/training', self.handle_training_sync)  # Training node priority sync
            app.router.add_post('/sync/push', self.handle_sync_push)            # Push data from GPU node (Dec 2025)
            app.router.add_post('/sync/receipt', self.handle_sync_receipt)      # Request sync receipt verification (Dec 2025)
            app.router.add_get('/sync/receipts', self.handle_sync_receipts_status)  # Get sync receipts stats (Dec 2025)
            app.router.add_get('/gpu/rankings', self.handle_gpu_rankings)      # GPU power rankings
            app.router.add_post('/cleanup/files', self.handle_cleanup_files)   # File-specific cleanup
            app.router.add_get('/admin/purge_retired', self.handle_purge_retired_peers)  # Purge retired peers (GET for auth bypass)
            app.router.add_get('/admin/purge_stale', self.handle_purge_stale_peers)      # Purge stale peers by heartbeat age
            app.router.add_post('/admin/unretire', self.handle_admin_unretire)           # Unretire specific node
            app.router.add_post('/admin/restart', self.handle_admin_restart)             # Force restart orchestrator
            app.router.add_post('/admin/reset_node_jobs', self.handle_admin_reset_node_jobs)  # Reset job counts for zombie nodes

            # Phase 5: Event subscription visibility (December 2025)
            app.router.add_get('/subscriptions', self.handle_subscriptions)              # Show event subscriptions

            # Phase 3: Training pipeline routes
            app.router.add_post('/training/start', self.handle_training_start)
            app.router.add_get('/training/status', self.handle_training_status)
            app.router.add_post('/training/update', self.handle_training_update)
            app.router.add_post('/training/nnue/start', self.handle_nnue_start)
            app.router.add_post('/training/cmaes/start', self.handle_cmaes_start_auto)

            # December 30, 2025: Training trigger RPC endpoints
            app.router.add_post('/training/trigger', self.handle_training_trigger)
            app.router.add_get('/training/trigger-decision/{config_key}', self.handle_training_trigger_decision)
            app.router.add_get('/training/trigger-configs', self.handle_training_trigger_configs)

            # Phase 5: Improvement cycle routes
            app.router.add_get('/improvement_cycles/status', self.handle_improvement_cycles_status)
            app.router.add_get('/improvement_cycles/leaderboard', self.handle_improvement_cycles_leaderboard)
            app.router.add_post('/improvement_cycles/training_complete', self.handle_improvement_training_complete)
            app.router.add_post('/improvement_cycles/evaluation_complete', self.handle_improvement_evaluation_complete)

            # Metrics observability routes
            app.router.add_get('/metrics', self.handle_metrics)
            app.router.add_get('/metrics/prometheus', self.handle_metrics_prometheus)

            # Canonical pipeline routes (for pipeline_orchestrator.py integration)
            app.router.add_post('/pipeline/start', self.handle_pipeline_start)
            app.router.add_get('/pipeline/status', self.handle_pipeline_status)
            app.router.add_post('/pipeline/selfplay_worker', self.handle_pipeline_selfplay_worker)

            # Phase 4: REST API and Dashboard routes
            app.router.add_get('/', self.handle_root)
            app.router.add_get('/api/cluster/status', self.handle_api_cluster_status)
            app.router.add_post('/api/cluster/git/update', self.handle_api_cluster_git_update)
            app.router.add_get('/api/selfplay/stats', self.handle_api_selfplay_stats)
            app.router.add_get('/api/elo/leaderboard', self.handle_api_elo_leaderboard)
            app.router.add_get('/elo/table', self.handle_elo_table)
            app.router.add_get('/elo/history', self.handle_elo_history)

            # Elo Database Sync routes (cluster-wide Elo consistency)
            app.router.add_get('/elo/sync/status', self.handle_elo_sync_status)
            app.router.add_post('/elo/sync/trigger', self.handle_elo_sync_trigger)
            app.router.add_get('/elo/sync/db', self.handle_elo_sync_download)
            app.router.add_post('/elo/sync/upload', self.handle_elo_sync_upload)
            app.router.add_get('/nodes/table', self.handle_nodes_table)
            app.router.add_get('/victory/table', self.handle_victory_table)
            app.router.add_get('/games/analytics', self.handle_games_analytics)
            app.router.add_get('/training/metrics', self.handle_training_metrics)
            app.router.add_get('/holdout/metrics', self.handle_holdout_metrics)
            app.router.add_get('/holdout/table', self.handle_holdout_table)
            app.router.add_get('/mcts/stats', self.handle_mcts_stats)
            app.router.add_get('/mcts/table', self.handle_mcts_table)
            # Feature endpoints
            app.router.add_get('/matchups/matrix', self.handle_matchup_matrix)
            app.router.add_get('/matchups/table', self.handle_matchup_table)
            app.router.add_get('/models/lineage', self.handle_model_lineage)
            app.router.add_get('/models/lineage/table', self.handle_model_lineage_table)
            app.router.add_get('/data/quality', self.handle_data_quality)
            app.router.add_get('/data/quality/table', self.handle_data_quality_table)
            app.router.add_get('/data/quality/issues', self.handle_data_quality_issues)
            app.router.add_get('/training/efficiency', self.handle_training_efficiency)
            app.router.add_get('/training/efficiency/table', self.handle_training_efficiency_table)
            app.router.add_get('/rollback/status', self.handle_rollback_status)
            app.router.add_get('/rollback/candidates', self.handle_rollback_candidates)
            app.router.add_post('/rollback/execute', self.handle_rollback_execute)
            app.router.add_post('/rollback/auto', self.handle_rollback_auto)
            app.router.add_get('/autoscale/metrics', self.handle_autoscale_metrics)
            app.router.add_get('/autoscale/recommendations', self.handle_autoscale_recommendations)
            app.router.add_get('/resource/optimizer', self.handle_resource_optimizer)
            app.router.add_get('/resource/history', self.handle_resource_utilization_history)
            app.router.add_post('/webhook/test', self.handle_webhook_test)
            app.router.add_get('/trends/summary', self.handle_trends_summary)
            app.router.add_get('/trends/history', self.handle_trends_history)
            app.router.add_get('/trends/table', self.handle_trends_table)

            # A/B Testing endpoints
            app.router.add_post('/abtest/create', self.handle_abtest_create)
            app.router.add_post('/abtest/result', self.handle_abtest_result)
            app.router.add_get('/abtest/status', self.handle_abtest_status)
            app.router.add_get('/abtest/list', self.handle_abtest_list)
            app.router.add_post('/abtest/cancel', self.handle_abtest_cancel)
            app.router.add_get('/abtest/table', self.handle_abtest_table)
            app.router.add_post('/abtest/run', self.handle_abtest_run)

            app.router.add_get('/api/training/status', self.handle_api_training_status)
            app.router.add_get('/api/canonical/health', self.handle_api_canonical_health)
            app.router.add_get('/api/canonical/jobs', self.handle_api_canonical_jobs_list)
            app.router.add_get('/api/canonical/jobs/{job_id}', self.handle_api_canonical_job_get)
            app.router.add_get('/api/canonical/jobs/{job_id}/log', self.handle_api_canonical_job_log)
            app.router.add_get('/api/canonical/logs', self.handle_api_canonical_logs_list)
            app.router.add_get('/api/canonical/logs/{log_name}/tail', self.handle_api_canonical_log_tail)
            app.router.add_post('/api/canonical/generate', self.handle_api_canonical_generate)
            app.router.add_post('/api/canonical/jobs/{job_id}/cancel', self.handle_api_canonical_job_cancel)
            app.router.add_get('/api/jobs', self.handle_api_jobs_list)
            app.router.add_post('/api/jobs/submit', self.handle_api_jobs_submit)
            app.router.add_get('/api/jobs/{job_id}', self.handle_api_job_get)
            app.router.add_post('/api/jobs/{job_id}/cancel', self.handle_api_job_cancel)
            app.router.add_get('/dashboard', self.handle_dashboard)
            app.router.add_get('/work_queue', self.handle_work_queue_dashboard)

        runner = web.AppRunner(app)
        await runner.setup()

        # Verify NFS sync before starting (prevents import errors from stale code)
        try:
            from scripts.verify_nfs_sync import verify_before_startup
            if not verify_before_startup():
                logger.warning("NFS sync verification found mismatches - check logs for details")
        except ImportError:
            logger.debug("NFS sync verification not available")
        except Exception as e:  # noqa: BLE001
            logger.warning(f"NFS sync verification failed: {e}")

        # Wire SyncRouter to event system for real-time sync triggers (December 2025)
        self._wire_sync_router_events()

        # Increase backlog to handle burst of connections from many nodes
        # Default is ~128, which can overflow when many vast nodes heartbeat simultaneously
        site = web.TCPSite(runner, self.host, self.port, reuse_address=True, backlog=1024)
        try:
            await site.start()
        except OSError as e:
            if "Address already in use" in str(e) or e.errno == 98:  # EADDRINUSE
                logger.error(f"Port {self.port} already in use. Is another P2P instance running?")
                logger.error(f"Try: lsof -i :{self.port} or pkill -f p2p_orchestrator")
                raise RuntimeError(f"Port {self.port} already bound - cannot start P2P daemon") from e
            elif "Invalid argument" in str(e):
                # macOS TCP keepalive socket option issue
                logger.warning(f"TCP socket configuration failed on {self.host}:{self.port}: {e}")
                logger.warning("This may be a macOS TCP keepalive compatibility issue")
                raise
            else:
                logger.error(f"Failed to bind to {self.host}:{self.port}: {e}")
                raise

        logger.info(f"HTTP server started on {self.host}:{self.port} (backlog=1024)")

        # Notify systemd that we're ready to serve
        systemd_notify_ready()

        # Start background tasks with exception isolation and restart support
        # CRITICAL FIX (Dec 2025): Each task is wrapped to prevent cascade failures.
        # Previously, a single exception in any task would crash all 18+ tasks.
        # Dec 2025 Update: Added factory functions for auto-restart on critical tasks.
        tasks = [
            # Critical heartbeat loop - auto-restart on failure
            self._create_safe_task(
                self._heartbeat_loop(), "heartbeat", factory=self._heartbeat_loop
            ),
            # NOTE: _manifest_collection_loop removed Dec 2025 - now runs via LoopManager (ManifestCollectionLoop)
            # See scripts/p2p/loops/manifest_collection_loop.py for implementation
            # Job management - auto-restart on failure
            self._create_safe_task(
                self._job_management_loop(), "job_management", factory=self._job_management_loop
            ),
            # Discovery - auto-restart on failure
            self._create_safe_task(
                self._discovery_loop(), "discovery", factory=self._discovery_loop
            ),
            # IMPROVED: Dedicated voter heartbeat loop for reliable leader election - auto-restart
            self._create_safe_task(
                self._voter_heartbeat_loop(), "voter_heartbeat", factory=self._voter_heartbeat_loop
            ),
            # NOTE: _nat_management_loop removed Dec 2025 - now runs via LoopManager (NATManagementLoop)
            # See scripts/p2p/loops/network_loops.py for implementation
            # SWIM gossip membership for leaderless failure detection (<5s vs 60s+)
            self._create_safe_task(
                self._swim_membership_loop(), "swim_membership", factory=self._swim_membership_loop
            ),
        ]

        # Add git update loop if enabled
        if AUTO_UPDATE_ENABLED:
            tasks.append(self._create_safe_task(
                self._git_update_loop(), "git_update", factory=self._git_update_loop
            ))

        # NOTE: _training_sync_loop removed Dec 2025 - now runs via LoopManager (TrainingSyncLoop)
        # See scripts/p2p/loops/training_sync_loop.py for implementation

        # Add cloud IP refresh loops (best-effort; no-op if not configured).
        if HAS_DYNAMIC_REGISTRY:
            tasks.append(self._create_safe_task(self._vast_ip_update_loop(), "vast_ip_update"))
            tasks.append(self._create_safe_task(self._aws_ip_update_loop(), "aws_ip_update"))
            tasks.append(self._create_safe_task(self._tailscale_ip_update_loop(), "tailscale_ip_update"))

        # NOTE: _tailscale_peer_recovery_loop removed Dec 2025 - now runs via LoopManager (TailscalePeerDiscoveryLoop)
        # See scripts/p2p/loops/network_loops.py for implementation

        # Phase 26: Continuous bootstrap loop - ensures isolated nodes can rejoin
        tasks.append(self._create_safe_task(self._continuous_bootstrap_loop(), "continuous_bootstrap"))

        # NOTE: _follower_discovery_loop removed Dec 2025 - now runs via LoopManager (FollowerDiscoveryLoop)
        # See scripts/p2p/loops/discovery_loop.py for implementation

        # NOTE: _data_management_loop removed Dec 2025 - now runs via LoopManager (DataManagementLoop)
        # See scripts/p2p/loops/data_loops.py for implementation

        # NOTE: _model_sync_loop removed Dec 2025 - now runs via LoopManager (ModelSyncLoop)
        # See scripts/p2p/loops/data_loops.py for implementation

        # NOTE: _elo_sync_loop removed Dec 2025 - now runs via LoopManager (EloSyncLoop)

        # NOTE: _worker_pull_loop removed Dec 2025 - now runs via LoopManager (WorkerPullLoop)
        # See scripts/p2p/loops/job_loops.py for implementation

        # NOTE: _work_queue_maintenance_loop removed Dec 2025 - now runs via LoopManager (WorkQueueMaintenanceLoop)
        # See scripts/p2p/loops/job_loops.py for implementation

        # NOTE: _idle_detection_loop removed Dec 2025 - now runs via LoopManager (IdleDetectionLoop)

        # === AUTOMATION LOOPS (2024-12) ===
        # These loops enable hands-free cluster operation

        # NOTE: _auto_scaling_loop removed Dec 2025 - now runs via LoopManager (AutoScalingLoop)

        # NOTE: _predictive_monitoring_loop removed Dec 2025 (~98 LOC)
        # Now runs via LoopManager as PredictiveMonitoringLoop.
        # See scripts/p2p/loops/resilience_loops.py for implementation.

        # NOTE: _self_healing_loop removed Dec 2025 (~71 LOC)
        # Now runs via LoopManager as SelfHealingLoop.
        # See scripts/p2p/loops/resilience_loops.py for implementation.

        # NOTE: _job_reaper_loop removed Dec 2025 - now runs via LoopManager (JobReaperLoop)

        # NOTE: _validation_loop removed Dec 2025 - now runs via LoopManager (ValidationLoop)
        # See scripts/p2p/loops/validation_loop.py for implementation

        # NOTE: _queue_populator_loop removed Dec 2025 - now runs via LoopManager (QueuePopulatorLoop)

        # Store tasks for shutdown handling
        self._background_tasks = tasks

        # Phase 4: Start extracted loops via LoopManager (Dec 2025)
        # These 11 loops now ONLY run via LoopManager (inline versions removed):
        # - EloSyncLoop, IdleDetectionLoop, AutoScalingLoop, JobReaperLoop, QueuePopulatorLoop
        # - WorkQueueMaintenanceLoop, NATManagementLoop, ManifestCollectionLoop, ValidationLoop
        # - DataManagementLoop, ModelSyncLoop
        job_reaper_started = False
        logger.info(f"[LoopManager] Phase 4 startup: EXTRACTED_LOOPS_ENABLED={EXTRACTED_LOOPS_ENABLED}")
        if EXTRACTED_LOOPS_ENABLED and self._register_extracted_loops():
            loop_manager = self._get_loop_manager()
            if loop_manager is not None:
                # Dec 27, 2025: start_all() now returns dict of {loop_name: started_successfully}
                # Check if job_reaper specifically started to avoid duplicate reapers
                startup_results = await loop_manager.start_all()
                job_reaper_started = startup_results.get("job_reaper", False)
                started_count = sum(1 for v in startup_results.values() if v)
                logger.info(
                    f"LoopManager: started {started_count}/{len(startup_results)} loops, "
                    f"job_reaper={'running' if job_reaper_started else 'FAILED'}"
                )

        # Phase 4.1: Inline job reaper fallback (Dec 27, 2025)
        # If JobReaperLoop specifically failed to start, run inline fallback for job cleanup
        # This ensures stuck jobs get cleaned up even if the modular loop system fails
        # Dec 27, 2025: Fixed race condition - now checks job_reaper loop status, not just
        # whether LoopManager.start_all() completed (which could mask loop startup failures)
        if JOB_REAPER_FALLBACK_ENABLED and not job_reaper_started:
            logger.info("[JobReaper] LoopManager not available, starting inline fallback")
            tasks.append(
                self._create_safe_task(
                    self._inline_job_reaper_fallback_loop(),
                    "job_reaper_fallback"
                )
            )

        # Best-effort bootstrap from seed peers before running elections. This
        # helps newly started cloud nodes quickly learn about the full cluster.
        with contextlib.suppress(Exception):
            await self._bootstrap_from_known_peers()

        # December 30, 2025: Immediate Tailscale discovery when no --peers provided
        # This fixes the bootstrap problem where nodes started without --peers
        # couldn't join the mesh because continuous_bootstrap_loop has a 30s delay.
        if not self.known_peers:
            logger.info("[Bootstrap] No --peers provided, running immediate Tailscale discovery...")
            with self.peers_lock:
                peers_before = len(self.peers)

            # Try direct Tailscale peer discovery first
            with contextlib.suppress(Exception):
                await self._discover_tailscale_peers()

            with self.peers_lock:
                peers_after = len(self.peers)

            if peers_after > peers_before:
                logger.info(f"[Bootstrap] Tailscale discovery found {peers_after - peers_before} new peer(s)")
            else:
                # Tailscale discovery didn't find peers - try config-based seeds
                logger.info("[Bootstrap] Tailscale discovery found no peers, trying config-based seeds...")
                config_seeds = self._load_bootstrap_seeds_from_config()
                if config_seeds:
                    logger.info(f"[Bootstrap] Loaded {len(config_seeds)} seed(s) from config")
                    self.known_peers = config_seeds
                    with contextlib.suppress(Exception):
                        await self._bootstrap_from_known_peers()

        # December 29, 2025: Extended startup election with retry mechanism
        # If no leader known, start election after allowing time for peer discovery.
        # Previously used 5s which was too short for cluster discovery.
        await asyncio.sleep(15)  # Increased from 5s to allow peer discovery
        if not self.leader_id and not self._maybe_adopt_leader_from_peers():
            # CRITICAL: Check quorum before starting election to prevent quorum bypass
            if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                logger.warning("Skipping startup election: no voter quorum available (will retry)")
            else:
                await self._start_election()

        # December 29, 2025: Add background task to retry election if still no leader
        # This handles cases where initial election fails or quorum wasn't available
        async def _delayed_election_retry():
            """Retry election periodically if no leader after startup."""
            retry_intervals = [30, 60, 120, 300]  # Exponential backoff: 30s, 1m, 2m, 5m
            retry_count = 0

            while self.running and retry_count < len(retry_intervals):
                wait_time = retry_intervals[retry_count]
                await asyncio.sleep(wait_time)

                if not self.running:
                    break

                if self.leader_id:
                    # Leader found, no need to retry
                    logger.info(f"Leader established ({self.leader_id}), stopping election retry task")
                    break

                # Still no leader, try to adopt from peers or start election
                if self._maybe_adopt_leader_from_peers():
                    logger.info(f"Adopted leader from peers: {self.leader_id}")
                    break

                # Check quorum and start election if possible
                if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                    retry_count += 1
                    voters_alive = self._count_alive_peers(self.voter_node_ids) if hasattr(self, '_count_alive_peers') else 0
                    logger.warning(
                        f"No voter quorum for election retry {retry_count}/{len(retry_intervals)} "
                        f"(alive={voters_alive}, need={getattr(self, 'voter_quorum_size', 3)})"
                    )
                    continue

                if not getattr(self, "election_in_progress", False):
                    logger.info(f"No leader after {wait_time}s, triggering election retry {retry_count + 1}")
                    await self._start_election()
                    retry_count += 1
                else:
                    logger.debug("Election already in progress, skipping retry")

            if not self.leader_id and self.running:
                logger.warning("Exhausted election retries, operating in leaderless mode")

        tasks.append(
            self._create_safe_task(
                _delayed_election_retry(),
                "delayed_election_retry"
            )
        )

        # Run forever
        # December 2025: Added return_exceptions=True to prevent task exceptions from crashing orchestrator
        try:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            # Log any task failures
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"Background task {i} failed: {result}")
        except asyncio.CancelledError:
            pass
        finally:
            self.running = False
            # Stop extracted loops via LoopManager (Dec 2025)
            loop_manager = self._get_loop_manager()
            if loop_manager is not None and loop_manager.is_started:
                try:
                    results = await loop_manager.stop_all(timeout=15.0)
                    stopped = sum(1 for ok in results.values() if ok)
                    logger.info(f"LoopManager: stopped {stopped}/{len(results)} loops")
                except Exception as e:  # noqa: BLE001
                    logger.warning(f"LoopManager: stop failed: {e}")
            try:
                await asyncio.wait_for(runner.cleanup(), timeout=30)
            except asyncio.TimeoutError:
                logger.warning("HTTP server cleanup timed out after 30s")


def _acquire_singleton_lock(
    kill_duplicates: bool = False,
    force_takeover: bool = False,
) -> bool:
    """Acquire singleton lock to prevent duplicate P2P orchestrator instances.

    Uses atomic file locking (fcntl) which is more reliable than PID file checks.
    Automatically handles stale locks from crashed processes.

    Args:
        kill_duplicates: If True, kill any duplicate P2P processes before acquiring
        force_takeover: If True, force-kill any lock holder (even if not P2P).
                        Use when lock is held by a recycled PID.

    Returns:
        True if lock acquired successfully
    """
    global _P2P_LOCK

    lock_dir = Path(__file__).parent.parent / "data" / "coordination"
    lock_dir.mkdir(parents=True, exist_ok=True)

    if kill_duplicates:
        # Find and kill any existing p2p_orchestrator processes
        pattern = r"p2p_orchestrator\.py"
        existing = find_processes_by_pattern(pattern, exclude_self=True)
        if existing:
            logger.info(f"[P2P] Found {len(existing)} duplicate processes, killing...")
            for proc in existing:
                logger.info(f"[P2P] Killing duplicate: PID {proc.pid}")
                if kill_process(proc.pid, wait=True, timeout=5.0):
                    logger.info(f"[P2P] Killed PID {proc.pid}")
                else:
                    logger.warning(f"[P2P] Failed to kill PID {proc.pid}")
            # Wait a moment for locks to release
            time.sleep(0.5)

    # Create lock with auto-cleanup of stale locks (from dead processes)
    _P2P_LOCK = SingletonLock(
        "p2p_orchestrator",
        lock_dir=lock_dir,
        auto_cleanup_stale=True,  # Automatically handle dead process locks
    )

    if not _P2P_LOCK.acquire():
        # Lock acquisition failed - provide detailed diagnostics
        status = _P2P_LOCK.get_lock_status()
        holder_pid = status.get("holder_pid")
        holder_alive = status.get("holder_alive", False)
        holder_command = status.get("holder_command", "")
        is_stale = status.get("is_stale", False)

        if is_stale:
            # This shouldn't happen with auto_cleanup_stale=True, but handle it
            logger.warning(
                f"[P2P] Stale lock detected (dead PID {holder_pid}). "
                f"Attempting force cleanup..."
            )
            if _P2P_LOCK.force_release():
                # Retry acquisition after cleanup
                if _P2P_LOCK.acquire():
                    logger.info(f"[P2P] Acquired lock after stale cleanup (PID {os.getpid()})")
                    return True
            logger.error("[P2P] Failed to clean up stale lock")
            return False

        if holder_pid and holder_alive:
            # Another live process is holding the lock
            is_p2p = _P2P_LOCK.is_holder_expected_process("p2p_orchestrator")
            if is_p2p:
                logger.error(
                    f"[P2P] Another P2P orchestrator is already running (PID {holder_pid}). "
                    f"Use --kill-duplicates to automatically terminate it."
                )
            else:
                # PID reuse - different process now holds the lock file
                # This happens when the old P2P crashed and the PID was reused
                if force_takeover:
                    logger.warning(
                        f"[P2P] Lock held by unexpected process (PID {holder_pid}: {holder_command[:80] if holder_command else 'unknown'}). "
                        f"Force takeover requested - killing holder."
                    )
                    if _P2P_LOCK.force_release(kill_holder=True):
                        if _P2P_LOCK.acquire():
                            logger.info(f"[P2P] Acquired lock after force takeover (PID {os.getpid()})")
                            return True
                    logger.error("[P2P] Force takeover failed")
                else:
                    logger.warning(
                        f"[P2P] Lock held by unexpected process (PID {holder_pid}: {holder_command[:80] if holder_command else 'unknown'}). "
                        f"This may indicate PID reuse after a crash. "
                        f"Use --force-takeover to automatically recover."
                    )
        else:
            logger.error(
                "[P2P] Failed to acquire lock (unknown reason). "
                f"Lock status: {status}"
            )
        return False

    logger.info(f"[P2P] Acquired singleton lock (PID {os.getpid()})")
    return True


def _release_singleton_lock() -> None:
    """Release the singleton lock on shutdown."""
    global _P2P_LOCK
    if _P2P_LOCK:
        _P2P_LOCK.release()
        logger.debug("[P2P] Released singleton lock")
        _P2P_LOCK = None


def main():
    # Parse lock-related args early (before full argparse)
    import sys
    kill_duplicates = "--kill-duplicates" in sys.argv
    force_takeover = "--force-takeover" in sys.argv

    # Acquire singleton lock (December 2025: improved atomic locking with stale cleanup)
    if not _acquire_singleton_lock(
        kill_duplicates=kill_duplicates,
        force_takeover=force_takeover,
    ):
        sys.exit(1)

    parser = argparse.ArgumentParser(description="P2P Orchestrator for RingRift cluster")
    parser.add_argument("--node-id", required=True, help="Unique identifier for this node")
    parser.add_argument("--host", default="0.0.0.0", help="Host to bind to")
    parser.add_argument("--port", type=int, default=DEFAULT_PORT, help="Port to listen on")
    parser.add_argument(
        "--advertise-host",
        default=None,
        help=f"Host to advertise to peers (or set {ADVERTISE_HOST_ENV})",
    )
    parser.add_argument(
        "--advertise-port",
        type=int,
        default=None,
        help=f"Port to advertise to peers (or set {ADVERTISE_PORT_ENV})",
    )
    parser.add_argument("--peers", help="Comma-separated list of known peers (host[:port] or http(s)://host[:port])")
    parser.add_argument("--relay-peers", help="Comma-separated list of peers to use relay heartbeats with (for NAT-blocked nodes)")
    parser.add_argument("--ringrift-path", help="Path to RingRift installation")
    parser.add_argument("--auth-token", help=f"Shared auth token (or set {AUTH_TOKEN_ENV})")
    parser.add_argument("--require-auth", action="store_true", help="Require auth token to be set")
    parser.add_argument("--storage-type", choices=["disk", "ramdrive", "auto"], default="auto",
                        help="Storage type: 'disk', 'ramdrive' (/dev/shm), or 'auto' (detect based on RAM/disk)")
    parser.add_argument("--sync-to-disk-interval", type=int, default=300,
                        help="When using ramdrive, sync to disk every N seconds (0 = no sync, default: 300)")
    parser.add_argument("--supervised", action="store_true",
                        help="Running under cluster_supervisor.py - disable self-restart logic")
    parser.add_argument("--kill-duplicates", action="store_true",
                        help="Kill any existing P2P orchestrator processes before starting")
    parser.add_argument("--force-takeover", action="store_true",
                        help="Force acquire lock even if held by another process (use when PID was recycled after crash)")

    args = parser.parse_args()

    known_peers = []
    if args.peers:
        known_peers = [p.strip() for p in args.peers.split(',')]

    relay_peers = []
    if args.relay_peers:
        relay_peers = [p.strip() for p in args.relay_peers.split(',')]

    # Wrap orchestrator creation and run in try/except to ensure crashes are logged
    orchestrator = None
    try:
        logger.info(f"Initializing P2P orchestrator: node_id={args.node_id}")
        orchestrator = P2POrchestrator(
            node_id=args.node_id,
            host=args.host,
            port=args.port,
            known_peers=known_peers,
            relay_peers=relay_peers,
            ringrift_path=args.ringrift_path,
            advertise_host=args.advertise_host,
            advertise_port=args.advertise_port,
            auth_token=args.auth_token,
            require_auth=args.require_auth,
            storage_type=args.storage_type,
            sync_to_disk_interval=args.sync_to_disk_interval,
        )
        logger.info(f"P2P orchestrator initialized successfully: {args.node_id}")

        # December 28, 2025: Validate event emitters at startup
        # This provides early warning if event system is not properly configured
        if _check_event_emitters():
            logger.info("[P2P] Event emitters available - P2P events will be published")
        else:
            logger.warning(
                "[P2P] Event emitters NOT available - P2P events will be silent. "
                "Ensure app.coordination.event_emitters is importable for full integration."
            )
    except Exception as e:  # noqa: BLE001
        logger.exception(f"Failed to initialize P2P orchestrator: {e}")
        sys.exit(1)

    # Handle shutdown gracefully - avoid race conditions with async tasks
    # December 2025: Fixed signal handler race condition that caused threading exceptions
    _shutdown_requested = False
    _start_time = time.time()

    def signal_handler(sig, frame):
        nonlocal _shutdown_requested
        import traceback

        uptime = time.time() - _start_time
        sig_name = signal.Signals(sig).name if hasattr(signal, 'Signals') else f"signal {sig}"

        if _shutdown_requested:
            # Force exit on second signal
            logger.warning(f"Forced shutdown (second {sig_name}) after {uptime:.1f}s uptime")
            os._exit(1)
        _shutdown_requested = True

        # Enhanced logging to identify what's sending signals
        logger.warning(f"=== SIGNAL RECEIVED: {sig_name} ===")
        logger.warning(f"PID: {os.getpid()}, Uptime: {uptime:.1f}s, Node: {args.node_id}")
        logger.warning(f"Stack trace at signal:\n{''.join(traceback.format_stack(frame))}")
        logger.info("Shutdown requested, stopping gracefully...")
        if orchestrator:
            orchestrator.running = False
            # Cancel all background tasks for graceful shutdown (Dec 2025)
            if hasattr(orchestrator, '_background_tasks'):
                for task in orchestrator._background_tasks:
                    if not task.done():
                        task.cancel()
            # Schedule ramdrive sync in a thread to avoid blocking signal handler
            # Don't call sys.exit() - let asyncio loop exit cleanly

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    # Run with exception logging
    try:
        logger.info(f"Starting P2P orchestrator main loop: {args.node_id}")
        asyncio.run(orchestrator.run())
    except Exception as e:  # noqa: BLE001
        logger.exception(f"P2P orchestrator crashed: {e}")
        sys.exit(1)
    finally:
        # Ensure ramdrive is synced on exit (moved from signal handler to avoid race)
        if orchestrator:
            try:
                orchestrator.stop_ramdrive_syncer(final_sync=True)
                logger.info("Ramdrive sync completed on shutdown")
            except Exception as e:  # noqa: BLE001
                logger.warning(f"Ramdrive sync on shutdown failed: {e}")
            # December 2025: Close webhook notifier to prevent memory leaks
            try:
                if hasattr(orchestrator, 'notifier') and orchestrator.notifier:
                    orchestrator.notifier.close_sync()
            except (RuntimeError, OSError, AttributeError) as e:
                # Dec 2025: Narrowed from bare Exception; best effort cleanup
                logger.debug(f"Notifier close failed (best effort): {e}")

            # December 2025: Close work queue to persist final stats
            try:
                from app.coordination.work_queue import reset_work_queue
                reset_work_queue()
            except (ImportError, RuntimeError, sqlite3.Error) as e:
                logger.debug(f"Work queue cleanup failed (best effort): {e}")

            # December 2025: Release singleton lock on shutdown
            _release_singleton_lock()


if __name__ == "__main__":
    main()
