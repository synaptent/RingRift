#!/usr/bin/env python3
"""Distributed P2P Orchestrator - Self-healing compute cluster for RingRift AI training.

This orchestrator runs on each node in the cluster and:
1. Discovers other nodes via broadcast UDP or known peer list
2. Participates in leader election for coordination tasks
3. Monitors local resources and shares status with peers
4. Auto-starts selfplay/training jobs based on cluster needs
5. Self-heals when nodes go offline or IPs change

Architecture:
- Each node runs this script as a daemon
- Nodes communicate via HTTP REST API (port 8770)
- Leader election uses Bully algorithm (highest node_id wins)
- Heartbeats every 30 seconds detect failures
- Nodes maintain local SQLite state for crash recovery

Usage:
    # On each node:
    python scripts/p2p_orchestrator.py --node-id mac-studio
    python scripts/p2p_orchestrator.py --node-id vast-5090-quad --port 8770

    # With known peers (for cloud nodes without broadcast):
    python scripts/p2p_orchestrator.py --node-id vast-3090 --peers <peer-ip>:8770,<peer-ip>:8770
"""
from __future__ import annotations

# Load .env.local BEFORE app.p2p.constants imports (for SWIM/Raft feature flags)
# This must happen before any app.* imports that read environment variables
def _load_env_local():
    """Load .env.local from script directory or ai-service root."""
    import os as _os
    from pathlib import Path as _Path
    for base in [_Path(__file__).parent.parent, _Path.cwd()]:
        env_file = base / ".env.local"
        if env_file.exists():
            try:
                with open(env_file) as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith("#") and "=" in line:
                            key, _, value = line.partition("=")
                            key = key.strip()
                            value = value.strip().strip('"').strip("'")
                            if key not in _os.environ:  # Don't override existing
                                _os.environ[key] = value
                break
            except (OSError, IOError, UnicodeDecodeError):
                pass  # Skip if .env.local can't be read

_load_env_local()

import argparse
import asyncio
import contextlib

# Python 3.10 compatibility: asyncio.timeout was added in 3.11
# Use a compatibility shim that works with Python 3.10+
try:
    from asyncio import timeout as async_timeout
except ImportError:
    # Python 3.10 fallback using wait_for
    from contextlib import asynccontextmanager

    @asynccontextmanager
    async def async_timeout(delay):
        """Compatibility shim for asyncio.timeout (Python 3.11+)."""
        task = asyncio.current_task()
        loop = asyncio.get_running_loop()

        def cancel_task():
            if task is not None:
                task.cancel()

        handle = loop.call_later(delay, cancel_task)
        try:
            yield
        except asyncio.CancelledError:
            raise asyncio.TimeoutError()
        finally:
            handle.cancel()
import gzip
import importlib
import ipaddress
import json
import os
import secrets
import shutil
import signal
import socket
import sqlite3
import subprocess
import sys

# Safe database connection context manager (December 2025)
try:
    from app.distributed.db_utils import safe_db_connection
except ImportError:
    # Fallback for when db_utils isn't available
    from contextlib import contextmanager as _cm
    @_cm
    def safe_db_connection(db_path, timeout=30):
        conn = sqlite3.connect(str(db_path), timeout=timeout)
        try:
            yield conn
            conn.commit()
        except sqlite3.Error:
            conn.rollback()
            raise
        finally:
            conn.close()
import threading
import time
import uuid
from collections.abc import Generator
from dataclasses import asdict
from datetime import datetime
from pathlib import Path
from typing import TYPE_CHECKING, Any, Optional
from urllib.parse import urlparse

if TYPE_CHECKING:
    from app.coordination.unified_queue_populator import UnifiedQueuePopulator as QueuePopulator
    from app.coordination.p2p_auto_deployer import P2PAutoDeployer
    from scripts.p2p.loops import LoopManager

# =============================================================================
# PRE-FLIGHT DEPENDENCY VALIDATION (January 2026)
# =============================================================================
# Critical dependencies that must be present before P2P startup.
# Failing fast with clear errors prevents cryptic runtime failures.

_CRITICAL_DEPENDENCIES = {
    "aiohttp": "HTTP server and client functionality",
    "psutil": "Process and system monitoring",
    "yaml": "Configuration file parsing",
}

_OPTIONAL_DEPENDENCIES = {
    "prometheus_client": "Metrics export (optional)",
    "paramiko": "SSH connections for remote operations",
}


def _validate_preflight_dependencies() -> tuple[bool, list[str]]:
    """Validate critical dependencies are available before startup.

    Returns:
        Tuple of (all_ok, list of error messages)
    """
    errors = []
    warnings = []

    # Check critical dependencies
    for module_name, purpose in _CRITICAL_DEPENDENCIES.items():
        try:
            importlib.import_module(module_name)
        except ImportError:
            errors.append(f"CRITICAL: Missing '{module_name}' - required for {purpose}")
            errors.append(f"  Fix: pip install {module_name}")

    # Check optional dependencies (warn only)
    for module_name, purpose in _OPTIONAL_DEPENDENCIES.items():
        try:
            importlib.import_module(module_name)
        except ImportError:
            warnings.append(f"Optional: Missing '{module_name}' - {purpose}")

    # Log warnings
    for warn in warnings:
        print(f"[P2P] {warn}", file=sys.stderr)

    return len(errors) == 0, errors


# =============================================================================
# Work queue for centralized work distribution (lazy import to avoid circular deps)
_work_queue = None
def get_work_queue():
    """Get the work queue singleton (lazy load)."""
    global _work_queue
    if _work_queue is None:
        try:
            from app.coordination.work_queue import get_work_queue as _get_wq
            _work_queue = _get_wq()
        except ImportError:
            _work_queue = None
    return _work_queue

# Automation managers (lazy imports to avoid circular deps)
_health_manager = None  # December 2025: Consolidated from recovery_manager
_predictive_alerts = None
# Dec 2025: Removed unused _tier_calibrator global (never used)
# Dec 28, 2025: Removed unused get_auto_scaler() - never called

def get_health_manager():
    """Get the health manager singleton (lazy load).

    December 2025: Consolidated from get_recovery_manager().
    Uses UnifiedHealthManager which combines recovery + error coordination.
    """
    global _health_manager
    if _health_manager is None:
        try:
            from app.coordination.unified_health_manager import (
                get_health_manager as _get_uhm,
            )
            _health_manager = _get_uhm()
        except ImportError:
            _health_manager = None
    return _health_manager


# Job Reaper Daemon (leader-only, kills stuck jobs and reassigns work)
_job_reaper = None
def get_job_reaper(work_queue=None, ssh_config=None):
    """Get the job reaper singleton (lazy load).

    The JobReaperDaemon enforces job timeouts by:
    1. Detecting jobs past their timeout
    2. Killing stuck processes via SSH
    3. Marking jobs as TIMEOUT
    4. Reassigning failed work to other nodes
    5. Blacklisting nodes that repeatedly fail
    """
    global _job_reaper
    if _job_reaper is None and work_queue is not None:
        try:
            from app.coordination.job_reaper import JobReaperDaemon
            _job_reaper = JobReaperDaemon(
                work_queue=work_queue,
                ssh_config=ssh_config,
            )
        except ImportError as e:
            logger.warning(f"JobReaperDaemon not available: {e}")
            _job_reaper = None
    return _job_reaper

def get_predictive_alerts():
    """Get the predictive alerts manager (lazy load)."""
    global _predictive_alerts
    if _predictive_alerts is None:
        try:
            from app.monitoring.predictive_alerts import PredictiveAlertManager
            _predictive_alerts = PredictiveAlertManager()
        except ImportError:
            _predictive_alerts = None
    return _predictive_alerts

# Dec 2025: Removed unused get_tier_calibrator() function


# SWIM membership manager for leaderless gossip-based membership
_swim_manager = None
SWIM_AVAILABLE = False


def get_swim_manager(node_id: str | None = None, bind_port: int = 7947):
    """Get the SWIM membership manager singleton (lazy load).

    SWIM (Scalable Weakly-consistent Infection-style Membership) provides:
    - O(1) message complexity per node (constant bandwidth)
    - Failure detection in <5 seconds (vs 60+ seconds with heartbeat-based)
    - No single leader required - truly distributed
    - Suspicion mechanism to reduce false positives

    Args:
        node_id: Node identifier (required for first initialization)
        bind_port: UDP port for SWIM protocol (default 7947)

    Returns:
        SwimMembershipManager instance or None if swim-p2p not installed
    """
    global _swim_manager, SWIM_AVAILABLE
    if _swim_manager is None and node_id is not None:
        try:
            from app.p2p.swim_adapter import SwimMembershipManager, SWIM_AVAILABLE as _swim_avail
            SWIM_AVAILABLE = _swim_avail
            if SWIM_AVAILABLE:
                _swim_manager = SwimMembershipManager.from_distributed_hosts(
                    node_id=node_id,
                    bind_port=bind_port,
                )
                logger.info(f"SWIM membership manager initialized for {node_id}")
            else:
                logger.warning("swim-p2p not installed - using HTTP heartbeats only")
        except ImportError as e:
            logger.warning(f"SWIM adapter not available: {e}")
            _swim_manager = None
    return _swim_manager


# Dead Peer Cooldown Manager (Jan 2026)
# Adaptive cooldown with probe-based early recovery
_dead_peer_cooldown_manager = None


def get_dead_peer_cooldown_manager():
    """Get the dead peer cooldown manager singleton (lazy load).

    The DeadPeerCooldownManager replaces the static 1-hour cooldown with:
    - Tiered cooldowns (30s -> 2min -> 10min -> 30min) based on failure frequency
    - Probe-based early recovery when gossip reports a dead node might be alive
    - Prevents 25-40% node loss from brief network blips
    """
    global _dead_peer_cooldown_manager
    if _dead_peer_cooldown_manager is None:
        try:
            from scripts.p2p.dead_peer_recovery import DeadPeerCooldownManager
            _dead_peer_cooldown_manager = DeadPeerCooldownManager()
            logger.info("DeadPeerCooldownManager initialized with adaptive cooldown")
        except ImportError as e:
            logger.warning(f"DeadPeerCooldownManager not available: {e}")
            _dead_peer_cooldown_manager = None
    return _dead_peer_cooldown_manager


# ============================================
# Phase 4: Extracted Background Loops (Dec 2025)
# ============================================
# These loops are extracted from the monolithic orchestrator for modularity.
# They use dependency injection via callbacks for testability.

# Feature flag for gradual rollout
EXTRACTED_LOOPS_ENABLED = os.environ.get("RINGRIFT_EXTRACTED_LOOPS", "true").lower() in ("true", "1", "yes")
JOB_REAPER_FALLBACK_ENABLED = os.environ.get("RINGRIFT_JOB_REAPER_FALLBACK_ENABLED", "true").lower() in ("true", "1", "yes")

# Lazy import to avoid circular dependencies
_loop_manager_instance = None
_loop_classes_loaded = False


def _load_loop_classes():
    """Lazy-load loop classes to avoid import-time dependencies."""
    global _loop_classes_loaded
    if _loop_classes_loaded:
        return True
    try:
        from scripts.p2p.loops import (
            LoopManager,
            QueuePopulatorLoop,
            EloSyncLoop,
            ModelSyncLoop,
            DataAggregationLoop,
            IpDiscoveryLoop,
            TailscaleRecoveryLoop,
            TailscalePeerDiscoveryLoop,
            FollowerDiscoveryLoop,
            AutoScalingLoop,
            HealthAggregationLoop,
            JobReaperLoop,
            IdleDetectionLoop,
            UdpDiscoveryLoop,
            SplitBrainDetectionLoop,
            QuorumCrisisDiscoveryLoop,
            QuorumCrisisConfig,
        )
        _loop_classes_loaded = True
        return True
    except ImportError as e:
        logger.error(f"[LoopManager] CRITICAL: Extracted loops import failed: {e}")
        logger.error("[LoopManager] WorkerPullLoop will NOT start - workers won't claim work!")
        return False


def get_loop_manager() -> "LoopManager | None":
    """Get or create the global LoopManager singleton.

    Returns None if extracted loops are disabled or unavailable.
    """
    global _loop_manager_instance
    if not EXTRACTED_LOOPS_ENABLED:
        return None
    if _loop_manager_instance is None:
        if not _load_loop_classes():
            return None
        try:
            from scripts.p2p.loops import LoopManager
            _loop_manager_instance = LoopManager(name="p2p_loops")
            logger.info("LoopManager: initialized for extracted background loops")
        except (ImportError, TypeError, ValueError, AttributeError) as e:
            # ImportError: loops module not available
            # TypeError: wrong constructor signature
            # ValueError: invalid argument
            # AttributeError: LoopManager not found in module
            logger.error(f"LoopManager: failed to initialize: {e}")
            return None
    return _loop_manager_instance


# Board priority overrides from unified_loop.yaml
# 0=CRITICAL, 1=HIGH, 2=MEDIUM, 3=LOW (lower value = higher priority)
_board_priority_cache: dict[str, int] | None = None
_board_priority_cache_time: float = 0


def get_board_priority_overrides() -> dict[str, int]:
    """Load board priority overrides from config, cached for 60 seconds.

    Returns dict mapping config keys (e.g., 'hexagonal_2p') to priority levels.
    Priority levels: 0=CRITICAL, 1=HIGH, 2=MEDIUM, 3=LOW
    """
    global _board_priority_cache, _board_priority_cache_time
    now = time.time()

    # Return cached value if fresh (60 second TTL)
    if _board_priority_cache is not None and now - _board_priority_cache_time < 60:
        return _board_priority_cache

    try:
        import yaml
        config_path = Path(__file__).parent.parent / "config" / "unified_loop.yaml"
        if config_path.exists():
            with open(config_path) as f:
                yaml_config = yaml.safe_load(f)
            selfplay_config = yaml_config.get("selfplay", {})
            overrides = selfplay_config.get("board_priority_overrides", {})
            # Convert config keys like "hexagonal_2p" -> priority int
            _board_priority_cache = {k: int(v) for k, v in overrides.items()}
            _board_priority_cache_time = now
            return _board_priority_cache
    except (OSError, ValueError, AttributeError, ImportError):
        pass

    # Default: empty (no overrides)
    return {}


# =============================================================================
# P2P Event Emission Helpers (December 2025 - CRITICAL gap fix)
# =============================================================================
# These helpers safely emit events for P2P lifecycle changes. Events enable:
# - LeadershipCoordinator to track leader changes
# - UnifiedHealthManager to respond to node failures
# - Cluster-wide coordination on membership changes

_p2p_event_emitters_available: bool | None = None
_p2p_event_emitters_last_check: float = 0.0
_P2P_EMITTER_CACHE_TTL: float = 30.0  # Retry every 30 seconds if failed


def _check_event_emitters() -> bool:
    """Check if event emitters are available (cached with TTL for retries).

    December 27, 2025: Fixed bug where negative result was cached permanently.
    Now retries every 30 seconds if event system becomes available later.
    """
    global _p2p_event_emitters_available, _p2p_event_emitters_last_check
    import time

    now = time.time()

    # Use cached positive result indefinitely
    if _p2p_event_emitters_available is True:
        return True

    # For negative results, retry after TTL expires
    if _p2p_event_emitters_available is False:
        if now - _p2p_event_emitters_last_check < _P2P_EMITTER_CACHE_TTL:
            return False
        # TTL expired, retry below

    try:
        from app.coordination.event_router import (
            emit_host_online,
            emit_host_offline,
            emit_leader_elected,
        )
        _p2p_event_emitters_available = True
        _p2p_event_emitters_last_check = now
        return True
    except ImportError:
        _p2p_event_emitters_available = False
        _p2p_event_emitters_last_check = now
        return False


# December 28, 2025: Module-level emit functions (27 methods, ~911 LOC) were moved to
# EventEmissionMixin in scripts/p2p/event_emission_mixin.py.
# P2POrchestrator now inherits from EventEmissionMixin and uses self._emit_* methods.
# See scripts/p2p/__init__.py for the mixin export.


# Add project root to path for scripts.lib imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scripts.lib.file_formats import open_jsonl_file
from scripts.lib.logging_config import setup_script_logging
from scripts.lib.process import (
    SingletonLock,
    find_processes_by_pattern,
    kill_process,
    is_process_running,
)

logger = setup_script_logging("p2p_orchestrator")

# Singleton lock for duplicate process prevention (December 2025)
_P2P_LOCK: SingletonLock | None = None


def _validate_p2p_dependencies() -> None:
    """Pre-flight check for required modules. Exits with code 2 if missing.

    This catches import errors early with a clear message, rather than
    failing deep in the call stack with confusing tracebacks.
    """
    required_modules = [
        ("aiohttp", "pip install aiohttp"),
        ("psutil", "pip install psutil"),
        ("yaml", "pip install pyyaml"),
    ]
    missing = []
    for module_name, install_hint in required_modules:
        try:
            __import__(module_name)
        except ImportError:
            missing.append(f"{module_name} ({install_hint})")

    if missing:
        # Use print since logger may not be fully initialized
        print(f"CRITICAL: Missing required dependencies: {', '.join(missing)}", file=sys.stderr)
        print("Run: pip install -r requirements.txt", file=sys.stderr)
        sys.exit(2)  # Exit code 2 = missing dependencies


# Validate dependencies before any heavy imports
_validate_p2p_dependencies()


@contextlib.contextmanager
def db_connection(db_path: str | Path, timeout: float = 30.0) -> Generator[sqlite3.Connection]:
    """Context manager for SQLite connections to prevent leaks.

    Usage:
        with db_connection(db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(...)
    """
    conn = None
    try:
        conn = sqlite3.connect(str(db_path), timeout=timeout)
        yield conn
    finally:
        if conn:
            with contextlib.suppress(Exception):
                conn.close()


# =============================================================================
# Async subprocess helper - Jan 19, 2026
# Prevents blocking the event loop during subprocess operations
# =============================================================================

async def async_subprocess_run(
    cmd: list[str],
    cwd: str | Path | None = None,
    timeout: float = 30.0,
    capture_output: bool = True,
    text: bool = True,
    env: dict | None = None,
) -> subprocess.CompletedProcess:
    """Run subprocess in thread pool to avoid blocking the event loop.

    This is a drop-in replacement for subprocess.run() in async contexts.
    Wraps the blocking subprocess.run() call in asyncio.to_thread().

    Args:
        cmd: Command and arguments to run
        cwd: Working directory for the command
        timeout: Timeout in seconds (default 30)
        capture_output: Capture stdout/stderr (default True)
        text: Return text instead of bytes (default True)
        env: Environment variables (default None = inherit)

    Returns:
        CompletedProcess with returncode, stdout, stderr

    Example:
        result = await async_subprocess_run(["git", "status"], cwd="/path")
        if result.returncode == 0:
            print(result.stdout)
    """
    def _run():
        return subprocess.run(
            cmd,
            cwd=cwd,
            timeout=timeout,
            capture_output=capture_output,
            text=text,
            env=env,
        )

    return await asyncio.to_thread(_run)


async def async_db_query(
    db_path: str | Path,
    query: str,
    params: tuple = (),
    timeout: float = 30.0,
    fetch_all: bool = True,
) -> list | None:
    """Run SQLite query in thread pool to avoid blocking the event loop.

    Args:
        db_path: Path to SQLite database
        query: SQL query to execute
        params: Query parameters (default empty)
        timeout: Connection timeout in seconds (default 30)
        fetch_all: If True, return all rows; if False, return one row

    Returns:
        Query results or None on error
    """
    def _query():
        with db_connection(db_path, timeout=timeout) as conn:
            cursor = conn.cursor()
            cursor.execute(query, params)
            if fetch_all:
                return cursor.fetchall()
            return cursor.fetchone()

    try:
        return await asyncio.to_thread(_query)
    except Exception as e:
        logger.warning(f"[async_db_query] Query failed on {db_path}: {e}")
        return None


async def async_db_execute(
    db_path: str | Path,
    query: str,
    params: tuple = (),
    timeout: float = 30.0,
) -> bool:
    """Execute SQLite write query in thread pool.

    Args:
        db_path: Path to SQLite database
        query: SQL query to execute (INSERT, UPDATE, DELETE)
        params: Query parameters (default empty)
        timeout: Connection timeout in seconds (default 30)

    Returns:
        True if successful, False on error
    """
    def _execute():
        with db_connection(db_path, timeout=timeout) as conn:
            cursor = conn.cursor()
            cursor.execute(query, params)
            conn.commit()
            return True

    try:
        return await asyncio.to_thread(_execute)
    except Exception as e:
        logger.warning(f"[async_db_execute] Execute failed on {db_path}: {e}")
        return False


# Centralized ramdrive utilities for auto-detection
# Shared database integrity utilities
from app.db.integrity import (
    check_and_repair_databases,
)

# Circuit breaker for fault-tolerant network operations
from app.distributed.circuit_breaker import (
    CircuitState,
    get_circuit_registry,
)
# Jan 2026: Adaptive budget selection based on config Elo
from app.coordination.budget_calculator import (
    get_adaptive_budget_for_elo,
)
from app.utils.ramdrive import (
    RamdriveSyncer,
    get_system_resources,
    log_storage_recommendation,
    should_use_ramdrive,
)
from scripts.p2p.cluster_config import (
    get_cluster_config,
    get_webhook_urls,
)
from scripts.p2p.handlers import (
    ABTestHandlersMixin,
    AdminHandlersMixin,
    CanonicalGateHandlersMixin,
    CMAESHandlersMixin,
    DeliveryHandlersMixin,
    ElectionHandlersMixin,
    EloSyncHandlersMixin,
    GauntletHandlersMixin,
    GossipHandlersMixin,
    ImprovementHandlersMixin,
    JobsApiHandlersMixin,
    MetricsHandlersMixin,  # January 2026 - P2P Modularization (Prometheus metrics)
    SelfplayHandlersMixin,  # January 2026 - P2P Modularization (Selfplay API)
    ClusterApiHandlersMixin,  # January 2026 - P2P Modularization (Cluster API)
    DashboardHandlersMixin,  # January 2026 - P2P Modularization (Dashboard)
    RecoveryHandlersMixin,  # January 2026 - P2P Modularization Phase 2b (Rollback)
    ConfigurationHandlersMixin,  # January 2026 - P2P Modularization Phase 2c (Config/Registration)
    TrainingControlHandlersMixin,  # January 2026 - P2P Modularization Phase 3a (Training Control)
    EloAnalyticsHandlersMixin,  # January 2026 - P2P Modularization Phase 4a (Elo Analytics)
    EvaluationPlayHandlersMixin,  # January 2026 - P2P Modularization Phase 5a (Elo Match Play)
    EventManagementHandlersMixin,  # January 2026 - P2P Modularization Phase 5b (Event Subscriptions)
    StatusHandlersMixin,  # January 2026 - P2P Modularization Phase 6a (Status/Health/Loops)
    ModelHandlersMixin,  # January 2026 - Comprehensive Model Evaluation Pipeline
    VoterConfigHandlersMixin,  # January 2026 - Consensus-safe voter config sync
    RegistryHandlersMixin,
    ManifestHandlersMixin,
    RelayHandlersMixin,
    SSHTournamentHandlersMixin,
    SyncHandlersMixin,
    TableHandlersMixin,
    TournamentHandlersMixin,
    WorkQueueHandlersMixin,
    setup_model_routes,  # January 2026 - Model inventory route setup
)
from scripts.p2p.network_utils import NetworkUtilsMixin
from scripts.p2p.peer_manager import PeerManagerMixin
from scripts.p2p.leader_election import LeaderElectionMixin
from scripts.p2p.gossip_protocol import GossipProtocolMixin  # Contains merged GossipMetricsMixin (Dec 28, 2025)

# Phase 5: SWIM + Raft integration mixins (Dec 26, 2025)
from scripts.p2p.membership_mixin import MembershipMixin
from scripts.p2p.consensus_mixin import ConsensusMixin
from scripts.p2p.handlers.swim import SwimHandlersMixin
from scripts.p2p.handlers.raft import RaftHandlersMixin
from scripts.p2p.handlers.network_health import NetworkHealthMixin, setup_network_health_routes

# Import constants from the refactored module (Phase 2 refactoring - consolidated)
from scripts.p2p.constants import (
    ADVERTISE_HOST_ENV,
    ADVERTISE_PORT_ENV,
    AGENT_MODE_ENABLED,
    ARBITER_URL,
    # Auth and build info
    AUTH_TOKEN_ENV,
    AUTH_TOKEN_FILE_ENV,
    AUTO_ASSIGN_ENABLED,
    AUTO_TRAINING_THRESHOLD_MB,
    AUTO_UPDATE_ENABLED,
    AUTO_WORK_BATCH_SIZE,
    BUILD_VERSION_ENV,
    COORDINATOR_URL,
    DATA_MANAGEMENT_INTERVAL,
    DB_EXPORT_THRESHOLD_MB,
    # Network configuration
    DEFAULT_PORT,
    DISCOVERY_INTERVAL,
    DISCOVERY_PORT,
    DISK_CLEANUP_THRESHOLD,
    # Resource thresholds
    DISK_CRITICAL_THRESHOLD,
    DISK_WARNING_THRESHOLD,
    # Dynamic voter management
    DYNAMIC_VOTER_ENABLED,
    DYNAMIC_VOTER_MAX_QUORUM,
    DYNAMIC_VOTER_MIN,
    DYNAMIC_VOTER_TARGET,
    ELECTION_TIMEOUT,
    ELO_K_FACTOR,
    GH200_MAX_SELFPLAY,
    GH200_MIN_SELFPLAY,
    GIT_BRANCH_NAME,
    GIT_REMOTE_NAME,
    # Auto-update settings
    GIT_UPDATE_CHECK_INTERVAL,
    # Safeguards
    GPU_IDLE_RESTART_TIMEOUT,
    GPU_IDLE_THRESHOLD,
    GPU_POWER_RANKINGS,
    GRACEFUL_SHUTDOWN_BEFORE_UPDATE,
    HEARTBEAT_INTERVAL,
    # Connection robustness
    HTTP_CONNECT_TIMEOUT,
    HTTP_TOTAL_TIMEOUT,
    IDLE_CHECK_INTERVAL,
    IDLE_GPU_THRESHOLD,
    IDLE_GRACE_PERIOD,
    # Elo constants (from app.config.thresholds)
    BASELINE_ELO_RANDOM,  # Random AI pinned at 400 Elo
    INITIAL_ELO_RATING,
    JOB_CHECK_INTERVAL,
    LEADER_DEGRADED_STEPDOWN_DELAY,
    LEADER_HEALTH_CHECK_INTERVAL,
    LEADER_LEASE_DURATION,
    LEADER_LEASE_RENEW_INTERVAL,
    LEADER_MIN_RESPONSE_RATE,
    LEADERLESS_TRAINING_TIMEOUT,
    LEADER_WORK_DISPATCH_TIMEOUT,
    # Leader stickiness (Jan 2, 2026)
    INCUMBENT_LEADER_GRACE_PERIOD,
    RECENT_LEADER_WINDOW,
    # Probabilistic fallback leadership (Jan 1, 2026)
    PROVISIONAL_LEADER_MIN_LEADERLESS_TIME,
    PROVISIONAL_LEADER_INITIAL_PROBABILITY,
    PROVISIONAL_LEADER_MAX_PROBABILITY,
    PROVISIONAL_LEADER_PROBABILITY_GROWTH_RATE,
    PROVISIONAL_LEADER_QUORUM_TIMEOUT,
    PROVISIONAL_LEADER_CHECK_INTERVAL,
    # Jan 2026: ULSM tiered fallback
    ELECTION_RETRY_COUNT_BEFORE_PROVISIONAL,
    DETERMINISTIC_FALLBACK_TIME,
    LOAD_AVERAGE_MAX_MULTIPLIER,
    LOAD_MAX_FOR_NEW_JOBS,
    MANIFEST_JSONL_LINECOUNT_CHUNK_BYTES,
    # Data management
    MANIFEST_JSONL_LINECOUNT_MAX_BYTES,
    MANIFEST_JSONL_SAMPLE_BYTES,
    MAX_CONCURRENT_EXPORTS,
    MAX_CONSECUTIVE_FAILURES,
    MAX_DISK_USAGE_PERCENT,
    MAX_GAUNTLET_RUNTIME,
    # Stale process cleanup
    MAX_SELFPLAY_RUNTIME,
    MAX_TOURNAMENT_RUNTIME,
    MAX_TRAINING_RUNTIME,
    MEMORY_CRITICAL_THRESHOLD,
    MEMORY_WARNING_THRESHOLD,
    MIN_GAMES_FOR_SYNC,
    MIN_MEMORY_GB_FOR_TASKS,
    MODEL_SYNC_INTERVAL,
    NAT_BLOCKED_PROBE_INTERVAL,
    NAT_BLOCKED_PROBE_TIMEOUT,
    NAT_BLOCKED_RECOVERY_TIMEOUT,
    NAT_EXTERNAL_IP_CACHE_TTL,
    NAT_HOLE_PUNCH_RETRY_COUNT,
    # NAT/Relay settings
    NAT_INBOUND_HEARTBEAT_STALE_SECONDS,
    NAT_RELAY_PREFERENCE_THRESHOLD,
    NAT_STUN_LIKE_PROBE_INTERVAL,
    NAT_SYMMETRIC_DETECTION_ENABLED,
    P2P_DATA_SYNC_BASE,
    P2P_DATA_SYNC_MAX,
    P2P_DATA_SYNC_MIN,
    P2P_MODEL_SYNC_BASE,
    P2P_MODEL_SYNC_MAX,
    P2P_MODEL_SYNC_MIN,
    P2P_SYNC_BACKOFF_FACTOR,
    P2P_SYNC_SPEEDUP_FACTOR,
    P2P_TRAINING_DB_SYNC_BASE,
    P2P_TRAINING_DB_SYNC_MAX,
    P2P_TRAINING_DB_SYNC_MIN,
    PEER_BOOTSTRAP_INTERVAL,
    PEER_BOOTSTRAP_MIN_PEERS,
    PEER_DEATH_RATE_LIMIT,
    PEER_PURGE_AFTER_SECONDS,
    PEER_RECOVERY_RETRY_INTERVAL,
    PEER_RETIRE_AFTER_SECONDS,
    PEER_TIMEOUT,
    PEER_TIMEOUT_JITTER_FACTOR,
    get_jittered_peer_timeout,
    get_cpu_adaptive_timeout,
    CPU_LOAD_HIGH_THRESHOLD,
    RELAY_COMMAND_MAX_ATTEMPTS,
    RELAY_COMMAND_MAX_BATCH,
    RELAY_COMMAND_TTL_SECONDS,
    RELAY_HEARTBEAT_INTERVAL,
    RELAY_MAX_PENDING_START_JOBS,
    RETRY_DEAD_NODE_INTERVAL,
    RETRY_RETIRED_NODE_INTERVAL,
    RUNAWAY_SELFPLAY_PROCESS_THRESHOLD,
    SPAWN_RATE_LIMIT_PER_MINUTE,
    STALE_PROCESS_CHECK_INTERVAL,
    STARTUP_GRACE_PERIOD,
    ELECTION_PARTICIPATION_DELAY,
    STALE_PROCESS_PATTERNS,
    STARTUP_JSONL_GRACE_PERIOD_SECONDS,
    # State directory
    STATE_DIR,
    TAILSCALE_CGNAT_NETWORK,
    TARGET_GPU_UTIL_MAX,
    # GPU configuration
    TARGET_GPU_UTIL_MIN,
    TRAINING_DATA_SYNC_THRESHOLD_MB,
    # Training node sync
    TRAINING_NODE_COUNT,
    TRAINING_SYNC_INTERVAL,
    # Unified inventory / Idle detection
    UNIFIED_DISCOVERY_INTERVAL,
    VOTER_DEMOTION_FAILURES,
    VOTER_HEALTH_THRESHOLD,
    VOTER_HEARTBEAT_INTERVAL,
    VOTER_HEARTBEAT_TIMEOUT,
    VOTER_MESH_REFRESH_INTERVAL,
    VOTER_MIN_QUORUM,
    VOTER_NAT_RECOVERY_AGGRESSIVE,
    VOTER_PROMOTION_UPTIME,
    # Phase 26: Multi-seed bootstrap and mesh resilience
    BOOTSTRAP_SEEDS,
    MIN_BOOTSTRAP_ATTEMPTS,
    ISOLATED_BOOTSTRAP_INTERVAL,
    MIN_CONNECTED_PEERS,
    # Phase 28: Gossip protocol
    GOSSIP_FANOUT,
    GOSSIP_INTERVAL,
    GOSSIP_MAX_PEER_ENDPOINTS,
    # Phase 27: Peer cache
    PEER_CACHE_TTL_SECONDS,
    PEER_CACHE_MAX_ENTRIES,
    PEER_REPUTATION_ALPHA,
    # Phase 29: Cluster epochs
    INITIAL_CLUSTER_EPOCH,
)
from scripts.p2p.models import (
    ClusterDataManifest,
    ClusterJob,
    ClusterSyncPlan,
    DataFileInfo,
    DataSyncJob,
    DistributedCMAESState,
    DistributedTournamentState,
    ImprovementLoopState,
    NodeDataManifest,
    NodeInfo,
    PeerCircuitBreaker,  # Jan 3, 2026: Sprint 10+ P2P hardening
    PeerHealthScore,     # Jan 3, 2026: Sprint 10+ P2P hardening
    SSHTournamentRun,
    TrainingJob,
    TrainingThresholds,
)
from scripts.p2p.p2p_mixin_base import SubscriptionRetryConfig
from scripts.p2p.network import (
    JobSnapshot,  # Jan 12, 2026: Lock-free job reads
    NonBlockingAsyncLockWrapper,
    PeerSnapshot,  # Jan 12, 2026: Lock-free peer reads
    TimeoutAsyncLockWrapper,
    get_client_session,
)

# Import refactored utilities (Phase 2 refactoring)
from scripts.p2p.resource_utils import (
    check_disk_has_capacity,
)

# Import refactored P2P types and models
# These were extracted from this file for modularity (Phase 1 refactoring)
from scripts.p2p.types import JobType, NodeRole
from scripts.p2p.utils import (
    safe_json_response,
    systemd_notify_ready,
    systemd_notify_watchdog,
)
from scripts.p2p.managers import (
    JobManager,
    JobOrchestrationConfig,
    JobOrchestrationManager,
    NodeSelector,
    SelfplayScheduler,
    StateManager,
    SyncPlanner,
    SyncPlannerConfig,
    TrainingCoordinator,
    create_job_orchestration_manager,
)
from scripts.p2p.managers.state_manager import PersistedLeaderState
from scripts.p2p.managers.voter_config_manager import (
    get_voter_config_manager,
    VoterConfigManager,
)
from scripts.p2p.managers.work_discovery_manager import (
    _is_selfplay_enabled_for_node,
    set_selfplay_disabled_override,
)
from scripts.p2p.metrics_manager import MetricsManager
from scripts.p2p.resource_detector import ResourceDetector, ResourceDetectorMixin
from scripts.p2p.event_emission_mixin import EventEmissionMixin
from scripts.p2p.failover_integration import FailoverIntegrationMixin
from scripts.p2p.relay_leader_propagator import RelayLeaderPropagatorMixin  # Phase 1: NAT-blocked leader propagation (Jan 4, 2026)
from scripts.p2p.leadership_state_machine import (
    LeadershipStateMachine,
    LeaderState,
    TransitionReason,
)

# Unified resource checking utilities (80% max utilization)
# Includes graceful degradation for dynamic workload management
try:
    from app.utils.resource_guard import (
        LIMITS as RESOURCE_LIMITS,
        OperationPriority,
        check_cpu as unified_check_cpu,
        check_disk_space as unified_check_disk,
        check_memory as unified_check_memory,
        get_degradation_level,
        should_proceed_with_priority,
    )
    HAS_RESOURCE_GUARD = True
except ImportError:
    HAS_RESOURCE_GUARD = False
    unified_check_disk = None
    unified_check_memory = None
    unified_check_cpu = None
    RESOURCE_LIMITS = None
    should_proceed_with_priority = None
    OperationPriority = None
    get_degradation_level = None

# ELO database sync manager for cluster-wide consistency
try:
    from app.tournament.elo_sync_manager import (
        EloSyncManager,
        ensure_elo_synced,
        get_elo_sync_manager,
        sync_elo_after_games,
    )
    HAS_ELO_SYNC = True
except ImportError:
    HAS_ELO_SYNC = False
    EloSyncManager = None
    get_elo_sync_manager = None
    sync_elo_after_games = None
    ensure_elo_synced = None

# Distributed data sync manager for model/data distribution
# Prefer new sync_coordinator, fallback to deprecated data_sync
try:
    from app.distributed.sync_coordinator import SyncCoordinator, full_cluster_sync
    HAS_SYNC_COORDINATOR = True

    def get_sync_coordinator():
        return SyncCoordinator.get_instance()
except ImportError:
    HAS_SYNC_COORDINATOR = False
    SyncCoordinator = None
    full_cluster_sync = None

# SyncRouter: Intelligent data routing with quality-based priority (December 2025)
try:
    from app.coordination.sync_router import get_sync_router, SyncRouter
    HAS_SYNC_ROUTER = True
except ImportError:
    HAS_SYNC_ROUTER = False
    get_sync_router = None
    SyncRouter = None

# Phase 3.1: Curriculum weights integration for selfplay prioritization
try:
    from scripts.unified_loop.curriculum import load_curriculum_weights
    HAS_CURRICULUM_WEIGHTS = True
except ImportError:
    HAS_CURRICULUM_WEIGHTS = False
    load_curriculum_weights = None

# Unified node inventory for multi-CLI discovery (Vast, Tailscale, Lambda, Hetzner)
try:
    from app.coordination.unified_inventory import UnifiedInventory, get_inventory
    HAS_UNIFIED_INVENTORY = True
except ImportError:
    HAS_UNIFIED_INVENTORY = False
    UnifiedInventory = None
    get_inventory = None

# HTTP server imports
try:
    import aiohttp
    from aiohttp import ClientSession, ClientTimeout, web
    HAS_AIOHTTP = True
except ImportError:
    HAS_AIOHTTP = False
    aiohttp = None
    logger.warning("aiohttp not installed. Install with: pip install aiohttp")

# SOCKS proxy support for userspace Tailscale networking
try:
    from aiohttp_socks import ProxyConnector
    HAS_SOCKS = True
except ImportError:
    HAS_SOCKS = False
    ProxyConnector = None

# Get SOCKS proxy from environment (e.g., socks5://localhost:1055)
SOCKS_PROXY = os.environ.get("RINGRIFT_SOCKS_PROXY", "")


# =============================================================================
# HTTP Handler Timeout Decorator (December 30, 2025)
# =============================================================================
# Added to fix P2P cluster connectivity issues where HTTP handlers blocked
# indefinitely on slow operations (lock acquisition, daemon status collection).

def with_request_timeout(timeout_seconds: float = 20.0):
    """Decorator to add timeout protection to HTTP handlers.

    December 30, 2025: Added to prevent HTTP endpoints from blocking indefinitely.
    January 10, 2026: Increased default from 10s to 20s to exceed typical lock wait
    times (reduced from 5s to 2s for gossip locks, but other operations can take longer).

    Usage:
        @with_request_timeout(5.0)
        async def handle_health(self, request):
            ...

    Args:
        timeout_seconds: Maximum time in seconds for handler to complete.

    Returns:
        Decorated handler that returns 504 Gateway Timeout on timeout.
    """
    import functools

    def decorator(handler):
        @functools.wraps(handler)
        async def wrapper(self_or_request, *args, **kwargs):
            # Handle both bound methods (self, request) and plain functions (request)
            try:
                return await asyncio.wait_for(
                    handler(self_or_request, *args, **kwargs),
                    timeout=timeout_seconds
                )
            except asyncio.TimeoutError:
                # Return 504 Gateway Timeout with details
                return web.json_response(
                    {
                        "error": "Request timed out",
                        "timeout_seconds": timeout_seconds,
                        "timestamp": time.time(),
                    },
                    status=504
                )
        return wrapper
    return decorator


# Systemd watchdog support for service health monitoring
# When running under systemd with WatchdogSec set, we need to periodically
# notify systemd that the service is healthy. If we miss the deadline,
# systemd will restart the service.
try:
    import sdnotify
    SYSTEMD_NOTIFIER = sdnotify.SystemdNotifier()
    HAS_SYSTEMD = True
except ImportError:
    SYSTEMD_NOTIFIER = None
    HAS_SYSTEMD = False


# ============================================
# Utilities (Refactored - Phase 2)
# ============================================
# The following utilities have been moved to scripts/p2p/ for modularity:
# - systemd_notify_watchdog, systemd_notify_ready (scripts/p2p/utils.py)
# - AsyncLockWrapper, get_client_session (scripts/p2p/network.py)
# - check_peer_circuit, record_peer_success, record_peer_failure (scripts/p2p/network.py)
# - peer_request (scripts/p2p/network.py)
# - get_disk_usage_percent, check_disk_has_capacity, check_all_resources (scripts/p2p/resource.py)
#
# They are imported at the top of this file for backward compatibility.
# ============================================

# Dynamic host registry for IP auto-update
try:
    from app.distributed.dynamic_registry import (
        NodeState,
        get_registry,
    )
    HAS_DYNAMIC_REGISTRY = True
except ImportError:
    HAS_DYNAMIC_REGISTRY = False
    get_registry = None
    NodeState = None

# Hybrid transport layer for HTTP/SSH fallback (self-healing Vast connectivity)
try:
    from app.distributed.hybrid_transport import (
        HybridTransport,
        diagnose_node_connectivity,
        get_hybrid_transport,
    )
    from app.distributed.ssh_transport import (
        SSHTransport,
        get_ssh_transport,
        probe_vast_nodes_via_ssh,
    )
    HAS_HYBRID_TRANSPORT = True
except ImportError:
    HAS_HYBRID_TRANSPORT = False
    HybridTransport = None
    get_hybrid_transport = None
    diagnose_node_connectivity = None
    SSHTransport = None
    get_ssh_transport = None
    probe_vast_nodes_via_ssh = None

# Improvement cycle manager for automated training
# Note: ImprovementCycleManager is deprecated - unified_ai_loop.py is the new approach
# Kept for backwards compatibility with older scripts
try:
    from scripts.improvement_cycle_manager import ImprovementCycleManager
    HAS_IMPROVEMENT_MANAGER = True
except ImportError:
    # Fallback - deprecated archive location removed in 2025-12
    HAS_IMPROVEMENT_MANAGER = False
    ImprovementCycleManager = None

# Task coordination safeguards - prevents runaway spawning
try:
    from app.coordination.safeguards import Safeguards, check_before_spawn
    HAS_SAFEGUARDS = True
    _safeguards = Safeguards.get_instance()
except ImportError:
    HAS_SAFEGUARDS = False
    _safeguards = None
    def check_before_spawn(task_type, node_id):
        return True, ""

# New coordination features: OrchestratorRole, backpressure, sync_lock, bandwidth
try:
    from app.coordination import (
        NodeResources,
        # Orchestrator role management (SQLite-backed with heartbeat)
        OrchestratorRole,
        # Queue backpressure
        QueueType,
        # Resource optimizer for cluster-wide PID-controlled optimization
        ResourceOptimizer,
        TransferPriority,
        acquire_orchestrator_role,
        get_cluster_utilization,
        get_host_targets,
        get_optimal_concurrency,
        get_resource_optimizer,
        # Resource targets for unified utilization management
        get_resource_targets,
        get_target_job_count,
        get_throttle_factor,
        record_utilization,
        release_bandwidth,
        release_orchestrator_role,
        # Bandwidth management
        request_bandwidth,
        should_scale_down,
        should_scale_up,
        should_stop_production,
        should_throttle_production,
        # Sync mutex for data transfer coordination
        sync_lock,
    )

    # Import rate negotiation functions for cooperative utilization (60-80% target)
    from app.coordination.resource_optimizer import (
        apply_feedback_adjustment,
        get_config_weights,
        get_current_selfplay_rate,
        get_hybrid_selfplay_limits,
        get_max_cpu_only_selfplay,
        # Hardware-aware selfplay limits (single source of truth)
        get_max_selfplay_for_node,
        get_utilization_status,
        negotiate_selfplay_rate,
        update_config_weights,
    )
    HAS_RATE_NEGOTIATION = True
    HAS_NEW_COORDINATION = True
    HAS_HW_AWARE_LIMITS = True
    # Get targets from unified source
    _unified_targets = get_resource_targets()
except ImportError:
    HAS_NEW_COORDINATION = False
    HAS_RATE_NEGOTIATION = False
    HAS_HW_AWARE_LIMITS = False
    OrchestratorRole = None
    _unified_targets = None
    negotiate_selfplay_rate = None
    get_current_selfplay_rate = None
    apply_feedback_adjustment = None
    get_utilization_status = None
    update_config_weights = None
    get_config_weights = None
    get_max_selfplay_for_node = None
    get_hybrid_selfplay_limits = None
    get_max_cpu_only_selfplay = None

# P2P-integrated monitoring management
try:
    from app.monitoring.p2p_monitoring import MonitoringManager
    HAS_P2P_MONITORING = True
except ImportError:
    HAS_P2P_MONITORING = False
    MonitoringManager = None

# Model sync across cluster
try:
    from scripts.sync_models import (
        HOSTS_MODULE_AVAILABLE as HAS_HOSTS_FOR_SYNC,
        ClusterModelState,
        scan_cluster as scan_cluster_models,
        sync_missing_models,
    )
    # Also import load_remote_hosts for scanning
    if HAS_HOSTS_FOR_SYNC:
        from app.distributed.hosts import filter_ready_hosts, load_remote_hosts
    HAS_MODEL_SYNC = True
except ImportError:
    HAS_MODEL_SYNC = False
    scan_cluster_models = None
    sync_missing_models = None
    ClusterModelState = None
    HAS_HOSTS_FOR_SYNC = False
    load_remote_hosts = None
    filter_ready_hosts = None

# PFSP (Prioritized Fictitious Self-Play) opponent pool
try:
    from app.training.advanced_training import (
        CMAESAutoTuner,
        OpponentStats,
        PFSPOpponentPool,
        PlateauConfig,
    )
    HAS_PFSP = True
except ImportError:
    HAS_PFSP = False
    PFSPOpponentPool = None
    OpponentStats = None
    CMAESAutoTuner = None
    PlateauConfig = None

# ============================================
# Configuration
# ============================================
# NOTE: All constants have been consolidated into scripts/p2p/constants.py
# and are imported at the top of this file (Phase 2 refactoring).
# See scripts/p2p/constants.py for configuration values and documentation.
# ============================================


# ============================================
# Types and Models (Refactored)
# ============================================
# The following types have been moved to scripts/p2p/ for modularity:
# - NodeRole, JobType (scripts/p2p/types.py)
# - NodeInfo, ClusterJob, DistributedCMAESState, DistributedTournamentState,
#   SSHTournamentRun, ImprovementLoopState, TrainingJob, TrainingThresholds,
#   DataFileInfo, NodeDataManifest, ClusterDataManifest, DataSyncJob,
#   ClusterSyncPlan (scripts/p2p/models.py)
#
# They are imported at the top of this file for backward compatibility.
# ============================================


class WebhookNotifier:
    """Sends alerts to Slack/Discord webhooks for important events.

    Configure via environment variables:
    - RINGRIFT_SLACK_WEBHOOK: Slack incoming webhook URL
    - RINGRIFT_DISCORD_WEBHOOK: Discord webhook URL
    - RINGRIFT_ALERT_LEVEL: Minimum level to alert (debug/info/warning/error) default: warning
    """

    LEVELS = {"debug": 0, "info": 1, "warning": 2, "error": 3}

    def __init__(self):
        # Try environment variables first, then fall back to cluster.yaml
        self.slack_webhook = os.environ.get("RINGRIFT_SLACK_WEBHOOK", "")
        self.discord_webhook = os.environ.get("RINGRIFT_DISCORD_WEBHOOK", "")

        # Fall back to cluster.yaml config if env vars not set
        if not self.slack_webhook or not self.discord_webhook:
            try:
                yaml_webhooks = get_webhook_urls()
                if not self.slack_webhook and "slack" in yaml_webhooks:
                    self.slack_webhook = yaml_webhooks["slack"]
                if not self.discord_webhook and "discord" in yaml_webhooks:
                    self.discord_webhook = yaml_webhooks["discord"]
            except (KeyError, IndexError, AttributeError):
                pass  # Ignore config loading errors

        self.min_level = self.LEVELS.get(
            os.environ.get("RINGRIFT_ALERT_LEVEL", "warning").lower(), 2
        )
        self._session: ClientSession | None = None
        self._last_alert: dict[str, float] = {}  # Throttle repeated alerts
        self._throttle_seconds = 300  # 5 minutes between duplicate alerts

    async def _get_session(self) -> ClientSession:
        if self._session is None or self._session.closed:
            self._session = ClientSession(timeout=ClientTimeout(total=10))
        return self._session

    async def close(self) -> None:
        """Close the HTTP session to prevent memory leaks.

        December 2025: Added to fix memory leak from unclosed sessions.
        """
        if self._session is not None and not self._session.closed:
            await self._session.close()
            self._session = None

    def close_sync(self) -> None:
        """Synchronously close the HTTP session (for finally blocks)."""
        if self._session is not None and not self._session.closed:
            try:
                loop = asyncio.new_event_loop()
                loop.run_until_complete(self._session.close())
                loop.close()
            except (RuntimeError, OSError, asyncio.CancelledError) as e:
                # Dec 2025: Narrowed from bare Exception; best effort cleanup
                logger.debug(f"HTTP session close failed (best effort): {e}")
            self._session = None

    def _should_throttle(self, alert_key: str) -> bool:
        """Check if this alert should be throttled (duplicate within window)."""
        now = time.time()
        if alert_key in self._last_alert and now - self._last_alert[alert_key] < self._throttle_seconds:
            return True
        self._last_alert[alert_key] = now
        return False

    async def send(
        self,
        title: str,
        message: str = "",
        level: str = "warning",
        fields: dict[str, str] | None = None,
        node_id: str = "",
        # Aliases for backward compatibility (December 28, 2025)
        severity: str | None = None,
        context: dict[str, str] | None = None,
    ):
        """Send an alert to configured webhooks.

        Args:
            title: Alert title/subject (or message if message not provided)
            message: Alert body text
            level: debug/info/warning/error
            fields: Additional key-value pairs to include
            node_id: Node ID for deduplication
            severity: Alias for level (backward compatibility)
            context: Alias for fields (backward compatibility)

        December 28, 2025: Added severity and context aliases to fix API mismatch
        with callers using the alternative parameter names.
        """
        # Handle aliases - severity takes precedence if provided
        if severity is not None:
            level = severity
        if context is not None:
            fields = context
        # If message is empty, use title as message (for single-arg callers)
        if not message:
            message = title
            title = "RingRift Alert"

        if self.LEVELS.get(level, 2) < self.min_level:
            return

        if not self.slack_webhook and not self.discord_webhook:
            return

        # Throttle duplicate alerts
        alert_key = f"{title}:{node_id}"
        if self._should_throttle(alert_key):
            return

        try:
            session = await self._get_session()

            # Color based on level
            colors = {"debug": "#808080", "info": "#36a64f", "warning": "#ff9800", "error": "#ff0000"}
            color = colors.get(level, "#808080")

            # Send to Slack
            if self.slack_webhook:
                slack_fields = []
                if fields:
                    for k, v in fields.items():
                        slack_fields.append({"title": k, "value": str(v), "short": True})

                slack_payload = {
                    "attachments": [{
                        "color": color,
                        "title": f"[{level.upper()}] {title}",
                        "text": message,
                        "fields": slack_fields,
                        "footer": f"RingRift AI | {node_id}" if node_id else "RingRift AI",
                        "ts": int(time.time()),
                    }]
                }
                try:
                    async with session.post(self.slack_webhook, json=slack_payload) as resp:
                        if resp.status != 200:
                            logger.warning(f"[Webhook] Slack alert failed: {resp.status}")
                except Exception as e:  # noqa: BLE001
                    logger.error(f"[Webhook] Slack error: {e}")

            # Send to Discord
            if self.discord_webhook:
                discord_fields = []
                if fields:
                    for k, v in fields.items():
                        discord_fields.append({"name": k, "value": str(v), "inline": True})

                discord_payload = {
                    "embeds": [{
                        "title": f"[{level.upper()}] {title}",
                        "description": message,
                        "color": int(color.lstrip("#"), 16),
                        "fields": discord_fields,
                        "footer": {"text": f"RingRift AI | {node_id}" if node_id else "RingRift AI"},
                        "timestamp": datetime.utcnow().isoformat(),
                    }]
                }
                try:
                    async with session.post(self.discord_webhook, json=discord_payload) as resp:
                        if resp.status not in (200, 204):
                            logger.warning(f"[Webhook] Discord alert failed: {resp.status}")
                except Exception as e:  # noqa: BLE001
                    logger.error(f"[Webhook] Discord error: {e}")

        except Exception as e:  # noqa: BLE001
            logger.error(f"[Webhook] Alert send error: {e}")

    # Dec 28, 2025: Removed duplicate close() method (was lines 2031-2033)
    # The proper close() is defined at line 1898 with docstring and session=None cleanup


class P2POrchestrator(
    WorkQueueHandlersMixin,
    ElectionHandlersMixin,
    RelayHandlersMixin,
    GauntletHandlersMixin,
    GossipHandlersMixin,
    AdminHandlersMixin,
    EloSyncHandlersMixin,
    TournamentHandlersMixin,
    CMAESHandlersMixin,
    SSHTournamentHandlersMixin,
    DeliveryHandlersMixin,  # Phase 3: Delivery verification (Dec 27, 2025)
    SyncHandlersMixin,      # Phase 8: Sync handlers extraction (Dec 28, 2025)
    TableHandlersMixin,     # Phase 8: Table/dashboard handlers extraction (Dec 28, 2025)
    RegistryHandlersMixin,  # Phase 8: Registry handlers extraction (Dec 28, 2025)
    ManifestHandlersMixin,  # Phase 8: Manifest handlers extraction (Dec 28, 2025)
    ABTestHandlersMixin,    # Phase 8: A/B test handlers extraction (Dec 28, 2025)
    ImprovementHandlersMixin,  # Phase 8: Improvement loop handlers extraction (Dec 28, 2025)
    CanonicalGateHandlersMixin,  # Phase 8: Canonical gate handlers extraction (Dec 28, 2025)
    JobsApiHandlersMixin,        # Phase 8: Jobs API handlers extraction (Dec 28, 2025)
    MetricsHandlersMixin,        # Prometheus metrics export (Jan 2026 - P2P Modularization)
    SelfplayHandlersMixin,       # Selfplay API endpoints (Jan 2026 - P2P Modularization)
    ClusterApiHandlersMixin,     # Cluster API endpoints (Jan 2026 - P2P Modularization)
    DashboardHandlersMixin,      # Dashboard endpoints (Jan 2026 - P2P Modularization)
    RecoveryHandlersMixin,       # Rollback endpoints (Jan 2026 - P2P Modularization Phase 2b)
    ConfigurationHandlersMixin,  # Config/Registration (Jan 2026 - P2P Modularization Phase 2c)
    TrainingControlHandlersMixin,  # Training Control (Jan 2026 - P2P Modularization Phase 3a)
    EloAnalyticsHandlersMixin,   # Elo Analytics (Jan 2026 - P2P Modularization Phase 4a)
    EvaluationPlayHandlersMixin,  # Elo Match Play (Jan 2026 - P2P Modularization Phase 5a)
    EventManagementHandlersMixin,  # Event Subscriptions (Jan 2026 - P2P Modularization Phase 5b)
    StatusHandlersMixin,         # Status/Health/Loops (Jan 2026 - P2P Modularization Phase 6a)
    ModelHandlersMixin,          # Model inventory endpoints (Jan 2026 - Comprehensive Eval Pipeline)
    NetworkHealthMixin,          # Network health endpoints (Dec 30, 2025)
    NetworkUtilsMixin,
    PeerManagerMixin,
    LeaderElectionMixin,
    GossipProtocolMixin,  # Provides gossip protocol + metrics (merged Dec 28, 2025)
    # Phase 5: SWIM + Raft integration (Dec 26, 2025)
    MembershipMixin,      # SWIM gossip-based membership
    ConsensusMixin,       # PySyncObj Raft consensus
    SwimHandlersMixin,    # /swim/* HTTP handlers
    RaftHandlersMixin,    # /raft/* HTTP handlers
    ResourceDetectorMixin,  # Resource detection delegation (Dec 28, 2025)
    RelayLeaderPropagatorMixin,  # NAT-blocked leader propagation via gossip (Jan 4, 2026 - Phase 1)
    EventEmissionMixin,     # Event emission consolidation (Dec 28, 2025 - Phase 8)
    FailoverIntegrationMixin,  # Multi-layer transport failover (Dec 30, 2025 - Phase 9)
    VoterConfigHandlersMixin,  # Voter config sync (Jan 20, 2026 - Consensus-safe config sync)
):
    """Main P2P orchestrator class that runs on each node.

    Inherits from:
    - WorkQueueHandlersMixin: Work queue HTTP handlers (handle_work_*)
    - ElectionHandlersMixin: Leader election handlers (handle_election*, handle_lease*, handle_voter*)
    - RelayHandlersMixin: NAT relay handlers (handle_relay_*)
    - GauntletHandlersMixin: Gauntlet evaluation handlers (handle_gauntlet_*)
    - GossipHandlersMixin: Gossip protocol handlers (handle_gossip*)
    - AdminHandlersMixin: Admin and git handlers (handle_git_*, handle_admin_*)
    - EloSyncHandlersMixin: Elo sync handlers (handle_elo_sync_*)
    - TournamentHandlersMixin: Tournament handlers (handle_tournament_*)
    - CMAESHandlersMixin: CMA-ES optimization handlers (handle_cmaes_*)
    - SSHTournamentHandlersMixin: SSH tournament handlers (handle_ssh_tournament_*)
    - NetworkUtilsMixin: Peer address parsing, URL building, Tailscale detection
    - PeerManagerMixin: Peer discovery, reputation tracking, cache management
    - RelayLeaderPropagatorMixin: NAT-blocked leader propagation via gossip (Jan 4, 2026)
    """

    def __init__(
        self,
        node_id: str,
        host: str = "0.0.0.0",
        port: int = DEFAULT_PORT,
        known_peers: list[str] | None = None,
        relay_peers: list[str] | None = None,
        ringrift_path: str | None = None,
        advertise_host: str | None = None,
        advertise_port: int | None = None,
        auth_token: str | None = None,
        require_auth: bool = False,
        storage_type: str = "auto",  # "disk", "ramdrive", or "auto"
        sync_to_disk_interval: int = 300,  # Sync ramdrive to disk every N seconds
    ):
        self.node_id = node_id
        self.host = host
        self.port = port

        # Phase 26: Multi-seed bootstrap - merge CLI peers with hardcoded seeds
        # Priority: CLI peers first, then hardcoded seeds
        # Shuffle seeds to distribute load across bootstrap attempts
        import random
        cli_peers = known_peers or []
        merged_seeds = list(cli_peers)  # CLI peers have highest priority
        for seed in BOOTSTRAP_SEEDS:
            if seed not in merged_seeds:
                merged_seeds.append(seed)
        # Shuffle only the hardcoded portion to avoid overloading any single seed
        if len(merged_seeds) > len(cli_peers):
            hardcoded_portion = merged_seeds[len(cli_peers):]
            random.shuffle(hardcoded_portion)
            merged_seeds = merged_seeds[:len(cli_peers)] + hardcoded_portion
        self.known_peers = merged_seeds
        self.bootstrap_seeds = list(BOOTSTRAP_SEEDS)  # Store original for reference
        logger.info(f"Bootstrap seeds: {len(cli_peers)} CLI + {len(BOOTSTRAP_SEEDS)} hardcoded = {len(self.known_peers)} total")

        # Peers that should always receive relay heartbeats (for NAT-blocked nodes)
        self.relay_peers: set[str] = set(relay_peers or [])
        self.ringrift_path = ringrift_path or self._detect_ringrift_path()

        # Jan 5, 2026: Force relay mode for NAT-blocked nodes
        # NAT-blocked nodes should send ALL outbound heartbeats via relay to ensure
        # other nodes can discover them (since direct inbound connections fail).
        # This is loaded from distributed_hosts.yaml: `nat_blocked: true` or `force_relay_mode: true`
        self._force_relay_mode: bool = self._load_force_relay_mode()

        # Phase 29: Cluster epoch tracking for split-brain resolution
        self._cluster_epoch: int = INITIAL_CLUSTER_EPOCH
        # P2P Health state tracking (Dec 2025)
        self._cluster_health_degraded: bool = False
        # Gossip-learned peer endpoints (Phase 28)
        self._gossip_learned_endpoints: dict[str, dict[str, Any]] = {}

        # Phase 2.4 (Dec 29, 2025): Partition read-only mode
        # When in minority partition, pause job dispatch to prevent data divergence
        self._partition_readonly_mode: bool = False
        self._partition_readonly_since: float = 0.0
        self._last_partition_check: float = 0.0
        self._partition_check_interval: float = 30.0  # Check every 30 seconds

        # Storage configuration: "disk", "ramdrive", or "auto" (detected)
        self.sync_to_disk_interval = sync_to_disk_interval
        self.ramdrive_path = "/dev/shm/ringrift/data"  # Standard ramdrive location
        self.ramdrive_syncer: RamdriveSyncer | None = None

        # Resolve "auto" storage type based on system resources
        if storage_type == "auto":
            resources = get_system_resources()
            if should_use_ramdrive():
                self.storage_type = "ramdrive"
                logger.info(f"Auto-detected storage: RAMDRIVE "
                      f"(RAM: {resources.total_ram_gb:.0f}GB, "
                      f"Disk: {resources.free_disk_gb:.0f}GB free / {resources.disk_usage_percent:.0f}% used)")
            else:
                self.storage_type = "disk"
                logger.info(f"Auto-detected storage: DISK "
                      f"(RAM: {resources.total_ram_gb:.0f}GB, "
                      f"Disk: {resources.free_disk_gb:.0f}GB free / {resources.disk_usage_percent:.0f}% used)")
            log_storage_recommendation()
        else:
            self.storage_type = storage_type
        # Git 2.35+ enforces safe.directory for repos with different ownership.
        # Many nodes run the orchestrator as root against a checkout owned by
        # another user (e.g. ubuntu), so always provide a safe.directory override
        # for all git operations.
        self._git_safe_directory = os.path.abspath(self.ringrift_path)
        self.build_version = self._detect_build_version()
        self.start_time = time.time()
        self.last_peer_bootstrap = 0.0

        # Resource detection delegation (Dec 28, 2025)
        self._resource_detector = ResourceDetector(
            ringrift_path=self.ringrift_path,
            start_time=self.start_time,
            startup_grace_period=STARTUP_JSONL_GRACE_PERIOD_SECONDS,
        )

        # Public endpoint peers should use to reach us. Peers learn our host from
        # the heartbeat socket address, but the port must be self-reported. This
        # matters for port-mapped environments like Vast.ai.
        self.advertise_host = (advertise_host or os.environ.get(ADVERTISE_HOST_ENV, "")).strip()
        if not self.advertise_host:
            # Prefer a stable mesh address (Tailscale) when available so nodes
            # behind NAT remain reachable and the cluster converges on a single
            # view of peer endpoints.
            #
            # Jan 12, 2026: Multi-fallback IP resolution with YAML config priority.
            # Order: 1) YAML config tailscale_ip, 2) Tailscale CLI with retry,
            #        3) Local IP (last resort)
            #
            # YAML fallback added because Tailscale CLI may not be ready at startup,
            # and the pre-configured tailscale_ip in distributed_hosts.yaml is a
            # reliable source for the correct IP.
            yaml_ip = self._get_yaml_tailscale_ip()
            if yaml_ip:
                self.advertise_host = yaml_ip
                logger.info(f"[P2P] Using YAML config tailscale_ip: {yaml_ip}")
            else:
                # Try Tailscale detection with retry (up to 90s - increased Jan 12, 2026)
                # Root cause: 30s was insufficient when mac-studio boots and Tailscale
                # takes 45-60s to initialize. This caused persistent local IP (10.0.0.62)
                # advertisement, breaking voter quorum.
                ts_ip = _wait_for_tailscale_ip(timeout_seconds=90, interval_seconds=1.0)
                self.advertise_host = ts_ip or self._get_local_ip()
                if not ts_ip:
                    logger.warning(
                        f"[P2P] Tailscale unavailable, using local IP: {self.advertise_host}. "
                        "Set RINGRIFT_ADVERTISE_HOST or ensure Tailscale is running."
                    )

        # Dec 30, 2025: Validate advertise_host to prevent private IP issues
        # that cause P2P quorum loss when nodes can't reach each other
        self._validate_and_fix_advertise_host()

        self.advertise_port = advertise_port if advertise_port is not None else self._infer_advertise_port()

        # Optional auth token used to protect mutating endpoints and cluster control.
        # Default is allow-all unless a token is configured.
        env_token = (os.environ.get(AUTH_TOKEN_ENV, "")).strip()
        token_from_arg = (auth_token or "").strip()
        token = token_from_arg or env_token

        if not token:
            token_file = (os.environ.get(AUTH_TOKEN_FILE_ENV, "")).strip()
            if token_file:
                try:
                    token = Path(token_file).read_text().strip()
                except Exception as e:  # noqa: BLE001
                    logger.info(f"Auth: failed to read {AUTH_TOKEN_FILE_ENV}={token_file}: {e}")

        self.auth_token = token.strip()
        self.require_auth = bool(require_auth)
        if self.require_auth and not self.auth_token:
            raise ValueError(
                f"--require-auth set but {AUTH_TOKEN_ENV}/{AUTH_TOKEN_FILE_ENV}/--auth-token is empty"
            )

        # Optional split-brain mitigation: require a majority of "voter" nodes
        # to be visible before assuming or renewing leadership.
        #
        # Voters can be configured via:
        # - env: RINGRIFT_P2P_VOTERS="node-a,node-b,..."
        # - ai-service/config/distributed_hosts.yaml: per-host `p2p_voter: true`
        self.voter_config_source: str = "none"  # env|config|state|learned|none
        self.voter_node_ids: list[str] = self._load_voter_node_ids()
        # SIMPLIFIED QUORUM: Fixed at 3 voters (or less if fewer voters exist)
        self.voter_quorum_size: int = min(VOTER_MIN_QUORUM, len(self.voter_node_ids)) if self.voter_node_ids else 0
        if self.voter_node_ids:
            print(
                f"[P2P] Voter quorum enabled: voters={len(self.voter_node_ids)}, "
                f"quorum={self.voter_quorum_size} ({', '.join(self.voter_node_ids)})"
            )

        # Jan 2, 2026: IP-to-node-name mapping for SWIM peer ID resolution
        # SWIM identifies peers by IP:port, but voters are configured by name.
        # This mapping translates between them.
        self._ip_to_node_map: dict[str, str] = self._build_ip_to_node_map()

        # Node state
        self.role = NodeRole.FOLLOWER
        self.leader_id: str | None = None

        # Unified Leadership State Machine (ULSM) - Jan 2026
        # Single source of truth for leadership state transitions.
        # Addresses silent step-down and split-brain issues.
        self._leadership_sm = LeadershipStateMachine(node_id=self.node_id)
        # Broadcast callback is set in _setup_routes() after app is created

        self.verbose = bool(os.environ.get("RINGRIFT_P2P_VERBOSE", "").strip())
        self.peers: dict[str, NodeInfo] = {}
        # Jan 12, 2026: Lock-free peer snapshot for read-heavy operations like /status
        # PeerSnapshot maintains an immutable copy that can be read without acquiring peers_lock
        self._peer_snapshot: PeerSnapshot[NodeInfo] = PeerSnapshot()
        # Jan 20, 2026: Adaptive dead peer cooldown with probe-based recovery.
        # Replaces the static 1-hour cooldown that was causing 25-40% node loss.
        # The manager uses tiered cooldowns (30s -> 30min) based on failure frequency.
        self._cooldown_manager = get_dead_peer_cooldown_manager()
        # Fallback dict for compatibility if cooldown manager fails to load
        self._dead_peer_timestamps: dict[str, float] = {}
        self.local_jobs: dict[str, ClusterJob] = {}
        self.active_jobs: dict[str, dict[str, Any]] = {}  # Track running jobs by type (selfplay, training, etc.)

        # Network health tracking (December 30, 2025)
        # Reference to TailscalePeerDiscoveryLoop for stats reporting in /network/health
        self._tailscale_discovery_loop: Any = None

        # Distributed job state tracking (leader-only)
        self.distributed_cmaes_state: dict[str, DistributedCMAESState] = {}
        self.distributed_tournament_state: dict[str, DistributedTournamentState] = {}
        self.ssh_tournament_runs: dict[str, SSHTournamentRun] = {}
        self.improvement_loop_state: dict[str, ImprovementLoopState] = {}
        # Limit CPU-heavy CMA-ES local evaluations to avoid runaway process
        # explosions that can starve the orchestrator (especially on relay hubs).
        try:
            raw = (os.environ.get("RINGRIFT_P2P_MAX_CONCURRENT_CMAES_EVALS", "") or "").strip()
            self.max_concurrent_cmaes_evals = max(1, int(raw)) if raw else 2
        except (ValueError, AttributeError):
            self.max_concurrent_cmaes_evals = 2
        self._cmaes_eval_semaphore = asyncio.Semaphore(int(self.max_concurrent_cmaes_evals))

        # Tournament match semaphore - limit concurrent Elo calibration matches to prevent OOM
        # Each match can potentially load neural networks which use significant memory
        # NOTE: Set to None here, created lazily in async context to avoid event loop issues
        self._tournament_match_semaphore: asyncio.Semaphore | None = None

        # Phase 2: Distributed data sync state
        self.local_data_manifest: NodeDataManifest | None = None
        self.cluster_data_manifest: ClusterDataManifest | None = None  # Leader-only or received from broadcast
        self._cluster_manifest_received_at: float = 0.0  # When broadcast was received (followers)
        self.manifest_collection_interval = 300.0  # Collect manifests every 5 minutes
        self.last_manifest_collection = 0.0

        # Dashboard/selfplay stats history (leader-only). Stored in-memory to
        # enable lightweight throughput charts without adding DB migrations.
        self.selfplay_stats_history: list[dict[str, Any]] = []
        self.selfplay_stats_history_max_samples: int = 288  # ~24h @ 5-min cadence

        # Canonical gate jobs (leader-only): dashboard-triggered runs of
        # scripts/generate_canonical_selfplay.py.
        self.canonical_gate_jobs: dict[str, dict[str, Any]] = {}
        self.canonical_gate_jobs_lock = threading.RLock()

        # Phase 2: P2P rsync coordination state
        self.active_sync_jobs: dict[str, DataSyncJob] = {}
        self.current_sync_plan: ClusterSyncPlan | None = None  # Leader-only
        self.pending_sync_requests: list[dict[str, Any]] = []  # Requests from non-leader nodes
        self.sync_in_progress = False
        self.last_sync_time = 0.0
        self.auto_sync_interval = 600.0  # Auto-sync every 10 minutes when data is missing

        # Training node priority sync state (leader-only)
        self.training_sync_interval = TRAINING_SYNC_INTERVAL
        self.last_training_sync_time = 0.0
        self.training_nodes_cache: list[str] = []  # Cached list of top GPU nodes
        self.training_nodes_cache_time = 0.0
        self.games_synced_to_training: dict[str, int] = {}  # node_id -> last synced game count

        # Circuit breaker for fault-tolerant peer communication
        self._circuit_registry = get_circuit_registry()

        # Jan 3, 2026 (Sprint 10+): Per-peer circuit breakers and health scoring
        # Finer-grained failure isolation than per-transport breakers
        self._peer_circuit_breakers: dict[str, PeerCircuitBreaker] = {}
        self._peer_health_scores: dict[str, PeerHealthScore] = {}

        # Phase 3: Training pipeline state (leader-only)
        self.training_jobs: dict[str, TrainingJob] = {}
        self.training_thresholds: TrainingThresholds = TrainingThresholds()
        self.last_training_check: float = 0.0
        self.training_check_interval: float = 300.0  # Check every 5 minutes
        self.games_at_last_nnue_train: dict[str, int] = {}  # board_type -> game_count
        self.games_at_last_cmaes_train: dict[str, int] = {}

        # Phase 5: Automated improvement cycle manager (leader-only)
        self.improvement_cycle_manager: ImprovementCycleManager | None = None
        if HAS_IMPROVEMENT_MANAGER:
            try:
                self.improvement_cycle_manager = ImprovementCycleManager(
                    db_path=STATE_DIR / f"{node_id}_improvement.db",
                    ringrift_path=self.ringrift_path,
                )
                logger.info("ImprovementCycleManager initialized")
            except Exception as e:  # noqa: BLE001
                logger.error(f"Failed to initialize ImprovementCycleManager: {e}")
        self.last_improvement_cycle_check: float = 0.0

        # P2P-integrated monitoring (leader starts Prometheus/Grafana)
        self.monitoring_manager: MonitoringManager | None = None
        if HAS_P2P_MONITORING:
            try:
                self.monitoring_manager = MonitoringManager(
                    node_id=node_id,
                    prometheus_port=9090,
                    grafana_port=3000,
                    config_dir=Path(self.ringrift_path) / "monitoring",
                )
                logger.info("MonitoringManager initialized")
            except Exception as e:  # noqa: BLE001
                logger.error(f"Failed to initialize MonitoringManager: {e}")
        self._monitoring_was_leader = False  # Track leadership changes
        self.improvement_cycle_check_interval: float = 600.0  # Check every 10 minutes

        # P2P Auto-Deployer (leader-only): ensures P2P runs on all cluster nodes
        self.p2p_auto_deployer: P2PAutoDeployer | None = None
        self._auto_deployer_task: asyncio.Task | None = None

        # Webhook notifications for alerts
        self.notifier = WebhookNotifier()

        # HTTP server components for graceful restart (Jan 2026)
        # These are set in run() and used by restart_http_server()
        self._http_app: "web.Application | None" = None
        self._http_runner: "web.AppRunner | None" = None
        self._http_sites: list["web.TCPSite"] = []
        self._http_restart_lock = asyncio.Lock()
        self._http_restart_count = 0

        # Diversity tracking metrics
        self.diversity_metrics = {
            "games_by_engine_mode": {},      # engine_mode -> count
            "games_by_board_config": {},     # "board_players" -> count
            "games_by_difficulty": {},       # difficulty -> count
            "asymmetric_games": 0,           # count of asymmetric games scheduled
            "symmetric_games": 0,            # count of symmetric games scheduled
            "training_triggers": 0,          # count of training triggers
            "cmaes_triggers": 0,             # count of CMA-ES triggers
            "promotions": 0,                 # count of model promotions
            "rollbacks": 0,                  # count of rollbacks
            "last_reset": time.time(),       # when metrics were last reset
        }

        # === CRITICAL SELF-IMPROVEMENT LOOP METRICS ===
        # Training progress tracking (populated by training callbacks)
        self.training_metrics: dict[str, dict[str, float]] = {}  # config -> {loss, val_loss, epoch}

        # Selfplay throughput tracking
        self.selfplay_throughput: dict[str, float] = {}  # config -> games/hour

        # Cost efficiency metrics
        self.cost_metrics: dict[str, float] = {
            "gpu_hours_total": 0.0,
            "estimated_cost_usd": 0.0,
            "elo_per_gpu_hour": 0.0,
        }

        # Promotion quality metrics
        self.promotion_metrics: dict[str, Any] = {
            "success_rate": 0.0,
            "avg_elo_gain": 0.0,
            "rejections": {},  # reason -> count
            "total_attempts": 0,
            "successful": 0,
        }

        # LEARNED LESSONS - Stuck job detection (leader-only)
        # Track when each node's GPU first went idle with running jobs
        self.gpu_idle_since: dict[str, float] = {}  # node_id -> timestamp when GPU went idle

        # A/B Testing Framework - Compare models head-to-head with statistical significance
        # Key: test_id (UUID), Value: ABTestState dict
        self.ab_tests: dict[str, dict[str, Any]] = {}
        self.ab_test_lock = threading.RLock()

        # Elo Sync Manager - Keeps unified_elo.db consistent across cluster
        self.elo_sync_manager: EloSyncManager | None = None
        if HAS_ELO_SYNC:
            try:
                db_path = Path(self._get_ai_service_path()) / "data" / "unified_elo.db"
                # Use env var for coordinator, fallback to nebius-backbone-1 (stable backbone node)
                elo_coordinator = os.environ.get("RINGRIFT_ELO_COORDINATOR", "nebius-backbone-1")
                self.elo_sync_manager = EloSyncManager(
                    db_path=db_path,
                    coordinator_host=elo_coordinator,
                    sync_interval=300,  # Sync every 5 minutes
                )
                logger.info(f"EloSyncManager initialized (db: {db_path})")
            except Exception as e:  # noqa: BLE001
                logger.error(f"Failed to initialize EloSyncManager: {e}")

        # Queue Populator - Maintains 50+ work items until 2000 Elo target met
        self._queue_populator: QueuePopulator | None = None
        # Jan 5, 2026 (Session 17.41): Reference to QueuePopulatorLoop for handler access
        self._queue_populator_loop: Any = None

        # PFSP (Prioritized Fictitious Self-Play) opponent pool (leader-only)
        # Maintains a pool of historical models weighted by difficulty for diverse training
        self.pfsp_pools: dict[str, Any] = {}  # config_key -> PFSPOpponentPool
        if HAS_PFSP:
            try:
                for config_key in ["square8_2p", "square8_4p", "hex8_2p", "hexagonal_2p"]:
                    self.pfsp_pools[config_key] = PFSPOpponentPool(
                        max_pool_size=30,
                        hard_opponent_weight=0.6,
                        diversity_weight=0.25,
                        recency_weight=0.15,
                    )
                logger.info(f"PFSP opponent pools initialized for {len(self.pfsp_pools)} configs")
            except Exception as e:  # noqa: BLE001
                logger.error(f"Failed to initialize PFSP pools: {e}")

        # CMA-ES Auto-Tuner (leader-only)
        # Automatically triggers hyperparameter optimization when Elo plateaus
        self.cmaes_auto_tuners: dict[str, Any] = {}  # config_key -> CMAESAutoTuner
        self.last_cmaes_elo: dict[str, float] = {}  # config_key -> last recorded Elo
        if HAS_PFSP and CMAESAutoTuner:
            try:
                for config_key in ["square8_2p", "square8_4p", "hex8_2p", "hexagonal_2p"]:
                    parts = config_key.rsplit("_", 1)
                    board_type = parts[0]
                    num_players = int(parts[1].replace("p", ""))
                    plateau_cfg = PlateauConfig(patience=10)
                    self.cmaes_auto_tuners[config_key] = CMAESAutoTuner(
                        board_type=board_type,
                        num_players=num_players,
                        plateau_config=plateau_cfg,
                        min_epochs_between_tuning=50,
                        max_auto_tunes=3,
                    )
                logger.info(f"CMA-ES auto-tuners initialized for {len(self.cmaes_auto_tuners)} configs")
            except Exception as e:  # noqa: BLE001
                logger.error(f"Failed to initialize CMA-ES auto-tuners: {e}")

        # Locks for thread safety
        # Use RLock (reentrant lock) to allow nested acquisitions from same thread
        # This prevents deadlocks when helper methods like _select_best_relay are
        # called while already holding the lock
        self.peers_lock = threading.RLock()
        self.jobs_lock = threading.RLock()
        self.manifest_lock = threading.RLock()
        self.sync_lock = threading.RLock()
        self.training_lock = threading.RLock()
        self.ssh_tournament_lock = threading.RLock()
        self.relay_lock = threading.RLock()
        # Jan 12, 2026: C1 fix - leader state lock prevents race conditions
        # during leader elections and state transitions. Protects self.leader_id
        # and self.role from concurrent modification in async contexts.
        self.leader_state_lock = threading.RLock()

        # Jan 12, 2026: Lock-free job snapshot for /status endpoint
        # Updates via _job_snapshot.update(self.local_jobs) after mutations
        self._job_snapshot = JobSnapshot()

        # ============================================
        # Phase 5: SWIM + Raft Integration (Dec 26, 2025)
        # ============================================
        # SWIM provides leaderless gossip-based membership with 5s failure detection
        # Raft provides replicated work queue with sub-second failover
        # Both are initialized here but started asynchronously in run()

        # Feature flag validation - warn if flags enabled but dependencies missing
        from scripts.p2p.constants import (
            SWIM_ENABLED, RAFT_ENABLED, MEMBERSHIP_MODE, CONSENSUS_MODE
        )
        try:
            from app.p2p.swim_adapter import SWIM_AVAILABLE
        except ImportError:
            SWIM_AVAILABLE = False
        try:
            from scripts.p2p.consensus_mixin import PYSYNCOBJ_AVAILABLE
        except ImportError:
            PYSYNCOBJ_AVAILABLE = False

        if SWIM_ENABLED and not SWIM_AVAILABLE:
            logger.warning(
                "RINGRIFT_SWIM_ENABLED=true but swim-p2p not installed or not compatible. "
                "SWIM features disabled. Install with: pip install swim-p2p>=1.2.0"
            )
        if RAFT_ENABLED and not PYSYNCOBJ_AVAILABLE:
            logger.warning(
                "RINGRIFT_RAFT_ENABLED=true but pysyncobj not installed. "
                "Raft features disabled. Install with: pip install pysyncobj>=0.3.14"
            )
        if MEMBERSHIP_MODE in ("swim", "hybrid") and not SWIM_AVAILABLE:
            logger.warning(
                f"RINGRIFT_MEMBERSHIP_MODE={MEMBERSHIP_MODE} but SWIM unavailable. "
                "Falling back to HTTP heartbeats."
            )
        if CONSENSUS_MODE in ("raft", "hybrid") and not PYSYNCOBJ_AVAILABLE:
            logger.warning(
                f"RINGRIFT_CONSENSUS_MODE={CONSENSUS_MODE} but PySyncObj unavailable. "
                "Falling back to Bully algorithm."
            )

        # Log active P2P protocol modes
        logger.info(
            f"P2P protocols: MEMBERSHIP_MODE={MEMBERSHIP_MODE} (SWIM={'available' if SWIM_AVAILABLE else 'unavailable'}), "
            f"CONSENSUS_MODE={CONSENSUS_MODE} (Raft={'available' if PYSYNCOBJ_AVAILABLE else 'unavailable'})"
        )

        # Initialize SWIM membership (from MembershipMixin)
        self._swim_initialized = self._init_swim_membership()
        if self._swim_initialized:
            logger.info("SWIM membership initialized (will start in run())")

        # Initialize Raft consensus (from ConsensusMixin)
        # Note: Raft requires advertise_host which is set above
        self._raft_init_attempted = False
        # Raft initialization deferred to after peers are discovered
        # to ensure we have partner addresses available

        # Try early Raft initialization if we have voter nodes
        if RAFT_ENABLED and PYSYNCOBJ_AVAILABLE and self.voter_node_ids:
            try:
                self._raft_init_attempted = True
                raft_ok = self._init_raft_consensus()
                if raft_ok:
                    logger.info("Raft consensus initialized (will sync with peers in run())")
            except Exception as e:  # noqa: BLE001
                logger.warning(f"Early Raft initialization failed (will retry later): {e}")

        # Initialize failover system (Phase 9: Multi-layer transport cascade)
        # Lazy initialization - will be fully set up on first use
        try:
            self._init_failover_system()
            logger.info("Failover system initialized (transport cascade + union discovery)")
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Failover system init deferred: {e}")

        # State persistence (Phase 1 refactoring: delegated to StateManager)
        self.db_path = STATE_DIR / f"{node_id}_state.db"
        self.state_manager = StateManager(self.db_path, verbose=self.verbose)
        self.state_manager.init_database()
        self._cluster_epoch = self.state_manager.load_cluster_epoch()

        # Sprint 4 (Jan 2, 2026): Wire circuit breaker state persistence
        # This enables crash recovery for the global circuit breaker
        # Sprint 5 (Jan 2, 2026): Wire transport metrics persistence
        try:
            from scripts.p2p.transport_cascade import GlobalCircuitBreaker, TransportCascade
            GlobalCircuitBreaker.set_state_manager(self.state_manager)
            TransportCascade.set_state_manager(self.state_manager)
            logger.debug("Circuit breaker and transport metrics persistence configured")
        except ImportError:
            logger.debug("Transport cascade not available for persistence")

        # Metrics recording (Phase 1 refactoring: delegated to MetricsManager)
        self.metrics_manager = MetricsManager(self.db_path)

        # Event flags
        self.running = True
        self.election_in_progress = False
        self.last_election_attempt: float = 0.0

        # LEARNED LESSONS - Lease-based leadership to prevent split-brain
        # Leader must continuously renew lease; if lease expires, leadership is void
        self.leader_lease_expires: float = 0.0  # timestamp when current leader's lease expires
        self.last_lease_renewal: float = 0.0  # when we last renewed our lease (if leader)
        self.leader_lease_id: str = ""  # unique ID for current leadership term
        # LEADERLESS FALLBACK: Track when we last had a functioning leader.
        # If leaderless for too long, nodes can trigger local training independently.
        self.last_leader_seen: float = time.time()  # When we last saw a functioning leader
        # Dec 31, 2025: Leader invalidation window - prevents gossip from re-setting stale leader
        # After we invalidate a leader, ignore gossip leader claims for this window
        self._leader_invalidation_until: float = 0.0  # timestamp until which we ignore gossip leader claims
        self.last_local_training_fallback: float = 0.0  # When we last triggered local training fallback
        # Dec 30, 2025: Track when we last received work from the leader
        # If leader exists but isn't dispatching work, nodes can self-assign after timeout
        self.last_work_from_leader: float = time.time()  # When we last got work from leader

        # Jan 2, 2026: Leader stickiness - prefer incumbent during elections
        # Track when we last held leadership to allow re-claiming with preference
        self._last_become_leader_time: float = 0.0  # When we last became leader
        self._last_step_down_time: float = 0.0  # When we last stepped down from leadership

        # Jan 1, 2026: Probabilistic Fallback Leadership (Provisional Leader state)
        # When normal elections repeatedly fail, nodes can claim provisional leadership
        # with increasing probability. Provisional leaders can dispatch work but must be
        # confirmed by quorum acknowledgment or node_id tiebreaker if contested.
        self._provisional_leader_claimed_at: float = 0.0  # When we claimed provisional leadership
        self._provisional_leader_acks: set[str] = set()  # Nodes that acknowledged our provisional claim
        self._provisional_leader_challengers: dict[str, float] = {}  # node_id -> challenge_time
        self._last_provisional_check: float = 0.0  # Last time we checked for probabilistic claim
        self._provisional_claim_probability: float = PROVISIONAL_LEADER_INITIAL_PROBABILITY

        # Voter-backed lease grants (split-brain resistance).
        #
        # When quorum gating is enabled, voters act as a lightweight consensus
        # group by granting an exclusive leader lease to a single node at a time.
        # A leader must renew its lease with a quorum of voters; otherwise it
        # steps down. This prevents split-brain even if multiple nodes think
        # they are eligible leaders.
        self.voter_grant_leader_id: str = ""
        self.voter_grant_lease_id: str = ""
        self.voter_grant_expires: float = 0.0

        # Phase 15.1.1: Fenced lease tokens with monotonic epoch
        # These prevent split-brain during network partitions by ensuring
        # only one leader per epoch can issue commands.
        self._lease_epoch: int = 0  # Monotonic, never decreases
        self._fence_token: str = ""  # Unique per lease grant: node_id:epoch:timestamp
        self._last_seen_epoch: int = 0  # Highest epoch seen from any leader

        # Job completion tracking for auto-restart
        self.completed_jobs: dict[str, float] = {}  # node_id -> last job completion time
        self.jobs_started_at: dict[str, dict[str, float]] = {}  # node_id -> {job_id: start_time}

        # NAT/relay support (for nodes without inbound connectivity).
        # NAT-blocked nodes poll a relay endpoint for commands; the leader enqueues
        # commands keyed by node_id.
        self.last_inbound_heartbeat: float = 0.0
        self.last_relay_heartbeat: float = 0.0
        self.relay_command_queue: dict[str, list[dict[str, Any]]] = {}
        self.pending_relay_acks: set[str] = set()
        self.pending_relay_results: list[dict[str, Any]] = []
        self.relay_command_attempts: dict[str, int] = {}
        # Background tasks list for graceful shutdown (Dec 2025)
        self._background_tasks: list[asyncio.Task] = []

        # SAFEGUARDS - Rate limiting and coordinator integration (added 2025-12-15)
        self.spawn_timestamps: list[float] = []  # Timestamps of recent process spawns
        self.agent_mode = AGENT_MODE_ENABLED
        self.coordinator_url = COORDINATOR_URL
        self.last_coordinator_check: float = 0.0
        self.coordinator_available: bool = False
        logger.info(f"Safeguards: rate_limit={SPAWN_RATE_LIMIT_PER_MINUTE}/min, "
              f"load_max={LOAD_AVERAGE_MAX_MULTIPLIER}x, agent_mode={self.agent_mode}")

        # Load persisted state
        self._load_state()
        # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
        # Note: This is called during __init__ so ULSM may not exist yet, but _set_leader handles that
        if self.leader_id == self.node_id:
            self._set_leader(self.node_id, reason="startup_restore_leadership", save_state=False)

        # Self info
        self.self_info = self._create_self_info()

        # Phase 1 Refactoring: NodeSelector for node ranking/selection
        self.node_selector = NodeSelector(
            get_peers=lambda: self.peers,
            get_self_info=lambda: self.self_info,
            peers_lock=self.peers_lock,
            get_training_jobs=lambda: self.training_jobs,
        )
        # December 2025: Subscribe to health events (HOST_OFFLINE, NODE_RECOVERED)
        # to track unhealthy nodes for filtering during selection
        # Note: NodeSelector uses its own subscribe_to_events (not mixin)
        self.node_selector.subscribe_to_events()

        # Phase 2A Refactoring: SyncPlanner for data synchronization
        # NOTE: request_peer_manifest is wired AFTER SyncPlanner creation
        # because _request_peer_manifest is a method on this class
        self.sync_planner = SyncPlanner(
            node_id=self.node_id,
            data_directory=self.get_data_directory(),
            get_peers=lambda: self.peers,
            get_self_info=lambda: self.self_info,
            peers_lock=self.peers_lock,
            is_leader=lambda: self._is_leader(),
            request_peer_manifest=lambda peer_id: self._request_peer_manifest_sync(peer_id),
            check_disk_capacity=lambda: check_disk_has_capacity(),
            config=SyncPlannerConfig(),
        )
        # December 2025: Subscribe to cluster events (LEADER_ELECTED, NODE_RECOVERED)
        # to invalidate cached manifests and trigger re-collection
        # Wave 7 Phase 1.1: Use retry mechanism for reliable subscription
        self.sync_planner.subscribe_to_events_with_retry()

        # Phase 2B Refactoring: SelfplayScheduler for priority-based selfplay allocation
        # All callbacks wired for full delegation (Dec 2025)
        self.selfplay_scheduler = SelfplayScheduler(
            get_cluster_elo_fn=lambda: self._get_cluster_elo_summary(),
            load_curriculum_weights_fn=lambda: self._load_curriculum_weights(),
            get_board_priority_overrides_fn=lambda: getattr(self, "board_priority_overrides", {}),
            # Backpressure callbacks (wired Dec 2025)
            should_stop_production_fn=should_stop_production if HAS_NEW_COORDINATION else None,
            should_throttle_production_fn=should_throttle_production if HAS_NEW_COORDINATION else None,
            get_throttle_factor_fn=get_throttle_factor if HAS_NEW_COORDINATION else None,
            # Resource targeting callbacks (wired Dec 2025)
            record_utilization_fn=record_utilization if HAS_NEW_COORDINATION else None,
            get_host_targets_fn=get_host_targets if HAS_NEW_COORDINATION else None,
            get_target_job_count_fn=get_target_job_count if HAS_NEW_COORDINATION else None,
            should_scale_up_fn=should_scale_up if HAS_NEW_COORDINATION else None,
            should_scale_down_fn=should_scale_down if HAS_NEW_COORDINATION else None,
            # Hardware-aware limits (wired Dec 2025)
            get_max_selfplay_for_node_fn=get_max_selfplay_for_node if HAS_HW_AWARE_LIMITS else None,
            get_hybrid_selfplay_limits_fn=get_hybrid_selfplay_limits if HAS_HW_AWARE_LIMITS else None,
            # Safeguards callback (wired Dec 2025) - halt selfplay during emergency
            is_emergency_active_fn=_safeguards.is_emergency_active if HAS_SAFEGUARDS and _safeguards else None,
            verbose=self.verbose,
        )
        # Subscribe to feedback loop events (December 2025)
        # Wave 7 Phase 1.1: Use retry mechanism for reliable subscription
        self.selfplay_scheduler.subscribe_to_events_with_retry()

        # Session 17.29: Seed game counts from canonical databases at startup
        # This enables bootstrap priority boosts for underserved configs immediately
        try:
            initial_game_counts = self._seed_selfplay_scheduler_game_counts_sync()
            if initial_game_counts:
                self.selfplay_scheduler.update_p2p_game_counts(initial_game_counts)
                logger.info(f"[P2P] Seeded SelfplayScheduler with {len(initial_game_counts)} config game counts from canonical DBs")
                for config_key, count in sorted(initial_game_counts.items(), key=lambda x: x[1]):
                    if count < 500:  # Log underserved configs
                        logger.info(f"[P2P] Underserved config: {config_key} = {count} games")
        except Exception as e:  # noqa: BLE001
            logger.debug(f"[P2P] Failed to seed initial game counts: {e}")

        # Phase 2B Refactoring: JobManager for job spawning and lifecycle
        self.job_manager = JobManager(
            ringrift_path=self.ringrift_path,
            node_id=self.node_id,
            peers=self.peers,
            peers_lock=self.peers_lock,
            active_jobs=self.active_jobs,
            jobs_lock=self.jobs_lock,
            improvement_loop_state=self.improvement_loop_state,
            distributed_tournament_state=self.distributed_tournament_state,
        )
        # December 2025: Subscribe to job-relevant events (HOST_OFFLINE, HOST_ONLINE)
        # Wave 7 Phase 1.1: Use retry mechanism for reliable subscription
        self.job_manager.subscribe_to_events_with_retry()

        # January 2026 Sprint 6: Wire job spawn verification between JobManager and SelfplayScheduler
        # This enables tracking of dispatched jobs to verify they actually start running
        self.job_manager.set_spawn_registration_callback(
            self.selfplay_scheduler.register_pending_spawn
        )
        self.selfplay_scheduler.set_job_status_callback(
            self.job_manager.get_job_status
        )
        logger.info("[P2P] Spawn verification wired: JobManager <-> SelfplayScheduler")

        # Phase 2B Refactoring: TrainingCoordinator for training dispatch and completion
        self.training_coordinator = TrainingCoordinator(
            ringrift_path=Path(self.ringrift_path),
            get_cluster_data_manifest=lambda: self.cluster_data_manifest,
            get_training_jobs=lambda: self.training_jobs,
            get_training_lock=lambda: self.training_lock,
            get_peers=lambda: self.peers,
            get_peers_lock=lambda: self.peers_lock,
            get_self_info=lambda: self.self_info,
            training_thresholds=self.training_thresholds,
            games_at_last_nnue_train=getattr(self, "games_at_last_nnue_train", None),
            games_at_last_cmaes_train=getattr(self, "games_at_last_cmaes_train", None),
            improvement_cycle_manager=getattr(self, "improvement_cycle_manager", None),
            auth_headers=lambda: self._auth_headers(),
            urls_for_peer=lambda node_id, endpoint: self._urls_for_peer(node_id, endpoint),
            save_state_callback=lambda: self._save_state(),
            has_voter_quorum=lambda: self._check_quorum_health(),  # Now returns QuorumHealthLevel for degraded-mode training
        )
        # December 2025: Subscribe to training-relevant events
        # (SELFPLAY_COMPLETE, DATA_SYNC_COMPLETED, EVALUATION_COMPLETED, REGRESSION_DETECTED)
        # Wave 7 Phase 1.1: Use retry mechanism for reliable subscription
        self.training_coordinator.subscribe_to_events_with_retry()

        # January 2026: Phase 1 P2P Orchestrator Deep Decomposition
        # JobOrchestrationManager handles job spawning, scaling, and cluster-wide coordination
        # NOTE: Using factory function which wires all callbacks automatically
        self.job_orchestration = create_job_orchestration_manager(self)
        logger.info("[P2P] JobOrchestrationManager initialized")

        # January 4, 2026: Phase 5 - WorkDiscoveryManager for multi-channel work discovery
        # This enables workers to find work even during leader elections or partitions
        self._initialize_work_discovery_manager()

        # December 2025: Wire feedback loops for self-improvement
        # This connects curriculum adjustments to Elo changes, evaluation results, etc.
        self._wire_feedback_loops()

        # December 2025: Subscribe to daemon status events for observability
        daemon_events_ok = self._subscribe_to_daemon_events()

        # December 2025: Subscribe to training feedback signals for dynamic orchestration
        feedback_signals_ok = self._subscribe_to_feedback_signals()

        # December 2025: Subscribe to manager lifecycle events for coordination
        manager_events_ok = self._subscribe_to_manager_events()

        # Dec 2025: Store subscription status for health checks via /status endpoint
        self._event_subscription_status = {
            "daemon_events": daemon_events_ok,
            "feedback_signals": feedback_signals_ok,
            "manager_events": manager_events_ok,
            "all_healthy": daemon_events_ok and feedback_signals_ok and manager_events_ok,
            "timestamp": time.time(),
        }

        # Log subscription status for debugging integration issues
        if self._event_subscription_status["all_healthy"]:
            logger.info("[P2P] Event subscriptions: daemon=, feedback=, manager=")
        else:
            logger.warning(
                f"[P2P] Event subscriptions incomplete: "
                f"daemon={'' if daemon_events_ok else ''}, "
                f"feedback={'' if feedback_signals_ok else ''}, "
                f"manager={'' if manager_events_ok else ''}"
            )

        # December 2025 (Wave 7 Phase 1.2): Critical subscription failure mode
        # These subscriptions are required for the training pipeline to function.
        # Without them, events like DATA_SYNC_COMPLETED and TRAINING_COMPLETED
        # won't trigger downstream actions, causing the pipeline to stall silently.
        CRITICAL_SUBSCRIPTION_GROUPS = ["manager_events"]  # Contains DATA_SYNC_COMPLETED, TRAINING_COMPLETED
        self._event_subscription_status["critical_failed"] = []

        for group in CRITICAL_SUBSCRIPTION_GROUPS:
            if not self._event_subscription_status.get(group, False):
                self._event_subscription_status["critical_failed"].append(group)

        if self._event_subscription_status["critical_failed"]:
            failed_groups = self._event_subscription_status["critical_failed"]
            msg = f"[P2P] CRITICAL: Event subscription groups failed: {failed_groups}"
            logger.critical(msg)

            # Optional: fail startup on critical subscription failure
            # Enable via environment variable for strict production deployments
            if os.environ.get("RINGRIFT_FAIL_ON_SUBSCRIPTION_FAILURE", "").lower() == "true":
                raise RuntimeError(
                    f"Critical event subscriptions failed: {failed_groups}. "
                    "Set RINGRIFT_FAIL_ON_SUBSCRIPTION_FAILURE=false to allow startup anyway."
                )

        # NOTE: _manager_health_status validation is deferred to after _loop_manager
        # initialization below (line ~2730). This avoids AttributeError on _loop_manager.

        print(
            f"[P2P] Initialized node {node_id} on {host}:{port} "
            f"(advertise {self.advertise_host}:{self.advertise_port})"
        )
        logger.info(f"RingRift path: {self.ringrift_path}")
        logger.info(f"Version: {self.build_version}")
        logger.info(f"Known peers: {self.known_peers}")
        if self.relay_peers:
            logger.info(f"Relay peers (forced relay mode): {list(self.relay_peers)}")
        if self.auth_token:
            logger.info(f"Auth: enabled via {AUTH_TOKEN_ENV}")
        else:
            logger.info(f"Auth: disabled (set {AUTH_TOKEN_ENV} to enable)")

        # Hybrid transport for HTTP/SSH fallback (self-healing Vast connectivity)
        self.hybrid_transport: HybridTransport | None = None
        if HAS_HYBRID_TRANSPORT:
            try:
                self.hybrid_transport = get_hybrid_transport()
                logger.info("HybridTransport: enabled (HTTP with SSH fallback for Vast)")
            except Exception as e:  # noqa: BLE001
                logger.info(f"HybridTransport: failed to initialize: {e}")

        # SWIM-based leaderless membership (gossip protocol)
        # This provides faster failure detection (<5s vs 60s+) and O(1) bandwidth
        self._swim_manager = get_swim_manager(node_id=node_id, bind_port=7947)
        self._swim_started = False

        # SyncRouter: Intelligent data routing with quality-based priority (December 2025)
        # Lazy-loaded to avoid import overhead on startup
        self._sync_router: SyncRouter | None = None
        self._sync_router_wired = False

        # Phase 4: LoopManager for extracted loops (Dec 2025)
        self._loop_manager: LoopManager | None = None
        self._loops_registered = False
        self._autonomous_queue_loop = None  # Jan 4, 2026: Phase 2 P2P Resilience
        self._quorum_crisis_loop = None  # Jan 2026: Aggressive peer discovery during quorum loss

        # Jan 11, 2026: Track startup time for voter health grace period
        # During the first STARTUP_GRACE_PERIOD seconds, we don't warn about offline voters
        # because heartbeats haven't been exchanged yet
        self._startup_time = time.time()

        # December 27, 2025: Validate manager health at startup
        # This catches initialization issues early rather than at first use
        # NOTE: Must be called AFTER _loop_manager is initialized (was causing AttributeError)
        self._manager_health_status = self._validate_manager_health()

    def _get_loop_manager(self) -> "LoopManager | None":
        """Get the LoopManager, initializing if needed."""
        if self._loop_manager is None:
            self._loop_manager = get_loop_manager()
        return self._loop_manager

    def _register_extracted_loops(self) -> bool:
        """Register extracted loops with the LoopManager.

        December 2025: Extracted loops run alongside inline loops during
        the gradual migration. Once verified, inline loops will be deprecated.
        """
        logger.info(f"[LoopManager] _register_extracted_loops called, already_registered={self._loops_registered}")
        if self._loops_registered:
            return True

        manager = self._get_loop_manager()
        logger.info(f"[LoopManager] Got manager: {manager}")
        if manager is None:
            logger.info("LoopManager: not available, using inline loops only")
            return False

        try:
            from scripts.p2p.loops import (
                QueuePopulatorLoop,
                EloSyncLoop,
                JobReaperLoop,
                JobReassignmentLoop,
                IdleDetectionLoop,
                AutoScalingLoop,
                SpawnVerificationLoop,
                PredictiveScalingLoop,
                WorkQueueMaintenanceLoop,
                NATManagementLoop,
                ManifestCollectionLoop,
                ValidationLoop,
                ModelSyncLoop,
                DataManagementLoop,
                HttpServerHealthLoop,
                # Phase 2 Decomposition (Jan 9, 2026): Unified context for loop callbacks
                OrchestratorContext,
            )

            # January 9, 2026 - Phase 2 P2P Deep Decomposition
            # Create unified context object that bundles all callbacks loops need.
            # This reduces constructor complexity and makes dependencies explicit.
            # Loops can use ctx.is_leader() instead of receiving 5-10 individual callbacks.
            ctx = OrchestratorContext.from_orchestrator(self)

            # QueuePopulatorLoop - maintains 50+ work items until 2000 Elo
            # Jan 3, 2026: CRITICAL FIX - Use _is_leader() instead of raw role check
            # The raw role check caused the queue populator to stay stopped even when
            # this node was the elected leader, because leader_id was set but role wasn't.
            queue_populator = QueuePopulatorLoop(
                get_role=lambda: NodeRole.LEADER if self._is_leader() else NodeRole.FOLLOWER,
                get_selfplay_scheduler=lambda: self.selfplay_scheduler,
                notifier=self.notifier,
            )
            manager.register(queue_populator)
            # Jan 5, 2026 (Session 17.41): Store loop reference so handler can access populator
            # The loop creates its populator lazily, so we store the loop and access
            # populator via loop.populator property when needed
            self._queue_populator_loop = queue_populator

            # EloSyncLoop - keeps unified_elo.db consistent across cluster
            # Jan 9, 2026: Now uses OrchestratorContext for unified callback access
            if ctx.elo_sync_manager is not None:
                elo_sync = EloSyncLoop(
                    get_elo_sync_manager=lambda: ctx.elo_sync_manager,
                    get_sync_in_progress=ctx.sync_in_progress or (lambda: False),
                )
                manager.register(elo_sync)

            # JobReaperLoop - enforces job timeouts and reassigns work
            # December 27, 2025: Fixed constructor to match JobReaperLoop signature
            # Previous version passed wrong parameters causing reaper to never run
            # Jan 9, 2026: Now uses OrchestratorContext for unified callback access
            job_reaper = JobReaperLoop(
                get_active_jobs=ctx.get_active_jobs,
                cancel_job=ctx.cancel_job,
                get_job_heartbeats=ctx.get_job_heartbeats,
            )
            manager.register(job_reaper)

            # IdleDetectionLoop - auto-assigns work to idle nodes
            # Jan 2, 2026: Added on_zombie_detected callback to kill stuck processes
            # Jan 9, 2026: Now uses OrchestratorContext for unified callback access
            idle_detection = IdleDetectionLoop(
                get_role=ctx.get_role,
                get_peers=ctx.get_peers,
                get_work_queue=ctx.get_work_queue,
                on_idle_detected=ctx.auto_start_selfplay,
                on_zombie_detected=ctx.handle_zombie_detected,
            )
            manager.register(idle_detection)

            # SpawnVerificationLoop - verifies dispatched jobs actually start running
            # January 2026 Sprint 6: Addresses 10-15% wasted capacity from unconfirmed spawns
            # Runs on leader only - verifies pending job spawns and emits events
            # Jan 9, 2026: Now uses OrchestratorContext for unified callback access
            if ctx.selfplay_scheduler is not None:
                spawn_verification = SpawnVerificationLoop(
                    verify_pending_spawns=ctx.selfplay_scheduler.verify_pending_spawns,
                    get_spawn_stats=lambda: {
                        "per_node": {
                            node_id: ctx.selfplay_scheduler.get_spawn_success_rate(node_id)
                            for node_id in list(ctx.get_peers().keys())[:20]  # Limit to avoid overhead
                        } if ctx.get_peers() else {},
                    },
                )
                manager.register(spawn_verification)
                logger.info("[P2P] SpawnVerificationLoop enabled for job spawn tracking")
            else:
                logger.debug("[P2P] SpawnVerificationLoop skipped: no selfplay_scheduler")

            # PredictiveScalingLoop - spawns jobs BEFORE nodes become idle
            # January 2026 Sprint 6: Addresses 5-10 minute launch latency from reactive-only scheduling
            # Monitors queue depth and node "approaching idle" status to spawn preemptively
            # Jan 9, 2026: Now uses OrchestratorContext for unified callback access
            predictive_scaling = PredictiveScalingLoop(
                get_role=ctx.get_role,
                get_peers=ctx.get_peers,
                get_queue_depth=ctx.get_work_queue_depth,
                get_pending_jobs_for_node=ctx.get_pending_jobs_for_node,
                spawn_preemptive_job=ctx.spawn_preemptive_job,
            )
            manager.register(predictive_scaling)
            logger.info("[P2P] PredictiveScalingLoop enabled for proactive job spawning")

            # JobReassignmentLoop - automatically reassigns orphaned jobs (P1 Sprint 6, Jan 2026)
            # Detects jobs without heartbeats and reassigns within 5 min (vs 1 hour for JobReaper)
            # Uses JobManager.process_stale_jobs() with exponential backoff
            # Jan 9, 2026: Now uses OrchestratorContext for unified callback access
            if ctx.job_manager is not None:
                job_reassignment = JobReassignmentLoop(
                    get_role=ctx.get_role,
                    check_and_reassign=ctx.job_manager.process_stale_jobs,
                    get_healthy_nodes=self._get_healthy_node_ids_for_reassignment,
                )
                manager.register(job_reassignment)
                logger.info("[P2P] JobReassignmentLoop enabled for faster orphaned job recovery")
            else:
                logger.debug("[P2P] JobReassignmentLoop skipped: no job_manager")

            # AutoScalingLoop - provision/deprovision cloud instances
            # December 28, 2025 (Wave 7 Phase 2.2): Enabled with CompositeScaleAdapter
            # Uses RELUCTANT TERMINATION - nodes only terminated after confirmed unusability
            try:
                from scripts.p2p.adapters.scale_adapters import (
                    CompositeScaleAdapter,
                    AutoScalingConfig,
                    create_scale_adapter,
                )

                # Create scale adapter with conservative (reluctant) configuration
                scale_adapter = create_scale_adapter(
                    work_queue=get_work_queue(),
                    peers_getter=lambda: self.peers,
                    config=AutoScalingConfig.conservative(),  # Reluctant termination
                )

                # Store adapter for job activity tracking
                self._scale_adapter = scale_adapter

                auto_scaling = AutoScalingLoop(
                    get_pending_work=scale_adapter.get_pending_work,
                    get_active_nodes=scale_adapter.get_active_nodes,
                    get_idle_nodes=scale_adapter.get_idle_nodes,
                    scale_up=scale_adapter.scale_up,
                    scale_down=scale_adapter.scale_down,
                )
                manager.register(auto_scaling)
                logger.info("[P2P] AutoScalingLoop enabled with reluctant termination")
            except ImportError as e:
                logger.debug(f"[P2P] AutoScalingLoop disabled: {e}")
                self._scale_adapter = None
            except Exception as e:  # noqa: BLE001
                logger.warning(f"[P2P] AutoScalingLoop initialization failed: {e}")
                self._scale_adapter = None

            # WorkQueueMaintenanceLoop - leader cleans up timeouts and old items
            # December 27, 2025: Migrated from inline _work_queue_maintenance_loop
            # Jan 9, 2026: Now uses OrchestratorContext for unified callback access
            work_queue_maint = WorkQueueMaintenanceLoop(
                is_leader=ctx.is_leader,
                get_work_queue=ctx.get_work_queue,
            )
            manager.register(work_queue_maint)

            # NATManagementLoop - STUN-like probing, symmetric NAT detection, relay selection
            # December 27, 2025: Migrated from inline _nat_management_loop
            # December 30, 2025: Added validate_relay_assignments for automatic failover
            nat_management = NATManagementLoop(
                detect_nat_type=self._detect_nat_type,
                probe_nat_blocked_peers=self._probe_nat_blocked_peers,
                update_relay_preferences=self._update_relay_preferences,
                validate_relay_assignments=self._validate_relay_assignments,
            )
            manager.register(nat_management)

            # ManifestCollectionLoop - periodic manifest collection for dashboard/training/sync
            # December 27, 2025: Migrated from inline _manifest_collection_loop
            # Jan 9, 2026: Now uses OrchestratorContext for get_role
            # Jan 14, 2026: Added broadcast to followers for cluster-wide data visibility
            manifest_collection = ManifestCollectionLoop(
                get_role=ctx.get_role,
                collect_cluster_manifest=self._collect_cluster_manifest,
                collect_local_manifest=self._collect_local_data_manifest,
                update_manifest=self._update_manifest_from_loop,
                update_improvement_cycle=self._update_improvement_cycle_from_loop,
                record_stats_sample=self._record_selfplay_stats_sample,
                get_alive_peers=self._get_alive_peers_for_broadcast,
                get_http_session=lambda: self.http_session,
                broadcast_enabled=True,
            )
            manager.register(manifest_collection)

            # DataManagementLoop - automatic data pipeline management
            # December 27, 2025: Migrated from inline _data_management_loop
            def _check_disk_capacity_wrapper(threshold: float) -> tuple[bool, float]:
                """Wrapper for check_disk_has_capacity."""
                try:
                    return check_disk_has_capacity(threshold)
                except Exception as e:
                    logger.debug(f"Disk capacity check error: {e}")
                    return True, 0.0  # Assume OK on error

            async def _check_db_integrity_wrapper(data_dir: Path) -> dict[str, int]:
                """Wrapper for check_and_repair_databases."""
                try:
                    return check_and_repair_databases(
                        data_dir=data_dir,
                        auto_repair=False,
                        log_prefix="[P2P]"
                    )
                except Exception as e:
                    logger.debug(f"DB integrity check error: {e}")
                    return {"checked": 0, "corrupted": 0, "failed": 0}

            # Jan 9, 2026: Use context for is_leader, remaining callbacks are specialized
            data_management = DataManagementLoop(
                is_leader=ctx.is_leader,
                check_disk_capacity=_check_disk_capacity_wrapper,
                cleanup_disk=self._cleanup_local_disk,
                convert_jsonl_to_db=self._convert_jsonl_to_db,
                convert_jsonl_to_npz=self._convert_jsonl_to_npz_for_training,
                check_db_integrity=_check_db_integrity_wrapper,
                trigger_export=self._trigger_export_for_loop,
                start_training=self._start_auto_training,
                get_data_dir=self.get_data_directory,
                get_games_dir=lambda: self.get_data_directory() / "games",
                get_training_dir=lambda: self.get_data_directory() / "training",
                is_gpu_node=lambda: self.self_info.is_gpu_node() if self.self_info else False,
                has_training_jobs=lambda: (self.self_info.training_jobs > 0) if self.self_info else False,
            )
            manager.register(data_management)

            # ModelSyncLoop - syncs NN/NNUE models across cluster
            # December 27, 2025: Migrated from inline _model_sync_loop
            # Note: The inline version is more complex (thread pool, batch sync, event emission)
            # This registration uses the simpler ModelSyncLoop pattern with wrapper callbacks
            if HAS_MODEL_SYNC and HAS_HOSTS_FOR_SYNC:
                async def _get_node_models_for_sync(node_id: str) -> dict[str, str]:
                    """Get model versions on a specific node."""
                    # Use manifest data if available
                    if hasattr(self, 'cluster_data_manifest') and self.cluster_data_manifest:
                        node_data = self.cluster_data_manifest.get(node_id, {})
                        models = node_data.get("models", {})
                        return {k: v.get("version", "") for k, v in models.items()} if isinstance(models, dict) else {}
                    return {}

                async def _sync_model_to_node(node_id: str, config_key: str, source_path: str) -> bool:
                    """Sync a model to a specific node."""
                    try:
                        # Use rsync for model distribution
                        peer = self.peers.get(node_id)
                        if not peer:
                            return False
                        host = peer.get("ip") or peer.get("host", "")
                        if not host:
                            return False
                        # Use sync_models infrastructure
                        cmd = [
                            "rsync", "-az", "--timeout=60",
                            source_path,
                            f"{host}:{source_path}",
                        ]
                        proc = await asyncio.create_subprocess_exec(
                            *cmd,
                            stdout=asyncio.subprocess.DEVNULL,
                            stderr=asyncio.subprocess.DEVNULL,
                        )
                        await proc.wait()
                        return proc.returncode == 0
                    except (asyncio.TimeoutError, OSError, subprocess.SubprocessError, ValueError):
                        return False

                model_sync = ModelSyncLoop(
                    get_model_versions=lambda: getattr(self, '_model_versions', {}),
                    get_node_models=_get_node_models_for_sync,
                    sync_model=_sync_model_to_node,
                    get_active_nodes=lambda: list(self.peers.keys()) if self.peers else [],
                )
                manager.register(model_sync)
            else:
                logger.debug("ModelSyncLoop: skipped (model sync not available)")

            # ModelFetchLoop - fetch trained models FROM training nodes TO coordinator
            # December 2025: Critical fix for model distribution gap
            # Training nodes (nebius-h100-*, lambda-gh200-*) produce models that need
            # to be fetched to the coordinator for evaluation/promotion/distribution.
            try:
                from scripts.p2p.loops.data_loops import ModelFetchLoop

                # Track which jobs have had models fetched
                _fetched_model_jobs: set[str] = set()

                def _get_completed_training_jobs() -> list:
                    """Get completed training jobs that need model fetch."""
                    with self.training_jobs_lock:
                        return [
                            job for job in self.training_jobs.values()
                            if getattr(job, "status", "") == "completed"
                        ]

                async def _fetch_model_for_job(job) -> bool:
                    """Fetch model from training node."""
                    try:
                        return await self.training_coordinator._fetch_model_from_training_node(job)
                    except (AttributeError, TypeError) as e:
                        logger.debug(f"ModelFetchLoop: fetch error: {e}")
                        return False

                def _mark_model_fetched(job_id: str) -> None:
                    """Mark a job's model as fetched."""
                    _fetched_model_jobs.add(job_id)

                def _is_model_fetched(job_id: str) -> bool:
                    """Check if model was already fetched."""
                    return job_id in _fetched_model_jobs

                # Jan 9, 2026: Now uses OrchestratorContext for is_leader
                model_fetch = ModelFetchLoop(
                    is_leader=ctx.is_leader,
                    get_completed_training_jobs=_get_completed_training_jobs,
                    fetch_model=_fetch_model_for_job,
                    mark_model_fetched=_mark_model_fetched,
                    is_model_fetched=_is_model_fetched,
                )
                manager.register(model_fetch)
                logger.info("ModelFetchLoop: registered for training node model fetching")
            except (ImportError, TypeError, ValueError, AttributeError) as e:
                logger.debug(f"ModelFetchLoop: skipped ({e})")

            # ValidationLoop - automatic model validation scheduling
            # December 27, 2025: Migrated from inline _validation_loop
            def _get_model_registry():
                try:
                    from app.training.model_registry import ModelRegistry
                    return ModelRegistry()
                except (ImportError, TypeError, ValueError) as e:
                    logger.debug(f"ValidationLoop: registry not available: {e}")
                    return None

            async def _send_validation_notification(msg: str, severity: str, context: dict) -> None:
                await self.notifier.send(msg, severity=severity, context=context)

            # Jan 9, 2026: Now uses OrchestratorContext for is_leader
            validation = ValidationLoop(
                is_leader=ctx.is_leader,
                get_model_registry=_get_model_registry,
                get_work_queue=get_work_queue,
                send_notification=_send_validation_notification,
            )
            manager.register(validation)

            # TailscalePeerDiscoveryLoop - December 27, 2025
            # Proactively discovers and connects to Tailscale nodes not in P2P network
            # Extracted from _tailscale_peer_recovery_loop (note: different from TailscaleRecoveryLoop
            # which handles service recovery; this handles peer discovery)
            def _get_current_peer_ids() -> set[str]:
                """Get node IDs of current P2P peers."""
                # Jan 2026: Use lock-free PeerSnapshot for read-only access
                return {p.node_id for p in self._peer_snapshot.get_snapshot().values()}

            def _get_alive_peer_count() -> int:
                """Count peers that are currently alive."""
                # Jan 2026: Use lock-free PeerSnapshot for read-only access
                return sum(1 for p in self._peer_snapshot.get_snapshot().values() if p.is_alive())

            async def _probe_and_connect_peer(ip: str, hostname: str) -> bool:
                """Probe peer health endpoint and send heartbeat to connect."""
                try:
                    url = f"http://{ip}:{DEFAULT_PORT}/health"
                    timeout = ClientTimeout(total=10)
                    async with get_client_session(timeout) as session:
                        async with session.get(url) as resp:
                            data, error = await safe_json_response(resp, default={}, log_errors=False)
                            if error:
                                logger.debug(f"TailscalePeerDiscovery: {hostname} response error: {error}")
                                return False
                            node_id = data.get("node_id", hostname)
                            logger.debug(f"TailscalePeerDiscovery: connected to {node_id}, sending heartbeat")
                            await self._send_heartbeat_to_peer(ip, DEFAULT_PORT)
                            return True
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"TailscalePeerDiscovery: failed to connect to {hostname}: {e}")
                return False

            try:
                from scripts.p2p.loops import TailscalePeerDiscoveryLoop
                # Jan 9, 2026: Now uses OrchestratorContext for is_leader
                ts_peer_discovery = TailscalePeerDiscoveryLoop(
                    is_leader=ctx.is_leader,
                    get_current_peers=_get_current_peer_ids,
                    get_alive_peer_count=_get_alive_peer_count,
                    probe_and_connect=_probe_and_connect_peer,
                )
                manager.register(ts_peer_discovery)
                logger.info("[LoopManager] TailscalePeerDiscoveryLoop registered")
            except (ImportError, TypeError) as e:
                logger.warning(f"[LoopManager] TailscalePeerDiscoveryLoop: not available: {e}")

            # PeerCleanupLoop - December 30, 2025
            # Automatically purges stale peers to maintain accurate health ratio
            # Prevents false "isolated" partition status from accumulated stale peers
            try:
                from scripts.p2p.loops import PeerCleanupLoop, PeerCleanupConfig

                async def _purge_single_peer_async(node_id: str) -> bool:
                    """Purge a single peer from the peers dict."""
                    with self.peers_lock:
                        if node_id in self.peers:
                            del self.peers[node_id]
                            logger.debug(f"[PeerCleanup] Removed peer from dict: {node_id}")
                            # Jan 12, 2026: Sync to lock-free snapshot after peer removal
                            self._sync_peer_snapshot()
                            return True
                    return False

                peer_cleanup = PeerCleanupLoop(
                    get_all_peers=lambda: dict(self.peers),
                    purge_peer=_purge_single_peer_async,
                    # emit_event omitted - optional, uses default None
                    config=PeerCleanupConfig(
                        cleanup_interval_seconds=float(
                            os.environ.get("RINGRIFT_PEER_CLEANUP_INTERVAL", "300")
                        ),
                        enabled=os.environ.get(
                            "RINGRIFT_PEER_CLEANUP_ENABLED", "1"
                        ).lower() in ("1", "true", "yes", "on"),
                    ),
                )
                manager.register(peer_cleanup)
                logger.info("[LoopManager] PeerCleanupLoop registered")
            except (ImportError, TypeError, AttributeError) as e:
                logger.warning(f"[LoopManager] PeerCleanupLoop: not available: {e}")

            # GossipStateCleanupLoop - January 7, 2026
            # TTL-based cleanup for unbounded gossip data structures
            # Prevents memory leaks causing OOM kills (fixes vultr-a100-20gb crash loop)
            try:
                from scripts.p2p.loops import GossipStateCleanupLoop, GossipStateCleanupConfig

                gossip_cleanup = GossipStateCleanupLoop(
                    get_orchestrator=lambda: self,
                    emit_event=self._safe_emit_p2p_event,
                    config=GossipStateCleanupConfig(),
                )
                manager.register(gossip_cleanup)
                logger.info("[LoopManager] GossipStateCleanupLoop registered")
            except (ImportError, TypeError, AttributeError) as e:
                logger.warning(f"[LoopManager] GossipStateCleanupLoop: not available: {e}")

            # QuorumCrisisDiscoveryLoop - January 2026
            # Aggressive peer discovery during quorum loss to speed up recovery
            # Reduces bootstrap interval from 60s to 10s when quorum drops
            try:
                from scripts.p2p.loops import QuorumCrisisDiscoveryLoop, QuorumCrisisConfig

                def _get_bootstrap_seeds_for_crisis() -> list[str]:
                    """Get bootstrap seed addresses for crisis discovery."""
                    seeds = getattr(self, "bootstrap_seeds", [])
                    return list(seeds) if seeds else []

                def _get_voter_endpoints_for_crisis() -> list[tuple[str, int]]:
                    """Get voter IP:port tuples for direct probing."""
                    voters = []
                    for peer_id, peer_info in self.state_manager.get_peers().items():
                        if hasattr(peer_info, "is_voter") and peer_info.is_voter:
                            ip = getattr(peer_info, "tailscale_ip", None) or getattr(peer_info, "ip", None)
                            port = getattr(peer_info, "port", 8770)
                            if ip:
                                voters.append((ip, port))
                    return voters

                async def _probe_endpoint_for_crisis(addr: str) -> bool:
                    """Probe a bootstrap seed for health."""
                    try:
                        return await self._probe_peer_health(addr)
                    except Exception:
                        return False

                async def _on_peer_discovered_during_crisis(peer_id: str, addr: str) -> None:
                    """Handle peer discovered during crisis mode."""
                    logger.info(f"[QuorumCrisis] Peer discovered: {peer_id} @ {addr}")
                    # Trigger immediate gossip sync if peer is new
                    if peer_id not in self.state_manager.get_peers():
                        await self._bootstrap_from_peer(addr)

                quorum_crisis = QuorumCrisisDiscoveryLoop(
                    get_bootstrap_seeds=_get_bootstrap_seeds_for_crisis,
                    get_voter_endpoints=_get_voter_endpoints_for_crisis,
                    probe_endpoint=_probe_endpoint_for_crisis,
                    on_peer_discovered=_on_peer_discovered_during_crisis,
                    emit_event=self._safe_emit_p2p_event,
                    config=QuorumCrisisConfig(),
                )
                manager.register(quorum_crisis)
                self._quorum_crisis_loop = quorum_crisis  # Store reference for event handling
                logger.info("[LoopManager] QuorumCrisisDiscoveryLoop registered")
            except (ImportError, TypeError, AttributeError) as e:
                logger.warning(f"[LoopManager] QuorumCrisisDiscoveryLoop: not available: {e}")

            # WorkerPullLoop - December 27, 2025
            # Workers poll leader for work (pull model instead of push)
            # Extracted from _worker_pull_loop
            def _get_self_metrics_for_pull() -> dict[str, Any]:
                """Get self node metrics for idle detection and slot-based work claiming.

                Jan 2, 2026: Added selfplay_jobs and max_selfplay_slots for slot-based
                capacity management. WorkerPullLoop uses these to determine if the node
                has capacity for more work, even when legacy selfplay is running.
                """
                return {
                    "gpu_percent": getattr(self.self_info, "gpu_percent", 0),
                    "cpu_percent": getattr(self.self_info, "cpu_percent", 0),
                    "training_jobs": getattr(self.self_info, "training_jobs", 0),
                    "has_gpu": getattr(self.self_info, "has_gpu", False),
                    # Jan 2, 2026: Slot-based capacity management
                    "selfplay_jobs": getattr(self.self_info, "selfplay_jobs", 0),
                    "max_selfplay_slots": getattr(self.self_info, "max_selfplay_slots", 8),
                }

            try:
                from scripts.p2p.loops import WorkerPullLoop

                # Jan 4, 2026: Autonomous queue fallback (Phase 2 P2P Resilience)
                async def _pop_autonomous_work_for_pull() -> dict[str, Any] | None:
                    """Get work from autonomous queue when leader unavailable."""
                    loop = getattr(self, "_autonomous_queue_loop", None)
                    if loop and hasattr(loop, "pop_local_work"):
                        try:
                            return await loop.pop_local_work()
                        except Exception as e:
                            logger.debug(f"[WorkerPull] Autonomous queue pop failed: {e}")
                    return None

                # Jan 4, 2026: Phase 5 - Add WorkDiscoveryManager for multi-channel work discovery
                def _get_work_discovery_manager_for_pull():
                    """Get WorkDiscoveryManager singleton if available."""
                    try:
                        from scripts.p2p.managers.work_discovery_manager import (
                            get_work_discovery_manager,
                        )
                        return get_work_discovery_manager()
                    except ImportError:
                        return None

                # Jan 6, 2026 (Session 17.41): Partition-aware backpressure callback
                def _set_work_queue_partition_state(is_partitioned: bool) -> None:
                    """Notify work queue of partition state for backpressure adjustment."""
                    wq = get_work_queue()
                    if wq and hasattr(wq, "set_partition_state"):
                        try:
                            wq.set_partition_state(is_partitioned)
                        except Exception as e:
                            logger.debug(f"[WorkerPull] Failed to set partition state: {e}")

                # Session 17.42: Split-brain detection - probe leader's /status endpoint
                async def _probe_leader_health_for_pull(leader_id: str) -> dict[str, Any] | None:
                    """Probe the claimed leader's /status endpoint to validate it's actually leading.

                    This detects split-brain scenarios where:
                    - This node thinks leader_id is the leader
                    - But leader_id doesn't claim to be leader (leader_id=null in its /status)

                    Args:
                        leader_id: The node ID we believe is the leader

                    Returns:
                        The leader's /status response if reachable, None otherwise
                    """
                    # Find peer info for the leader
                    with self.peers_lock:
                        peer = self.peers.get(leader_id)
                        if not peer:
                            logger.debug(f"[WorkerPull] Leader {leader_id} not in peer list")
                            return None

                        host = peer.host
                        port = peer.port

                    if not host or not port:
                        logger.debug(f"[WorkerPull] Leader {leader_id} has no host/port")
                        return None

                    try:
                        import aiohttp
                        async with aiohttp.ClientSession() as session:
                            async with session.get(
                                f"http://{host}:{port}/status",
                                timeout=aiohttp.ClientTimeout(total=5.0),
                            ) as resp:
                                if resp.status == 200:
                                    return await resp.json()
                                logger.debug(f"[WorkerPull] Leader {leader_id} /status returned {resp.status}")
                                return None
                    except Exception as e:
                        logger.debug(f"[WorkerPull] Leader {leader_id} probe failed: {e}")
                        return None

                # Jan 9, 2026: Use OrchestratorContext for leader state
                worker_pull = WorkerPullLoop(
                    is_leader=ctx.is_leader,
                    get_leader_id=ctx.get_leader_id,
                    get_self_metrics=_get_self_metrics_for_pull,
                    claim_work_from_leader=self._claim_work_from_leader,
                    execute_work=self._execute_claimed_work,
                    report_work_result=self._report_work_result,
                    pop_autonomous_work=_pop_autonomous_work_for_pull,
                    get_work_discovery_manager=_get_work_discovery_manager_for_pull,
                    # Session 17.34: Batch claiming for +30-40% utilization improvement
                    claim_work_batch_from_leader=self._claim_work_batch_from_leader,
                    # Session 17.42: Split-brain detection - validate leader is actually leading
                    probe_leader_health=_probe_leader_health_for_pull,
                    # Session 17.41: Partition-aware backpressure
                    set_partition_state=_set_work_queue_partition_state,
                )
                manager.register(worker_pull)
                logger.info("[LoopManager] WorkerPullLoop registered (with work discovery manager)")
            except (ImportError, TypeError) as e:
                logger.error(f"[LoopManager] WorkerPullLoop registration FAILED: {e}")
                logger.error("[LoopManager] This node will NOT claim work from leader!")

            # FollowerDiscoveryLoop - December 27, 2025
            # Simple peer list discovery for followers
            def _get_known_peer_addresses() -> list[str]:
                """Get list of known peer addresses for discovery.

                Returns alive peers from runtime list, falling back to bootstrap
                seeds when no peers are known yet.
                """
                # Jan 10, 2026: Copy-on-read - minimize lock hold time
                with self.peers_lock:
                    peers_snapshot = list(self.peers.values())
                addresses = [
                    f"{p.host}:{p.port}"
                    for p in peers_snapshot
                    if p.is_alive() and p.host and p.port
                ]

                # Fall back to bootstrap seeds if no alive peers known
                # This enables discovery when starting fresh
                if not addresses and self.known_peers:
                    addresses = list(self.known_peers)
                    logger.debug(f"No alive peers, using {len(addresses)} bootstrap seeds for discovery")

                return addresses

            async def _query_peer_list(peer_addr: str) -> list[str] | None:
                """Query a peer for its peer list."""
                try:
                    host, port_str = peer_addr.rsplit(":", 1)
                    port = int(port_str)
                    async with aiohttp.ClientSession() as session:
                        async with session.get(
                            f"http://{host}:{port}/peers",
                            timeout=aiohttp.ClientTimeout(total=5.0),
                        ) as resp:
                            if resp.status == 200:
                                data = await resp.json()
                                return data.get("peers", [])
                except (aiohttp.ClientError, ValueError, asyncio.TimeoutError):
                    pass
                return None

            def _add_discovered_peer(peer_addr: str) -> None:
                """Add a newly discovered peer address."""
                try:
                    host, port_str = peer_addr.rsplit(":", 1)
                    port = int(port_str)
                    # Queue for background probing
                    asyncio.create_task(
                        self._send_heartbeat_to_peer(host, port),
                        name=f"discover_peer_{peer_addr}",
                    )
                except ValueError:
                    logger.debug(f"Invalid peer address format: {peer_addr}")

            try:
                from scripts.p2p.loops import FollowerDiscoveryLoop
                # Jan 9, 2026: Now uses OrchestratorContext for is_leader
                follower_discovery = FollowerDiscoveryLoop(
                    get_known_peers=_get_known_peer_addresses,
                    query_peer_list=_query_peer_list,
                    add_peer=_add_discovered_peer,
                    is_leader=ctx.is_leader,
                )
                manager.register(follower_discovery)
            except (ImportError, TypeError) as e:
                logger.warning(f"[LoopManager] FollowerDiscoveryLoop: not available: {e}")

            # SelfHealingLoop - December 28, 2025
            # Migrated from inline _self_healing_loop (~71 LOC removed)
            # Recovers stuck jobs (leader) and cleans stale processes (all nodes)
            # Jan 2026: Added loop auto-restart for 48h autonomous operation
            try:
                from scripts.p2p.loops import SelfHealingLoop

                # Jan 2026: Callback to restart stopped loops - critical for 48h autonomy
                async def _restart_stopped_loops_callback() -> dict[str, bool]:
                    """Restart any stopped but enabled loops."""
                    lm = self._get_loop_manager()
                    if lm is not None:
                        return await lm.restart_stopped_loops()
                    return {}

                # Jan 9, 2026: Now uses OrchestratorContext for is_leader
                self_healing = SelfHealingLoop(
                    is_leader=ctx.is_leader,
                    get_health_manager=get_health_manager,
                    get_work_queue=get_work_queue,
                    cleanup_stale_processes=self._cleanup_stale_processes,
                    restart_stopped_loops=_restart_stopped_loops_callback,
                )
                manager.register(self_healing)
            except (ImportError, TypeError) as e:
                logger.debug(f"SelfHealingLoop: not available: {e}")

            # RemoteP2PRecoveryLoop - December 31, 2025
            # Automatically starts P2P on cluster nodes that should be running it
            # but aren't currently in the mesh.
            # Jan 2, 2026: Register on ALL nodes - the is_leader callback in the loop
            # already prevents non-leaders from doing any work. Previous restriction
            # to coordinator/voter nodes caused a bug where the leader (elected after
            # startup) didn't have the loop registered because voter_node_ids was
            # empty at loop registration time.
            try:
                from scripts.p2p.loops import RemoteP2PRecoveryLoop

                def _get_alive_peer_ids_for_recovery() -> list[str]:
                    """Get list of alive peer IDs."""
                    # Jan 10, 2026: Copy-on-read - minimize lock hold time
                    with self.peers_lock:
                        peers_snapshot = list(self.peers.values())
                    return [p.node_id for p in peers_snapshot if p.is_alive()]

                # Note: emit_event omitted - uses internal logging instead
                # P2POrchestrator doesn't have _emit_event but the loop is optional
                # Jan 2, 2026: Added is_leader callback - CRITICAL to prevent
                # all nodes from trying to restart each other simultaneously
                # Jan 9, 2026: Now uses OrchestratorContext for is_leader
                remote_recovery = RemoteP2PRecoveryLoop(
                    get_alive_peer_ids=_get_alive_peer_ids_for_recovery,
                    is_leader=ctx.is_leader,
                )
                manager.register(remote_recovery)
                logger.info(f"[LoopManager] RemoteP2PRecoveryLoop registered (node: {self.node_id}, leader-only execution)")
            except (ImportError, TypeError) as e:
                logger.debug(f"RemoteP2PRecoveryLoop: not available: {e}")

            # LeaderProbeLoop - January 4, 2026
            # Phase 5 of P2P Resilience: Fast leader health probe with forced election
            # Probes leader every 10s, triggers election after 6 consecutive failures (60s)
            try:
                from scripts.p2p.loops import LeaderProbeLoop
                leader_probe = LeaderProbeLoop(orchestrator=self)
                manager.register(leader_probe)
                logger.info("[LoopManager] LeaderProbeLoop registered (fast leader recovery)")
            except (ImportError, TypeError) as e:
                logger.debug(f"LeaderProbeLoop: not available: {e}")

            # RelayHealthLoop - January 15, 2026
            # Phase 3 of P2P Resilience Plan: Proactive relay health monitoring
            # Probes relay nodes every 30s, triggers failover after 5 consecutive failures
            try:
                from scripts.p2p.loops import RelayHealthLoop

                def _get_relay_nodes_for_loop() -> list[str]:
                    """Get list of active relay nodes from config."""
                    relay_nodes = []
                    try:
                        hosts = self.cluster_config.get("hosts", {})
                        for node_id, config in hosts.items():
                            if config.get("relay_capable", False) and config.get("status") == "active":
                                relay_nodes.append(node_id)
                    except Exception:
                        pass
                    return relay_nodes or ["hetzner-cpu1", "hetzner-cpu2", "hetzner-cpu3"]

                def _get_node_info_for_relay(node_id: str) -> dict[str, Any] | None:
                    """Get node info for relay health probing."""
                    try:
                        hosts = self.cluster_config.get("hosts", {})
                        return hosts.get(node_id)
                    except Exception:
                        return None

                def _get_nat_blocked_peers_for_relay() -> dict[str, str]:
                    """Get NAT-blocked peers and their primary relays."""
                    result: dict[str, str] = {}
                    try:
                        hosts = self.cluster_config.get("hosts", {})
                        for node_id, config in hosts.items():
                            relay_primary = config.get("relay_primary")
                            if relay_primary and config.get("status") == "active":
                                result[node_id] = relay_primary
                    except Exception:
                        pass
                    return result

                async def _trigger_relay_failover_for_loop(
                    peer_id: str, old_relay: str, new_relay: str
                ) -> bool:
                    """Trigger relay failover for a NAT-blocked peer.

                    Note: This is a notification mechanism. Actual relay updates
                    require config changes or dynamic relay selection by the peer.
                    """
                    logger.info(
                        f"[RelayHealth] Failover recommended: {peer_id} "
                        f"should switch from {old_relay} to {new_relay}"
                    )
                    # Emit event for monitoring/alerting
                    self._safe_emit_event("RELAY_FAILOVER_RECOMMENDED", {
                        "peer_id": peer_id,
                        "old_relay": old_relay,
                        "new_relay": new_relay,
                    })
                    return True

                relay_health = RelayHealthLoop(
                    get_relay_nodes=_get_relay_nodes_for_loop,
                    get_node_info=_get_node_info_for_relay,
                    get_nat_blocked_peers=_get_nat_blocked_peers_for_relay,
                    trigger_relay_failover=_trigger_relay_failover_for_loop,
                    emit_event=self._safe_emit_event,
                )
                manager.register(relay_health)
                logger.info("[LoopManager] RelayHealthLoop registered (NAT relay monitoring)")
            except (ImportError, TypeError) as e:
                logger.debug(f"RelayHealthLoop: not available: {e}")

            # PredictiveMonitoringLoop - December 28, 2025
            # Migrated from inline _predictive_monitoring_loop (~98 LOC removed)
            # Proactive monitoring and alerting for cluster health
            def _get_peers_for_monitoring() -> list[Any]:
                """Get list of alive peers for monitoring."""
                # Jan 10, 2026: Copy-on-read - minimize lock hold time
                with self.peers_lock:
                    peers_snapshot = list(self.peers.values())
                return [p for p in peers_snapshot if p.is_alive()]

            def _get_production_models() -> tuple[list[str], float]:
                """Get production model info for alert checks."""
                model_ids = []
                last_training = time.time() - 3600  # Default to 1 hour ago
                try:
                    from app.training.model_registry import ModelRegistry, ModelStage
                    registry = ModelRegistry()
                    production_models = registry.get_versions_by_stage(ModelStage.PRODUCTION)
                    model_ids = [f"{m['model_id']}_v{m['version']}" for m in production_models]
                    if production_models:
                        from datetime import datetime
                        latest_update = max(
                            datetime.fromisoformat(m['updated_at'].replace('Z', '+00:00'))
                            for m in production_models
                            if m.get('updated_at')
                        )
                        last_training = latest_update.timestamp()
                except Exception as e:
                    logger.debug(f"Model registry lookup failed: {e}")
                return model_ids, last_training

            try:
                from scripts.p2p.loops import PredictiveMonitoringLoop
                predictive_monitoring = PredictiveMonitoringLoop(
                    is_leader=lambda: self.role == NodeRole.LEADER,
                    get_alert_manager=get_predictive_alerts,
                    get_work_queue=get_work_queue,
                    get_peers=_get_peers_for_monitoring,
                    get_notifier=lambda: self.notifier,
                    get_production_models=_get_production_models,
                )
                manager.register(predictive_monitoring)
            except (ImportError, TypeError) as e:
                logger.debug(f"PredictiveMonitoringLoop: not available: {e}")

            # TrainingSyncLoop - December 28, 2025
            # CRITICAL for training: Syncs selfplay data to training nodes
            # Only runs when this node is the cluster leader
            # Emits DATA_SYNC_STARTED/COMPLETED/FAILED events for pipeline coordination
            try:
                from scripts.p2p.loops import TrainingSyncLoop

                # Jan 9, 2026: Use context for is_leader callback
                training_sync = TrainingSyncLoop(
                    is_leader=ctx.is_leader,
                    sync_to_training_nodes=self._sync_selfplay_to_training_nodes,
                    get_last_sync_time=lambda: getattr(self, 'last_training_sync_time', 0.0),
                    check_disk_capacity=lambda: check_disk_has_capacity(70.0),
                )
                manager.register(training_sync)
                logger.info("[P2P] TrainingSyncLoop registered (critical for training pipeline)")
            except (ImportError, TypeError) as e:
                logger.warning(f"TrainingSyncLoop: failed to register (CRITICAL): {e}")

            # DataAggregationLoop - December 28, 2025
            # IMPORTANT for training: Collects selfplay game databases from distributed nodes
            # Consolidates data to central storage for training pipeline
            try:
                from scripts.p2p.loops import DataAggregationLoop

                def _get_node_game_counts_for_aggregation() -> dict[str, int]:
                    """Get game counts per node from manifest."""
                    result: dict[str, int] = {}
                    if hasattr(self, 'cluster_data_manifest') and self.cluster_data_manifest:
                        for node_id, node_data in self.cluster_data_manifest.node_manifests.items():
                            # NodeDataManifest is a dataclass with selfplay_games field
                            if hasattr(node_data, 'selfplay_games'):
                                result[node_id] = node_data.selfplay_games
                            elif isinstance(node_data, dict):
                                # Fallback for dict representation
                                result[node_id] = node_data.get('selfplay_games', 0)
                            else:
                                result[node_id] = 0
                    return result

                async def _aggregate_from_node_for_loop(node_id: str) -> dict[str, Any]:
                    """Aggregate selfplay data from a specific node."""
                    try:
                        peer = self.peers.get(node_id)
                        if not peer:
                            return {"success": False, "error": "peer_not_found"}
                        # Use SyncPlanner if available
                        if hasattr(self, 'sync_planner') and self.sync_planner:
                            result = await self.sync_planner.sync_from_node(node_id)
                            return {"success": result, "node_id": node_id}
                        return {"success": False, "error": "sync_planner_unavailable"}
                    except Exception as e:
                        return {"success": False, "error": str(e)}

                data_aggregation = DataAggregationLoop(
                    get_node_game_counts=_get_node_game_counts_for_aggregation,
                    aggregate_from_node=_aggregate_from_node_for_loop,
                )
                manager.register(data_aggregation)
                logger.info("[P2P] DataAggregationLoop registered (important for training data)")
            except (ImportError, TypeError) as e:
                logger.warning(f"[LoopManager] DataAggregationLoop: not available (training sync affected): {e}")

            # HealthAggregationLoop - December 28, 2025
            # IMPORTANT for scheduling: Aggregates health metrics from all nodes
            # Provides cluster-wide health view for SelfplayScheduler decisions
            try:
                from scripts.p2p.loops import HealthAggregationLoop

                def _get_node_ids_for_health() -> list[str]:
                    """Get list of alive peer node IDs."""
                    # Jan 10, 2026: Copy-on-read - minimize lock hold time
                    with self.peers_lock:
                        peers_snapshot = list(self.peers.values())
                    return [p.node_id for p in peers_snapshot if p.is_alive()]

                async def _fetch_node_health_for_loop(node_id: str) -> dict[str, Any]:
                    """Fetch health metrics from a specific node."""
                    try:
                        peer = self.peers.get(node_id)
                        if not peer:
                            return {"healthy": False, "error": "peer_not_found"}
                        host = peer.host or peer.ip
                        port = peer.port or DEFAULT_PORT
                        url = f"http://{host}:{port}/health"
                        timeout = ClientTimeout(total=10)
                        async with get_client_session(timeout) as session:
                            async with session.get(url) as resp:
                                if resp.status == 200:
                                    data = await resp.json()
                                    return {
                                        "healthy": True,
                                        "cpu_percent": data.get("cpu_percent", 0),
                                        "memory_percent": data.get("memory_percent", 0),
                                        "gpu_percent": data.get("gpu_percent", 0),
                                        "disk_percent": data.get("disk_percent", 0),
                                    }
                                return {"healthy": False, "error": f"status_{resp.status}"}
                    except Exception as e:
                        return {"healthy": False, "error": str(e)}

                def _on_health_updated_callback(health_data: dict[str, dict[str, Any]]) -> None:
                    """Store aggregated health data for scheduling decisions."""
                    self._aggregated_node_health = health_data

                def _recover_unhealthy_nodes_callback() -> list[str]:
                    """Attempt to recover nodes from unhealthy state.

                    January 5, 2026 (Session 17.26): This callback fixes the critical issue
                    where nodes were permanently excluded from work distribution after
                    transient failures. The NodeSelector.recover_unhealthy_nodes() method
                    existed but was never called, causing cluster utilization to drop to
                    40-60% instead of 85%+.

                    Returns:
                        List of recovered node IDs.
                    """
                    if hasattr(self, "node_selector") and self.node_selector:
                        return self.node_selector.recover_unhealthy_nodes()
                    return []

                health_aggregation = HealthAggregationLoop(
                    get_node_ids=_get_node_ids_for_health,
                    fetch_node_health=_fetch_node_health_for_loop,
                    on_health_updated=_on_health_updated_callback,
                    on_recover_unhealthy=_recover_unhealthy_nodes_callback,
                )
                manager.register(health_aggregation)
                logger.info("[P2P] HealthAggregationLoop registered with auto-recovery (Session 17.26)")
            except (ImportError, TypeError) as e:
                logger.warning(f"[LoopManager] HealthAggregationLoop: not available: {e}")

            # IpDiscoveryLoop - December 28, 2025
            # Discovers and updates node IP addresses for dynamic cloud instances
            try:
                from scripts.p2p.loops import IpDiscoveryLoop

                def _get_nodes_for_ip_discovery() -> dict[str, dict[str, Any]]:
                    """Get node info dict for IP discovery."""
                    with self.peers_lock:
                        return {
                            p.node_id: {
                                "ip": p.host or p.ip,
                                "tailscale_ip": getattr(p, "tailscale_ip", None),
                                "public_ip": getattr(p, "public_ip", None),
                                "hostname": getattr(p, "hostname", None),
                            }
                            for p in self.peers.values()
                        }

                async def _update_node_ip_for_discovery(node_id: str, new_ip: str) -> None:
                    """Update a node's IP address."""
                    with self.peers_lock:
                        if node_id in self.peers:
                            peer = self.peers[node_id]
                            peer.host = new_ip
                            peer.ip = new_ip
                            logger.info(f"[IpDiscovery] Updated {node_id} IP to {new_ip}")

                ip_discovery = IpDiscoveryLoop(
                    get_nodes=_get_nodes_for_ip_discovery,
                    update_node_ip=_update_node_ip_for_discovery,
                )
                manager.register(ip_discovery)
                logger.info("[P2P] IpDiscoveryLoop registered (handles dynamic IPs)")
            except (ImportError, TypeError) as e:
                logger.debug(f"IpDiscoveryLoop: not available: {e}")

            # TailscaleRecoveryLoop - December 28, 2025
            # Monitors and recovers Tailscale connections
            try:
                from scripts.p2p.loops import TailscaleRecoveryLoop

                def _get_tailscale_status_for_recovery() -> dict[str, Any]:
                    """Get Tailscale status for all nodes."""
                    result = {}
                    with self.peers_lock:
                        for p in self.peers.values():
                            result[p.node_id] = {
                                "tailscale_state": getattr(p, "tailscale_state", "unknown"),
                                "tailscale_online": getattr(p, "tailscale_online", True),
                            }
                    return result

                async def _run_ssh_for_tailscale_recovery(node_id: str, cmd: str) -> Any:
                    """Run SSH command for Tailscale recovery."""
                    try:
                        peer = self.peers.get(node_id)
                        if not peer:
                            return type("Result", (), {"returncode": 1, "stdout": "", "stderr": "peer not found"})()
                        host = peer.host or peer.ip
                        proc = await asyncio.create_subprocess_exec(
                            "ssh", "-o", "ConnectTimeout=10", "-o", "StrictHostKeyChecking=no",
                            f"root@{host}", cmd,
                            stdout=asyncio.subprocess.PIPE,
                            stderr=asyncio.subprocess.PIPE,
                        )
                        stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=30)
                        return type("Result", (), {"returncode": proc.returncode, "stdout": stdout.decode(), "stderr": stderr.decode()})()
                    except (asyncio.TimeoutError, OSError) as e:
                        return type("Result", (), {"returncode": 1, "stdout": "", "stderr": str(e)})()

                tailscale_recovery = TailscaleRecoveryLoop(
                    get_tailscale_status=_get_tailscale_status_for_recovery,
                    run_ssh_command=_run_ssh_for_tailscale_recovery,
                )
                manager.register(tailscale_recovery)
                logger.info("[P2P] TailscaleRecoveryLoop registered (handles Tailscale failures)")
            except (ImportError, TypeError) as e:
                logger.debug(f"TailscaleRecoveryLoop: not available: {e}")

            # TailscaleKeepaliveLoop - January 2026
            # Keeps connections warm, especially for userspace/container nodes
            try:
                from scripts.p2p.loops import TailscaleKeepaliveLoop, TailscaleKeepaliveConfig

                def _get_peer_tailscale_ips_for_keepalive() -> dict[str, str]:
                    """Get Tailscale IPs for all peers."""
                    result = {}
                    with self.peers_lock:
                        for p in self.peers.values():
                            ts_ip = getattr(p, "tailscale_ip", None)
                            if ts_ip and ts_ip != "0.0.0.0":
                                result[p.node_id] = ts_ip
                    return result

                def _is_userspace_mode() -> bool:
                    """Check if running in userspace/container mode."""
                    # Container nodes typically don't have /dev/net/tun
                    try:
                        return not os.path.exists("/dev/net/tun")
                    except OSError:
                        return True  # Assume userspace if we can't check

                async def _on_connection_quality_change(node_id: str, ip: str, is_direct: bool) -> None:
                    """Log connection quality changes."""
                    conn_type = "direct" if is_direct else "DERP relay"
                    logger.info(f"[TailscaleKeepalive] {node_id} ({ip}): now using {conn_type}")

                keepalive_config = TailscaleKeepaliveConfig(
                    interval_seconds=60.0,  # Normal mode: ping every 60s
                    userspace_interval_seconds=30.0,  # Container mode: every 30s
                    derp_recovery_interval_seconds=180.0,  # Try to escape DERP every 3 min
                )

                tailscale_keepalive = TailscaleKeepaliveLoop(
                    get_peer_tailscale_ips=_get_peer_tailscale_ips_for_keepalive,
                    is_userspace_mode=_is_userspace_mode,
                    on_connection_quality_change=_on_connection_quality_change,
                    config=keepalive_config,
                )
                manager.register(tailscale_keepalive)
                is_userspace = _is_userspace_mode()
                logger.info(
                    f"[P2P] TailscaleKeepaliveLoop registered "
                    f"(userspace_mode={is_userspace}, interval={keepalive_config.userspace_interval_seconds if is_userspace else keepalive_config.interval_seconds}s)"
                )
            except (ImportError, TypeError) as e:
                logger.debug(f"TailscaleKeepaliveLoop: not available: {e}")

            # ClusterHealingLoop - January 2026
            # Automatically connects missing nodes from distributed_hosts.yaml
            try:
                from scripts.p2p.loops import ClusterHealingLoop, ClusterHealingConfig

                def _get_current_peer_ids() -> set[str]:
                    """Get set of known peer node_ids."""
                    with self.peers_lock:
                        return set(self.peers.keys())

                def _get_alive_peer_addresses() -> list[str]:
                    """Get list of alive peer HTTP addresses."""
                    # Jan 10, 2026: Copy-on-read - minimize lock hold time
                    with self.peers_lock:
                        peers_snapshot = list(self.peers.values())
                    return [
                        f"http://{p.tailscale_ip or p.host or p.ip}:{p.port or DEFAULT_PORT}"
                        for p in peers_snapshot
                        if p.is_alive() and (p.tailscale_ip or p.host or p.ip)
                    ]

                def _on_node_joined(node_id: str) -> None:
                    """Callback when a node is healed and joins."""
                    logger.info(f"[ClusterHealing] Node {node_id} joined the cluster")

                healing_config = ClusterHealingConfig(
                    check_interval_seconds=300.0,  # Check every 5 minutes
                    max_heal_per_cycle=5,  # Heal up to 5 nodes per cycle
                    ssh_timeout_seconds=30.0,
                    p2p_startup_wait_seconds=15.0,
                )

                cluster_healing = ClusterHealingLoop(
                    get_current_peers=_get_current_peer_ids,
                    get_alive_peer_addresses=_get_alive_peer_addresses,
                    emit_event=self._safe_emit_event if hasattr(self, "_safe_emit_event") else None,
                    on_node_joined=_on_node_joined,
                    config=healing_config,
                )
                manager.register(cluster_healing)
                logger.info(
                    f"[P2P] ClusterHealingLoop registered "
                    f"(interval={healing_config.check_interval_seconds}s, "
                    f"max_heal={healing_config.max_heal_per_cycle})"
                )
            except (ImportError, TypeError) as e:
                logger.debug(f"ClusterHealingLoop: not available: {e}")

            # UdpDiscoveryLoop - December 28, 2025
            # LAN peer discovery via UDP broadcast (useful for network partition recovery)
            try:
                from scripts.p2p.loops import UdpDiscoveryLoop

                def _get_known_peer_addrs_for_udp() -> list[str]:
                    """Get list of known peer addresses."""
                    with self.peers_lock:
                        return [
                            f"{p.host or p.ip}:{p.port or DEFAULT_PORT}"
                            for p in self.peers.values()
                            if p.host or p.ip
                        ]

                def _add_peer_from_udp_discovery(peer_addr: str) -> None:
                    """Add a peer discovered via UDP."""
                    try:
                        host, port_str = peer_addr.rsplit(":", 1)
                        port = int(port_str)
                        asyncio.create_task(
                            self._send_heartbeat_to_peer(host, port),
                            name=f"udp_discover_{peer_addr}",
                        )
                    except ValueError:
                        logger.debug(f"[UdpDiscovery] Invalid peer address: {peer_addr}")

                udp_discovery = UdpDiscoveryLoop(
                    get_node_id=lambda: self.node_id,
                    get_host=lambda: self.host or "0.0.0.0",
                    get_port=lambda: self.port,
                    get_known_peers=_get_known_peer_addrs_for_udp,
                    add_peer=_add_peer_from_udp_discovery,
                )
                manager.register(udp_discovery)
                logger.info("[P2P] UdpDiscoveryLoop registered (LAN peer discovery)")
            except (ImportError, TypeError) as e:
                logger.debug(f"UdpDiscoveryLoop: not available: {e}")

            # SplitBrainDetectionLoop - December 28, 2025
            # Detects multiple leaders (network partition) and emits alerts
            try:
                from scripts.p2p.loops import SplitBrainDetectionLoop

                def _get_peer_endpoint_for_split_brain(peer_id: str) -> str | None:
                    """Get HTTP endpoint for a peer."""
                    peer = self.peers.get(peer_id)
                    if not peer:
                        return None
                    host = peer.host or peer.ip
                    port = peer.port or DEFAULT_PORT
                    return f"http://{host}:{port}"

                async def _on_split_brain_detected_callback(leaders: list[str], epoch: int) -> None:
                    """Handle split-brain detection."""
                    logger.critical(
                        f"[SplitBrain] DETECTED: {len(leaders)} leaders in cluster: {leaders} "
                        f"(epoch={epoch})"
                    )
                    await self._emit_split_brain_detected(
                        detected_leaders=leaders,
                        resolution_action="election",
                    )

                split_brain_detection = SplitBrainDetectionLoop(
                    get_peers=lambda: dict(self.peers) if self.peers else {},
                    get_peer_endpoint=_get_peer_endpoint_for_split_brain,
                    get_own_leader_id=lambda: self.leader_id,
                    get_cluster_epoch=lambda: getattr(self, "cluster_epoch", 0),
                    on_split_brain_detected=_on_split_brain_detected_callback,
                )
                manager.register(split_brain_detection)
                logger.info("[P2P] SplitBrainDetectionLoop registered (partition detection)")
            except (ImportError, TypeError) as e:
                logger.debug(f"SplitBrainDetectionLoop: not available: {e}")

            # AutonomousQueuePopulationLoop - January 4, 2026 (Phase 2 P2P Resilience)
            # Fallback queue population when leader is unavailable or queue is starved.
            # Runs on ALL nodes, activates when:
            # - No leader for >5 minutes, OR
            # - Queue depth <10 for >2 minutes
            try:
                from scripts.p2p.loops import AutonomousQueuePopulationLoop

                autonomous_queue = AutonomousQueuePopulationLoop(
                    orchestrator=self,
                    config=None,  # Uses env-based config
                )
                manager.register(autonomous_queue)
                # Store reference for worker pull loop integration
                self._autonomous_queue_loop = autonomous_queue
                logger.info("[P2P] AutonomousQueuePopulationLoop registered (fallback queue population)")
            except (ImportError, TypeError) as e:
                logger.debug(f"AutonomousQueuePopulationLoop: not available: {e}")
                self._autonomous_queue_loop = None

            # HttpServerHealthLoop - January 7, 2026 (Zombie Detection)
            # Detects zombie state where HTTP server crashes but process continues.
            # Probes localhost:8770/health every 10s, terminates after 6 consecutive failures.
            # Jan 13, 2026: Now attempts graceful restart via restart_http_server callback
            # before falling back to os._exit() for systemd restart.
            try:
                http_health = HttpServerHealthLoop(
                    port=self.port,
                    restart_callback=self.restart_http_server,
                )
                manager.register(http_health)
                logger.info("[P2P] HttpServerHealthLoop registered (with graceful restart support)")
            except (ImportError, TypeError) as e:
                logger.debug(f"HttpServerHealthLoop: not available: {e}")

            # ComprehensiveEvaluationLoop - January 2026 (Model Evaluation Pipeline)
            # Runs every 6 hours on leader to ensure all models across the cluster are
            # evaluated under all compatible harnesses. Tracks (model, harness, config)
            # combinations and prioritizes unevaluated or stale combinations.
            try:
                from scripts.p2p.loops import (
                    ComprehensiveEvaluationLoop,
                    ComprehensiveEvaluationConfig,
                )
                comp_eval_loop = ComprehensiveEvaluationLoop(
                    get_role=lambda: self._role,
                    get_orchestrator=lambda: self,
                    config=ComprehensiveEvaluationConfig(
                        interval=6 * 3600,  # Every 6 hours
                        max_evaluations_per_cycle=50,
                        stale_threshold_days=7,
                        games_per_harness=50,
                        save_games=True,
                        register_with_elo=True,
                    ),
                )
                manager.register(comp_eval_loop)
                logger.info("[P2P] ComprehensiveEvaluationLoop registered (6-hour model evaluation cycle)")
            except (ImportError, TypeError) as e:
                logger.debug(f"ComprehensiveEvaluationLoop: not available: {e}")

            # TournamentDataPipelineLoop - January 2026 (Training Data Export)
            # Runs every hour on leader to discover tournament/gauntlet game databases,
            # apply quality gates, export to NPZ format, and trigger training events.
            try:
                from scripts.p2p.loops import (
                    TournamentDataPipelineLoop,
                    TournamentDataPipelineConfig,
                )
                tournament_pipeline = TournamentDataPipelineLoop(
                    get_role=lambda: self._role,
                    config=TournamentDataPipelineConfig(
                        interval=3600,  # Every hour
                        min_games_for_export=100,
                        quality_threshold=0.6,
                        emit_training_events=True,
                    ),
                )
                manager.register(tournament_pipeline)
                logger.info("[P2P] TournamentDataPipelineLoop registered (hourly tournament data export)")
            except (ImportError, TypeError) as e:
                logger.debug(f"TournamentDataPipelineLoop: not available: {e}")

            # CircuitBreakerDecayLoop - January 2026 (Session 17.x)
            # Prevents circuits from being stuck OPEN indefinitely after transient failures.
            # Decays old circuit breakers (operation, transport, node) with configurable TTL.
            # Default: 1 hour TTL, checked every 5 minutes.
            # Jan 20, 2026: Added external_alive_check for gossip-integrated recovery.
            try:
                from scripts.p2p.loops.maintenance_loops import (
                    CircuitBreakerDecayLoop,
                    CircuitBreakerDecayConfig,
                )

                cb_decay_config = CircuitBreakerDecayConfig(
                    enabled=os.environ.get("RINGRIFT_CB_DECAY_ENABLED", "1").lower() in ("1", "true", "yes"),
                    check_interval_seconds=float(os.environ.get("RINGRIFT_CB_DECAY_INTERVAL", "300")),
                    ttl_seconds=float(os.environ.get("RINGRIFT_CB_DECAY_TTL", "3600")),
                )

                cb_decay_loop = CircuitBreakerDecayLoop(config=cb_decay_config)
                # Jan 20, 2026: Wire up gossip-based alive check for faster circuit recovery
                cb_decay_loop.set_external_alive_check(self._is_peer_alive_for_circuit_breaker)
                manager.register(cb_decay_loop)
                self._cb_decay_loop = cb_decay_loop  # Store reference
                logger.info(
                    f"[P2P] CircuitBreakerDecayLoop registered "
                    f"(enabled={cb_decay_config.enabled}, ttl={cb_decay_config.ttl_seconds}s, external_check=True)"
                )
            except (ImportError, TypeError) as e:
                logger.debug(f"CircuitBreakerDecayLoop: not available: {e}")

            # VoterConfigSyncLoop - January 20, 2026
            # Detects voter config drift across cluster and auto-syncs from peers
            # with higher version numbers. Part of consensus-safe config sync system.
            try:
                from scripts.p2p.loops.voter_config_sync_loop import VoterConfigSyncLoop

                voter_config_sync = VoterConfigSyncLoop(orchestrator=self)
                manager.register(voter_config_sync)
                self._voter_config_sync_loop = voter_config_sync
                logger.info("[LoopManager] VoterConfigSyncLoop registered")
            except (ImportError, TypeError) as e:
                logger.debug(f"VoterConfigSyncLoop: not available: {e}")

            self._loops_registered = True
            logger.info(f"LoopManager: registered {len(manager.loop_names)} loops")
            return True

        except Exception as e:  # noqa: BLE001
            logger.error(f"LoopManager: failed to register loops: {e}")
            return False

    # =========================================================================
    # JobReaperLoop callbacks - December 27, 2025
    # =========================================================================

    def _get_all_active_jobs_for_reaper(self) -> dict[str, Any]:
        """Get all active jobs across all job types for the job reaper.

        Returns a flat dict of job_id -> job_info, where job_info includes:
        - started_at: timestamp when job started
        - claimed_at: timestamp when job was claimed (if applicable)
        - status: current job status
        - pid: process ID (for killing stuck processes)
        - node_id: which node is running the job
        """
        result: dict[str, Any] = {}
        with self.jobs_lock:
            for job_type, jobs in self.active_jobs.items():
                for job_id, job_info in jobs.items():
                    if isinstance(job_info, dict):
                        result[job_id] = {
                            **job_info,
                            "job_type": job_type,
                        }
                    else:
                        # Handle non-dict job objects (legacy)
                        result[job_id] = {
                            "job_id": job_id,
                            "job_type": job_type,
                            "status": getattr(job_info, "status", "unknown"),
                            "started_at": getattr(job_info, "started_at", 0),
                            "pid": getattr(job_info, "pid", None),
                        }
        return result

    async def _cancel_job_for_reaper(self, job_id: str) -> bool:
        """Cancel a job by ID for the job reaper.

        Jan 21, 2026: Enhanced to escalate SIGTERM -> SIGKILL for stuck processes.

        Attempts to:
        1. Kill the process with SIGTERM, wait 3s, then SIGKILL if still alive
        2. Update job status to 'cancelled'
        3. Remove from active jobs dict
        4. Emit TASK_ABANDONED event

        Returns True if job was successfully cancelled.
        """
        import os
        import signal

        with self.jobs_lock:
            # Find the job across all job types
            for job_type, jobs in self.active_jobs.items():
                if job_id in jobs:
                    job_info = jobs[job_id]
                    pid = job_info.get("pid") if isinstance(job_info, dict) else getattr(job_info, "pid", None)

                    # Kill the process if we have a PID
                    if pid:
                        process_killed = False
                        try:
                            # First try SIGTERM
                            os.kill(pid, signal.SIGTERM)
                            logger.info(f"[JobReaper] Sent SIGTERM to pid {pid} for job {job_id}")

                            # Wait up to 3 seconds for graceful termination
                            for _ in range(6):  # 6 x 0.5s = 3s
                                await asyncio.sleep(0.5)
                                try:
                                    # Check if process still exists (signal 0 = check only)
                                    os.kill(pid, 0)
                                except ProcessLookupError:
                                    # Process is dead
                                    process_killed = True
                                    logger.debug(f"[JobReaper] Process {pid} terminated gracefully")
                                    break

                            # If still alive after 3s, escalate to SIGKILL
                            if not process_killed:
                                try:
                                    os.kill(pid, signal.SIGKILL)
                                    logger.warning(
                                        f"[JobReaper] SIGTERM failed for pid {pid}, sent SIGKILL for job {job_id}"
                                    )
                                    # Wait briefly for SIGKILL to take effect
                                    await asyncio.sleep(0.5)
                                except ProcessLookupError:
                                    pass  # Died between check and kill

                        except ProcessLookupError:
                            logger.debug(f"[JobReaper] Process {pid} already dead for job {job_id}")
                        except OSError as e:
                            logger.warning(f"[JobReaper] Failed to kill pid {pid}: {e}")

                    # Update status and remove from active jobs
                    if isinstance(job_info, dict):
                        job_info["status"] = "reaped"
                    del jobs[job_id]

                    # Emit event for coordination (fire-and-forget async task)
                    try:
                        asyncio.create_task(self._emit_task_abandoned(
                            task_id=job_id,
                            task_type=job_type,
                            reason="reaped_by_job_reaper",
                            node_id=job_info.get("node_id", "") if isinstance(job_info, dict) else "",
                        ))
                    except RuntimeError:
                        pass  # No event loop running

                    logger.info(f"[JobReaper] Cancelled job {job_id} (type: {job_type})")
                    return True

        logger.debug(f"[JobReaper] Job {job_id} not found in active jobs")
        return False

    def _get_job_heartbeats_for_reaper(self) -> dict[str, float]:
        """Get job heartbeat timestamps for the job reaper.

        Returns dict of job_id -> last_heartbeat_time.
        Jobs without recent heartbeats may be considered abandoned.

        Phase 15.1.9 (Dec 29, 2025): Updated to use JobManager.get_job_heartbeats()
        for actual heartbeat tracking instead of just job start times.
        """
        result: dict[str, float] = {}

        # Phase 15.1.9: Get actual heartbeats from JobManager
        if hasattr(self, "job_manager") and self.job_manager is not None:
            try:
                job_heartbeats = self.job_manager.get_job_heartbeats()
                result.update(job_heartbeats)
            except Exception as e:  # noqa: BLE001
                logger.debug(f"Failed to get heartbeats from JobManager: {e}")

        # Fallback: Also include jobs_started_at for jobs without heartbeat tracking
        # This ensures older jobs (started before heartbeat tracking) are still monitored
        if hasattr(self, "jobs_started_at"):
            for _node_id, jobs in self.jobs_started_at.items():
                for job_id, start_time in jobs.items():
                    # Only add if not already in result from heartbeat tracking
                    if job_id not in result:
                        result[job_id] = start_time

        return result

    # =========================================================================
    # ManifestCollectionLoop callbacks - December 27, 2025
    # =========================================================================

    def _update_manifest_from_loop(self, manifest: Any, is_cluster: bool) -> None:
        """Update stored manifest from ManifestCollectionLoop.

        Args:
            manifest: The collected manifest (cluster or local)
            is_cluster: True if this is a cluster-wide manifest, False for local
        """
        import time
        with self.manifest_lock:
            if is_cluster:
                self.cluster_data_manifest = manifest
            else:
                self.local_data_manifest = manifest
            self.last_manifest_collection = time.time()

        # Session 17.29: Feed game counts to selfplay scheduler for priority allocation
        # ROOT CAUSE FIX: _p2p_game_counts was never populated, causing all configs
        # to show 0 games in queue populator, breaking bootstrap priority boosts
        if is_cluster and hasattr(self, 'selfplay_scheduler') and self.selfplay_scheduler:
            try:
                game_counts: dict[str, int] = {}
                if hasattr(manifest, 'by_board_type') and manifest.by_board_type:
                    for config_key, config_data in manifest.by_board_type.items():
                        if isinstance(config_data, dict):
                            game_counts[config_key] = config_data.get("total_games", 0)
                        elif hasattr(config_data, 'total_games'):
                            game_counts[config_key] = getattr(config_data, 'total_games', 0)
                if game_counts:
                    self.selfplay_scheduler.update_p2p_game_counts(game_counts)
                    logger.debug(f"[ManifestUpdate] Fed {len(game_counts)} config game counts to SelfplayScheduler")
            except Exception as e:  # noqa: BLE001
                logger.debug(f"[ManifestUpdate] Failed to update selfplay scheduler game counts: {e}")

    def _get_alive_peers_for_broadcast(self) -> list[Any]:
        """Get list of alive peers for manifest broadcast.

        Jan 2026: Added for leader broadcast functionality.

        Returns:
            List of NodeInfo objects for alive, non-retired peers
        """
        with self.peers_lock:
            alive_peers = []
            for peer_id, peer in self.peers.items():
                # Skip self
                if peer_id == self.node_id:
                    continue
                # Check if peer is alive
                is_alive = getattr(peer, 'is_alive', lambda: False)
                if callable(is_alive):
                    is_alive = is_alive()
                # Check if peer is retired
                is_retired = getattr(peer, 'retired', False)
                if is_alive and not is_retired:
                    alive_peers.append(peer)
            return alive_peers

    def _update_improvement_cycle_from_loop(self, by_board_type: dict[str, Any]) -> None:
        """Update ImprovementCycleManager from ManifestCollectionLoop.

        Args:
            by_board_type: Dict of board_type -> game counts from manifest
        """
        if self.improvement_cycle_manager:
            try:
                self.improvement_cycle_manager.update_from_cluster_totals(by_board_type)
            except Exception as e:  # noqa: BLE001
                logger.debug(f"ImprovementCycleManager update error: {e}")

    # =========================================================================
    # DataManagementLoop callbacks - December 27, 2025
    # =========================================================================

    async def _trigger_export_for_loop(
        self,
        db_path: Path,
        output_path: Path,
        board_type: str,
    ) -> bool:
        """Trigger export job for DataManagementLoop.

        Args:
            db_path: Path to database file to export
            output_path: Path for output NPZ file
            board_type: Board type (square8, hex8, etc.)

        Returns:
            True if export started successfully
        """
        import subprocess

        try:
            cmd = [
                sys.executable,
                self._get_script_path("export_replay_dataset.py"),
                "--db", str(db_path),
                "--board-type", board_type,
                "--num-players", "2",
                "--board-aware-encoding",
                "--require-completed",
                "--min-moves", "10",
                "--output", str(output_path),
            ]

            env = os.environ.copy()
            env["PYTHONPATH"] = self._get_ai_service_path()

            log_file = Path(f"/tmp/auto_export_{db_path.stem}.log")

            # Jan 19, 2026: Run subprocess in thread pool to avoid blocking event loop
            def _start_export_process():
                with open(log_file, "w") as log_fh:
                    subprocess.Popen(
                        cmd,
                        stdout=log_fh,
                        stderr=subprocess.STDOUT,
                        env=env,
                        cwd=self._get_ai_service_path(),
                    )

            await asyncio.to_thread(_start_export_process)
            logger.info(f"[DataManagement] Started export job for {db_path.name}")
            return True

        except Exception as e:
            logger.error(f"[DataManagement] Failed to start export for {db_path.name}: {e}")
            return False

    async def _inline_job_reaper_fallback_loop(self) -> None:
        """Inline job reaper fallback loop.

        December 27, 2025: Fallback implementation that runs if the extracted
        JobReaperLoop fails to start or hits persistent errors. Uses the same
        callbacks and thresholds as the extracted loop.

        This is NOT a replacement for JobReaperLoop - it's a safety net that
        ensures job cleanup continues even if the modular loop system fails.

        Thresholds:
        - STALE: Jobs older than 1 hour without heartbeat
        - STUCK: Jobs older than 2 hours regardless of heartbeat
        - INTERVAL: Checks every 5 minutes

        Environment:
        - RINGRIFT_JOB_REAPER_FALLBACK_ENABLED: Enable/disable (default: true)
        """
        STALE_THRESHOLD_SECONDS = 3600.0   # 1 hour
        STUCK_THRESHOLD_SECONDS = 7200.0   # 2 hours
        CHECK_INTERVAL_SECONDS = 300.0      # 5 minutes
        MAX_JOBS_PER_CYCLE = 10             # Limit to avoid overload

        logger.info("[JobReaper Fallback] Started inline fallback loop")
        stats = {"checks": 0, "reaped": 0, "errors": 0}

        while self.running:
            try:
                await asyncio.sleep(CHECK_INTERVAL_SECONDS)
                if not self.running:
                    break

                stats["checks"] += 1
                now = time.time()
                reaped_this_cycle = 0

                # Get all active jobs
                try:
                    active_jobs = self._get_all_active_jobs_for_reaper()
                except Exception as e:
                    logger.warning(f"[JobReaper Fallback] Failed to get active jobs: {e}")
                    stats["errors"] += 1
                    continue

                if not active_jobs:
                    continue

                # Get heartbeat info
                try:
                    heartbeats = self._get_job_heartbeats_for_reaper()
                except Exception as e:
                    logger.debug(f"[JobReaper Fallback] Failed to get heartbeats: {e}")
                    heartbeats = {}

                # Identify stale and stuck jobs
                jobs_to_reap: list[tuple[str, str]] = []  # [(job_id, reason), ...]

                for job_id, job_info in active_jobs.items():
                    if reaped_this_cycle >= MAX_JOBS_PER_CYCLE:
                        break

                    started_at = job_info.get("started_at", 0)
                    if not started_at:
                        continue

                    job_age = now - started_at
                    last_heartbeat = heartbeats.get(job_id, started_at)
                    heartbeat_age = now - last_heartbeat

                    # Check for stuck jobs (absolute age threshold)
                    if job_age > STUCK_THRESHOLD_SECONDS:
                        jobs_to_reap.append((job_id, "stuck"))
                        reaped_this_cycle += 1
                        continue

                    # Check for stale jobs (no recent heartbeat)
                    if heartbeat_age > STALE_THRESHOLD_SECONDS:
                        jobs_to_reap.append((job_id, "stale"))
                        reaped_this_cycle += 1

                # Reap identified jobs
                for job_id, reason in jobs_to_reap:
                    try:
                        success = await self._cancel_job_for_reaper(job_id)
                        if success:
                            stats["reaped"] += 1
                            logger.info(
                                f"[JobReaper Fallback] Reaped {reason} job {job_id} "
                                f"(total: {stats['reaped']})"
                            )
                    except Exception as e:
                        logger.warning(f"[JobReaper Fallback] Failed to reap {job_id}: {e}")
                        stats["errors"] += 1

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"[JobReaper Fallback] Unexpected error: {e}")
                stats["errors"] += 1
                await asyncio.sleep(60)  # Back off on error

        logger.info(
            f"[JobReaper Fallback] Stopped after {stats['checks']} checks, "
            f"{stats['reaped']} reaped, {stats['errors']} errors"
        )

    def _get_sync_router(self) -> SyncRouter | None:
        """Lazy-load SyncRouter singleton for intelligent sync routing."""
        if not HAS_SYNC_ROUTER:
            return None
        if self._sync_router is None:
            try:
                self._sync_router = get_sync_router()
                logger.info("SyncRouter: initialized for intelligent data routing")
            except Exception as e:  # noqa: BLE001
                logger.warning(f"SyncRouter: failed to initialize: {e}")
                return None
        return self._sync_router

    def _wire_sync_router_events(self) -> bool:
        """Wire SyncRouter to event system for real-time sync triggers."""
        if self._sync_router_wired:
            return True
        router = self._get_sync_router()
        if router is None:
            return False
        try:
            if hasattr(router, 'wire_to_event_router'):
                router.wire_to_event_router()
                self._sync_router_wired = True
                logger.info("SyncRouter: wired to event system")
                return True
        except Exception as e:  # noqa: BLE001
            logger.warning(f"SyncRouter: failed to wire events: {e}")
        return False

    def _initialize_work_discovery_manager(self) -> bool:
        """Initialize WorkDiscoveryManager for multi-channel work discovery.

        January 4, 2026: Phase 5 of P2P Cluster Resilience.
        Enables workers to find work through multiple channels:
        1. Leader work queue (fastest)
        2. Peer discovery (query other peers)
        3. Autonomous queue (from AutonomousQueueLoop)
        4. Direct selfplay (last resort)

        Returns True if initialization succeeded, False otherwise.
        """
        try:
            from scripts.p2p.managers.work_discovery_manager import (
                WorkDiscoveryManager,
                WorkDiscoveryConfig,
                set_work_discovery_manager,
            )

            # Create manager with callbacks to this orchestrator
            manager = WorkDiscoveryManager(
                # Channel 1: Leader
                get_leader_id=lambda: self.leader_id,
                claim_from_leader=self._claim_work_from_leader,
                # Channel 2: Peer discovery
                get_alive_peers=lambda: [
                    p.node_id for p in self.peers.values() if p.is_alive()
                ],
                query_peer_work=self._query_peer_for_work,
                # Channel 3: Autonomous queue
                pop_autonomous_work=self._pop_autonomous_queue_work,
                # Channel 4: Direct selfplay
                create_direct_selfplay_work=self._create_direct_selfplay_work,
                # Config from environment
                config=WorkDiscoveryConfig.from_env(),
            )

            # Set as singleton for WorkerPullLoop access
            set_work_discovery_manager(manager)
            logger.info("WorkDiscoveryManager: initialized with 4 discovery channels")
            return True

        except ImportError as e:
            logger.debug(f"WorkDiscoveryManager: not available: {e}")
            return False
        except Exception as e:  # noqa: BLE001
            logger.warning(f"WorkDiscoveryManager: initialization failed: {e}")
            return False

    async def _query_peer_for_work(
        self, peer_id: str, capabilities: list[str]
    ) -> dict[str, Any] | None:
        """Query a peer for available work (used by WorkDiscoveryManager).

        January 4, 2026: Phase 5 - Peer discovery channel.
        """
        try:
            peer = self.peers.get(peer_id)
            if not peer or not peer.is_alive():
                return None

            # Query peer's work queue via HTTP
            urls = self._urls_for_peer(peer_id, "/work_queue/claim")
            for url in urls:
                try:
                    async with aiohttp.ClientSession() as session:
                        async with session.post(
                            url,
                            json={"capabilities": capabilities},
                            headers=self._auth_headers(),
                            timeout=aiohttp.ClientTimeout(total=5.0),
                        ) as resp:
                            if resp.status == 200:
                                data = await resp.json()
                                if data.get("work_item"):
                                    return data["work_item"]
                except Exception:
                    continue
            return None
        except Exception:
            return None

    async def _pop_autonomous_queue_work(self) -> dict[str, Any] | None:
        """Pop work from autonomous queue (used by WorkDiscoveryManager).

        January 4, 2026: Phase 5 - Autonomous queue channel.
        """
        try:
            loop = getattr(self, "_autonomous_queue_loop", None)
            if loop and hasattr(loop, "pop_local_work"):
                return await loop.pop_local_work()
            return None
        except Exception:
            return None

    def _create_direct_selfplay_work(
        self, capabilities: list[str]
    ) -> dict[str, Any] | None:
        """Create direct selfplay work item (used by WorkDiscoveryManager).

        January 4, 2026: Phase 5 - Direct selfplay channel (last resort).
        Only used when all other channels fail.
        """
        if "selfplay" not in capabilities:
            return None

        try:
            # Get next config from selfplay scheduler
            config_key = self.selfplay_scheduler.get_next_config()
            if not config_key:
                return None

            return {
                "work_id": f"direct-{self.node_id}-{int(time.time())}",
                "work_type": "selfplay",
                "config_key": config_key,
                "source": "direct_discovery",
                "games": 10,  # Small batch for direct selfplay
                "priority": 50,  # Lower priority than leader-assigned work
            }
        except Exception:
            return None

    def _wire_feedback_loops(self) -> bool:
        """Wire curriculum feedback loops for self-improvement.

        December 2025: Connects P2P orchestrator to the training feedback system:
        - Curriculum weights adjust based on Elo velocity
        - Weak configs get boosted/penalized based on evaluation results
        - Quality scores influence exploration temperature
        - Failed promotions reduce config priority

        Returns True if wiring succeeded, False otherwise.
        """
        try:
            from app.coordination.curriculum_integration import wire_all_feedback_loops

            status = wire_all_feedback_loops()
            if status.get("success", False):
                wired_count = status.get("wired_count", 0)
                logger.info(f"Feedback loops: wired {wired_count} bridges successfully")
                return True
            else:
                error = status.get("error", "Unknown error")
                logger.warning(f"Feedback loops: partial wiring - {error}")
                return False
        except ImportError as e:
            logger.debug(f"Feedback loops: curriculum_integration not available: {e}")
            return False
        except Exception as e:  # noqa: BLE001
            logger.warning(f"Feedback loops: failed to wire: {e}")
            return False

    def _validate_manager_health(self) -> dict[str, Any]:
        """Validate health of all P2P managers at startup.

        December 28, 2025: Checks that all 8 managers initialized correctly and
        are healthy. This catches initialization issues early rather than
        at first use, improving debuggability.

        Managers validated:
        - state_manager: SQLite persistence and cluster epoch tracking
        - node_selector: Node ranking for job dispatch
        - sync_planner: Manifest collection and sync planning
        - selfplay_scheduler: Priority-based config selection
        - job_manager: Job spawning and lifecycle management
        - training_coordinator: Training dispatch and model promotion
        - loop_manager: Background loop orchestration (Dec 2025)
        - job_orchestration: Job spawning, scaling, cluster coordination (Jan 2026)

        Returns:
            dict with manager health status and overall healthy flag
        """
        managers = [
            ("state_manager", self.state_manager),
            ("node_selector", self.node_selector),
            ("sync_planner", self.sync_planner),
            ("selfplay_scheduler", self.selfplay_scheduler),
            ("job_manager", self.job_manager),
            ("training_coordinator", self.training_coordinator),
            # December 2025: Include LoopManager in health validation
            ("loop_manager", self._get_loop_manager()),
            # January 2026: Phase 1 Decomposition - JobOrchestrationManager
            ("job_orchestration", getattr(self, "job_orchestration", None)),
        ]

        status = {
            "managers": {},
            "all_healthy": True,
            "unhealthy_count": 0,
            "timestamp": time.time(),
        }

        # Jan 2026: Startup grace period - during first STARTUP_GRACE_PERIOD seconds,
        # treat unhealthy managers as "starting" instead of failing. This prevents
        # false alarms during initialization when managers are still subscribing to
        # events, connecting to databases, or waiting for first sync.
        uptime = time.time() - getattr(self, "start_time", time.time())
        in_grace_period = uptime < STARTUP_GRACE_PERIOD
        if in_grace_period:
            status["in_startup_grace_period"] = True
            status["grace_period_remaining"] = round(STARTUP_GRACE_PERIOD - uptime, 1)

        for name, manager in managers:
            try:
                if manager is None:
                    status["managers"][name] = {"status": "not_initialized", "error": "Manager is None"}
                    status["all_healthy"] = False
                    status["unhealthy_count"] += 1
                elif hasattr(manager, "health_check"):
                    health = manager.health_check()
                    # Handle both dict and HealthCheckResult return types
                    # Jan 2026: Fixed to accept "running" status from managers (CoordinatorStatus.RUNNING)
                    # Jan 2026: Added "starting" and "initializing" for startup grace period
                    healthy_statuses = ("healthy", "ready", "running", "starting", "initializing")
                    if hasattr(health, "status"):
                        is_healthy = str(health.status).lower() in healthy_statuses
                        health_status = str(health.status)
                    else:
                        is_healthy = health.get("status") in healthy_statuses
                        health_status = health.get("status", "unknown")
                    status["managers"][name] = {
                        "status": health_status,
                        "operations": health.get("operations_count", 0) if isinstance(health, dict) else 0,
                        "errors": health.get("errors_count", 0) if isinstance(health, dict) else 0,
                    }
                    if not is_healthy:
                        # Jan 2026: During startup grace period, treat unhealthy as "starting"
                        # Managers may still be initializing - give them time to become healthy
                        if in_grace_period:
                            status["managers"][name]["status"] = "starting"
                            status["managers"][name]["original_status"] = health_status
                            # Don't mark all_healthy=False during grace period
                        else:
                            status["all_healthy"] = False
                            status["unhealthy_count"] += 1
                else:
                    # Manager initialized but no health_check method
                    status["managers"][name] = {"status": "initialized", "health_check": "not_available"}
            except Exception as e:  # noqa: BLE001
                logger.error(
                    f"[P2P] Manager {name} health check failed: {type(e).__name__}: {e}",
                    exc_info=True
                )
                status["managers"][name] = {
                    "status": "error",
                    "error": str(e),
                    "exception_type": type(e).__name__
                }
                status["all_healthy"] = False
                status["unhealthy_count"] += 1

        # Log results
        manager_count = len(managers)
        if status["all_healthy"]:
            if in_grace_period:
                starting = [n for n, s in status["managers"].items() if s.get("status") == "starting"]
                if starting:
                    logger.info(
                        f"[P2P] Manager health: {manager_count - len(starting)}/{manager_count} healthy, "
                        f"{len(starting)} starting (grace period: {status.get('grace_period_remaining', 0):.0f}s remaining)"
                    )
                else:
                    logger.info(f"[P2P] Manager health: all {manager_count} managers healthy ")
            else:
                logger.info(f"[P2P] Manager health: all {manager_count} managers healthy ")
        else:
            unhealthy = [n for n, s in status["managers"].items() if s.get("status") not in ("healthy", "initialized", "ready", "running", "starting")]
            logger.warning(f"[P2P] Manager health: {len(unhealthy)}/{manager_count} unhealthy: {unhealthy}")

        return status

    def health_check(self) -> "HealthCheckResult":
        """Return health check result for daemon protocol compliance.

        December 27, 2025: Added for DaemonManager integration. Returns a
        HealthCheckResult that can be used by the daemon infrastructure for
        health monitoring, auto-restart decisions, and liveness probes.

        Returns:
            HealthCheckResult with overall orchestrator health status
        """
        # Import from contracts (zero-dependency module)
        from app.coordination.contracts import CoordinatorStatus, HealthCheckResult

        # Get manager health status
        manager_health = self._validate_manager_health()

        # Calculate cluster metrics
        uptime_seconds = time.time() - getattr(self, "start_time", time.time())
        active_peers = sum(
            1 for p in self.peers.values()
            if time.time() - p.last_heartbeat < 120
        )

        details = {
            "node_id": self.node_id,
            "role": self.role.value if hasattr(self.role, "value") else str(self.role),
            "leader_id": self.leader_id,
            "active_peers": active_peers,
            "total_peers": len(self.peers),
            "uptime_seconds": uptime_seconds,
            "managers_healthy": manager_health.get("all_healthy", False),
            "unhealthy_managers": manager_health.get("unhealthy_count", 0),
            "selfplay_jobs": self.self_info.selfplay_jobs if hasattr(self, "self_info") else 0,
            "training_jobs": self.self_info.training_jobs if hasattr(self, "self_info") else 0,
        }

        # Determine overall health
        is_healthy = manager_health.get("all_healthy", False)

        # Additional health checks
        if uptime_seconds < 10:
            # Grace period for startup
            is_healthy = True
            message = "P2P Orchestrator starting up"
            status = CoordinatorStatus.RUNNING
        elif not is_healthy:
            message = f"P2P Orchestrator unhealthy: {manager_health.get('unhealthy_count', 0)} unhealthy managers"
            status = CoordinatorStatus.ERROR
        else:
            message = f"P2P Orchestrator healthy, {active_peers} peers active"
            status = CoordinatorStatus.RUNNING

        return HealthCheckResult(
            healthy=is_healthy,
            status=status,
            message=message,
            details=details,
        )

    async def _subscribe_with_retry(
        self,
        event_name: str,
        handler: Any,
        max_attempts: int = 3,
        is_critical: bool = False,
    ) -> bool:
        """Subscribe to event with exponential backoff retry.

        December 2025 (Wave 7 Phase 1.1): Implements reliable event subscription
        with automatic retry on failure. This prevents pipeline stalls caused by
        transient subscription failures at startup.

        Args:
            event_name: Name of the event to subscribe to
            handler: Handler function to call when event fires
            max_attempts: Maximum subscription attempts (default: 3)
            is_critical: If True, failure is logged at ERROR level

        Returns:
            True if subscription succeeded, False otherwise
        """
        from app.coordination.event_router import subscribe

        for attempt in range(max_attempts):
            try:
                subscribe(event_name, handler)
                logger.info(f"[P2P] Subscribed to {event_name}")
                return True
            except Exception as e:  # noqa: BLE001
                delay = 2 ** attempt  # 1s, 2s, 4s exponential backoff
                level = logger.error if is_critical else logger.warning
                level(
                    f"[P2P] Subscription to {event_name} failed "
                    f"(attempt {attempt + 1}/{max_attempts}): {e}"
                )
                if attempt < max_attempts - 1:
                    await asyncio.sleep(delay)

        if is_critical:
            logger.critical(
                f"[P2P] CRITICAL: Failed to subscribe to {event_name} "
                f"after {max_attempts} attempts"
            )
        return False

    def _subscribe_single(
        self,
        event_name: str,
        handler: Any,
        is_critical: bool = False,
    ) -> bool:
        """Synchronous single subscription attempt.

        Used for initial subscription setup. Returns True on success.
        For retry logic, use _subscribe_with_retry in async context.
        """
        try:
            from app.coordination.event_router import subscribe
            subscribe(event_name, handler)
            return True
        except Exception as e:  # noqa: BLE001
            level = logger.error if is_critical else logger.debug
            level(f"[P2P] Failed to subscribe to {event_name}: {e}")
            return False

    def _subscribe_to_daemon_events(self) -> bool:
        """Subscribe to daemon status events for observability.

        December 2025: Receives DAEMON_STATUS_CHANGED events from daemon_manager
        to track daemon health across the cluster. This enables:
        - Tracking which daemons are running/crashed on each node
        - Auto-recovery of critical daemons
        - Cluster-wide daemon health reporting via /status endpoint

        Returns True if subscription succeeded, False otherwise.
        """
        try:
            from app.coordination.event_router import subscribe

            def handle_daemon_status(event) -> None:
                """Handle daemon status change events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    daemon_name = payload.get("daemon_name", "unknown")
                    new_status = payload.get("new_status", "unknown")
                    hostname = payload.get("hostname", "unknown")
                    error = payload.get("error")

                    # Track daemon states for cluster health reporting
                    if not hasattr(self, "_daemon_states"):
                        self._daemon_states = {}
                    self._daemon_states[f"{hostname}:{daemon_name}"] = {
                        "status": new_status,
                        "last_update": time.time(),
                        "error": error,
                    }

                    # Log critical daemon failures
                    if new_status in ("crashed", "failed") and error:
                        logger.warning(
                            f"Daemon {daemon_name} on {hostname} {new_status}: {error}"
                        )
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling daemon status event: {e}")

            subscribe("DAEMON_STATUS_CHANGED", handle_daemon_status)
            logger.info("Subscribed to daemon status events")
            return True
        except ImportError as e:
            logger.debug(f"Daemon events: event_router not available: {e}")
            return False
        except Exception as e:  # noqa: BLE001
            logger.warning(f"Daemon events: failed to subscribe: {e}")
            return False

    def _subscribe_to_feedback_signals(self) -> bool:
        """Subscribe to training feedback signals for dynamic orchestration.

        December 2025: Subscribes to key feedback events that should influence
        cluster orchestration decisions:
        - ELO_VELOCITY_CHANGED: Adjust selfplay allocation based on training velocity
        - QUALITY_DEGRADED: Pause/slow selfplay when data quality drops
        - EVALUATION_COMPLETED: Trigger model promotion decisions
        - PROMOTION_FAILED: Revert curriculum weights if promotion fails
        - PLATEAU_DETECTED: Trigger hyperparameter search or curriculum changes

        Returns True if subscription succeeded, False otherwise.
        """
        try:
            from app.coordination.event_router import subscribe

            def handle_quality_degraded(event) -> None:
                """Handle quality degradation events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    config_key = payload.get("config_key", "unknown")
                    quality_score = payload.get("quality_score", 0)
                    threshold = payload.get("threshold", 0)

                    logger.warning(
                        f"Quality degraded for {config_key}: {quality_score:.2f} < {threshold:.2f}"
                    )
                    # Could pause selfplay for this config or trigger data cleanup
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling quality degraded event: {e}")

            def handle_elo_velocity_changed(event) -> None:
                """Handle Elo velocity change events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    config_key = payload.get("config_key", "unknown")
                    velocity = payload.get("velocity", 0)

                    if velocity < -50:  # Significant regression
                        logger.warning(f"Elo regression for {config_key}: velocity={velocity}")
                    elif velocity > 50:  # Good progress
                        logger.info(f"Elo progress for {config_key}: velocity={velocity}")
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling Elo velocity event: {e}")

            def handle_evaluation_completed(event) -> None:
                """Handle evaluation completion events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    config_key = payload.get("config_key", "unknown")
                    win_rate = payload.get("win_rate", 0)
                    opponent = payload.get("opponent", "unknown")

                    logger.info(
                        f"Evaluation completed for {config_key}: {win_rate:.1%} vs {opponent}"
                    )
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling evaluation completed event: {e}")

            def handle_plateau_detected(event) -> None:
                """Handle training plateau detection events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    config_key = payload.get("config_key", "unknown")
                    epochs_stalled = payload.get("epochs_stalled", 0)

                    logger.warning(
                        f"Training plateau for {config_key}: stalled {epochs_stalled} epochs"
                    )
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling plateau detected event: {e}")

            def handle_exploration_boost(event) -> None:
                """Handle exploration boost events from training feedback.

                P0.1 (Dec 2025): Added missing handler for EXPLORATION_BOOST events.
                When training anomalies (loss spikes, stalls) are detected, this
                signals that we should boost exploration in selfplay to generate
                more diverse training data.
                """
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    config_key = payload.get("config_key", payload.get("config", "unknown"))
                    boost_factor = payload.get("boost_factor", payload.get("boost", 1.0))
                    reason = payload.get("reason", "training_anomaly")
                    duration = payload.get("duration_seconds", 900)

                    logger.info(
                        f"Exploration boost for {config_key}: {boost_factor:.2f}x "
                        f"(reason={reason}, duration={duration}s)"
                    )

                    # Forward to selfplay scheduler if available
                    if hasattr(self, "selfplay_scheduler") and self.selfplay_scheduler:
                        self.selfplay_scheduler.set_exploration_boost(
                            config_key, boost_factor, duration
                        )
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling exploration boost event: {e}")

            def handle_promotion_failed(event) -> None:
                """Handle model promotion failure events.

                December 27, 2025: Added missing handler for PROMOTION_FAILED events.
                When model promotion fails (gauntlet failure, threshold not met),
                we should revert curriculum weights and potentially pause training
                for that config until issues are resolved.
                """
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    config_key = payload.get("config_key", "unknown")
                    model_path = payload.get("model_path", "unknown")
                    reason = payload.get("reason", "unknown")
                    win_rate = payload.get("win_rate", 0.0)

                    logger.warning(
                        f"[P2P] Promotion FAILED for {config_key}: {reason} "
                        f"(model={model_path}, win_rate={win_rate:.1%})"
                    )

                    # Revert curriculum weights if selfplay scheduler available
                    if hasattr(self, "selfplay_scheduler") and self.selfplay_scheduler:
                        # Reduce priority for this config temporarily
                        self.selfplay_scheduler.record_promotion_failure(config_key)
                        logger.info(f"[P2P] Reduced selfplay priority for {config_key} after promotion failure")

                    # Track failed promotions for monitoring
                    if not hasattr(self, "_promotion_failures"):
                        self._promotion_failures = {}
                    if config_key not in self._promotion_failures:
                        self._promotion_failures[config_key] = []
                    self._promotion_failures[config_key].append({
                        "timestamp": time.time(),
                        "reason": reason,
                        "win_rate": win_rate,
                    })
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling promotion failed event: {e}")

            def handle_handler_failed(event) -> None:
                """Handle event handler failure events.

                December 27, 2025: Added missing handler for HANDLER_FAILED events.
                When a coordination event handler throws an exception, this event
                is emitted. We need to track these for monitoring and potentially
                trigger alerts for critical handler failures.
                """
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    handler_name = payload.get("handler_name", "unknown")
                    event_type = payload.get("event_type", "unknown")
                    error = payload.get("error", "unknown")
                    coordinator = payload.get("coordinator", "unknown")

                    logger.error(
                        f"[P2P] Handler FAILED: {handler_name} for {event_type} "
                        f"in {coordinator}: {error}"
                    )

                    # Track handler failures for health monitoring
                    if not hasattr(self, "_handler_failures"):
                        self._handler_failures = {}
                    failure_key = f"{coordinator}.{handler_name}"
                    if failure_key not in self._handler_failures:
                        self._handler_failures[failure_key] = []
                    self._handler_failures[failure_key].append({
                        "timestamp": time.time(),
                        "event_type": event_type,
                        "error": str(error)[:200],  # Truncate long errors
                    })

                    # Keep only last 10 failures per handler
                    if len(self._handler_failures[failure_key]) > 10:
                        self._handler_failures[failure_key] = self._handler_failures[failure_key][-10:]
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling handler failed event: {e}")

            # Subscribe to all feedback signals
            subscribe("QUALITY_DEGRADED", handle_quality_degraded)
            subscribe("ELO_VELOCITY_CHANGED", handle_elo_velocity_changed)
            subscribe("EVALUATION_COMPLETED", handle_evaluation_completed)
            subscribe("PLATEAU_DETECTED", handle_plateau_detected)
            subscribe("EXPLORATION_BOOST", handle_exploration_boost)
            subscribe("PROMOTION_FAILED", handle_promotion_failed)
            subscribe("HANDLER_FAILED", handle_handler_failed)

            logger.info("Subscribed to training feedback signals")
            return True
        except ImportError as e:
            logger.debug(f"Feedback signals: event_router not available: {e}")
            return False
        except Exception as e:  # noqa: BLE001
            logger.warning(f"Feedback signals: failed to subscribe: {e}")
            return False

    def _subscribe_to_manager_events(self) -> bool:
        """Subscribe to manager lifecycle events for coordination.

        December 2025: Subscribes to critical manager events that were previously
        missing from P2P orchestrator integration:
        - TRAINING_STARTED/COMPLETED: Coordinate training transitions
        - TASK_SPAWNED/COMPLETED/FAILED: Track job lifecycle
        - DATA_SYNC_STARTED/COMPLETED: Coordinate data freshness

        Returns True if subscription succeeded, False otherwise.
        """
        try:
            from app.coordination.event_router import subscribe

            def handle_training_started(event) -> None:
                """Handle training start events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    config_key = payload.get("config_key", "unknown")
                    node_id = payload.get("node_id", "unknown")
                    logger.info(f"[P2P] Training started: {config_key} on {node_id}")
                    # Track active training in cluster state
                    if not hasattr(self, "_active_training"):
                        self._active_training = {}
                    self._active_training[config_key] = {
                        "node_id": node_id,
                        "started_at": time.time(),
                    }
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling training started event: {e}")

            def handle_training_completed(event) -> None:
                """Handle training completion events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    config_key = payload.get("config_key", "unknown")
                    model_path = payload.get("model_path", "")
                    final_loss = payload.get("final_loss", 0)
                    logger.info(
                        f"[P2P] Training completed: {config_key} "
                        f"(loss={final_loss:.4f}, model={model_path})"
                    )
                    # Clear from active training
                    if hasattr(self, "_active_training"):
                        self._active_training.pop(config_key, None)
                    # Trigger selfplay allocation refresh
                    if hasattr(self, "selfplay_scheduler"):
                        self.selfplay_scheduler.on_training_complete(config_key)

                    # Jan 3, 2026: Bridge to coordination event bus for EvaluationDaemon
                    # This enables the Training  Evaluation  Promotion pipeline
                    try:
                        from app.coordination.event_router import emit_event
                        from app.coordination.data_events import DataEventType
                        emit_event(DataEventType.TRAINING_COMPLETED, {
                            "config_key": config_key,
                            "model_path": model_path,
                            "final_loss": final_loss,
                            "source": "p2p_bridge",
                        })
                        logger.debug(f"[P2P] Bridged TRAINING_COMPLETED to coordination bus")
                    except Exception as bridge_err:  # noqa: BLE001
                        logger.debug(f"Could not bridge TRAINING_COMPLETED: {bridge_err}")
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling training completed event: {e}")

            def handle_task_spawned(event) -> None:
                """Handle task spawn events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    job_id = payload.get("job_id", "unknown")
                    job_type = payload.get("job_type", "unknown")
                    node_id = payload.get("node_id", "unknown")
                    logger.debug(f"[P2P] Task spawned: {job_type} {job_id} on {node_id}")
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling task spawned event: {e}")

            def handle_task_completed(event) -> None:
                """Handle task completion events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    job_id = payload.get("job_id", "unknown")
                    job_type = payload.get("job_type", "unknown")
                    duration = payload.get("duration", 0)
                    logger.debug(f"[P2P] Task completed: {job_type} {job_id} ({duration:.1f}s)")
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling task completed event: {e}")

            def handle_task_failed(event) -> None:
                """Handle task failure events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    job_id = payload.get("job_id", "unknown")
                    job_type = payload.get("job_type", "unknown")
                    error = payload.get("error", "unknown error")
                    logger.warning(f"[P2P] Task failed: {job_type} {job_id} - {error}")
                    # Could trigger recovery or rebalancing here
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling task failed event: {e}")

            def handle_data_sync_started(event) -> None:
                """Handle data sync start events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    sync_type = payload.get("sync_type", "unknown")
                    target_count = payload.get("target_nodes", 0)
                    logger.info(f"[P2P] Data sync started: {sync_type} to {target_count} nodes")
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling data sync started event: {e}")

            def handle_data_sync_completed(event) -> None:
                """Handle data sync completion events."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    sync_type = payload.get("sync_type", "unknown")
                    duration = payload.get("duration", 0)
                    files_synced = payload.get("files_synced", 0)
                    logger.info(
                        f"[P2P] Data sync completed: {sync_type} "
                        f"({files_synced} files in {duration:.1f}s)"
                    )
                    # Could trigger training readiness check here
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling data sync completed event: {e}")

            # P2P Health event handlers (Dec 2025)
            def handle_node_unhealthy(event) -> None:
                """Handle NODE_UNHEALTHY events - pause jobs on unhealthy nodes."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    node_id = payload.get("node_id", "unknown")
                    reason = payload.get("reason", "")
                    logger.warning(f"[P2P] Node {node_id} unhealthy: {reason}")
                    # Mark node as unhealthy for job routing
                    if hasattr(self, "node_selector") and self.node_selector:
                        self.node_selector.mark_node_unhealthy(node_id, reason)
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling node unhealthy event: {e}")

            def handle_node_recovered(event) -> None:
                """Handle NODE_RECOVERED events - resume jobs on recovered nodes."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    node_id = payload.get("node_id", "unknown")
                    logger.info(f"[P2P] Node {node_id} recovered")
                    # Mark node as healthy for job routing
                    if hasattr(self, "node_selector") and self.node_selector:
                        self.node_selector.mark_node_healthy(node_id)
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling node recovered event: {e}")

            def handle_cluster_healthy(event) -> None:
                """Handle P2P_CLUSTER_HEALTHY events."""
                try:
                    logger.info("[P2P] Cluster is healthy - resuming normal operations")
                    self._cluster_health_degraded = False
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling cluster healthy event: {e}")

            def handle_cluster_unhealthy(event) -> None:
                """Handle P2P_CLUSTER_UNHEALTHY events - pause non-critical operations."""
                try:
                    payload = event.payload if hasattr(event, "payload") else event
                    reason = payload.get("reason", "")
                    alive_nodes = payload.get("alive_nodes", 0)
                    logger.warning(
                        f"[P2P] Cluster unhealthy: {reason} (alive_nodes={alive_nodes})"
                    )
                    self._cluster_health_degraded = True
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Error handling cluster unhealthy event: {e}")

            # Subscribe to all manager events
            subscribe("TRAINING_STARTED", handle_training_started)
            subscribe("TRAINING_COMPLETED", handle_training_completed)
            subscribe("TASK_SPAWNED", handle_task_spawned)
            subscribe("TASK_COMPLETED", handle_task_completed)
            subscribe("TASK_FAILED", handle_task_failed)
            subscribe("DATA_SYNC_STARTED", handle_data_sync_started)
            subscribe("DATA_SYNC_COMPLETED", handle_data_sync_completed)

            # Subscribe to health events (Dec 2025)
            subscribe("NODE_UNHEALTHY", handle_node_unhealthy)
            subscribe("NODE_RECOVERED", handle_node_recovered)
            subscribe("P2P_CLUSTER_HEALTHY", handle_cluster_healthy)
            subscribe("P2P_CLUSTER_UNHEALTHY", handle_cluster_unhealthy)

            logger.info("Subscribed to manager lifecycle and health events")
            return True
        except ImportError as e:
            logger.debug(f"Manager events: event_router not available: {e}")
            return False
        except Exception as e:  # noqa: BLE001
            logger.warning(f"Manager events: failed to subscribe: {e}")
            return False

    # =========================================================================
    # Leadership State Management - Single Source of Truth (Jan 3, 2026)
    # =========================================================================

    def _set_leader(
        self,
        new_leader_id: str | None,
        reason: str = "unknown",
        *,
        sync_to_ulsm: bool = True,
        save_state: bool = True,
    ) -> bool:
        """Atomically set the leader and role to ensure consistency.

        This is the CANONICAL method for modifying self.leader_id and self.role.
        Using this method ensures both tracking systems (direct fields and ULSM)
        stay synchronized and prevents the leader self-recognition desync bug.

        Jan 3, 2026: Created to fix critical bug where leader node didn't recognize
        itself as leader due to divergent state between leader_id and role fields.

        Jan 12, 2026: C1 fix - Added leader_state_lock to prevent race conditions
        during concurrent leader elections and state transitions.

        Args:
            new_leader_id: The new leader ID (None to clear leader)
            reason: Human-readable reason for logging/debugging
            sync_to_ulsm: Whether to sync state to LeadershipStateMachine
            save_state: Whether to persist state after change

        Returns:
            True if this node is now the leader
        """
        # C1 fix: Acquire lock to prevent race conditions during leader transitions
        with self.leader_state_lock:
            old_leader_id = self.leader_id
            old_role = self.role

            # Determine new role based on leader_id
            if new_leader_id is None:
                new_role = NodeRole.FOLLOWER
                is_now_leader = False
            elif new_leader_id == self.node_id:
                new_role = NodeRole.LEADER
                is_now_leader = True
            else:
                new_role = NodeRole.FOLLOWER
                is_now_leader = False

            # Atomic update of both fields
            self.leader_id = new_leader_id
            self.role = new_role

            # Sync to ULSM (Unified Leadership State Machine) if enabled
            if sync_to_ulsm and hasattr(self, "_leadership_sm") and self._leadership_sm is not None:
                try:
                    from scripts.p2p.leadership_state_machine import LeaderState

                    self._leadership_sm._leader_id = new_leader_id
                    self._leadership_sm._state = (
                        LeaderState.LEADER if is_now_leader else LeaderState.FOLLOWER
                    )
                except (ImportError, AttributeError) as e:
                    logger.debug(f"[LeaderSet] ULSM sync skipped: {e}")

            # Log if changed
            if old_leader_id != new_leader_id or old_role != new_role:
                logger.info(
                    f"[LeaderSet] {old_role.value if hasattr(old_role, 'value') else old_role}->"
                    f"{new_role.value if hasattr(new_role, 'value') else new_role}, "
                    f"leader_id={old_leader_id}->{new_leader_id}, reason={reason}"
                )

            # Persist state if requested
            if save_state and (old_leader_id != new_leader_id or old_role != new_role):
                self._save_state()

            return is_now_leader

    def _is_leader(self) -> bool:
        """Check if this node is the current cluster leader with valid lease."""
        # Dec 31, 2025: Enhanced logging for leader self-recognition debugging
        if self.leader_id != self.node_id:
            logger.debug(
                f"[LeaderCheck] Not leader: leader_id={self.leader_id}, "
                f"self.node_id={self.node_id}, role={self.role.value if hasattr(self.role, 'value') else self.role}"
            )
            # Consistency: we should never claim role=leader/provisional while leader_id points elsewhere (or is None).
            # Jan 1, 2026: Also check PROVISIONAL_LEADER for consistency
            if self.role in (NodeRole.LEADER, NodeRole.PROVISIONAL_LEADER):
                logger.info("Inconsistent leadership state (role=leader but leader_id!=self); stepping down")
                # C1 fix: Use leader_state_lock for role changes
                with self.leader_state_lock:
                    self.role = NodeRole.FOLLOWER
                    self.last_lease_renewal = 0.0
                    if not self.leader_id:
                        self.leader_lease_id = ""
                        self.leader_lease_expires = 0.0
                self._release_voter_grant_if_self()
                self._save_state()
                # Only force an election when we have no known leader; otherwise we
                # may already be following a healthy leader and shouldn't flap.
                if not self.leader_id:
                    # CRITICAL: Check quorum before starting election to prevent quorum bypass
                    if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                        logger.warning("Skipping election: no voter quorum available")
                    else:
                        with contextlib.suppress(RuntimeError):
                            asyncio.get_running_loop().create_task(self._start_election())
            return False
        # Consistency: we should never claim leader_id=self while being a follower/candidate.
        # Jan 1, 2026: PROVISIONAL_LEADER is also valid for dispatching work (fallback leadership)
        if self.role not in (NodeRole.LEADER, NodeRole.PROVISIONAL_LEADER):
            logger.info("Inconsistent leadership state (leader_id=self but role!=leader/provisional); clearing leader_id")
            # C1 fix: Use leader_state_lock for role/leader_id changes
            with self.leader_state_lock:
                self.role = NodeRole.FOLLOWER
                self.leader_id = None
                self.leader_lease_id = ""
                self.leader_lease_expires = 0.0
                self.last_lease_renewal = 0.0
            self._release_voter_grant_if_self()
            self._save_state()
            # CRITICAL: Check quorum before starting election to prevent quorum bypass
            if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                logger.warning("Skipping election after inconsistent state: no voter quorum available")
            else:
                with contextlib.suppress(RuntimeError):
                    asyncio.get_running_loop().create_task(self._start_election())
            return False

        # LEARNED LESSONS - Lease-based leadership prevents split-brain
        # Must have valid lease to act as leader
        if self.leader_lease_expires > 0 and time.time() >= self.leader_lease_expires:
            logger.info("Leadership lease expired, stepping down via ULSM")
            # Jan 2026: Use ULSM step-down which broadcasts to peers BEFORE local mutation
            # This ensures peers learn about step-down immediately, preventing split-brain
            self._schedule_step_down_sync(TransitionReason.LEASE_EXPIRED)
            # Note: Election is triggered after step-down completes (in _complete_step_down_async)
            # if quorum is available
            return False
        # Dec 31, 2025: Add grace period for quorum failures in _is_leader() too
        # This prevents rapid step-downs from transient network issues
        # Jan 2026: Use ULSM QuorumHealth for unified quorum tracking
        if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
            # Jan 2, 2026: Use _count_alive_voters() to check IP:port matches
            voters_alive = self._count_alive_voters()
            quorum_size = getattr(self, "voter_quorum_size", 0)
            # Use ULSM QuorumHealth for unified tracking (threshold=5 vs old 3)
            threshold_exceeded = self._leadership_sm.quorum_health.record_failure(voters_alive)
            fail_count = self._leadership_sm.quorum_health.consecutive_failures
            threshold = self._leadership_sm.quorum_health.failure_threshold
            logger.debug(
                f"[LeaderCheck] Quorum check failed ({fail_count}/{threshold}): "
                f"voters_alive={voters_alive}, quorum_size={quorum_size}"
            )
            if threshold_exceeded:
                logger.info(f"Leadership without voter quorum ({threshold} consecutive failures), stepping down via ULSM")
                # Jan 2026: Use ULSM step-down which broadcasts to peers BEFORE local mutation
                self._schedule_step_down_sync(TransitionReason.QUORUM_LOST)
                # NOTE: Don't start election here - we just lost quorum, so election would fail
                # Wait for quorum to be restored before attempting election
                logger.warning("Skipping election after quorum loss: no voter quorum available")
                # Jan 2026: Trigger aggressive peer discovery during quorum crisis
                if hasattr(self, "_quorum_crisis_loop") and self._quorum_crisis_loop:
                    self._quorum_crisis_loop.enter_crisis_mode(reason="quorum_lost")
            return False
        else:
            # Reset quorum health counter on success
            # Jan 2, 2026: Use _count_alive_voters() to check IP:port matches
            voters_alive = self._count_alive_voters()
            self._leadership_sm.quorum_health.record_success(voters_alive)
            # Jan 2026: Exit crisis mode when quorum is restored
            if hasattr(self, "_quorum_crisis_loop") and self._quorum_crisis_loop:
                if self._quorum_crisis_loop.in_crisis_mode:
                    self._quorum_crisis_loop.exit_crisis_mode(reason="quorum_restored")
        return True

    @property
    def is_leader(self) -> bool:
        """Property alias for _is_leader() - required by WorkQueueHandlersMixin."""
        return self._is_leader()

    def _get_leadership_consistency_metrics(self) -> dict:
        """Get metrics for detecting leadership state desyncs.

        Jan 3, 2026: Added to monitor and debug the leader self-recognition bug
        where leader_id is set correctly but role doesn't match.

        Returns:
            Dictionary with consistency check results for monitoring.
        """
        try:
            from scripts.p2p.leadership_state_machine import LeaderState
        except ImportError:
            LeaderState = None

        # Get ULSM state if available
        ulsm_state = None
        ulsm_leader = None
        if hasattr(self, "_leadership_sm") and self._leadership_sm is not None:
            if LeaderState is not None:
                ulsm_state = self._leadership_sm._state.value if hasattr(self._leadership_sm._state, "value") else str(self._leadership_sm._state)
            ulsm_leader = self._leadership_sm._leader_id

        # Check for inconsistencies
        role_ulsm_mismatch = False
        leader_ulsm_mismatch = False

        if hasattr(self, "_leadership_sm") and self._leadership_sm is not None and LeaderState is not None:
            # Role should match ULSM state
            local_is_leader = self.role in (NodeRole.LEADER, NodeRole.PROVISIONAL_LEADER)
            ulsm_is_leader = self._leadership_sm._state == LeaderState.LEADER
            role_ulsm_mismatch = (local_is_leader != ulsm_is_leader) and self._leadership_sm._state != LeaderState.STEPPING_DOWN
            # Leader IDs should match
            leader_ulsm_mismatch = (self._leadership_sm._leader_id != self.leader_id)

        # Self-recognition check: If we're the elected leader, do we recognize it?
        leader_id_is_self = (self.leader_id == self.node_id)
        role_is_leader = self.role in (NodeRole.LEADER, NodeRole.PROVISIONAL_LEADER)
        is_leader_call = self._is_leader()

        # Desync conditions
        # Case 1: leader_id=self but role!=LEADER (gossip bug)
        gossip_desync = leader_id_is_self and not role_is_leader
        # Case 2: role=LEADER but leader_id!=self (should not happen)
        role_desync = role_is_leader and not leader_id_is_self

        return {
            "role": self.role.value if hasattr(self.role, "value") else str(self.role),
            "leader_id": self.leader_id,
            "node_id": self.node_id,
            "is_leader_call": is_leader_call,
            "leader_id_is_self": leader_id_is_self,
            "role_is_leader": role_is_leader,
            "ulsm_state": ulsm_state,
            "ulsm_leader_id": ulsm_leader,
            "role_ulsm_mismatch": role_ulsm_mismatch,
            "leader_ulsm_mismatch": leader_ulsm_mismatch,
            "gossip_desync": gossip_desync,  # leader_id=self but role!=LEADER
            "role_desync": role_desync,  # role=LEADER but leader_id!=self
            "self_recognition_ok": leader_id_is_self == is_leader_call,  # Quick health check
        }

    def _recover_leadership_desync(self) -> bool:
        """Auto-recover from leadership state desynchronization.

        Jan 20, 2026: Added to fix the root cause of cluster instability where
        nodes get stuck in inconsistent states (candidate claiming to be leader,
        or leader_id set but role not matching).

        Recovery actions:
        1. gossip_desync (leader_id=self but role!=LEADER):
            Accept leadership since other nodes already see us as leader
        2. role_desync (role=LEADER but leader_id!=self):
            Step down since another node is the recognized leader

        Returns:
            True if recovery action was taken, False if state was consistent.
        """
        leader_id_is_self = (self.leader_id == self.node_id)
        role_is_leader = self.role in (NodeRole.LEADER, NodeRole.PROVISIONAL_LEADER)

        # Case 1: gossip_desync - leader_id=self but role!=LEADER
        # Other nodes see us as leader, but we don't recognize it
        if leader_id_is_self and not role_is_leader:
            logger.warning(
                f"[LeadershipRecovery] Fixing gossip_desync: leader_id={self.leader_id}, "
                f"role={self.role} -> LEADER"
            )
            self.role = NodeRole.LEADER
            # Update state machine if available
            if hasattr(self, "_leadership_sm") and self._leadership_sm:
                try:
                    from scripts.p2p.leadership_state_machine import LeaderState
                    # Direct state update (bypassing normal transition) for recovery
                    self._leadership_sm._state = LeaderState.LEADER
                    self._leadership_sm._leader_id = self.node_id
                except Exception as e:
                    logger.warning(f"[LeadershipRecovery] Failed to update state machine: {e}")
            return True

        # Case 2: role_desync - role=LEADER but leader_id!=self
        # We think we're leader, but leader_id points elsewhere
        if role_is_leader and not leader_id_is_self:
            logger.warning(
                f"[LeadershipRecovery] Fixing role_desync: role={self.role}, "
                f"leader_id={self.leader_id} -> FOLLOWER"
            )
            self.role = NodeRole.FOLLOWER
            # Update state machine if available
            if hasattr(self, "_leadership_sm") and self._leadership_sm:
                try:
                    from scripts.p2p.leadership_state_machine import LeaderState
                    self._leadership_sm._state = LeaderState.FOLLOWER
                    self._leadership_sm._leader_id = self.leader_id
                except Exception as e:
                    logger.warning(f"[LeadershipRecovery] Failed to update state machine: {e}")
            return True

        return False  # State was consistent

    def _get_config_version(self) -> dict:
        """Get config file version info for drift detection.

        Jan 13, 2026: Phase 1 of P2P Cluster Stability Plan
        Enables gossip-based config drift detection across the cluster.

        Returns:
            Dictionary with config hash, timestamp, and metadata.
        """
        import hashlib
        from pathlib import Path

        config_paths = [
            Path(__file__).parent.parent / "config" / "distributed_hosts.yaml",
            Path.cwd() / "config" / "distributed_hosts.yaml",
        ]

        for config_path in config_paths:
            if config_path.exists():
                try:
                    content = config_path.read_text()
                    stat = config_path.stat()

                    # Compute hash of content
                    content_hash = hashlib.sha256(content.encode()).hexdigest()

                    return {
                        "hash": content_hash[:16],  # First 16 chars for display
                        "full_hash": content_hash,
                        "timestamp": stat.st_mtime,
                        "mtime": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime(stat.st_mtime)),
                        "path": str(config_path),
                        "size_bytes": stat.st_size,
                    }
                except (OSError, PermissionError) as e:
                    return {
                        "hash": None,
                        "error": str(e),
                        "path": str(config_path),
                    }

        return {
            "hash": None,
            "error": "config_not_found",
            "searched_paths": [str(p) for p in config_paths],
        }

    def _was_recently_leader(self) -> bool:
        """Check if this node was the cluster leader within RECENT_LEADER_WINDOW.

        Jan 2, 2026: Leader stickiness - allows previous leader to reclaim
        leadership with preference during elections, reducing oscillation.

        Returns:
            True if we were leader and stepped down within RECENT_LEADER_WINDOW seconds.
        """
        now = time.time()
        # Check if we became leader at some point
        if self._last_become_leader_time <= 0:
            return False
        # Check if we stepped down recently (within window)
        if self._last_step_down_time <= 0:
            return False
        time_since_step_down = now - self._last_step_down_time
        return time_since_step_down < RECENT_LEADER_WINDOW

    def _in_incumbent_grace_period(self) -> bool:
        """Check if we're within the incumbent grace period after stepping down.

        Jan 2, 2026: During this period, the previous leader gets priority
        to reclaim leadership without competition.

        Returns:
            True if within INCUMBENT_LEADER_GRACE_PERIOD seconds of step-down.
        """
        if self._last_step_down_time <= 0:
            return False
        time_since_step_down = time.time() - self._last_step_down_time
        return time_since_step_down < INCUMBENT_LEADER_GRACE_PERIOD

    # =========================================================================
    # UNIFIED LEADERSHIP STATE MACHINE (ULSM) - Jan 2026
    # Broadcast callback for state machine step-down notifications
    # =========================================================================

    async def _broadcast_leader_state_change(
        self,
        new_state: str,
        epoch: int,
        reason: "TransitionReason",
    ) -> None:
        """Broadcast leadership state change to all peers.

        Called by LeadershipStateMachine.transition_to() when stepping down.
        This ensures peers learn about step-down BEFORE local state mutation.

        Args:
            new_state: New leadership state (e.g., "stepping_down")
            epoch: Current leadership epoch
            reason: Reason for the transition
        """
        message = {
            "node_id": self.node_id,
            "new_state": new_state,
            "epoch": epoch,
            "reason": reason.value if hasattr(reason, "value") else str(reason),
            "timestamp": time.time(),
        }

        tasks = []
        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        peers_snapshot = self._peer_snapshot.get_snapshot()

        for peer_id, peer_info in peers_snapshot.items():
            if not peer_info.is_alive():
                continue

            # Build URL for peer
            # Jan 10, 2026: Fixed - NodeInfo uses host/port, not advertise_host/advertise_port
            url = f"http://{peer_info.host}:{peer_info.port}/leader-state-change"
            tasks.append(self._broadcast_to_peer(url, message, peer_id))

        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            success = sum(1 for r in results if r is True)
            logger.info(
                f"Broadcast step-down to {success}/{len(tasks)} peers "
                f"(epoch={epoch}, reason={reason.value if hasattr(reason, 'value') else reason})"
            )

    async def _broadcast_to_peer(
        self,
        url: str,
        message: dict[str, Any],
        peer_id: str,
    ) -> bool:
        """Send state change message to a single peer with timeout."""
        try:
            async with get_client_session(timeout=2.0) as session:
                async with session.post(
                    url,
                    json=message,
                    headers=self._auth_headers(),
                ) as resp:
                    if resp.status == 200:
                        return True
                    logger.debug(f"Broadcast to {peer_id} returned {resp.status}")
                    return False
        except (aiohttp.ClientError, asyncio.TimeoutError, OSError) as e:
            logger.debug(f"Broadcast to {peer_id} failed: {e}")
            return False

    # =========================================================================
    # ULSM STEP-DOWN HELPERS - Sync-to-async bridge for _is_leader() calls
    # =========================================================================

    def _schedule_step_down_sync(self, reason: "TransitionReason") -> None:
        """Schedule async step-down from sync context (e.g., _is_leader()).

        This is a sync-to-async bridge that schedules the full ULSM step-down
        process via asyncio.create_task(). The step-down includes:
        1. State machine transition to STEPPING_DOWN (broadcasts to peers)
        2. Brief delay for broadcast propagation
        3. Transition to FOLLOWER
        4. Legacy field synchronization

        Args:
            reason: Why we're stepping down (LEASE_EXPIRED, QUORUM_LOST, etc.)
        """
        try:
            loop = asyncio.get_running_loop()
            loop.create_task(self._complete_step_down_async(reason))
            logger.info(f"ULSM: Scheduled step-down for reason={reason.value}")
        except RuntimeError:
            # No running loop - this shouldn't happen in normal operation
            logger.warning("ULSM: No event loop to schedule step-down")

    async def _complete_step_down_async(self, reason: "TransitionReason") -> None:
        """Complete the step-down process via ULSM state machine.

        This is the async implementation that:
        1. Transitions to STEPPING_DOWN (triggers broadcast to peers)
        2. Waits briefly for broadcast propagation
        3. Transitions to FOLLOWER
        4. Syncs legacy fields and saves state

        Args:
            reason: Why we're stepping down
        """
        try:
            # Step 1: Transition to STEPPING_DOWN (triggers broadcast)
            old_leader_id = self.leader_id
            success = await self._leadership_sm.transition_to(
                LeaderState.STEPPING_DOWN,
                reason,
            )
            if not success:
                logger.warning(f"ULSM: Failed transition to STEPPING_DOWN, current state={self._leadership_sm.state}")
                return

            # Step 2: Brief delay to allow broadcast propagation
            await asyncio.sleep(0.5)

            # Step 3: Transition to FOLLOWER
            await self._leadership_sm.transition_to(
                LeaderState.FOLLOWER,
                TransitionReason.STEP_DOWN_COMPLETE,
            )

            # Step 4: Sync legacy fields (for backward compatibility)
            # C1 fix: Use leader_state_lock for role/leader_id changes
            with self.leader_state_lock:
                self.role = NodeRole.FOLLOWER
                self.leader_id = None
                self.leader_lease_id = ""
                self.leader_lease_expires = 0.0
                self.last_lease_renewal = 0.0
                self._is_leader_quorum_fail_count = 0
                # Jan 2, 2026: Track when we stepped down for leader stickiness
                self._last_step_down_time = time.time()
            self._release_voter_grant_if_self()
            self._save_state()

            # Emit LEADER_LOST event for other components
            self._emit_leader_lost_sync(old_leader_id, reason.value)

            logger.info(f"ULSM: Step-down complete, reason={reason.value}")

            # Step 5: Trigger new election for certain step-down reasons
            # Skip for QUORUM_LOST since election would fail anyway
            if reason != TransitionReason.QUORUM_LOST:
                if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                    logger.warning("ULSM: Skipping post-step-down election - no voter quorum")
                else:
                    logger.info("ULSM: Starting election after step-down")
                    asyncio.create_task(self._start_election())

        except Exception as e:
            logger.error(f"ULSM: Error during step-down: {e}", exc_info=True)
            # Fallback: Force follower state
            # C1 fix: Use leader_state_lock for role/leader_id changes
            with self.leader_state_lock:
                self.role = NodeRole.FOLLOWER
                self.leader_id = None
            self._save_state()

    # =========================================================================
    # TASK ISOLATION - Prevent single task failure from crashing all tasks
    # =========================================================================

    # Task factory registry for restart support
    _task_factories: dict[str, "Callable[[], Coroutine]"] = {}

    async def _safe_task_wrapper(
        self,
        coro,
        task_name: str,
        factory: "Callable[[], Coroutine] | None" = None,
    ) -> None:
        """Wrap a coroutine to catch exceptions and prevent cascade failures.

        This is a CRITICAL stability fix: without isolation, a single exception
        in any of 18+ background tasks will crash the entire P2P orchestrator
        via asyncio.gather() propagating the exception.

        Args:
            coro: The coroutine to wrap
            task_name: Human-readable task name for logging
            factory: Optional callable that returns a new coroutine for restarts

        Returns:
            None - exceptions are logged but not raised
        """
        # Register factory for potential restarts
        if factory is not None:
            self._task_factories[task_name] = factory

        restart_count = 0
        max_restarts = 5

        while True:
            try:
                await coro
                return  # Normal completion
            except asyncio.CancelledError:
                logger.debug(f"Task '{task_name}' cancelled (shutdown)")
                raise  # Re-raise CancelledError for graceful shutdown
            except SystemExit:
                # SystemExit from main loop exit - ignore in background tasks
                # This prevents "Task exception was never retrieved" log pollution
                logger.debug(f"Task '{task_name}' received SystemExit (orchestrator shutdown)")
                return
            except Exception as e:  # noqa: BLE001
                # Log but don't propagate - other tasks continue running
                logger.error(f"Task '{task_name}' crashed: {e}", exc_info=True)

                # Check if we can restart
                restart_factory = factory or self._task_factories.get(task_name)
                if not self.running or restart_factory is None:
                    logger.warning(f"Task '{task_name}' cannot restart (no factory or shutdown)")
                    return

                restart_count += 1
                if restart_count > max_restarts:
                    logger.error(
                        f"Task '{task_name}' exceeded max restarts ({max_restarts}), giving up"
                    )
                    return

                # Exponential backoff: 30s, 60s, 120s, 240s, 480s
                delay = min(30 * (2 ** (restart_count - 1)), 480)
                logger.info(
                    f"Restarting task '{task_name}' in {delay}s "
                    f"(attempt {restart_count}/{max_restarts})..."
                )
                await asyncio.sleep(delay)

                if not self.running:
                    return

                # Create new coroutine from factory
                try:
                    coro = restart_factory()
                    logger.info(f"Restarted task '{task_name}'")
                except Exception as restart_error:
                    logger.error(f"Failed to restart task '{task_name}': {restart_error}")
                    return

    def _create_safe_task(
        self,
        coro,
        name: str,
        factory: "Callable[[], Coroutine] | None" = None,
    ) -> asyncio.Task:
        """Create a task wrapped with exception isolation and restart support.

        Args:
            coro: The coroutine to run
            name: Task name for logging
            factory: Optional callable that returns a new coroutine for restarts.
                     If not provided, task cannot be automatically restarted.

        Returns:
            asyncio.Task wrapped with safe error handling
        """
        return asyncio.create_task(
            self._safe_task_wrapper(coro, name, factory),
            name=name,
        )

    # =========================================================================
    # BOUNDED COLLECTIONS - Prevent unbounded memory growth
    # =========================================================================

    # Maximum pending relay items before cleanup
    MAX_PENDING_RELAY_ACKS = 10000
    MAX_PENDING_RELAY_RESULTS = 10000

    def _add_pending_relay_ack(self, cmd_id: str) -> None:
        """Add a relay ack with bounds checking."""
        if len(self.pending_relay_acks) >= self.MAX_PENDING_RELAY_ACKS:
            # Evict oldest entries (set doesn't have order, so clear half)
            half = len(self.pending_relay_acks) // 2
            to_remove = list(self.pending_relay_acks)[:half]
            for item in to_remove:
                self.pending_relay_acks.discard(item)
            logger.warning(f"Evicted {half} pending_relay_acks (max {self.MAX_PENDING_RELAY_ACKS})")
        self.pending_relay_acks.add(cmd_id)

    def _add_pending_relay_result(self, result: dict) -> None:
        """Add a relay result with bounds checking."""
        if len(self.pending_relay_results) >= self.MAX_PENDING_RELAY_RESULTS:
            # Evict oldest entries (keep most recent half)
            half = len(self.pending_relay_results) // 2
            self.pending_relay_results = self.pending_relay_results[half:]
            logger.warning(f"Evicted {half} pending_relay_results (max {self.MAX_PENDING_RELAY_RESULTS})")
        self.pending_relay_results.append(result)

    # =========================================================================
    # SAFEGUARDS - Load, rate limiting, and coordinator integration
    # =========================================================================

    def _check_spawn_rate_limit(self) -> tuple[bool, str]:
        """Check if we're within the spawn rate limit.

        SAFEGUARD: Prevents runaway process spawning by limiting spawns per minute.

        Returns:
            (can_spawn, reason) - True if within rate limit
        """
        now = time.time()
        # Clean old timestamps (older than 60 seconds)
        self.spawn_timestamps = [t for t in self.spawn_timestamps if now - t < 60]

        if len(self.spawn_timestamps) >= SPAWN_RATE_LIMIT_PER_MINUTE:
            return False, f"Rate limit: {len(self.spawn_timestamps)}/{SPAWN_RATE_LIMIT_PER_MINUTE} spawns in last minute"

        return True, f"Rate OK: {len(self.spawn_timestamps)}/{SPAWN_RATE_LIMIT_PER_MINUTE}"

    def _record_spawn(self) -> None:
        """Record a process spawn for rate limiting."""
        self.spawn_timestamps.append(time.time())

    async def _check_coordinator_available(self) -> bool:
        """Check if the unified coordinator is available.

        SAFEGUARD: In agent mode, defer job decisions to coordinator.

        Returns:
            True if coordinator is reachable
        """
        if not self.coordinator_url:
            return False

        # Cache check for 30 seconds
        now = time.time()
        if now - self.last_coordinator_check < 30:
            return self.coordinator_available

        self.last_coordinator_check = now

        try:
            async with get_client_session(timeout=ClientTimeout(total=5)) as session:
                async with session.get(f"{self.coordinator_url}/api/health") as resp:
                    self.coordinator_available = resp.status == 200
                    if self.coordinator_available:
                        logger.info(f"Coordinator available at {self.coordinator_url}")
                    return self.coordinator_available
        except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
            self.coordinator_available = False
            return False

    def _can_spawn_process(self, reason: str = "job") -> tuple[bool, str]:
        """Combined safeguard check before spawning any process.

        SAFEGUARD: Checks load average, rate limit, and agent mode.

        Args:
            reason: Description of why we want to spawn (for logging)

        Returns:
            (can_spawn, explanation) - True if all checks pass
        """
        # Check 1: Load average
        load_ok, load_reason = self.self_info.check_load_average_safe()
        if not load_ok:
            logger.info(f"BLOCKED spawn ({reason}): {load_reason}")
            return False, load_reason

        # Check 2: Rate limit
        rate_ok, rate_reason = self._check_spawn_rate_limit()
        if not rate_ok:
            logger.info(f"BLOCKED spawn ({reason}): {rate_reason}")
            return False, rate_reason

        # Check 3: Agent mode - if coordinator is available and we're in agent mode,
        # we should not autonomously spawn jobs (let coordinator decide)
        if self.agent_mode and self.coordinator_available:
            msg = "Agent mode: deferring to coordinator"
            logger.info(f"BLOCKED spawn ({reason}): {msg}")
            return False, msg

        # Check 4: Backpressure (new coordination) - if training queue is saturated,
        # don't spawn more selfplay jobs that would produce more data
        if HAS_NEW_COORDINATION and "selfplay" in reason.lower():
            if should_stop_production(QueueType.TRAINING_DATA):
                msg = "Backpressure: training queue at STOP level"
                logger.info(f"BLOCKED spawn ({reason}): {msg}")
                return False, msg
            if should_throttle_production(QueueType.TRAINING_DATA):
                throttle = get_throttle_factor(QueueType.TRAINING_DATA)
                import random
                if random.random() > throttle:
                    msg = f"Backpressure: throttled (factor={throttle:.2f})"
                    logger.info(f"BLOCKED spawn ({reason}): {msg}")
                    return False, msg

        # Check 5: Graceful degradation - don't spawn under heavy resource pressure
        if HAS_RESOURCE_GUARD and get_degradation_level is not None:
            degradation = get_degradation_level()
            if degradation >= 4:  # CRITICAL - resources at/above limits
                msg = f"Graceful degradation: critical resource pressure (level {degradation})"
                logger.info(f"BLOCKED spawn ({reason}): {msg}")
                return False, msg
            elif degradation >= 3:  # HEAVY - only critical ops proceed
                # Selfplay is NORMAL priority, blocked under heavy pressure
                if should_proceed_with_priority is not None and not should_proceed_with_priority(OperationPriority.NORMAL):
                    msg = f"Graceful degradation: heavy resource pressure (level {degradation})"
                    logger.info(f"BLOCKED spawn ({reason}): {msg}")
                    return False, msg

        return True, "All safeguards passed"

    def _detect_build_version(self) -> str:
        env_version = (os.environ.get(BUILD_VERSION_ENV, "") or "").strip()
        if env_version:
            return env_version

        commit = ""
        branch = ""
        try:
            result = subprocess.run(
                self._git_cmd("rev-parse", "--short", "HEAD"),
                cwd=self.ringrift_path,
                capture_output=True,
                text=True,
                timeout=3,
            )
            if result.returncode == 0:
                commit = result.stdout.strip()
        except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError, AttributeError):
            commit = ""

        try:
            result = subprocess.run(
                self._git_cmd("rev-parse", "--abbrev-ref", "HEAD"),
                cwd=self.ringrift_path,
                capture_output=True,
                text=True,
                timeout=3,
            )
            if result.returncode == 0:
                branch = result.stdout.strip()
        except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError, AttributeError):
            branch = ""

        if commit and branch:
            return f"{branch}@{commit}"
        return commit or "unknown"

    def _git_cmd(self, *args: str) -> list[str]:
        safe_dir = getattr(self, "_git_safe_directory", "") or os.path.abspath(self.ringrift_path)
        return ["git", "-c", f"safe.directory={safe_dir}", *args]

    def _detect_ringrift_path(self) -> str:
        """Detect the RingRift installation path."""
        # Try common locations
        candidates = [
            Path.home() / "Development" / "RingRift",
            Path.home() / "ringrift",
            Path("/home/ubuntu/ringrift"),
            Path("/root/ringrift"),
        ]
        for path in candidates:
            if (path / "ai-service").exists():
                return str(path)
        return str(Path(__file__).parent.parent.parent)

    def _get_ai_service_path(self) -> str:
        """Get the path to the ai-service directory.

        Handles both cases:
        - ringrift_path = /path/to/RingRift (root directory)
        - ringrift_path = /path/to/RingRift/ai-service (already ai-service)

        Returns:
            Path to ai-service directory.
        """
        if self.ringrift_path.rstrip("/").endswith("ai-service"):
            return self.ringrift_path
        return os.path.join(self.ringrift_path, "ai-service")

    def _get_script_path(self, script_name: str) -> str:
        """Get the full path to a script in ai-service/scripts/.

        Args:
            script_name: Name of the script (e.g., "run_self_play_soak.py")

        Returns:
            Full path to the script.
        """
        return os.path.join(self._get_ai_service_path(), "scripts", script_name)

    def _check_yaml_gpu_config(self, node_id: str | None = None) -> tuple[bool, str, int]:
        """Check if YAML config indicates a node has a GPU.

        Used as fallback when runtime GPU detection fails (e.g., vGPU, containers,
        driver issues causing torch.cuda.is_available() to return False).

        Args:
            node_id: Node ID to check. If None, uses self.node_id.

        Returns:
            Tuple of (has_gpu, gpu_name, gpu_vram_gb)

        Session 17.50 (Jan 2026): Added to fix GPU nodes running CPU selfplay
        when torch.cuda.is_available() returns False due to driver issues.
        """
        target_node = node_id or self.node_id
        try:
            from app.config.cluster_config import get_config_cache
            config = get_config_cache().get_config()
            host_cfg = config.hosts_raw.get(target_node, {})

            # Check multiple indicators
            gpu_name = str(host_cfg.get("gpu", ""))
            gpu_vram = int(host_cfg.get("gpu_vram_gb", 0) or 0)
            role = str(host_cfg.get("role", ""))

            has_gpu = bool(gpu_name) or gpu_vram > 0 or "gpu" in role.lower()

            if has_gpu:
                logger.debug(
                    f"[YAML GPU] Node {target_node}: gpu={gpu_name}, "
                    f"vram={gpu_vram}GB, role={role}"
                )
            return has_gpu, gpu_name, gpu_vram
        except Exception as e:
            logger.debug(f"Could not check YAML GPU config for {target_node}: {e}")
            return False, "", 0

    def get_data_directory(self) -> Path:
        """Get the data directory path based on storage configuration.

        Returns:
            Path to data directory:
            - ramdrive: /dev/shm/ringrift/data (for disk-constrained Vast instances)
            - disk: {ringrift_path}/ai-service/data (default)

        The ramdrive option uses tmpfs for high-speed I/O and to work around
        limited disk space on some cloud instances. Data stored in ramdrive
        is volatile and should be synced to permanent storage periodically.
        """
        if self.storage_type == "ramdrive":
            ramdrive = Path(self.ramdrive_path)
            try:
                ramdrive.mkdir(parents=True, exist_ok=True)
            except (PermissionError, OSError) as e:
                # /dev/shm doesn't exist on macOS or may be inaccessible
                logger.warning(f"Cannot create ramdrive at {ramdrive}: {e}. Falling back to disk storage.")
                self.storage_type = "disk"
                return Path(self._get_ai_service_path()) / "data"

            # Set up automatic sync to persistent storage
            if self.ramdrive_syncer is None and self.sync_to_disk_interval > 0:
                persistent_path = Path(self._get_ai_service_path()) / "data"
                persistent_path.mkdir(parents=True, exist_ok=True)
                self.ramdrive_syncer = RamdriveSyncer(
                    source_dir=ramdrive,
                    target_dir=persistent_path,
                    interval=self.sync_to_disk_interval,
                    patterns=["*.db", "*.jsonl", "*.json", "*.npz"],
                )
                self.ramdrive_syncer.start()
                logger.info(f"Started ramdrive -> disk sync: {ramdrive} -> {persistent_path} "
                           f"every {self.sync_to_disk_interval}s")

            return ramdrive
        return Path(self._get_ai_service_path()) / "data"

    def stop_ramdrive_syncer(self, final_sync: bool = True) -> None:
        """Stop the ramdrive syncer and optionally perform final sync."""
        if self.ramdrive_syncer:
            logger.info("Stopping ramdrive syncer...")
            self.ramdrive_syncer.stop(final_sync=final_sync)
            logger.info(f"Ramdrive sync stats: {self.ramdrive_syncer.stats}")
            self.ramdrive_syncer = None

    # =========================================================================
    # GPU Job Tracking (Jan 7, 2026)
    # =========================================================================
    # These methods track GPU job lifecycle for adaptive dispatch decisions.
    # GPU nodes should run GPU-accelerated selfplay, not fall back to CPU.
    # =========================================================================

    def _get_node_job_preference(self, node_id: str) -> str:
        """Get preferred job type based on node role from YAML config.

        Jan 7, 2026: Added to enforce role-based job selection.
        GPU-only nodes should not fall back to CPU selfplay.

        Returns one of:
        - 'cpu_only': Node should only run CPU jobs (coordinator, cpu_selfplay)
        - 'gpu_only': Node should only run GPU jobs (gpu_selfplay role)
        - 'training_only': Node should only run training (gpu_training_primary)
        - 'both': Node can run both GPU selfplay and training (default)
        """
        try:
            from app.config.cluster_config import get_config_cache
            config = get_config_cache().get_config()
            host_cfg = config.hosts_raw.get(node_id, {})
            role = str(host_cfg.get("role", "")).lower()

            if role in ("coordinator", "cpu_selfplay"):
                return "cpu_only"
            if role == "gpu_selfplay":
                return "gpu_only"
            if role == "gpu_training_primary":
                # Training-primary nodes can still do selfplay when idle
                return "both"
            if role == "gpu_training_selfplay":
                return "both"
            return "both"
        except Exception as e:
            logger.debug(f"Could not get job preference for {node_id}: {e}")
            return "both"

    def _record_gpu_job_result(self, success: bool) -> None:
        """Record GPU job completion result for adaptive dispatch decisions.

        Jan 7, 2026: Added for GPU failure tracking.
        Consecutive failures indicate driver issues and should trigger CPU fallback.

        Args:
            success: True if GPU job completed successfully, False otherwise.
        """
        try:
            now = time.time()
            if success:
                self.self_info.last_gpu_job_success = now
                self.self_info.gpu_failure_count = 0  # Reset on success
            else:
                self.self_info.last_gpu_job_failure = now
                self.self_info.gpu_failure_count = getattr(self.self_info, "gpu_failure_count", 0) + 1
            logger.debug(f"GPU job result: success={success}, failure_count={self.self_info.gpu_failure_count}")
        except Exception as e:
            logger.debug(f"Could not record GPU job result: {e}")

    def _update_gpu_job_count(self, delta: int) -> None:
        """Update running GPU job count.

        Jan 7, 2026: Added for accurate GPU job tracking.
        Used to detect driver issues (jobs running but 0% utilization).

        Args:
            delta: Amount to change count by (+1 for start, -1 for completion).
        """
        try:
            current = getattr(self.self_info, "gpu_job_count", 0) or 0
            self.self_info.gpu_job_count = max(0, current + delta)
            logger.debug(f"GPU job count: {current} -> {self.self_info.gpu_job_count}")
        except Exception as e:
            logger.debug(f"Could not update GPU job count: {e}")

    def _infer_advertise_port(self) -> int:
        """Infer the externally reachable port for this node.

        - Explicit `RINGRIFT_ADVERTISE_PORT` always wins.
        - Vast.ai exposes container ports via `VAST_TCP_PORT_<PORT>`; when set,
          use that public port so peers can reach us.
        - Default to the listening port.
        """
        explicit = (os.environ.get(ADVERTISE_PORT_ENV, "")).strip()
        if explicit:
            try:
                return int(explicit)
            except ValueError:
                pass

        vast_key = f"VAST_TCP_PORT_{self.port}"
        mapped = (os.environ.get(vast_key, "")).strip()
        if mapped:
            try:
                return int(mapped)
            except ValueError:
                pass

        return int(self.port)

    def _validate_and_fix_advertise_host(self) -> None:
        """Validate advertise_host, fix private IP issues, and populate alternate_ips.

        December 30, 2025: Added to prevent P2P quorum loss caused by nodes
        advertising private LAN IPs (10.x, 192.168.x, 172.16-31.x) that other
        nodes in the mesh cannot reach.

        January 2, 2026: Extended for dual-stack IPv4/IPv6 support.
        - Discovers all IPs (IPv4 + IPv6) for this node
        - Prefers IPv4 for primary (broader compatibility)
        - Populates alternate_ips with all other addresses including IPv6

        This method:
        1. Discovers all reachable IPs (both address families)
        2. Selects best primary (prefer Tailscale IPv4, then any IPv4, then IPv6)
        3. Populates alternate_ips with remaining addresses
        4. Emits warnings/errors for operator awareness if using private IP
        """
        import ipaddress

        # Discover all IPs for this node (IPv4 + IPv6)
        all_ips = self._discover_all_ips()

        if not all_ips:
            logger.warning("[P2P] No IPs discovered for this node")
            return

        # If we already have a valid advertise_host in our discovered IPs, just update alternates
        if self.advertise_host and self.advertise_host in all_ips:
            # Check if current host is IPv6 but we have IPv4 available
            is_current_ipv6 = ":" in self.advertise_host
            ipv4_ips = {ip for ip in all_ips if ":" not in ip}

            if is_current_ipv6 and ipv4_ips:
                # Prefer IPv4 for primary
                primary, alternates = self._select_primary_advertise_host(all_ips)
                if primary and primary != self.advertise_host:
                    # Jan 12, 2026: Use setter to ensure self.self_info.host is also updated
                    self._set_advertise_host(primary, "ipv6_to_ipv4_switch")
                    self.alternate_ips = alternates
                    return

            # Current host is valid, just update alternates
            self.alternate_ips = all_ips - {self.advertise_host}
            logger.debug(f"[P2P] Updated alternate_ips: {len(self.alternate_ips)} addresses")
            return

        # Need to select a new primary host
        if self.advertise_host:
            # Current advertise_host is not in discovered IPs (possibly private/loopback)
            try:
                ip = ipaddress.ip_address(self.advertise_host)
                is_unreachable = ip.is_private or ip.is_loopback
                if is_unreachable:
                    ip_type = "loopback" if ip.is_loopback else "private"
                    logger.warning(
                        f"[P2P] Current advertise_host {self.advertise_host} is {ip_type}, selecting new primary"
                    )
            except ValueError:
                pass  # Not an IP address (maybe hostname)

        # Select best primary (IPv4 preferred)
        primary, alternates = self._select_primary_advertise_host(all_ips)

        if primary:
            old_host = self.advertise_host
            # Jan 12, 2026: Use setter to ensure self.self_info.host is also updated
            changed = self._set_advertise_host(primary, "primary_selection")
            self.alternate_ips = alternates

            if changed and old_host:
                print(
                    f"[P2P] WARNING: advertise_host auto-fixed: {old_host} -> {primary} "
                    f"(alternate IPs: {len(alternates)})"
                )
                logger.warning(
                    f"P2P advertise_host auto-fixed: {old_host} -> {primary} "
                    f"(discovered {len(all_ips)} IPs, {len(alternates)} alternates)"
                )
            elif not old_host:
                logger.info(
                    f"[P2P] advertise_host set to {primary} with {len(alternates)} alternate IPs"
                )
        else:
            # No valid IPs found - emit error
            print(
                f"[P2P] ERROR: advertise_host {self.advertise_host} is unreachable by peers! "
                f"Set RINGRIFT_P2P_ADVERTISE_HOST to your public/Tailscale IP."
            )
            logger.error(
                f"P2P advertise_host {self.advertise_host} is unreachable - mesh connectivity will fail!"
            )

    async def _periodic_ip_validation_loop(self) -> None:
        """Periodically revalidate advertise_host for late Tailscale availability.

        Dec 31, 2025: Added for 48-hour autonomous operation.
        Jan 12, 2026: Enhanced with aggressive startup checking and Tailscale enforcement.

        Problem: If Tailscale is not ready at startup, coordinator advertises private
        IP (10.x.x.x) which breaks mesh connectivity. Tailscale may become available
        later but the private IP persists.

        Solution: Check every 15s for first 5 minutes (aggressive startup), then
        every 5 minutes. If advertising private IP and Tailscale is available, switch
        to Tailscale IP, update peer info, and re-announce to bootstrap seeds.

        Jan 12, 2026 Enhancement: Added explicit Tailscale enforcement that checks
        specifically for private IP + Tailscale availability scenario.
        """
        interval = 15.0  # Very fast checking during startup (was 30s)
        stable_count = 0
        startup_fast_period = 300.0  # 5 minutes of fast checking (was 3 min)
        start_time = time.time()

        await asyncio.sleep(5)  # Brief initial delay (was 10s)

        while self.running:
            try:
                old_host = self.advertise_host
                is_private = self._is_advertising_private_ip()

                # Jan 12, 2026: Explicit Tailscale enforcement for private IP scenarios
                if is_private:
                    logger.info(f"[IP_ENFORCE] Advertising private IP {old_host}, checking for Tailscale...")
                    ts_ip = self._try_get_tailscale_ip()
                    if ts_ip:
                        # Tailscale is available - switch immediately
                        self._set_advertise_host(ts_ip, "tailscale_enforcement")
                        logger.warning(f"[IP_ENFORCE] Switched from private to Tailscale: {old_host} -> {ts_ip}")

                        # Emit event for monitoring/alerting
                        self._safe_emit_private_ip_alert(old_host, ts_ip, switched=True)

                        # Re-announce to bootstrap seeds
                        try:
                            await self._announce_to_bootstrap_seeds()
                            logger.info("[IP_ENFORCE] Re-announced to bootstrap seeds with Tailscale IP")
                        except Exception as announce_err:  # noqa: BLE001
                            logger.debug(f"[IP_ENFORCE] Re-announce failed: {announce_err}")

                        stable_count = 0
                        await asyncio.sleep(interval)
                        continue
                    else:
                        # Still no Tailscale - emit warning event periodically
                        if stable_count % 10 == 0:  # Every ~150s during startup
                            self._safe_emit_private_ip_alert(old_host, None, switched=False)
                            logger.warning(f"[IP_ENFORCE] Still advertising private IP {old_host}, Tailscale unavailable")

                # Run normal validation
                self._validate_and_fix_advertise_host()

                if old_host != self.advertise_host:
                    logger.warning(f"[P2P] IP revalidation detected change: {old_host} -> {self.advertise_host}")

                    # Re-announce to bootstrap seeds with corrected IP
                    try:
                        await self._announce_to_bootstrap_seeds()
                        logger.info("[P2P] Re-announced to bootstrap seeds with corrected IP")
                    except Exception as announce_err:  # noqa: BLE001
                        logger.debug(f"[P2P] Failed to re-announce after IP correction: {announce_err}")

                    stable_count = 0
                else:
                    stable_count += 1

                # Slow down after startup fast period and stable checks
                elapsed = time.time() - start_time
                if elapsed > startup_fast_period and stable_count >= 6:
                    if interval < 300.0:
                        interval = 300.0  # Slow to 5-minute checks
                        logger.debug("[P2P] IP validation: switching to 5-minute interval")

            except Exception as e:  # noqa: BLE001
                logger.debug(f"[P2P] IP revalidation error: {e}")

            await asyncio.sleep(interval)

    def _is_advertising_private_ip(self) -> bool:
        """Check if currently advertising a private/unreachable IP.

        Jan 12, 2026: Helper for Tailscale enforcement loop.

        Returns:
            True if advertise_host is a private IP (10.x, 192.168.x, 172.16-31.x)
        """
        import ipaddress

        if not self.advertise_host:
            return False

        try:
            ip = ipaddress.ip_address(self.advertise_host)
            # Tailscale CGNAT (100.x.x.x) is "private" technically but globally routable via mesh
            if self.advertise_host.startswith("100."):
                return False
            return ip.is_private or ip.is_loopback
        except ValueError:
            return False

    def _try_get_tailscale_ip(self) -> str:
        """Try to get Tailscale IP without waiting/blocking.

        Jan 12, 2026: Helper for Tailscale enforcement loop.

        Returns:
            Tailscale IP if available, else empty string
        """
        try:
            from scripts.p2p.resource_detector import ResourceDetector
            detector = ResourceDetector()
            # Prefer IPv4 for broader compatibility
            ts_ipv4 = detector.get_tailscale_ipv4()
            if ts_ipv4:
                return ts_ipv4
            ts_ipv6 = detector.get_tailscale_ipv6()
            if ts_ipv6:
                return ts_ipv6
        except Exception:  # noqa: BLE001
            pass
        return ""

    def _safe_emit_private_ip_alert(self, private_ip: str, tailscale_ip: str | None, switched: bool) -> None:
        """Emit event for private IP detection (for monitoring/alerting).

        Jan 12, 2026: Part of Tailscale enforcement for autonomous operation.
        """
        try:
            from app.coordination.data_events import DataEventType
            event_type = DataEventType.PRIVATE_IP_ADVERTISED if hasattr(DataEventType, "PRIVATE_IP_ADVERTISED") else None
            if event_type:
                self._emit_event(
                    str(event_type.value),
                    {
                        "node_id": self.node_id,
                        "private_ip": private_ip,
                        "tailscale_ip": tailscale_ip,
                        "switched": switched,
                        "severity": "info" if switched else "warning",
                    },
                )
        except Exception:  # noqa: BLE001
            pass  # Event emission is best-effort

    def _discover_all_ips(self, exclude_primary: str | None = None) -> set[str]:
        """Discover all IP addresses this node can be reached at (IPv4 AND IPv6).

        January 2026: Multi-IP advertising for improved mesh resilience.
        January 2, 2026: Extended for dual-stack IPv4/IPv6 support.

        Collects IPs from:
        1. Tailscale IPs (100.x.x.x IPv4, fd7a:115c:a1e0:: IPv6)
        2. Hostname resolution (both address families)
        3. Local network interfaces (both address families)
        4. YAML config (tailscale_ip, ssh_host if resolvable)

        Returns:
            Set of IP addresses (excluding the primary advertise_host)
        """
        import ipaddress
        import socket

        ips: set[str] = set()

        # 1. Tailscale IPs (both IPv4 and IPv6)
        # IMPORTANT: Explicitly fetch BOTH address families since _get_tailscale_ip()
        # defaults to IPv6, which may not be reachable from IPv4-only peers.
        try:
            from scripts.p2p.resource_detector import ResourceDetector
            detector = ResourceDetector()
            # Get Tailscale IPv4 explicitly (100.x.x.x)
            ts_ipv4 = detector.get_tailscale_ipv4()
            if ts_ipv4:
                ips.add(ts_ipv4)
                logger.debug(f"[P2P] Discovered Tailscale IPv4: {ts_ipv4}")
            # Get Tailscale IPv6 explicitly (fd7a:115c:a1e0::)
            ts_ipv6 = detector.get_tailscale_ipv6()
            if ts_ipv6:
                ips.add(ts_ipv6)
                logger.debug(f"[P2P] Discovered Tailscale IPv6: {ts_ipv6}")
        except Exception as e:
            logger.debug(f"[P2P] ResourceDetector Tailscale lookup failed: {e}")
            # Fall back to legacy method
            ts_ip = self._get_tailscale_ip()
            if ts_ip:
                ips.add(ts_ip)

        # 2. Try to get IPs from hostname - BOTH address families
        try:
            hostname = socket.gethostname()
            # IPv4
            for addr_info in socket.getaddrinfo(hostname, None, socket.AF_INET):
                ip = addr_info[4][0]
                if ip and ip != "127.0.0.1":
                    ips.add(ip)
            # IPv6
            for addr_info in socket.getaddrinfo(hostname, None, socket.AF_INET6):
                ip = addr_info[4][0]
                # Skip link-local (fe80::) and loopback (::1)
                if ip and not ip.startswith("fe80:") and ip != "::1":
                    ips.add(ip)
        except Exception:
            pass

        # 3. Get IPs from network interfaces - BOTH address families
        try:
            import netifaces
            for iface in netifaces.interfaces():
                # Skip loopback interfaces
                if iface.startswith("lo"):
                    continue
                addrs = netifaces.ifaddresses(iface)
                # IPv4
                for addr_info in addrs.get(netifaces.AF_INET, []):
                    ip = addr_info.get("addr")
                    if ip and ip != "127.0.0.1":
                        try:
                            ip_obj = ipaddress.ip_address(ip)
                            # Include Tailscale and public IPs, skip other private
                            if not ip_obj.is_private or ip.startswith("100."):
                                ips.add(ip)
                        except ValueError:
                            pass
                # IPv6 (NEW - dual-stack support)
                for addr_info in addrs.get(netifaces.AF_INET6, []):
                    ip = addr_info.get("addr", "")
                    if ip:
                        # Strip zone ID (e.g., "fe80::1%eth0" -> "fe80::1")
                        ip = ip.split("%")[0]
                        # Skip link-local (fe80::) and loopback (::1)
                        if not ip.startswith("fe80:") and ip != "::1":
                            ips.add(ip)
        except ImportError:
            # netifaces not available, try socket approach
            try:
                # Get primary outbound IPv4
                s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
                s.connect(("8.8.8.8", 80))
                local_ip = s.getsockname()[0]
                s.close()
                if local_ip and local_ip != "127.0.0.1":
                    ips.add(local_ip)
            except Exception:
                pass
        except Exception:
            pass

        # 4. Check YAML config for this node
        try:
            from app.config.cluster_config import load_cluster_config
            config = load_cluster_config()
            nodes = getattr(config, "hosts_raw", {}) or {}
            node_cfg = nodes.get(self.node_id, {})

            # Add Tailscale IP from config
            cfg_ts_ip = node_cfg.get("tailscale_ip")
            if cfg_ts_ip:
                ips.add(cfg_ts_ip)

            # Try to resolve ssh_host if it's a hostname - both address families
            ssh_host = node_cfg.get("ssh_host")
            if ssh_host and not ssh_host.startswith("ssh"):  # Skip vast.ai ssh gateway
                try:
                    # IPv4
                    for addr_info in socket.getaddrinfo(ssh_host, None, socket.AF_INET):
                        ip = addr_info[4][0]
                        if ip and ip != "127.0.0.1":
                            ips.add(ip)
                    # IPv6
                    for addr_info in socket.getaddrinfo(ssh_host, None, socket.AF_INET6):
                        ip = addr_info[4][0]
                        if ip and not ip.startswith("fe80:") and ip != "::1":
                            ips.add(ip)
                except Exception:
                    pass
        except Exception:
            pass

        # Remove primary host to avoid duplication
        if exclude_primary and exclude_primary in ips:
            ips.discard(exclude_primary)

        # Remove localhost variants (127.0.0.0/8 loopback range and bind-all)
        ips = {ip for ip in ips if not ip.startswith("127.") and ip != "0.0.0.0" and ip != "::1"}

        logger.debug(f"[P2P] Discovered alternate IPs: {ips}")
        return ips

    def _format_ip_for_url(self, ip: str) -> str:
        """Format IP for URL (bracket IPv6 addresses).

        January 2, 2026: Added for dual-stack IPv4/IPv6 URL construction.

        Args:
            ip: IP address string (IPv4 or IPv6)

        Returns:
            IP formatted for URL: IPv4 unchanged, IPv6 wrapped in brackets
        """
        if ":" in ip and not ip.startswith("["):
            return f"[{ip}]"
        return ip

    def _select_primary_advertise_host(self, all_ips: set[str]) -> tuple[str, set[str]]:
        """Select best primary address (prefer IPv4 for compatibility) and return alternates.

        January 2, 2026: Added for dual-stack IPv4/IPv6 support.
        Ensures primary advertise_host is IPv4 when available (broader compatibility),
        with IPv6 addresses available in alternate_ips for dual-stack peers.

        Preference order for primary:
        1. Tailscale CGNAT IPv4 (100.x.x.x) - globally routable via Tailscale mesh
        2. Other IPv4 addresses
        3. Tailscale IPv6 (fd7a:115c:a1e0::) - if no IPv4 available
        4. Other IPv6 addresses

        Args:
            all_ips: Set of all discovered IP addresses

        Returns:
            Tuple of (primary_host, alternate_ips)
        """
        if not all_ips:
            return "", set()

        ipv4_ips: set[str] = set()
        ipv6_ips: set[str] = set()

        for ip in all_ips:
            if ":" in ip:
                ipv6_ips.add(ip)
            else:
                ipv4_ips.add(ip)

        # Preference 1: Tailscale CGNAT IPv4 (100.x.x.x)
        tailscale_v4 = [ip for ip in ipv4_ips if ip.startswith("100.")]
        if tailscale_v4:
            primary = tailscale_v4[0]
            alternates = all_ips - {primary}
            return primary, alternates

        # Preference 2: Any other IPv4
        if ipv4_ips:
            primary = next(iter(ipv4_ips))
            alternates = all_ips - {primary}
            return primary, alternates

        # Preference 3: Tailscale IPv6 (fd7a:115c:a1e0::)
        tailscale_v6 = [ip for ip in ipv6_ips if ip.startswith("fd7a:115c:a1e0:")]
        if tailscale_v6:
            primary = tailscale_v6[0]
            alternates = all_ips - {primary}
            return primary, alternates

        # Preference 4: Any other IPv6
        if ipv6_ips:
            primary = next(iter(ipv6_ips))
            alternates = all_ips - {primary}
            return primary, alternates

        return "", set()

    def _load_force_relay_mode(self) -> bool:
        """Load force_relay_mode from distributed_hosts.yaml for this node.

        January 5, 2026: NAT-blocked nodes need to send ALL outbound heartbeats
        via relay to ensure other nodes can discover them. This is configured in
        distributed_hosts.yaml with either:
        - `nat_blocked: true` - Node is behind NAT and can't receive inbound connections
        - `force_relay_mode: true` - Explicitly enable relay mode

        Returns:
            True if this node should use relay mode for all outbound heartbeats.
        """
        # Priority 1: Environment variable override
        env = (os.environ.get("RINGRIFT_FORCE_RELAY_MODE") or "").strip().lower()
        if env in ("1", "true", "yes"):
            logger.info(f"[P2P] Force relay mode enabled via RINGRIFT_FORCE_RELAY_MODE env var")
            return True

        # Priority 2: Load from distributed_hosts.yaml
        try:
            from app.config.cluster_config import load_cluster_config
            config = load_cluster_config()
            nodes = getattr(config, "hosts_raw", {}) or {}
            node_cfg = nodes.get(self.node_id, {})

            nat_blocked = node_cfg.get("nat_blocked", False)
            force_relay = node_cfg.get("force_relay_mode", False)

            if nat_blocked or force_relay:
                reason = "nat_blocked" if nat_blocked else "force_relay_mode"
                logger.info(f"[P2P] Force relay mode enabled for {self.node_id} ({reason})")
                return True
        except ImportError:
            logger.debug("[P2P] cluster_config not available for force_relay_mode check")
        except Exception as e:  # noqa: BLE001
            logger.debug(f"[P2P] Failed to load force_relay_mode from config: {e}")

        return False

    def _get_yaml_tailscale_ip(self) -> str | None:
        """Get Tailscale IP from distributed_hosts.yaml for this node.

        Jan 12, 2026: Added to fix IP advertisement timing issue. When Tailscale
        CLI isn't ready at startup, we now fall back to the pre-configured
        tailscale_ip from YAML before falling back to local IP.

        This provides a reliable source for the correct IP even when Tailscale
        daemon is slow to start, preventing nodes from advertising unreachable
        local IPs (e.g., 10.0.0.62).

        Returns:
            Tailscale IP from config if available for this node, else None.
        """
        try:
            from app.config.cluster_config import load_cluster_config
            config = load_cluster_config()
            nodes = getattr(config, "hosts_raw", {}) or {}
            node_cfg = nodes.get(self.node_id, {})

            tailscale_ip = node_cfg.get("tailscale_ip")
            if tailscale_ip:
                logger.debug(f"[P2P] Found tailscale_ip in YAML config: {tailscale_ip}")
                return tailscale_ip
        except ImportError:
            logger.debug("[P2P] cluster_config not available for tailscale_ip lookup")
        except Exception as e:  # noqa: BLE001
            logger.debug(f"[P2P] Failed to load tailscale_ip from config: {e}")

        return None

    def _load_voter_node_ids(self) -> list[str]:
        """Load the set of P2P voter node_ids (for quorum-based leadership).

        If no voters are configured, returns an empty list and quorum checks are
        disabled (backwards compatible).

        December 2025 (Phase 7.1.2): Consolidated to use cluster_config.get_p2p_voters()
        for YAML loading, while preserving env var priority for overrides.
        """
        # Priority 1: Environment variable override (highest priority)
        env = (os.environ.get("RINGRIFT_P2P_VOTERS") or "").strip()
        if env:
            self.voter_config_source = "env"
            voters = [t.strip() for t in env.split(",") if t.strip()]
            return sorted(set(voters))

        # Priority 2: Use cluster_config for YAML-based voter loading
        # This handles both p2p_voters list and legacy per-host p2p_voter: true
        try:
            from app.config.cluster_config import get_p2p_voters
            voters = get_p2p_voters()
            if voters:
                self.voter_config_source = "config"
                return voters
        except ImportError:
            logger.debug("[P2P] cluster_config not available, falling back to direct YAML load")
        except Exception as e:  # noqa: BLE001
            logger.warning(f"[P2P] Failed to load voters via cluster_config: {e}")

        # Priority 3: Fallback - direct YAML load for legacy compatibility
        cfg_path = Path(self._get_ai_service_path()) / "config" / "distributed_hosts.yaml"
        if not cfg_path.exists():
            self.voter_config_source = "none"
            return []

        try:
            import yaml
            data = yaml.safe_load(cfg_path.read_text()) or {}
            p2p_voters_list = data.get("p2p_voters", []) or []
            if p2p_voters_list and isinstance(p2p_voters_list, list):
                voters = sorted({str(v).strip() for v in p2p_voters_list if str(v).strip()})
                if voters:
                    self.voter_config_source = "config"
                    return voters
        except (OSError, yaml.YAMLError, ValueError, KeyError) as e:
            # Dec 2025: Narrowed from bare Exception; config file may not exist
            logger.debug(f"Failed to load voter config from file: {e}")

        self.voter_config_source = "none"
        return []

    def _build_voter_ip_mapping(self) -> dict[str, set[str]]:
        """Build a mapping from voter node_ids to their known IPs.

        Jan 2, 2026: Added to support voter matching when peers are discovered
        via SWIM as IP:port format instead of proper node_ids.

        Returns:
            Dict mapping voter node_id -> set of known IPs (tailscale_ip, ssh_host)
        """
        voter_ids = getattr(self, "voter_node_ids", []) or []
        if not voter_ids:
            return {}

        # Load config to get IP mappings
        # Jan 2, 2026: Handle ringrift_path that already includes ai-service suffix
        rp = Path(self.ringrift_path)
        if rp.name == "ai-service":
            cfg_path = rp / "config" / "distributed_hosts.yaml"
        else:
            cfg_path = rp / "ai-service" / "config" / "distributed_hosts.yaml"
        if not cfg_path.exists():
            return {}

        try:
            import yaml
            data = yaml.safe_load(cfg_path.read_text()) or {}
            hosts = data.get("hosts", {}) or {}
        except Exception:
            return {}

        voter_ip_map: dict[str, set[str]] = {}
        for voter_id in voter_ids:
            host_cfg = hosts.get(voter_id, {})
            ips: set[str] = set()

            # Jan 13, 2026: Add BOTH Tailscale IP and ssh_host to voter IP set.
            # This enables voter matching regardless of which IP the peer advertises.
            # Previously used elif which caused voter matching to fail when peers
            # advertised ssh_host but config had tailscale_ip.
            if host_cfg.get("tailscale_ip"):
                ips.add(host_cfg["tailscale_ip"])
            # Also add ssh_host if it's a valid IP (not a hostname)
            if host_cfg.get("ssh_host"):
                ssh_host = host_cfg["ssh_host"]
                # Only add if it's an IP (not a hostname like "ssh5.vast.ai")
                if ssh_host and not any(c.isalpha() for c in ssh_host.replace(".", "")):
                    ips.add(ssh_host)

            if ips:
                voter_ip_map[voter_id] = ips

        return voter_ip_map

    def _build_ip_to_node_map(self) -> dict[str, str]:
        """Build a reverse mapping from IP addresses to node names.

        Jan 2, 2026: Added for translating SWIM peer IDs (IP:port) to node names.
        """
        cfg_path = Path(self._get_ai_service_path()) / "config" / "distributed_hosts.yaml"
        if not cfg_path.exists():
            return {}
        try:
            import yaml
            data = yaml.safe_load(cfg_path.read_text()) or {}
            hosts = data.get("hosts", {}) or {}
        except Exception:
            return {}
        ip_to_node: dict[str, str] = {}
        for node_id, host_cfg in hosts.items():
            # Jan 13, 2026: Add BOTH Tailscale IP and ssh_host for node resolution.
            # This enables IP->node mapping regardless of which IP the peer uses.
            ts_ip = host_cfg.get("tailscale_ip")
            if ts_ip:
                ip_to_node[ts_ip] = node_id
            # Also add ssh_host if it's a valid IP
            if host_cfg.get("ssh_host"):
                ssh_host = host_cfg["ssh_host"]
                if ssh_host and not any(c.isalpha() for c in ssh_host.replace(".", "")):
                    ip_to_node[ssh_host] = node_id
        return ip_to_node

    def _resolve_peer_id_to_node_name(self, peer_id: str) -> str:
        """Translate a SWIM peer ID (IP:port) to a node name if possible."""
        if ":" not in peer_id or not peer_id.split(":")[0].replace(".", "").isdigit():
            return peer_id
        ip = peer_id.split(":")[0]
        ip_map = getattr(self, "_ip_to_node_map", {})
        return ip_map.get(ip, peer_id)

    def _get_cached_peer_snapshot(self, max_age_seconds: float = 1.0) -> list:
        """Get cached peer snapshot to reduce lock acquisitions.

        Jan 12, 2026: Added to reduce lock contention in read-only contexts.
        Returns cached copy if < max_age_seconds old, otherwise takes new snapshot.

        Args:
            max_age_seconds: Maximum age of cached snapshot before refreshing (default 1.0s)

        Returns:
            List of peer NodeInfo objects (may be up to max_age_seconds stale)
        """
        now = time.time()
        cache_key = "_peer_snapshot_cache"
        cache_time_key = "_peer_snapshot_cache_time"

        cached = getattr(self, cache_key, None)
        cached_time = getattr(self, cache_time_key, 0)

        if cached is not None and (now - cached_time) < max_age_seconds:
            return cached

        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        snapshot = list(self._peer_snapshot.get_snapshot().values())

        setattr(self, cache_key, snapshot)
        setattr(self, cache_time_key, now)
        return snapshot

    def _find_voter_peer_by_ip(self, voter_id: str) -> tuple[str | None, "NodeInfo | None"]:
        """Find a voter's peer entry by matching their known IPs against peers dict.

        Jan 2, 2026: Added to fix voter heartbeat loop which was looking up voters
        by friendly name (e.g., 'hetzner-cpu1') but peers dict uses IP:port keys
        from SWIM (e.g., '135.181.39.239:7947').

        Jan 5, 2026: Fixed to try node_id matching first and check peer info 'host'
        field. The SWIM protocol (port 7947) is disabled - P2P runs on port 8770.
        Peers now register as node_id (friendly name) or IP:8770 format.

        Args:
            voter_id: The voter's friendly node_id (e.g., 'hetzner-cpu1')

        Returns:
            Tuple of (peer_key, peer_info) where peer_key is the key in
            self.peers, or (None, None) if not found.
        """
        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        peers_snapshot = self._peer_snapshot.get_snapshot()

        # Strategy 1: Direct node_id match (most reliable when peers use friendly names)
        if voter_id in peers_snapshot:
            return voter_id, peers_snapshot[voter_id]

        # Strategy 2: Get voter's known IPs from config
        voter_ip_map = self._build_voter_ip_mapping()
        voter_ips = voter_ip_map.get(voter_id, set())

        if not voter_ips:
            return None, None

        # Strategy 3: Check peer info 'host' field for IP match
        for peer_key, peer_info in peers_snapshot.items():
            if isinstance(peer_info, dict):
                peer_host = peer_info.get("host", "")
                if peer_host in voter_ips:
                    return peer_key, peer_info
            elif hasattr(peer_info, "host") and peer_info.host in voter_ips:
                return peer_key, peer_info

        # Strategy 4: Extract IP from peer_key (IP:PORT format, typically IP:8770)
        for peer_key, peer_info in peers_snapshot.items():
            if ":" in peer_key:
                peer_ip = peer_key.rsplit(":", 1)[0]
                if peer_ip in voter_ips:
                    return peer_key, peer_info

        return None, None

    def _count_alive_voters(self) -> int:
        """Count alive voters by checking both node_id and IP:port matches.

        Jan 2, 2026: Added because SWIM discovers peers as IP:port format
        (e.g., "135.181.39.239:7947") but voter_node_ids are proper names
        (e.g., "hetzner-cpu1"). This method checks both.

        Jan 5, 2026: Fixed to check peer info 'host' field. SWIM (port 7947) is
        disabled - P2P runs on port 8770. Peers register as node_id or IP:8770.

        Returns:
            Number of alive voters (including self if we are a voter)
        """
        voter_ids = getattr(self, "voter_node_ids", []) or []
        if not voter_ids:
            return 0

        alive_count = 0

        # Build voter IP mapping
        voter_ip_map = self._build_voter_ip_mapping()

        # Build reverse map: IP -> voter_id for quick lookup
        ip_to_voter: dict[str, str] = {}
        for voter_id, ips in voter_ip_map.items():
            for ip in ips:
                ip_to_voter[ip] = voter_id

        # Track which voters we've counted to avoid double-counting
        counted_voters: set[str] = set()

        # Jan 12, 2026: Get our advertised host for self-recognition
        # Fixes issue where node_id != voter_id but host matches voter's Tailscale IP
        self_host = getattr(self, "advertise_host", None) or getattr(self, "host", None)

        # Check each voter
        for voter_id in voter_ids:
            if voter_id in counted_voters:
                continue

            # Check 1a: Is this voter us? (by node_id)
            if voter_id == self.node_id:
                alive_count += 1
                counted_voters.add(voter_id)
                continue

            # Check 1b: Is this voter us? (by IP - for node_id mismatch cases)
            # Jan 12, 2026 Root cause fix: Lambda nodes derive node_id from hostname
            # (e.g., "192-222-51-195") but voter list uses configured names
            # (e.g., "lambda-gh200-training"). Check if our host matches voter's IPs.
            voter_ips = voter_ip_map.get(voter_id, set())
            if self_host and self_host in voter_ips:
                alive_count += 1
                counted_voters.add(voter_id)
                logger.debug(f"[VoterSelfRecognition] Matched self as voter {voter_id} via host {self_host}")
                continue

            # Check 2: Direct node_id match in peers
            peer = self.peers.get(voter_id)
            if peer and peer.is_alive():
                alive_count += 1
                counted_voters.add(voter_id)
                continue

            # Check 3: Peer info 'host' field match
            voter_ips = voter_ip_map.get(voter_id, set())
            for peer_id, peer in self.peers.items():
                if voter_id in counted_voters:
                    break
                # Jan 7, 2026: Skip SWIM protocol entries (IP:7947) - they pollute voter health
                if self._is_swim_peer_id(peer_id):
                    continue
                # Check peer info 'host' field
                if isinstance(peer, dict):
                    peer_host = peer.get("host", "")
                    if peer_host in voter_ips and self._is_peer_alive(peer):
                        alive_count += 1
                        counted_voters.add(voter_id)
                        break
                elif hasattr(peer, "host") and peer.host in voter_ips:
                    if peer.is_alive():
                        alive_count += 1
                        counted_voters.add(voter_id)
                        break

            if voter_id in counted_voters:
                continue

            # Check 4: Multi-address matching (Jan 13, 2026)
            # Peers now advertise ALL their addresses in heartbeats.
            # Check if any of the peer's advertised addresses match voter IPs.
            for peer_id, peer in self.peers.items():
                if voter_id in counted_voters:
                    break
                if self._is_swim_peer_id(peer_id):
                    continue

                # Get peer's advertised addresses
                peer_addresses: set[str] = set()
                if isinstance(peer, dict):
                    peer_addresses.update(peer.get("addresses", []))
                    if peer.get("tailscale_ip"):
                        peer_addresses.add(peer["tailscale_ip"])
                    if peer.get("host"):
                        peer_addresses.add(peer["host"])
                elif hasattr(peer, "addresses"):
                    peer_addresses.update(getattr(peer, "addresses", []) or [])
                    if getattr(peer, "tailscale_ip", None):
                        peer_addresses.add(peer.tailscale_ip)
                    if getattr(peer, "host", None):
                        peer_addresses.add(peer.host)

                # Check for intersection with voter IPs
                if peer_addresses & voter_ips:
                    is_alive = (
                        peer.is_alive()
                        if hasattr(peer, "is_alive")
                        else self._is_peer_alive(peer)
                    )
                    if is_alive:
                        alive_count += 1
                        counted_voters.add(voter_id)
                        logger.debug(
                            f"[VoterMultiAddr] Matched voter {voter_id} via multi-address: "
                            f"peer={peer_id}, matching={peer_addresses & voter_ips}"
                        )
                        break

            if voter_id in counted_voters:
                continue

            # Check 5: IP:port extraction from peer_id (format: "IP:8770")
            for peer_id, peer in self.peers.items():
                if voter_id in counted_voters:
                    break
                # Jan 7, 2026: Skip SWIM protocol entries (IP:7947) - they pollute voter health
                if self._is_swim_peer_id(peer_id):
                    continue
                if ":" in peer_id:
                    peer_ip = peer_id.rsplit(":", 1)[0]
                    if peer_ip in voter_ips:
                        is_alive = (
                            peer.is_alive()
                            if hasattr(peer, "is_alive")
                            else self._is_peer_alive(peer)
                        )
                        if is_alive:
                            alive_count += 1
                            counted_voters.add(voter_id)
                            break

        return alive_count

    def _is_peer_alive(self, peer_info: dict) -> bool:
        """Check if a peer (as dict) is alive based on its status field."""
        if isinstance(peer_info, dict):
            status = peer_info.get("status", "unknown")
            return status in ("alive", "healthy", "connected")
        return False

    def _is_swim_peer_id(self, peer_id: str) -> bool:
        """Check if peer_id is a SWIM protocol entry (IP:7947 format).

        SWIM entries use port 7947 and should not be in the HTTP peer list.
        These leak from the SWIM membership layer and cause peer pollution.

        Jan 7, 2026 Session 17.43: Added as defensive filter for peer iteration.
        Primary fix is in the SWIM membership loop, but this provides defense-in-depth.

        Args:
            peer_id: Node identifier to check.

        Returns:
            True if this is a SWIM-format peer ID (should be skipped).
        """
        if not peer_id or ":" not in peer_id:
            return False
        parts = peer_id.rsplit(":", 1)
        if len(parts) == 2 and parts[1] == "7947":
            return True
        return False

    def _check_voter_health(self) -> dict[str, any]:
        """Check health status of all configured voters and emit alerts.

        Jan 2, 2026: Voter health monitoring for proactive alerting.
        Returns a dict with voter health status and emits events for:
        - Individual voter offline
        - Quorum threatened (voters_alive < quorum_size + 1)
        - Quorum lost (voters_alive < quorum_size)

        Returns:
            Dict with voters_total, voters_alive, voters_offline, quorum_size,
            quorum_ok, quorum_threatened, and per-voter status.
        """
        voter_ids = getattr(self, "voter_node_ids", []) or []
        if not voter_ids:
            return {
                "voters_total": 0,
                "voters_alive": 0,
                "voters_offline": [],
                "quorum_size": 0,
                "quorum_ok": True,
                "quorum_threatened": False,
                "voter_status": {},
            }

        quorum_size = getattr(self, "voter_quorum_size", 0) or 0
        voter_ip_map = self._build_voter_ip_mapping()

        # Check each voter's status
        voter_status: dict[str, dict] = {}
        alive_voters: list[str] = []
        offline_voters: list[str] = []

        # Track last known status for change detection
        prev_offline = set(getattr(self, "_last_offline_voters", []))

        # Jan 12, 2026: Get our advertised host for self-recognition by IP
        self_host = getattr(self, "advertise_host", None) or getattr(self, "host", None)

        for voter_id in voter_ids:
            is_alive = False
            status_detail = "unknown"

            # Check 1a: Is this voter us? (by node_id)
            if voter_id == self.node_id:
                is_alive = True
                status_detail = "self"
            # Check 1b: Is this voter us? (by IP - for node_id mismatch cases)
            # Jan 12, 2026 Root cause fix: node_id may differ from voter name
            elif self_host and self_host in voter_ip_map.get(voter_id, set()):
                is_alive = True
                status_detail = "self_via_ip"
            else:
                # Check 2: Direct node_id match in peers
                peer = self.peers.get(voter_id)
                if peer and peer.is_alive():
                    is_alive = True
                    status_detail = "direct"
                else:
                    # Check 3: Peer host or IP:port match
                    # Jan 12, 2026: Also check peer.host field for peers with different node_ids
                    voter_ips = voter_ip_map.get(voter_id, set())
                    for peer_id, p in self.peers.items():
                        # Jan 7, 2026: Skip SWIM protocol entries (IP:7947) - they pollute voter health
                        if self._is_swim_peer_id(peer_id):
                            continue
                        # Check peer's host field
                        peer_host = getattr(p, "host", None) or (p.get("host") if isinstance(p, dict) else None)
                        if peer_host and peer_host in voter_ips and p.is_alive():
                            is_alive = True
                            status_detail = f"host_match:{peer_host}"
                            break
                        # Check IP:port format peer_id
                        if ":" in peer_id:
                            peer_ip = peer_id.split(":")[0]
                            if peer_ip in voter_ips and p.is_alive():
                                is_alive = True
                                status_detail = f"ip_match:{peer_ip}"
                                break
                    if not is_alive:
                        status_detail = "unreachable"

            voter_status[voter_id] = {
                "alive": is_alive,
                "detail": status_detail,
            }
            if is_alive:
                alive_voters.append(voter_id)
            else:
                offline_voters.append(voter_id)

        # Store for next comparison
        self._last_offline_voters = offline_voters

        voters_alive = len(alive_voters)
        voters_total = len(voter_ids)
        quorum_ok = voters_alive >= quorum_size
        quorum_threatened = voters_alive <= quorum_size  # Just at or below threshold

        # Emit events for status changes
        newly_offline = set(offline_voters) - prev_offline
        newly_online = prev_offline - set(offline_voters)

        # Jan 11, 2026: Skip warnings during startup grace period
        # Voters appear offline until initial heartbeats are exchanged
        in_grace_period = (time.time() - getattr(self, "_startup_time", 0)) < STARTUP_GRACE_PERIOD

        for voter_id in newly_offline:
            if in_grace_period:
                logger.debug(
                    f"[VoterHealth] Voter {voter_id} appears offline during startup grace period "
                    f"(waiting for heartbeats, {STARTUP_GRACE_PERIOD - (time.time() - self._startup_time):.0f}s remaining)"
                )
            else:
                logger.warning(
                    f"[VoterHealth] Voter {voter_id} went OFFLINE "
                    f"({voters_alive}/{voters_total} alive, quorum={quorum_size})"
                )

        for voter_id in newly_online:
            logger.info(
                f"[VoterHealth] Voter {voter_id} came ONLINE "
                f"({voters_alive}/{voters_total} alive, quorum={quorum_size})"
            )

        # Periodic summary log (suppress during grace period)
        if offline_voters and not in_grace_period:
            logger.warning(
                f"[VoterHealth] Status: {voters_alive}/{voters_total} voters alive, "
                f"quorum={'OK' if quorum_ok else 'LOST'}, "
                f"offline: {offline_voters}"
            )

        return {
            "voters_total": voters_total,
            "voters_alive": voters_alive,
            "voters_offline": offline_voters,
            "quorum_size": quorum_size,
            "quorum_ok": quorum_ok,
            "quorum_threatened": quorum_threatened,
            "voter_status": voter_status,
        }

    def _maybe_adopt_voter_node_ids(self, voter_node_ids: list[str], *, source: str) -> bool:
        """Adopt/override the voter set when it's not explicitly configured.

        This is a convergence mechanism: some nodes may boot without local
        config (or with stale config), which would disable quorum gating and
        allow non-voter nodes to become leaders. Leaders propagate the stable
        voter set via `/coordinator` so the cluster converges.

        Dec 2025: Don't override if config source is 'env' or 'config' (YAML).
        Dec 2025: Added strict validation to prevent voter set flapping.
        """
        # If explicitly configured via env var, never override
        if (os.environ.get("RINGRIFT_P2P_VOTERS") or "").strip():
            return False

        # If explicitly configured via YAML, never override from gossip
        if getattr(self, "voter_config_source", "") == "config":
            return False

        normalized = sorted({str(v).strip() for v in (voter_node_ids or []) if str(v).strip()})
        if not normalized:
            return False

        # Dec 2025: Strict validation to prevent voter set flapping
        # Reject voter sets that are too small (need at least 3 for meaningful quorum)
        if len(normalized) < 3:
            return False

        # Dec 2025: Canonical voters that MUST be in any valid voter set
        # These are stable, always-on nodes that should always be voters
        canonical_voters = {"nebius-backbone-1", "vultr-a100-20gb"}
        has_canonical = bool(canonical_voters & set(normalized))
        if not has_canonical:
            # Reject voter sets that don't include any canonical voter
            return False

        current = sorted(set(getattr(self, "voter_node_ids", []) or []))
        if current == normalized:
            return False

        # Dec 2025: Once we have a learned voter set with 5+ voters, don't downgrade
        if len(current) >= 5 and len(normalized) < len(current):
            return False

        self.voter_node_ids = normalized
        # SIMPLIFIED QUORUM: Fixed at 3 voters (or less if fewer voters exist)
        self.voter_quorum_size = min(VOTER_MIN_QUORUM, len(normalized))
        self.voter_config_source = source or "learned"
        print(
            f"[P2P] Updated voter set ({self.voter_config_source}): voters={len(normalized)}, "
            f"quorum={self.voter_quorum_size} ({', '.join(normalized)})"
        )
        return True

    # _has_voter_quorum: Provided by LeaderElectionMixin
    # _release_voter_grant_if_self: Provided by LeaderElectionMixin

    def _enable_partition_local_election(self) -> bool:
        """Enable local leader election for partitioned nodes.

        When a partition is detected and no voters are reachable, this method
        temporarily adds reachable nodes to the voter set so they can elect a
        local leader and continue operating autonomously.

        This is a self-healing mechanism for network splits. When connectivity
        is restored, the partition will merge back with the main cluster.

        Returns:
            True if local election was enabled
        """
        # Don't override env-configured voters
        if (os.environ.get("RINGRIFT_P2P_VOTERS") or "").strip():
            return False

        # Check if we have any voters configured
        voters = list(getattr(self, "voter_node_ids", []) or [])

        # Count how many voters are reachable
        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        peers_by_id = self._peer_snapshot.get_snapshot()
        reachable_voters = 0
        for voter_id in voters:
            if voter_id == self.node_id:
                reachable_voters += 1
                continue
            peer = peers_by_id.get(voter_id)
            if peer and peer.is_alive():
                reachable_voters += 1

        # If we have quorum (simplified: 3 voters), no need for partition election
        quorum = min(VOTER_MIN_QUORUM, len(voters)) if voters else 1
        if reachable_voters >= quorum:
            return False

        # Build local partition voter set from reachable nodes
        local_voters = [self.node_id]  # Always include self
        for node_id, peer in peers_by_id.items():
            if peer.is_alive() and node_id not in local_voters:
                local_voters.append(node_id)

        if len(local_voters) < 2:
            # Need at least 2 nodes for meaningful election
            return False

        # Store original voters for restoration
        if not hasattr(self, "_original_voters"):
            self._original_voters = voters.copy()
            self._partition_election_started = time.time()

        # Enable partition-local election
        self.voter_node_ids = sorted(local_voters)
        self.voter_quorum_size = min(VOTER_MIN_QUORUM, len(local_voters))
        self.voter_config_source = "partition-local"
        print(
            f"[P2P] PARTITION: Enabling local election with {len(local_voters)} nodes: "
            f"{', '.join(local_voters)} (quorum={self.voter_quorum_size})"
        )
        return True

    def _restore_original_voters(self) -> bool:
        """Restore original voter configuration after partition heals.

        Called when connectivity to the main cluster is restored.

        Returns:
            True if voters were restored
        """
        if not hasattr(self, "_original_voters"):
            return False

        original = getattr(self, "_original_voters", [])
        if not original:
            return False

        # Check if we can reach any original voters
        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        peers_by_id = self._peer_snapshot.get_snapshot()
        for voter_id in original:
            if voter_id == self.node_id:
                continue
            peer = peers_by_id.get(voter_id)
            if peer and peer.is_alive():
                # We can reach at least one original voter, restore config
                self.voter_node_ids = original.copy()
                # SIMPLIFIED QUORUM: Fixed at 3 voters (or less if fewer voters exist)
                self.voter_quorum_size = min(VOTER_MIN_QUORUM, len(original))
                self.voter_config_source = "restored"
                delattr(self, "_original_voters")
                if hasattr(self, "_partition_election_started"):
                    delattr(self, "_partition_election_started")
                logger.info(f"Partition healed: restored original voters {', '.join(original)}")
                return True
        return False

    def _get_eligible_voters(self) -> list[str]:
        """Get list of nodes eligible to be voters (GPU nodes with good health)."""
        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        peers = self._peer_snapshot.get_snapshot()

        eligible = []
        now = time.time()

        for node_id, peer in peers.items():
            # Skip retired or NAT-blocked without relay
            if getattr(peer, "retired", False):
                continue

            # Must be alive
            if not peer.is_alive():
                continue

            # Must have GPU (CUDA or MPS)
            has_gpu = getattr(peer, "has_gpu", False)
            gpu_name = str(getattr(peer, "gpu_name", "") or "")
            if not has_gpu and "GH200" not in gpu_name and "H100" not in gpu_name and "A10" not in gpu_name and "aws" not in node_id.lower():
                continue

            # Must have been up for minimum time
            first_seen = getattr(peer, "first_seen", 0) or peer.last_heartbeat
            if now - first_seen < VOTER_PROMOTION_UPTIME:
                continue

            # Check health score (response rate)
            failures = getattr(peer, "consecutive_failures", 0)
            if failures >= VOTER_DEMOTION_FAILURES:
                continue

            eligible.append(node_id)

        # Always include self if we have GPU
        if self.node_id not in eligible:
            self_gpu = getattr(self, "has_gpu", False)
            if self_gpu or "aws" in self.node_id.lower() or "lambda" in self.node_id.lower():
                eligible.append(self.node_id)

        return sorted(eligible)

    def _manage_dynamic_voters(self) -> bool:
        """Manage dynamic voter pool - promote/demote voters as needed.

        Returns True if voter set was changed.
        """
        if not DYNAMIC_VOTER_ENABLED:
            return False

        # Don't override env-configured voters
        if (os.environ.get("RINGRIFT_P2P_VOTERS") or "").strip():
            return False

        current_voters = list(getattr(self, "voter_node_ids", []) or [])
        eligible = self._get_eligible_voters()

        # Count how many current voters are healthy
        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        peers = self._peer_snapshot.get_snapshot()

        healthy_voters = []
        unhealthy_voters = []

        for voter_id in current_voters:
            if voter_id == self.node_id:
                healthy_voters.append(voter_id)
                continue
            peer = peers.get(voter_id)
            if peer and peer.is_alive():
                failures = getattr(peer, "consecutive_failures", 0)
                if failures < VOTER_DEMOTION_FAILURES:
                    healthy_voters.append(voter_id)
                else:
                    unhealthy_voters.append(voter_id)
            else:
                unhealthy_voters.append(voter_id)

        changed = False
        new_voters = healthy_voters.copy()

        # Demote unhealthy voters
        if unhealthy_voters:
            logger.info(f"Dynamic voters: demoting unhealthy voters: {unhealthy_voters}")
            changed = True

        # Promote new voters if below target
        if len(new_voters) < DYNAMIC_VOTER_TARGET:
            candidates = [n for n in eligible if n not in new_voters]
            # Sort by reliability (fewer failures = better)
            candidates.sort(key=lambda n: getattr(peers.get(n), "consecutive_failures", 0) if peers.get(n) else 999)

            needed = DYNAMIC_VOTER_TARGET - len(new_voters)
            for candidate in candidates[:needed]:
                new_voters.append(candidate)
                logger.info(f"Dynamic voters: promoting {candidate} to voter")
                changed = True

        if changed and new_voters:
            new_voters = sorted(set(new_voters))
            self.voter_node_ids = new_voters
            # SIMPLIFIED QUORUM: Fixed at 3 voters (or less if fewer voters exist)
            self.voter_quorum_size = min(VOTER_MIN_QUORUM, len(new_voters))
            self.voter_config_source = "dynamic"
            print(
                f"[P2P] Dynamic voter set updated: {len(new_voters)} voters, "
                f"quorum={self.voter_quorum_size} ({', '.join(new_voters)})"
            )
            return True

        return False

    def _check_leader_health(self) -> bool:
        """Check leader health based on peer response rates.

        Returns True if health is good, False if degraded.
        """
        if self.role != NodeRole.LEADER:
            return True

        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        peers = list(self._peer_snapshot.get_snapshot().values())

        if not peers:
            return True

        # Calculate response rate (peers that responded recently)
        now = time.time()
        alive_count = sum(1 for p in peers if p.is_alive() and not getattr(p, "retired", False))
        total_count = sum(1 for p in peers if not getattr(p, "retired", False))

        if total_count == 0:
            return True

        response_rate = alive_count / total_count

        # Track degraded state
        if not hasattr(self, "_leader_degraded_since"):
            self._leader_degraded_since = 0.0

        if response_rate < LEADER_MIN_RESPONSE_RATE:
            if self._leader_degraded_since == 0.0:
                self._leader_degraded_since = now
                logger.info(f"Leader health degraded: {response_rate:.1%} response rate (min: {LEADER_MIN_RESPONSE_RATE:.0%})")
            elif now - self._leader_degraded_since > LEADER_DEGRADED_STEPDOWN_DELAY:
                logger.info(f"Leader health critically degraded for {LEADER_DEGRADED_STEPDOWN_DELAY}s, stepping down")
                self._leader_degraded_since = 0.0
                return False
        else:
            if self._leader_degraded_since > 0:
                logger.info(f"Leader health recovered: {response_rate:.1%} response rate")
            self._leader_degraded_since = 0.0

        return True

    async def _acquire_voter_lease_quorum(self, lease_id: str, duration: int) -> float | None:
        """Acquire/renew an exclusive leader lease from a quorum of voters.

        December 29, 2025: Added retry with exponential backoff when initial
        quorum acquisition fails. This handles transient network issues.

        Returns the effective lease expiry timestamp if a quorum granted the
        lease; otherwise returns None.
        """
        voter_ids = list(getattr(self, "voter_node_ids", []) or [])
        if not voter_ids:
            return time.time() + float(duration)

        quorum = int(getattr(self, "voter_quorum_size", 0) or 0)
        if quorum <= 0:
            # SIMPLIFIED QUORUM: Fixed at 3 voters (or less if fewer voters exist)
            quorum = min(VOTER_MIN_QUORUM, len(voter_ids))

        duration = max(10, min(int(duration), int(LEADER_LEASE_DURATION * 2)))

        # December 29, 2025: Retry with exponential backoff
        max_retries = 3
        retry_delays = [0, 2, 5]  # Immediate, then 2s, then 5s

        for attempt in range(max_retries):
            if attempt > 0:
                await asyncio.sleep(retry_delays[attempt])
                logger.info(f"Voter lease acquisition retry {attempt + 1}/{max_retries}")

            now = time.time()
            acks = 0
            lease_ttls: list[float] = []

            # Self-grant (as a voter).
            if self.node_id in voter_ids:
                self.voter_grant_leader_id = self.node_id
                self.voter_grant_lease_id = lease_id
                self.voter_grant_expires = now + float(duration)
                lease_ttls.append(float(duration))
                acks += 1

            # Jan 2026: Use lock-free PeerSnapshot for read-only access
            peers_by_id = self._peer_snapshot.get_snapshot()

            # STABILITY FIX: Use 15s timeout for voter lease operations (was 5s).
            # Cross-geographic Tailscale connections can have latency spikes.
            timeout = ClientTimeout(total=15)

            # Dec 29, 2025: Parallel lease acquisition for faster leadership transitions
            # Instead of sequential requests, we fire all lease requests in parallel
            async def _request_lease_from_voter(
                session: aiohttp.ClientSession,
                voter_id: str,
                voter: NodeInfo,
            ) -> tuple[bool, float | None]:
                """Request lease from a single voter. Returns (success, ttl)."""
                payload = {
                    "leader_id": self.node_id,
                    "lease_id": lease_id,
                    "lease_duration": duration,
                    "lease_epoch": self._lease_epoch + 1,
                }
                for url in self._tailscale_urls_for_voter(voter, "/election/lease"):
                    try:
                        async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                            if resp.status != 200:
                                continue
                            data, json_error = await safe_json_response(resp, default={}, log_errors=False)
                            if json_error or not data.get("granted"):
                                return False, None
                            ttl_raw = data.get("lease_ttl_seconds") or data.get("ttl_seconds")
                            if ttl_raw is not None:
                                try:
                                    return True, float(ttl_raw)
                                except (ValueError, TypeError):
                                    pass
                            return True, float(duration)
                    except (aiohttp.ClientError, asyncio.TimeoutError, ValueError, AttributeError, OSError):
                        continue
                return False, None

            async with get_client_session(timeout) as session:
                # Build list of voters to request from (excluding self and dead peers)
                voter_tasks = []
                for voter_id in voter_ids:
                    if voter_id == self.node_id:
                        continue
                    voter = peers_by_id.get(voter_id)
                    if not voter or not voter.is_alive():
                        continue
                    voter_tasks.append(_request_lease_from_voter(session, voter_id, voter))

                # Fire all requests in parallel
                if voter_tasks:
                    results = await asyncio.gather(*voter_tasks, return_exceptions=True)
                    for result in results:
                        if isinstance(result, Exception):
                            continue
                        success, ttl = result
                        if success:
                            acks += 1
                            if ttl is not None and ttl > 0:
                                lease_ttls.append(ttl)
                            else:
                                lease_ttls.append(float(duration))

            if acks >= quorum:
                # Use a relative TTL (computed by each voter on its own clock) to avoid
                # leader lease flapping under clock skew. Convert back to a local expiry.
                effective_ttl = min(lease_ttls) if lease_ttls else float(duration)
                effective_ttl = max(10.0, min(float(duration), float(effective_ttl)))
                if attempt > 0:
                    logger.info(f"Voter lease acquired on retry {attempt + 1}")
                return now + float(effective_ttl)

            # Log retry info
            if attempt < max_retries - 1:
                logger.warning(
                    f"Voter lease quorum not reached: {acks}/{quorum} acks, "
                    f"retrying in {retry_delays[attempt + 1]}s..."
                )

        # All retries exhausted
        logger.error(f"Failed to acquire voter lease quorum after {max_retries} attempts")
        return None

    # =========================================================================
    # Phase 15.1.1: Fence Token Helpers (December 29, 2025)
    # =========================================================================

    def get_fence_token(self) -> str:
        """Get the current fence token for including in leader operations.

        Phase 15.1.1: Fence tokens provide split-brain protection by ensuring
        workers can reject commands from stale leaders.

        Returns:
            Current fence token or empty string if not leader
        """
        if self.role != NodeRole.LEADER:
            return ""
        return self._fence_token

    def get_lease_epoch(self) -> int:
        """Get the current lease epoch.

        Phase 15.1.1: The epoch is monotonically increasing and helps
        resolve split-brain by allowing workers to compare epochs.

        Returns:
            Current lease epoch (0 if never been leader)
        """
        return self._lease_epoch

    def validate_fence_token(self, token: str) -> tuple[bool, str]:
        """Validate an incoming fence token from a claimed leader.

        Phase 15.1.1: Workers use this to reject commands from stale leaders.
        A token is valid if:
        1. It's from the current known leader
        2. Its epoch is >= our known epoch

        Args:
            token: Fence token to validate (format: node_id:epoch:timestamp)

        Returns:
            (valid, reason) tuple
        """
        if not token:
            return False, "empty_fence_token"

        try:
            parts = token.split(":")
            if len(parts) != 3:
                return False, "malformed_token"

            token_node_id = parts[0]
            token_epoch = int(parts[1])

            # Check if token is from known leader
            if self.leader_id and token_node_id != self.leader_id:
                return False, f"token_from_unknown_leader:{token_node_id}"

            # Check epoch - reject if lower than what we've seen
            if hasattr(self, "_last_seen_epoch"):
                if token_epoch < self._last_seen_epoch:
                    return False, f"stale_epoch:{token_epoch}<{self._last_seen_epoch}"
                self._last_seen_epoch = max(self._last_seen_epoch, token_epoch)
            else:
                self._last_seen_epoch = token_epoch

            return True, "valid"

        except (ValueError, IndexError) as e:
            return False, f"parse_error:{e}"

    def update_fence_token_from_leader(self, token: str, leader_id: str) -> bool:
        """Update internal fence token state when receiving leader announcement.

        Phase 15.1.1: Called when a follower receives a coordinator announcement
        to update the last seen epoch for fence token validation.

        Args:
            token: Fence token from leader
            leader_id: Leader node ID

        Returns:
            True if update was successful
        """
        if not token:
            return False

        try:
            parts = token.split(":")
            if len(parts) != 3:
                return False

            token_epoch = int(parts[1])

            # Update last seen epoch (but never decrease)
            if hasattr(self, "_last_seen_epoch"):
                if token_epoch >= self._last_seen_epoch:
                    self._last_seen_epoch = token_epoch
                    return True
                else:
                    # Reject older epoch
                    logger.warning(
                        f"Rejecting fence token with stale epoch {token_epoch} "
                        f"(current: {self._last_seen_epoch}) from {leader_id}"
                    )
                    return False
            else:
                self._last_seen_epoch = token_epoch
                return True

        except (ValueError, IndexError):
            return False

    async def _determine_leased_leader_from_voters(self) -> str | None:
        """Return the current lease-holder as reported by a quorum of voters.

        This is a read-only reconciliation step used to resolve split-brain once
        partitions heal. It queries the current voter grant state via
        `/election/grant` and selects the leader_id that has >= quorum votes with
        non-expired grants.
        """
        voter_ids = list(getattr(self, "voter_node_ids", []) or [])
        if not voter_ids:
            return None

        quorum = int(getattr(self, "voter_quorum_size", 0) or 0)
        if quorum <= 0:
            # SIMPLIFIED QUORUM: Fixed at 3 voters (or less if fewer voters exist)
            quorum = min(VOTER_MIN_QUORUM, len(voter_ids))

        now = time.time()
        counts: dict[str, int] = {}

        # Include local voter state.
        if self.node_id in voter_ids:
            leader_id = str(getattr(self, "voter_grant_leader_id", "") or "")
            expires = float(getattr(self, "voter_grant_expires", 0.0) or 0.0)
            if leader_id and expires > now:
                counts[leader_id] = counts.get(leader_id, 0) + 1

        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        peers_by_id = self._peer_snapshot.get_snapshot()

        # STABILITY FIX: Use 15s timeout for voter operations (was 5s).
        timeout = ClientTimeout(total=15)
        async with get_client_session(timeout) as session:
            for voter_id in voter_ids:
                if voter_id == self.node_id:
                    continue
                voter = peers_by_id.get(voter_id)
                if not voter or not voter.is_alive():
                    continue

                # Use Tailscale-exclusive URLs for voter communication to avoid NAT issues
                for url in self._tailscale_urls_for_voter(voter, "/election/grant"):
                    try:
                        async with session.get(url, headers=self._auth_headers()) as resp:
                            if resp.status != 200:
                                continue
                            data = await resp.json()
                        leader_id = str((data or {}).get("leader_id") or "")
                        if not leader_id:
                            break
                        ttl_raw = (data or {}).get("lease_ttl_seconds")
                        if ttl_raw is None:
                            ttl_raw = (data or {}).get("ttl_seconds")
                        ttl_val: float | None = None
                        if ttl_raw is not None:
                            try:
                                ttl_val = float(ttl_raw)
                            except (ValueError):
                                ttl_val = None

                        if ttl_val is not None:
                            if ttl_val <= 0:
                                break
                        else:
                            # Back-compat: use absolute expiry as best-effort, with
                            # a generous skew tolerance (1 lease duration).
                            expires = float((data or {}).get("lease_expires") or 0.0)
                            if expires <= 0:
                                break
                            if expires + float(LEADER_LEASE_DURATION) < now:
                                break
                        counts[leader_id] = counts.get(leader_id, 0) + 1
                        break
                    except (ValueError, AttributeError):
                        continue

        winners = [leader_id for leader_id, count in counts.items() if count >= quorum]
        if not winners:
            return None
        # Deterministic: if multiple satisfy quorum (shouldn't), pick highest node_id.
        return sorted(winners)[-1]

    async def _query_arbiter_for_leader(self) -> str | None:
        """Query the arbiter for the authoritative leader when voter quorum fails.

        The arbiter is a reliably-reachable node that maintains its view of
        who the leader should be. Used as a fallback when split-brain causes
        voter quorum to be unreachable.

        Returns:
            The leader_id from the arbiter, or None if arbiter is unreachable
        """
        arbiter_url = ARBITER_URL
        if not arbiter_url:
            return None

        # Try the configured arbiter URL
        urls_to_try = [arbiter_url]

        # Also try known peers as arbiters if main arbiter fails
        for peer_addr in (self.known_peers or []):
            if peer_addr not in urls_to_try:
                urls_to_try.append(peer_addr)

        timeout = ClientTimeout(total=5)
        try:
            async with get_client_session(timeout) as session:
                for url in urls_to_try:
                    try:
                        base_url = url.rstrip("/")
                        # Query the arbiter's election/grant endpoint to see who they think is leader
                        async with session.get(
                            f"{base_url}/election/grant",
                            headers=self._auth_headers()
                        ) as resp:
                            if resp.status == 200:
                                data = await resp.json()
                                leader_id = str((data or {}).get("leader_id") or "")
                                if leader_id:
                                    logger.info(f"Arbiter {base_url} reports leader: {leader_id}")
                                    return leader_id
                    except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
                        # Try next arbiter
                        continue
        except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
            pass

        return None

    # _parse_peer_address, _url_for_peer, _urls_for_peer provided by NetworkUtilsMixin

    def _auth_headers(self) -> dict[str, str]:
        if not self.auth_token:
            return {}
        return {"Authorization": f"Bearer {self.auth_token}"}

    def _get_leader_peer(self) -> NodeInfo | None:
        if self._is_leader():
            return self.self_info

        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        peers_snapshot = list(self._peer_snapshot.get_snapshot().values())

        conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])

        leader_id = self.leader_id
        if leader_id and self._is_leader_lease_valid():
            for peer in peers_snapshot:
                if (
                    peer.node_id == leader_id
                    and peer.role == NodeRole.LEADER
                    and peer.is_alive()
                    and self._is_leader_eligible(peer, conflict_keys)
                ):
                    # Jan 8, 2026: Validate consensus - check that other peers agree
                    consensus_count = self._count_peers_reporting_leader(leader_id, peers_snapshot)
                    if consensus_count < 2 and len(peers_snapshot) >= 3:
                        # Low consensus - log warning but still return leader
                        logger.warning(
                            f"[LeaderConsensus] Low consensus for leader {leader_id}: "
                            f"only {consensus_count} peers agree out of {len(peers_snapshot)}"
                        )
                    return peer

        eligible_leaders = [
            peer for peer in peers_snapshot
            if peer.role == NodeRole.LEADER and self._is_leader_eligible(peer, conflict_keys)
        ]
        if eligible_leaders:
            return sorted(eligible_leaders, key=lambda p: p.node_id)[-1]

        return None

    def _count_peers_reporting_leader(
        self, leader_id: str, peers_snapshot: list[NodeInfo]
    ) -> int:
        """Count how many peers report the same leader_id.

        Jan 8, 2026: Added for leader consensus validation.

        Args:
            leader_id: The leader ID to check for consensus
            peers_snapshot: List of peer NodeInfo objects

        Returns:
            Number of peers reporting this leader_id
        """
        count = 0
        for peer in peers_snapshot:
            if not peer.is_alive():
                continue
            # Check if peer reports this leader
            peer_leader = getattr(peer, "leader_id", None)
            if peer_leader == leader_id:
                count += 1
        return count

    async def _proxy_to_leader(self, request: web.Request) -> web.StreamResponse:
        """Best-effort proxy for leader-only APIs when the dashboard hits a follower."""
        leader = self._get_leader_peer()
        if not leader:
            return web.json_response(
                {"success": False, "error": "leader_unknown", "leader_id": self.leader_id},
                status=503,
            )

        candidate_urls = self._urls_for_peer(leader, request.raw_path)
        if not candidate_urls:
            candidate_urls = [self._url_for_peer(leader, request.raw_path)]
        forward_headers: dict[str, str] = {}
        for h in ("Authorization", "X-RingRift-Auth", "Content-Type"):
            if h in request.headers:
                forward_headers[h] = request.headers[h]

        body: bytes | None = None
        if request.method not in ("GET", "HEAD", "OPTIONS"):
            body = await request.read()

        # Keep leader-proxy responsive: unreachable "leaders" (often NAT/firewall)
        # should fail fast so the dashboard doesn't hang for a full minute.
        timeout = ClientTimeout(total=10)
        last_exc: Exception | None = None
        async with get_client_session(timeout) as session:
            for target_url in candidate_urls:
                try:
                    async with session.request(
                        request.method,
                        target_url,
                        data=body,
                        headers=forward_headers,
                    ) as resp:
                        payload = await resp.read()
                        content_type = resp.headers.get("Content-Type")
                        headers: dict[str, str] = {}
                        if content_type:
                            headers["Content-Type"] = content_type
                        headers["X-RingRift-Proxied-By"] = self.node_id
                        headers["X-RingRift-Proxied-To"] = target_url
                        return web.Response(body=payload, status=resp.status, headers=headers)
                except Exception as exc:
                    last_exc = exc
                    continue

        return web.json_response(
            {
                "success": False,
                "error": "leader_proxy_failed",
                "message": str(last_exc) if last_exc else "unknown_error",
                "leader_id": self.leader_id,
                "attempted_urls": candidate_urls,
            },
            status=502,
        )

    def _is_request_authorized(self, request: web.Request) -> bool:
        if not self.auth_token:
            return True

        auth_header = request.headers.get("Authorization", "")
        token = ""
        if auth_header.lower().startswith("bearer "):
            token = auth_header[7:].strip()
        if not token:
            token = request.headers.get("X-RingRift-Auth", "").strip()
        if not token:
            return False

        return secrets.compare_digest(token, self.auth_token)

    def _init_database(self):
        """Initialize SQLite database for state persistence.

        Phase 1 Refactoring: Delegated to StateManager.
        Kept for backward compatibility with any external callers.
        """
        self.state_manager.init_database()

    def _db_connect(self) -> sqlite3.Connection:
        """Create a database connection with proper settings.

        Phase 1 Refactoring: Delegated to StateManager.
        Kept for backward compatibility with other internal methods.
        """
        return self.state_manager._db_connect()

    def _load_state(self):
        """Load persisted state from database.

        Phase 1 Refactoring: Delegated to StateManager.
        The StateManager returns a PersistedState object which is then
        applied to the orchestrator's instance variables.
        """
        try:
            state = self.state_manager.load_state(self.node_id)

            # P2P Hardening Phase 2 (Dec 2025): Validate and clean stale state
            is_valid, issues = self.state_manager.validate_loaded_state(state)
            if issues:
                # Clean up stale entries before applying state
                jobs_removed, peers_removed = self.state_manager.clean_stale_state(state)
                if self.verbose:
                    logger.info(
                        f"[P2POrchestrator] Startup cleanup: removed "
                        f"{jobs_removed} stale jobs, {peers_removed} stale peers"
                    )

            # Apply loaded peers
            for node_id, info_dict in state.peers.items():
                try:
                    info = NodeInfo.from_dict(info_dict)
                    self.peers[node_id] = info
                except Exception as e:  # noqa: BLE001
                    logger.error(f"Failed to load peer {node_id}: {e}")
            # C2 fix: Sync peer snapshot after loading persisted peers
            self._sync_peer_snapshot()

            # Apply loaded jobs
            for job_dict in state.jobs:
                try:
                    job = ClusterJob(
                        job_id=job_dict["job_id"],
                        job_type=JobType(job_dict["job_type"]),
                        node_id=job_dict["node_id"],
                        board_type=job_dict.get("board_type", "square8"),
                        num_players=job_dict.get("num_players", 2),
                        engine_mode=job_dict.get("engine_mode", "descent-only"),
                        pid=job_dict.get("pid", 0),
                        started_at=job_dict.get("started_at", 0.0),
                        status=job_dict.get("status", "running"),
                    )
                    self.local_jobs[job.job_id] = job
                except Exception as e:  # noqa: BLE001
                    logger.error(f"Failed to load job: {e}")

            # Apply leader state
            # C1 fix: Use leader_state_lock for role/leader_id changes
            ls = state.leader_state
            with self.leader_state_lock:
                if ls.leader_id:
                    self.leader_id = ls.leader_id
                if ls.leader_lease_id:
                    self.leader_lease_id = ls.leader_lease_id
                if ls.leader_lease_expires:
                    self.leader_lease_expires = ls.leader_lease_expires
                if ls.last_lease_renewal:
                    self.last_lease_renewal = ls.last_lease_renewal
                if ls.role:
                    with contextlib.suppress(Exception):
                        self.role = NodeRole(ls.role)

            # Voter grant state
            if ls.voter_grant_leader_id:
                self.voter_grant_leader_id = ls.voter_grant_leader_id
            if ls.voter_grant_lease_id:
                self.voter_grant_lease_id = ls.voter_grant_lease_id
            if ls.voter_grant_expires:
                self.voter_grant_expires = ls.voter_grant_expires

            # Phase 15.1.1: Restore fenced lease token state
            # These fields may not exist in older state files, so use getattr with defaults
            persisted_epoch = getattr(ls, "lease_epoch", 0) or 0
            persisted_fence = getattr(ls, "fence_token", "") or ""
            persisted_last_seen = getattr(ls, "last_seen_epoch", 0) or 0
            # Only restore if higher than current (monotonic guarantee)
            if persisted_epoch > self._lease_epoch:
                self._lease_epoch = persisted_epoch
            if persisted_fence and not self._fence_token:
                self._fence_token = persisted_fence
            if persisted_last_seen > self._last_seen_epoch:
                self._last_seen_epoch = persisted_last_seen
            if persisted_epoch > 0:
                logger.info(
                    f"[P2POrchestrator] Restored lease fencing: epoch={self._lease_epoch}, "
                    f"last_seen={self._last_seen_epoch}"
                )

            # Optional persisted voter configuration (convergence helper). Only
            # apply when voters are not explicitly configured via env/config.
            if (
                ls.voter_node_ids
                and not (getattr(self, "voter_node_ids", []) or [])
                and str(getattr(self, "voter_config_source", "none") or "none") == "none"
            ):
                self._maybe_adopt_voter_node_ids(ls.voter_node_ids, source="state")

            # Self-heal inconsistent persisted leader state (can happen after
            # abrupt shutdowns or partial writes): never keep role=leader without
            # a matching leader_id.
            if self.role == NodeRole.LEADER and not self.leader_id:
                logger.info("Loaded role=leader but leader_id is empty; stepping down to follower")
                # C1 fix: Use leader_state_lock for role changes
                with self.leader_state_lock:
                    self.role = NodeRole.FOLLOWER
                    self.leader_lease_id = ""
                    self.leader_lease_expires = 0.0
                self.last_lease_renewal = 0.0

            logger.info(f"Loaded state: {len(self.peers)} peers, {len(self.local_jobs)} jobs")

            # December 2025 P2P Hardening: Validate loaded state on startup
            # This detects stale jobs, stale peers, and expired leases
            is_valid, issues = self.state_manager.validate_loaded_state(state)
            if not is_valid:
                logger.warning(f"[P2P] Startup state validation found {len(issues)} issues:")
                for issue in issues:
                    logger.warning(f"  - {issue}")
                # Clean up stale entries
                stale_jobs_cleared = self.state_manager.clear_stale_jobs_by_age(max_age_hours=24.0)
                stale_peers_cleared = self.state_manager.clear_stale_peers(max_stale_seconds=300.0)
                if stale_jobs_cleared or stale_peers_cleared:
                    logger.info(f"[P2P] Cleared {stale_jobs_cleared} stale jobs, {stale_peers_cleared} stale peers")
            else:
                logger.info("[P2P] Startup state validation passed")

            # Dec 28, 2025 (Phase 7): Load persisted peer health state
            try:
                peer_health_states = self.state_manager.load_all_peer_health(max_age_seconds=3600.0)
                if peer_health_states:
                    self._apply_loaded_peer_health(peer_health_states)
                    logger.info(f"[P2P] Loaded {len(peer_health_states)} peer health records")
            except Exception as e:  # noqa: BLE001
                logger.warning(f"[P2P] Failed to load peer health state: {e}")

            # Jan 12, 2026: Initialize job snapshot with loaded jobs
            try:
                self._job_snapshot.update(self.local_jobs)
            except Exception as e:  # noqa: BLE001
                logger.warning(f"[P2P] Failed to initialize job snapshot: {e}")

        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to load state: {e}")

    def _apply_loaded_peer_health(self, peer_health_states: dict) -> None:
        """Apply loaded peer health state to circuit breakers and gossip tracker.

        Dec 28, 2025 (Phase 7): Restores peer health history after restart.
        """
        try:
            # Try to get node circuit breaker for restoring circuit states
            from app.coordination.node_circuit_breaker import get_node_circuit_breaker

            breaker = get_node_circuit_breaker("health_check")
            restored_circuits = 0

            for node_id, health_state in peer_health_states.items():
                if node_id == self.node_id:
                    continue

                # Restore circuit breaker state if circuit was open
                if health_state.circuit_state == "open":
                    breaker.force_open(node_id)
                    restored_circuits += 1
                    logger.debug(
                        f"[P2P] Restored open circuit for {node_id} "
                        f"(failures: {health_state.failure_count})"
                    )

                # Update peer's last_seen if we have fresh data
                if node_id in self.peers and health_state.last_seen > 0:
                    peer = self.peers[node_id]
                    if hasattr(peer, "last_heartbeat"):
                        # Only update if our persisted data is fresher
                        if health_state.last_seen > (peer.last_heartbeat or 0):
                            peer.last_heartbeat = health_state.last_seen

            if restored_circuits > 0:
                logger.info(f"[P2P] Restored {restored_circuits} open circuit breakers from state")

            # Restore gossip health tracker state if available
            if hasattr(self, "_gossip_health_tracker"):
                for node_id, health_state in peer_health_states.items():
                    if health_state.gossip_failure_count >= 5:
                        # Mark as suspected in gossip tracker
                        for _ in range(health_state.gossip_failure_count):
                            self._gossip_health_tracker.record_gossip_failure(node_id)

        except ImportError:
            logger.debug("[P2P] Node circuit breaker not available for health state restoration")
        except Exception as e:  # noqa: BLE001
            logger.warning(f"[P2P] Error applying peer health state: {e}")

    def _collect_peer_health_states(self) -> list:
        """Collect peer health states from circuit breakers and gossip tracker.

        Dec 28, 2025 (Phase 7): Gathers health state for persistence.

        Returns:
            List of PeerHealthState objects to persist
        """
        from scripts.p2p.managers.state_manager import PeerHealthState

        health_states = []

        # Get circuit breaker states
        circuit_states = {}
        try:
            from app.coordination.node_circuit_breaker import get_node_circuit_breaker

            breaker = get_node_circuit_breaker("health_check")
            for node_id, status in breaker.get_all_states().items():
                circuit_states[node_id] = {
                    "state": status.state.value,
                    "failure_count": status.failure_count,
                    "opened_at": status.opened_at or 0.0,
                    "last_failure": status.last_failure_time or 0.0,
                }
        except ImportError:
            pass
        except Exception as e:  # noqa: BLE001
            if self.verbose:
                logger.debug(f"[P2P] Error collecting circuit states: {e}")

        # Get gossip health tracker states
        gossip_failures = {}
        try:
            if hasattr(self, "_gossip_health_tracker"):
                tracker = self._gossip_health_tracker
                for node_id in tracker.get_suspected_peers():
                    gossip_failures[node_id] = tracker.get_failure_count(node_id)
        except Exception as e:  # noqa: BLE001
            if self.verbose:
                logger.debug(f"[P2P] Error collecting gossip states: {e}")

        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        peers_snapshot = self._peer_snapshot.get_snapshot()

        # Process snapshot outside lock to avoid blocking other operations
        for node_id, peer in peers_snapshot.items():
            if node_id == self.node_id:
                continue

            # Determine peer state
            is_retired = getattr(peer, "retired", False)
            is_alive = peer.is_alive() if hasattr(peer, "is_alive") else True

            if is_retired:
                peer_state = "retired"
            elif not is_alive:
                peer_state = "dead"
            else:
                peer_state = "alive"

            # Get circuit info
            circuit_info = circuit_states.get(node_id, {})
            gossip_fail_count = gossip_failures.get(node_id, 0)

            # Adjust state if circuit is open
            if circuit_info.get("state") == "open" and peer_state == "alive":
                peer_state = "suspect"

            health_states.append(
                PeerHealthState(
                    node_id=node_id,
                    state=peer_state,
                    failure_count=circuit_info.get("failure_count", 0),
                    gossip_failure_count=gossip_fail_count,
                    last_seen=getattr(peer, "last_heartbeat", 0.0) or 0.0,
                    last_failure=circuit_info.get("last_failure", 0.0),
                    circuit_state=circuit_info.get("state", "closed"),
                    circuit_opened_at=circuit_info.get("opened_at", 0.0),
                )
            )

        return health_states

    def _save_state(self):
        """Save current state to database.

        Phase 1 Refactoring: Delegated to StateManager.
        Creates a PersistedLeaderState from instance variables and
        passes it to the StateManager for persistence.
        """
        try:
            # Build leader state from instance variables
            role_value = self.role.value if hasattr(self.role, "value") else str(self.role)
            leader_state = PersistedLeaderState(
                leader_id=self.leader_id or "",
                leader_lease_id=self.leader_lease_id or "",
                leader_lease_expires=float(self.leader_lease_expires or 0.0),
                last_lease_renewal=float(self.last_lease_renewal or 0.0),
                role=role_value,
                voter_grant_leader_id=str(getattr(self, "voter_grant_leader_id", "") or ""),
                voter_grant_lease_id=str(getattr(self, "voter_grant_lease_id", "") or ""),
                voter_grant_expires=float(getattr(self, "voter_grant_expires", 0.0) or 0.0),
                voter_node_ids=list(getattr(self, "voter_node_ids", []) or []),
                voter_config_source=str(getattr(self, "voter_config_source", "") or ""),
                # Phase 15.1.1: Fenced lease token state
                lease_epoch=int(getattr(self, "_lease_epoch", 0) or 0),
                fence_token=str(getattr(self, "_fence_token", "") or ""),
                last_seen_epoch=int(getattr(self, "_last_seen_epoch", 0) or 0),
            )

            # Delegate to StateManager
            self.state_manager.save_state(
                node_id=self.node_id,
                peers=self.peers,
                jobs=self.local_jobs,
                leader_state=leader_state,
                peers_lock=self.peers_lock,
                jobs_lock=self.jobs_lock,
            )

            # Dec 28, 2025 (Phase 7): Save peer health state
            try:
                peer_health_states = self._collect_peer_health_states()
                if peer_health_states:
                    saved = self.state_manager.save_peer_health_batch(peer_health_states)
                    if saved > 0 and self.verbose:
                        logger.debug(f"[P2P] Saved {saved} peer health records")
            except Exception as e:  # noqa: BLE001
                if self.verbose:
                    logger.debug(f"[P2P] Error saving peer health state: {e}")

            # Jan 12, 2026: Sync job snapshot for lock-free /status reads
            try:
                self._job_snapshot.update(self.local_jobs)
            except Exception as e:  # noqa: BLE001
                if self.verbose:
                    logger.debug(f"[P2P] Error syncing job snapshot: {e}")

        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to save state: {e}")

    # =========================================================================
    # Phase 27: Peer Cache and Reputation Tracking
    # Provided by PeerManagerMixin:
    # - _update_peer_reputation: EMA-based reputation updates
    # - _save_peer_to_cache: SQLite peer persistence with pruning
    # - _get_bootstrap_peers_by_reputation: Prioritized peer list for bootstrap
    # - _get_cached_peer_count, _clear_peer_cache, _prune_stale_peers
    # =========================================================================

    # =========================================================================
    # Phase 29: Cluster Epoch Persistence
    # Phase 1 Refactoring: Delegated to StateManager
    # =========================================================================

    def _load_cluster_epoch(self) -> None:
        """Load cluster epoch from database.

        Phase 1 Refactoring: Delegated to StateManager.
        Kept for backward compatibility.
        """
        self._cluster_epoch = self.state_manager.load_cluster_epoch()

    def _save_cluster_epoch(self) -> None:
        """Save cluster epoch to database.

        Phase 1 Refactoring: Delegated to StateManager.
        Kept for backward compatibility.
        """
        self.state_manager.set_cluster_epoch(self._cluster_epoch)
        self.state_manager.save_cluster_epoch()

    def _increment_cluster_epoch(self) -> None:
        """Increment cluster epoch (called on leader change).

        Phase 1 Refactoring: Delegated to StateManager.
        Kept for backward compatibility.
        """
        self._cluster_epoch = self.state_manager.increment_cluster_epoch()

    # Class-level metrics buffer (legacy, kept for backward compatibility)
    # Phase 1 Refactoring: Delegated to MetricsManager
    _metrics_buffer: list[tuple] = []
    _metrics_buffer_lock = threading.Lock()
    _metrics_last_flush: float = 0.0
    _metrics_flush_interval: float = 30.0
    _metrics_max_buffer: int = 100

    def record_metric(
        self,
        metric_type: str,
        value: float,
        board_type: str | None = None,
        num_players: int | None = None,
        metadata: dict[str, Any] | None = None,
    ):
        """Record a metric to the history table for observability.

        Phase 1 Refactoring: Delegated to MetricsManager.

        Metric types:
        - training_loss: NNUE training loss
        - elo_rating: Model Elo rating
        - gpu_utilization: GPU utilization percentage
        - selfplay_games_per_hour: Game generation rate
        - validation_rate: GPU selfplay validation rate
        - tournament_win_rate: Tournament win rate for new model
        """
        self.metrics_manager.record_metric(
            metric_type=metric_type,
            value=value,
            board_type=board_type,
            num_players=num_players,
            metadata=metadata,
        )

    def get_metrics_history(
        self,
        metric_type: str,
        board_type: str | None = None,
        num_players: int | None = None,
        hours: float = 24,
        limit: int = 1000,
    ) -> list[dict[str, Any]]:
        """Get metrics history for a specific metric type."""
        conn = None
        try:
            conn = sqlite3.connect(str(self.db_path), timeout=10.0)
            cursor = conn.cursor()

            since = time.time() - (hours * 3600)
            query = """
                SELECT timestamp, value, board_type, num_players, metadata
                FROM metrics_history
                WHERE metric_type = ? AND timestamp > ?
            """
            params: list[Any] = [metric_type, since]

            if board_type:
                query += " AND board_type = ?"
                params.append(board_type)
            if num_players:
                query += " AND num_players = ?"
                params.append(num_players)

            query += " ORDER BY timestamp DESC LIMIT ?"
            params.append(limit)

            cursor.execute(query, params)
            results = []
            for row in cursor.fetchall():
                results.append({
                    "timestamp": row[0],
                    "value": row[1],
                    "board_type": row[2],
                    "num_players": row[3],
                    "metadata": json.loads(row[4]) if row[4] else None,
                })
            return results
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to get metrics history: {e}")
            return []
        finally:
            if conn:
                conn.close()

    def get_metrics_summary(self, hours: float = 24) -> dict[str, Any]:
        """Get summary of all metrics over the specified time period."""
        conn = None
        try:
            conn = sqlite3.connect(str(self.db_path), timeout=10.0)
            cursor = conn.cursor()

            since = time.time() - (hours * 3600)

            cursor.execute("""
                SELECT metric_type, COUNT(*), AVG(value), MIN(value), MAX(value)
                FROM metrics_history
                WHERE timestamp > ?
                GROUP BY metric_type
            """, (since,))

            summary: dict[str, Any] = {}
            for row in cursor.fetchall():
                summary[row[0]] = {
                    "count": row[1],
                    "avg": row[2],
                    "min": row[3],
                    "max": row[4],
                }

            cursor.execute("""
                SELECT metric_type, value, timestamp
                FROM metrics_history m1
                WHERE timestamp = (
                    SELECT MAX(timestamp) FROM metrics_history m2
                    WHERE m2.metric_type = m1.metric_type
                )
            """)
            for row in cursor.fetchall():
                if row[0] in summary:
                    summary[row[0]]["latest"] = row[1]
                    summary[row[0]]["latest_time"] = row[2]

            return {"period_hours": hours, "since": since, "metrics": summary}
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to get metrics summary: {e}")
            return {}
        finally:
            if conn:
                conn.close()

    def _create_self_info(self) -> NodeInfo:
        """Create NodeInfo for this node."""
        # Detect GPU
        has_gpu, gpu_name = self._detect_gpu()

        cpu_count = int(os.cpu_count() or 0)

        # Detect memory
        memory_gb = self._detect_memory()

        # Detect capabilities based on hardware
        # Dec 2025: RINGRIFT_IS_COORDINATOR=true restricts to coordinator-only
        # Dec 29, 2025: Also check distributed_hosts.yaml for role/enabled flags
        is_coordinator = os.environ.get("RINGRIFT_IS_COORDINATOR", "").lower() in ("true", "1", "yes")

        # Check YAML config for this node's settings
        if not is_coordinator:
            try:
                from app.config.cluster_config import load_cluster_config
                config = load_cluster_config()
                # ClusterConfig stores hosts in hosts_raw attribute
                nodes = getattr(config, "hosts_raw", {}) or {}
                node_cfg = nodes.get(self.node_id, {})
                # Check role or explicit enabled flags
                if node_cfg.get("role") == "coordinator":
                    is_coordinator = True
                    logger.info(f"[P2P] Node {self.node_id} is coordinator (from YAML)")
                elif node_cfg.get("selfplay_enabled") is False and node_cfg.get("training_enabled") is False:
                    is_coordinator = True
                    logger.info(f"[P2P] Node {self.node_id} has selfplay/training disabled (from YAML)")
            except Exception as e:
                logger.debug(f"[P2P] Could not load cluster config: {e}")

        if is_coordinator:
            # Dec 30, 2025: Warn if GPU node is misconfigured as coordinator
            if has_gpu:
                logger.warning(
                    f"[P2P] GPU node {self.node_id} is marked as coordinator - "
                    f"this may be a misconfiguration. GPU: {gpu_name}. "
                    "Unset RINGRIFT_IS_COORDINATOR or remove role:coordinator from YAML "
                    "to enable training capabilities."
                )
            capabilities = []  # Coordinator nodes don't run compute tasks
            logger.info("[P2P] Coordinator-only mode: no selfplay/training/cmaes capabilities")
        else:
            capabilities = ["selfplay"]
            if has_gpu:
                capabilities.extend(["training", "cmaes"])
            if memory_gb >= 64:
                capabilities.append("large_boards")

        info = NodeInfo(
            node_id=self.node_id,
            host=self.advertise_host,
            port=self.advertise_port,
            role=self.role,
            last_heartbeat=time.time(),
            cpu_count=cpu_count,
            has_gpu=has_gpu,
            gpu_name=gpu_name,
            memory_gb=memory_gb,
            capabilities=capabilities,
            version=self.build_version,
        )
        # Advertise an alternate mesh endpoint (Tailscale) for NAT traversal and
        # multi-path retries. Peers persist the observed reachable endpoint in
        # `host`/`port` but keep our `reported_host`/`reported_port` as an
        # additional candidate (see `_heartbeat_loop` multi-path retry).
        ts_ip = self._get_tailscale_ip()
        if ts_ip and ts_ip != info.host:
            info.reported_host = ts_ip
            # Use the actual listening port for mesh endpoints (port-mapped
            # advertise ports may not be reachable inside overlays).
            info.reported_port = int(self.port)

        # Jan 2026: Populate alternate_ips with all reachable IPs for partition healing
        # Peers can try multiple IPs to reach us, improving mesh resilience
        info.alternate_ips = self._discover_all_ips(exclude_primary=info.host)

        # Jan 13, 2026: Multi-address advertisement for voter counting fix
        # Nodes advertise ALL addresses they're reachable at in heartbeats.
        # This fixes voter quorum issues where voters are listed by config name
        # but peers report via Tailscale/public IPs that don't match.
        info.tailscale_ip = ts_ip or ""
        info.addresses = self._collect_all_addresses(ts_ip, info.host)

        return info

    def _collect_all_addresses(
        self, tailscale_ip: str | None, primary_host: str
    ) -> list[str]:
        """Collect all addresses this node is reachable at.

        Jan 13, 2026: For multi-address advertisement to fix voter counting.

        Returns addresses in priority order:
        1. Tailscale IP (100.x.x.x) - most reliable for P2P mesh
        2. Primary host (advertise_host) - what we're currently advertising
        3. SSH host from config - public/direct access
        4. Local interface IP - same-network access

        Args:
            tailscale_ip: Tailscale VPN IP if available
            primary_host: Current advertise_host

        Returns:
            List of addresses, deduplicated, in priority order
        """
        addresses: list[str] = []
        seen: set[str] = set()

        def add_if_new(addr: str | None) -> None:
            if addr and addr not in seen and addr not in ("", "0.0.0.0", "127.0.0.1"):
                addresses.append(addr)
                seen.add(addr)

        # Priority 1: Tailscale IP (best for mesh)
        add_if_new(tailscale_ip)

        # Priority 2: Current advertise host
        add_if_new(primary_host)

        # Priority 3: SSH host from config (may be public IP)
        try:
            from app.config.cluster_config import load_cluster_config
            config = load_cluster_config()
            nodes = getattr(config, "hosts_raw", {}) or {}
            node_cfg = nodes.get(self.node_id, {})
            if node_cfg:
                add_if_new(node_cfg.get("ssh_host"))
                add_if_new(node_cfg.get("tailscale_ip"))
        except Exception:
            pass

        # Priority 4: Local interface IPs
        for ip in self._discover_all_ips(exclude_primary=None):
            add_if_new(ip)

        return addresses

    # NOTE: _detect_gpu, _detect_memory, _get_local_ip, _get_tailscale_ip
    # delegated to ResourceDetectorMixin (Dec 28, 2025). ~75 LOC removed.
    # Use: self._detect_gpu(), self._detect_memory(), etc.

    @staticmethod
    def _infer_capabilities_from_hardware(
        has_gpu: bool,
        memory_gb: int = 0,
        gpu_name: str = "",
    ) -> list[str]:
        """Infer capabilities from hardware info.

        December 30, 2025: Fallback for nodes reporting empty capabilities but
        having detectable hardware. Used to populate capabilities for peers
        that may have misconfigured coordinator settings.

        Args:
            has_gpu: Whether the node has a GPU
            memory_gb: RAM in gigabytes
            gpu_name: GPU name for logging

        Returns:
            List of inferred capabilities
        """
        capabilities = ["selfplay"]  # All nodes can at least do CPU selfplay
        if has_gpu:
            capabilities.extend(["training", "cmaes"])
        if memory_gb >= 64:
            capabilities.append("large_boards")
        return capabilities

    def _register_self_in_peers(self) -> None:
        """Register this node in the peers dict.

        Jan 5, 2026: Ensures the leader (and any node) is visible in self.peers
        for components that iterate over peers directly. This fixes an issue
        where the leader was not in its own peers dict after becoming leader.

        This is idempotent - calling multiple times is safe.
        """
        self._update_self_info()  # Ensure self_info is current

        with self.peers_lock:
            was_present = self.node_id in self.peers
            self.peers[self.node_id] = self.self_info
            if not was_present:
                logger.info(f"[SelfReg] Registered self in peers: {self.node_id}")

        # Jan 12, 2026: Sync to lock-free snapshot
        self._sync_peer_snapshot()

        # Emit HOST_ONLINE if this is first registration (consistency with peer discovery)
        if not was_present:
            try:
                asyncio.create_task(self._emit_host_online_for_self())
            except RuntimeError:
                # No event loop running, use sync path if available
                pass

    async def _emit_host_online_for_self(self) -> None:
        """Emit HOST_ONLINE event for self-registration."""
        try:
            from app.distributed.data_events import DataEventType
            from app.coordination.event_router import emit_event

            emit_event(DataEventType.HOST_ONLINE.value, {
                "node_id": self.node_id,
                "host": self.self_info.host,
                "port": self.self_info.port,
                "has_gpu": self.self_info.has_gpu,
                "gpu_name": self.self_info.gpu_name,
                "capabilities": list(self.self_info.capabilities) if self.self_info.capabilities else [],
                "source": "leader_self_registration",
            })
            logger.debug(f"[SelfReg] Emitted HOST_ONLINE for self: {self.node_id}")
        except ImportError:
            pass  # Event system not available
        except Exception as e:
            logger.debug(f"[SelfReg] Failed to emit HOST_ONLINE: {e}")

    # =========================================================================
    # H2 fix: Lifecycle event emission methods (Jan 12, 2026)
    # These methods emit HOST_ONLINE, HOST_OFFLINE, P2P_NODE_DEAD, and
    # CLUSTER_CAPACITY_CHANGED events for cluster coordination.
    # =========================================================================

    async def _emit_host_online(self, node_id: str, capabilities: list[str] | None = None) -> None:
        """Emit HOST_ONLINE event for a peer coming online."""
        try:
            from app.distributed.data_events import DataEventType
            from app.coordination.event_router import emit_event

            peer_info = self.peers.get(node_id)
            emit_event(DataEventType.HOST_ONLINE.value, {
                "node_id": node_id,
                "host": getattr(peer_info, "host", "") if peer_info else "",
                "port": getattr(peer_info, "port", 0) if peer_info else 0,
                "has_gpu": getattr(peer_info, "has_gpu", False) if peer_info else False,
                "gpu_name": getattr(peer_info, "gpu_name", "") if peer_info else "",
                "capabilities": capabilities or [],
                "source": "peer_discovery",
            })
            logger.debug(f"[P2P] Emitted HOST_ONLINE for peer: {node_id}")
        except ImportError:
            pass
        except Exception as e:
            logger.debug(f"[P2P] Failed to emit HOST_ONLINE for {node_id}: {e}")

    def _emit_host_online_sync(self, node_id: str, capabilities: list[str] | None = None) -> None:
        """Sync version of _emit_host_online for non-async contexts."""
        try:
            from app.distributed.data_events import DataEventType
            from app.coordination.event_router import emit_event

            peer_info = self.peers.get(node_id)
            emit_event(DataEventType.HOST_ONLINE.value, {
                "node_id": node_id,
                "host": getattr(peer_info, "host", "") if peer_info else "",
                "port": getattr(peer_info, "port", 0) if peer_info else 0,
                "has_gpu": getattr(peer_info, "has_gpu", False) if peer_info else False,
                "gpu_name": getattr(peer_info, "gpu_name", "") if peer_info else "",
                "capabilities": capabilities or [],
                "source": "peer_recovery_sync",
            })
            logger.debug(f"[P2P] Emitted HOST_ONLINE (sync) for peer: {node_id}")
        except ImportError:
            pass
        except Exception as e:
            logger.debug(f"[P2P] Failed to emit HOST_ONLINE (sync) for {node_id}: {e}")

    async def _emit_host_offline(self, node_id: str, reason: str, last_heartbeat: float | None) -> None:
        """Emit HOST_OFFLINE event for a peer going offline."""
        try:
            from app.distributed.data_events import DataEventType
            from app.coordination.event_router import emit_event

            emit_event(DataEventType.HOST_OFFLINE.value, {
                "node_id": node_id,
                "reason": reason,
                "last_heartbeat": last_heartbeat,
                "source": "peer_retirement",
            })
            logger.debug(f"[P2P] Emitted HOST_OFFLINE for peer: {node_id} (reason={reason})")
        except ImportError:
            pass
        except Exception as e:
            logger.debug(f"[P2P] Failed to emit HOST_OFFLINE for {node_id}: {e}")

    def _emit_host_offline_sync(self, node_id: str, reason: str, last_heartbeat: float | None) -> None:
        """Sync version of _emit_host_offline for non-async contexts."""
        try:
            from app.distributed.data_events import DataEventType
            from app.coordination.event_router import emit_event

            emit_event(DataEventType.HOST_OFFLINE.value, {
                "node_id": node_id,
                "reason": reason,
                "last_heartbeat": last_heartbeat,
                "source": "peer_retirement_sync",
            })
            logger.debug(f"[P2P] Emitted HOST_OFFLINE (sync) for peer: {node_id} (reason={reason})")
        except ImportError:
            pass
        except Exception as e:
            logger.debug(f"[P2P] Failed to emit HOST_OFFLINE (sync) for {node_id}: {e}")

    async def _emit_node_dead(self, node_id: str, reason: str, last_heartbeat: float | None, dead_for: float) -> None:
        """Emit P2P_NODE_DEAD event for a dead peer."""
        try:
            from app.distributed.data_events import DataEventType
            from app.coordination.event_router import emit_event

            emit_event(DataEventType.P2P_NODE_DEAD.value, {
                "node_id": node_id,
                "reason": reason,
                "last_heartbeat": last_heartbeat,
                "dead_for_seconds": dead_for,
                "source": "peer_timeout",
            })
            logger.debug(f"[P2P] Emitted P2P_NODE_DEAD for peer: {node_id} (dead_for={dead_for:.0f}s)")
        except ImportError:
            pass
        except Exception as e:
            logger.debug(f"[P2P] Failed to emit P2P_NODE_DEAD for {node_id}: {e}")

    def _emit_node_dead_sync(self, node_id: str, reason: str, last_heartbeat: float | None, dead_for: float) -> None:
        """Sync version of _emit_node_dead for non-async contexts."""
        try:
            from app.distributed.data_events import DataEventType
            from app.coordination.event_router import emit_event

            emit_event(DataEventType.P2P_NODE_DEAD.value, {
                "node_id": node_id,
                "reason": reason,
                "last_heartbeat": last_heartbeat,
                "dead_for_seconds": dead_for,
                "source": "peer_timeout_sync",
            })
            logger.debug(f"[P2P] Emitted P2P_NODE_DEAD (sync) for peer: {node_id} (dead_for={dead_for:.0f}s)")
        except ImportError:
            pass
        except Exception as e:
            logger.debug(f"[P2P] Failed to emit P2P_NODE_DEAD (sync) for {node_id}: {e}")

    async def _emit_cluster_capacity_changed(
        self,
        total_nodes: int,
        alive_nodes: int,
        gpu_nodes: int,
        training_nodes: int,
        change_type: str,
        change_details: dict | None = None,
    ) -> None:
        """Emit CLUSTER_CAPACITY_CHANGED event when cluster capacity changes."""
        try:
            from app.distributed.data_events import DataEventType
            from app.coordination.event_router import emit_event

            emit_event(DataEventType.CLUSTER_CAPACITY_CHANGED.value, {
                "total_nodes": total_nodes,
                "alive_nodes": alive_nodes,
                "gpu_nodes": gpu_nodes,
                "training_nodes": training_nodes,
                "change_type": change_type,
                "change_details": change_details or {},
                "source": "peer_management",
            })
            logger.debug(f"[P2P] Emitted CLUSTER_CAPACITY_CHANGED: {change_type}, alive={alive_nodes}")
        except ImportError:
            pass
        except Exception as e:
            logger.debug(f"[P2P] Failed to emit CLUSTER_CAPACITY_CHANGED: {e}")

    def _emit_cluster_capacity_changed_sync(
        self,
        change_type: str,
        node_id: str,
        total_nodes: int,
        gpu_nodes: int,
        reason: str,
    ) -> None:
        """Sync version of _emit_cluster_capacity_changed for non-async contexts."""
        try:
            from app.distributed.data_events import DataEventType
            from app.coordination.event_router import emit_event

            emit_event(DataEventType.CLUSTER_CAPACITY_CHANGED.value, {
                "total_nodes": total_nodes,
                "alive_nodes": total_nodes,  # Sync version has less info
                "gpu_nodes": gpu_nodes,
                "training_nodes": 0,  # Not tracked in sync version
                "change_type": change_type,
                "change_details": {"node_id": node_id, "reason": reason},
                "source": "peer_management_sync",
            })
            logger.debug(f"[P2P] Emitted CLUSTER_CAPACITY_CHANGED (sync): {change_type}, node={node_id}")
        except ImportError:
            pass
        except Exception as e:
            logger.debug(f"[P2P] Failed to emit CLUSTER_CAPACITY_CHANGED (sync): {e}")

    def _safe_emit_p2p_event(self, event_type: Any, payload: dict) -> None:
        """Safely emit a P2P-related event via the event router.

        This is a generic event emitter for P2P loops (QuorumCrisisDiscoveryLoop,
        GossipStateCleanupLoop, etc.) that need to emit events without knowing
        the specific event type at compile time.

        January 12, 2026: Added to fix AttributeError in P2P loops that referenced
        this method but it didn't exist. The loops pass emit_event=self._safe_emit_p2p_event
        but this method was never implemented.

        Args:
            event_type: Event type (string or DataEventType enum)
            payload: Event payload dictionary
        """
        try:
            from app.distributed.data_events import DataEventType
            from app.coordination.event_router import emit_event

            # Handle both string and enum event types
            event_value = None
            if isinstance(event_type, str):
                # Try to convert string to DataEventType
                try:
                    event_value = DataEventType(event_type).value
                except ValueError:
                    # Unknown event type - log and skip
                    logger.debug(f"[P2P] Unknown event type: {event_type}, skipping emission")
                    return
            elif hasattr(event_type, "value"):
                # It's an enum, get its value
                event_value = event_type.value
            else:
                # Pass through as-is
                event_value = str(event_type)

            emit_event(event_value, payload)
            logger.debug(f"[P2P] Emitted event: {event_value}")
        except ImportError:
            pass  # Event router not available
        except Exception as e:
            logger.debug(f"[P2P] Failed to emit event {event_type}: {e}")

    def _sync_peer_snapshot(self) -> None:
        """Synchronize PeerSnapshot with current peers dictionary.

        January 12, 2026: Added for lock-free reads in handle_status.
        Call this after any operation that modifies self.peers.

        This uses bulk_update for efficiency when there are many peers.
        The PeerSnapshot will be atomically updated with the current state.
        """
        try:
            # Use bulk update for efficiency - single lock acquisition, single snapshot refresh
            with self._peer_snapshot.bulk_update():
                # Clear and repopulate (handles removes and updates)
                self._peer_snapshot.clear()
                for node_id, info in self.peers.items():
                    self._peer_snapshot.update_peer(node_id, info)
        except Exception as e:  # noqa: BLE001
            # Log but don't fail - reads will use stale snapshot
            logger.warning(f"[PeerSnapshot] Sync failed: {e}")

    # _is_tailscale_host provided by NetworkUtilsMixin

    def _local_has_tailscale(self) -> bool:
        """Best-effort: True when this node appears to have a Tailscale address."""
        try:
            info = getattr(self, "self_info", None)
            if not info:
                return False
            host = str(getattr(info, "host", "") or "").strip()
            reported_host = str(getattr(info, "reported_host", "") or "").strip()
            return self._is_tailscale_host(host) or self._is_tailscale_host(reported_host)
        except (AttributeError):
            return False

    # _get_tailscale_ip_for_peer: Provided by NetworkUtilsMixin

    def _detect_network_partition(self) -> bool:
        """Detect if we're in a network partition (>50% peers unreachable via primary IP).

        Used to trigger Tailscale-first connectivity mode when the public network
        is fragmented but mesh connectivity remains intact.

        Returns:
            True if partition detected (majority of peers unreachable)
        """
        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        peers_snapshot = [p for p in self._peer_snapshot.get_snapshot().values() if p.node_id != self.node_id]

        if len(peers_snapshot) < 2:
            return False

        # Count peers with recent heartbeat failures
        # Jan 19, 2026: Use jittered timeout to prevent synchronized partition detection
        now = time.time()
        jittered_timeout = get_jittered_peer_timeout(PEER_TIMEOUT)
        unreachable = 0
        for peer in peers_snapshot:
            if peer.consecutive_failures >= 3 or (now - peer.last_heartbeat > jittered_timeout):
                unreachable += 1

        partition_ratio = unreachable / len(peers_snapshot)
        if partition_ratio > 0.5:
            logger.info(f"Network partition detected: {unreachable}/{len(peers_snapshot)} peers unreachable ({partition_ratio:.0%})")
            return True
        return False

    def _get_tailscale_priority_mode(self) -> bool:
        """Check if Tailscale-first mode is enabled (partition recovery)."""
        return getattr(self, "_tailscale_priority", False)

    def _enable_tailscale_priority(self) -> None:
        """Enable Tailscale-first mode for heartbeats during partition recovery."""
        if not getattr(self, "_tailscale_priority", False):
            logger.info("Enabling Tailscale-priority mode for partition recovery")
            self._tailscale_priority = True
            self._tailscale_priority_until = time.time() + 300  # 5 minutes

    def _disable_tailscale_priority(self) -> None:
        """Disable Tailscale-first mode when connectivity recovers."""
        if getattr(self, "_tailscale_priority", False):
            logger.info("Disabling Tailscale-priority mode (connectivity recovered)")
            self._tailscale_priority = False

    # =========================================================================
    # Network Health Methods (December 30, 2025)
    # Required by NetworkHealthMixin for cross-verification of P2P vs Tailscale
    # =========================================================================

    async def _get_tailscale_status(self) -> dict[str, bool]:
        """Query Tailscale status and return peer online status.

        Returns:
            Dict mapping Tailscale IP to online status {ip: is_online}
        """
        try:
            proc = await asyncio.create_subprocess_exec(
                "tailscale",
                "status",
                "--json",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
            stdout, stderr = await asyncio.wait_for(
                proc.communicate(),
                timeout=10.0,
            )

            if proc.returncode != 0:
                logger.debug(f"Tailscale status failed: {stderr.decode()[:100]}")
                return {}

            data = json.loads(stdout.decode())

            # Extract peer IPs and online status
            result: dict[str, bool] = {}
            for peer_key, peer_data in data.get("Peer", {}).items():
                is_online = peer_data.get("Online", False)
                # Extract Tailscale IPs
                for ip in peer_data.get("TailscaleIPs", []):
                    result[ip] = is_online

            return result

        except asyncio.TimeoutError:
            logger.debug("Tailscale status timed out")
            return {}
        except json.JSONDecodeError as e:
            logger.debug(f"Failed to parse Tailscale status JSON: {e}")
            return {}
        except FileNotFoundError:
            logger.debug("Tailscale command not found")
            return {}
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Error querying Tailscale status: {e}")
            return {}

    async def _reconnect_discovered_peer(
        self, node_id: str, host: str, port: int
    ) -> bool:
        """Attempt to reconnect to a peer discovered via Tailscale.

        Probes the peer's health endpoint and sends a heartbeat to establish
        P2P connection.

        Args:
            node_id: Peer node identifier
            host: Tailscale IP address
            port: P2P port (usually 8770)

        Returns:
            True if reconnection successful, False otherwise
        """
        try:
            # Probe health endpoint
            url = f"http://{host}:{port}/health"
            timeout = ClientTimeout(total=5)
            async with get_client_session(timeout) as session:
                async with session.get(url) as resp:
                    if resp.status != 200:
                        return False
                    data, error = await safe_json_response(resp, default={}, log_errors=False)
                    if error:
                        return False

            # Extract node_id from response if available
            actual_node_id = data.get("node_id", node_id)

            # Send heartbeat to establish connection
            await self._send_heartbeat_to_peer(host, port)

            # Check if peer is now in our peers dict
            async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                if actual_node_id not in self.peers or not self.peers[actual_node_id].is_alive():
                    # Register the peer
                    self.peers[actual_node_id] = PeerInfo(
                        node_id=actual_node_id,
                        host=host,
                        port=port,
                        last_heartbeat=time.time(),
                        state="alive",
                    )
                    # C2 fix: Sync peer snapshot after adding new peer
                    self._sync_peer_snapshot()
                    logger.info(f"Reconnected peer via network health: {actual_node_id} ({host}:{port})")
                    await self._emit_host_online(actual_node_id)
                    return True

            return True  # Already connected

        except Exception as e:  # noqa: BLE001
            logger.debug(f"Failed to reconnect {node_id}: {e}")
            return False

    async def reconnect_missing_peers(self) -> list[str]:
        """Reconnect to all peers that are online in Tailscale but not in P2P.

        Returns:
            List of node IDs that were successfully reconnected
        """
        ts_peers = await self._get_tailscale_status()
        config_hosts = self._load_distributed_hosts().get("hosts", {})

        # Build IP to node mapping
        ip_to_node: dict[str, tuple[str, dict]] = {}
        for name, h in config_hosts.items():
            ts_ip = h.get("tailscale_ip")
            if ts_ip and h.get("p2p_enabled", True):
                ip_to_node[ts_ip] = (name, h)

        # Get current alive peer IDs
        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        current_ids: set[str] = set()
        for peer in self._peer_snapshot.get_snapshot().values():
            if peer.is_alive():
                current_ids.add(peer.node_id)

        # Find and reconnect missing peers
        reconnected: list[str] = []
        for ts_ip, is_online in ts_peers.items():
            if not is_online:
                continue

            if ts_ip not in ip_to_node:
                continue

            node_id, node_config = ip_to_node[ts_ip]

            # Skip if already connected
            if node_id in current_ids:
                continue

            # Skip self
            if node_id == self.node_id:
                continue

            # Attempt reconnection
            port = node_config.get("p2p_port", DEFAULT_PORT)
            if await self._reconnect_discovered_peer(node_id, ts_ip, port):
                reconnected.append(node_id)

        if reconnected:
            logger.info(f"Reconnected {len(reconnected)} missing peers: {reconnected}")

        return reconnected

    # =========================================================================
    # Partition Read-Only Mode (Phase 2.4 - Dec 29, 2025)
    # =========================================================================

    def _check_partition_mode(self) -> None:
        """Check partition status and enable/disable read-only mode.

        December 2025 (Phase 2.4): Prevent data divergence during network partitions.

        When this node is in a minority partition (<50% of peers alive):
        - Pause training job dispatch
        - Pause selfplay job dispatch
        - Continue serving existing data (read-only)
        - Allow sync operations to help recovery

        This prevents split-brain scenarios where both partitions continue
        generating training data that later conflicts during merge.
        """
        now = time.time()

        # Rate limit partition checks
        if now - self._last_partition_check < self._partition_check_interval:
            return
        self._last_partition_check = now

        # Use gossip protocol's partition detection
        status, ratio = self.detect_partition_status()

        if status in ("minority", "isolated"):
            if not self._partition_readonly_mode:
                logger.warning(
                    f"[P2P] Entering partition read-only mode: "
                    f"status={status}, health_ratio={ratio:.2%}"
                )
                self._partition_readonly_mode = True
                self._partition_readonly_since = now

                # Emit event for monitoring
                self._safe_emit_event("PARTITION_READONLY_ENTERED", {
                    "node_id": self.node_id,
                    "status": status,
                    "health_ratio": ratio,
                    "timestamp": now,
                })
        else:
            if self._partition_readonly_mode:
                readonly_duration = now - self._partition_readonly_since
                logger.info(
                    f"[P2P] Exiting partition read-only mode: "
                    f"status={status}, health_ratio={ratio:.2%}, "
                    f"was_readonly_for={readonly_duration:.0f}s"
                )
                self._partition_readonly_mode = False
                self._partition_readonly_since = 0.0

                # Emit event for monitoring
                self._safe_emit_event("PARTITION_READONLY_EXITED", {
                    "node_id": self.node_id,
                    "status": status,
                    "health_ratio": ratio,
                    "readonly_duration_seconds": readonly_duration,
                    "timestamp": now,
                })

    def is_partition_readonly(self) -> bool:
        """Check if this node is in partition read-only mode.

        December 2025 (Phase 2.4): Query method for dispatch gates.

        Returns:
            True if job dispatch should be paused due to partition status.
        """
        # Do a fresh check if it's been a while
        self._check_partition_mode()
        return self._partition_readonly_mode

    def get_partition_status(self) -> dict[str, Any]:
        """Get current partition status details.

        December 2025 (Phase 2.4): Status API for monitoring/debugging.

        Returns:
            Dict with partition status, mode, and duration.
        """
        status, ratio = self.detect_partition_status()
        now = time.time()

        result = {
            "partition_status": status,
            "health_ratio": round(ratio, 3),
            "readonly_mode": self._partition_readonly_mode,
            "readonly_since": self._partition_readonly_since,
            "readonly_duration_seconds": (
                now - self._partition_readonly_since
                if self._partition_readonly_mode else 0.0
            ),
            "last_check": self._last_partition_check,
        }

        # Add detailed peer info if available
        if hasattr(self, "get_partition_details"):
            result["details"] = self.get_partition_details()

        return result

    # _tailscale_urls_for_voter: Provided by NetworkUtilsMixin

    # NOTE: _is_in_startup_grace_period, _get_resource_usage, _check_nfs_accessible,
    # _detect_local_external_work delegated to ResourceDetectorMixin (Dec 28, 2025).
    # ~170 LOC removed. Use: self._is_in_startup_grace_period(), self._get_resource_usage(), etc.

    # NOTE: _get_diversity_metrics() and _track_selfplay_diversity() removed.
    # Delegated to self.selfplay_scheduler.get_diversity_metrics() and
    # self.selfplay_scheduler.track_diversity(). Removed ~66 LOC Dec 2025.

    def _get_db_game_count_sync(self, db_path: Path) -> int:
        """Get game count from database synchronously.

        IMPORTANT: This is a blocking operation. Call via asyncio.to_thread() from async code.
        Added Dec 2025 to fix P2P orchestrator CPU spikes from blocking SQLite in async loops.
        """
        try:
            with safe_db_connection(db_path, timeout=5) as conn:
                result = conn.execute("SELECT COUNT(*) FROM games").fetchone()
                return result[0] if result else 0
        except (sqlite3.Error, OSError):
            return 0

    def _seed_selfplay_scheduler_game_counts_sync(self) -> dict[str, int]:
        """Seed game counts from canonical databases synchronously.

        IMPORTANT: This is a blocking operation. Call via asyncio.to_thread() from async code.
        Added Jan 2026 (Session 17.29) to fix bootstrap priority for underserved configs.

        Returns:
            Dict mapping config_key -> game_count from canonical databases
        """
        game_counts: dict[str, int] = {}
        # Jan 7, 2026: Use _get_ai_service_path() to avoid doubled ai-service/ path
        canonical_dir = Path(self._get_ai_service_path()) / "data" / "games"

        # Pattern: canonical_<board_type>_<num_players>p.db
        for db_path in canonical_dir.glob("canonical_*_*p.db"):
            try:
                # Extract config_key from filename: canonical_hex8_2p.db -> hex8_2p
                stem = db_path.stem  # canonical_hex8_2p
                if stem.startswith("canonical_"):
                    config_key = stem[len("canonical_"):]  # hex8_2p
                    game_count = self._get_db_game_count_sync(db_path)
                    if game_count > 0:
                        game_counts[config_key] = game_count
            except (ValueError, AttributeError):
                continue

        return game_counts

    async def _fetch_game_counts_from_peers(self) -> dict[str, int]:
        """Fetch game counts from coordinator or other peers with canonical databases.

        Session 17.41: Cluster nodes don't have canonical databases, so they need to
        fetch game counts from the coordinator which has them. This enables the
        starvation multipliers to work correctly on all nodes.

        Returns:
            Dict mapping config_key -> game_count from peers
        """
        # Try coordinator nodes first (they have canonical databases)
        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        peers_snapshot = self._peer_snapshot.get_snapshot()
        coordinator_candidates = []
        for peer_id, peer in peers_snapshot.items():
            # Coordinator nodes or nodes with role=coordinator
            role_str = getattr(peer.role, "value", str(peer.role)) if peer.role else ""
            if "coordinator" in role_str.lower() or "mac-studio" in peer_id.lower():
                coordinator_candidates.append(peer)

        # Fallback to any alive peer
        if not coordinator_candidates:
            coordinator_candidates = [p for p in peers_snapshot.values() if p.is_alive()]

        for peer in coordinator_candidates[:3]:  # Try up to 3 candidates
            try:
                # Get best endpoint for peer
                key = self._endpoint_key(peer)
                if not key:
                    continue
                scheme, host, port = key
                url = f"{scheme}://{host}:{port}/game_counts"

                async with aiohttp.ClientSession() as session:
                    async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            game_counts = data.get("game_counts", {})
                            if game_counts:
                                source_node = data.get("node_id", peer.node_id)
                                logger.info(f"[P2P] Fetched {len(game_counts)} game counts from {source_node}")
                                return game_counts
            except (aiohttp.ClientError, asyncio.TimeoutError, json.JSONDecodeError) as e:
                logger.debug(f"[P2P] Failed to fetch game counts from {peer.node_id}: {e}")
                continue

        # Session 17.48: Fallback to known coordinator IPs from config if peer discovery failed
        # This handles the case where P2P network hasn't converged yet (no heartbeats from coordinator)
        fallback_coordinator_ips = [
            "100.69.164.58",  # macbook-pro-2-1 Tailscale IP (has canonical DBs)
        ]
        for ip in fallback_coordinator_ips:
            try:
                url = f"http://{ip}:8770/game_counts"
                async with aiohttp.ClientSession() as session:
                    async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            game_counts = data.get("game_counts", {})
                            if game_counts:
                                source_node = data.get("node_id", "unknown")
                                logger.info(f"[P2P] Fetched {len(game_counts)} game counts from fallback {source_node}")
                                return game_counts
            except (aiohttp.ClientError, asyncio.TimeoutError, json.JSONDecodeError) as e:
                logger.debug(f"[P2P] Fallback fetch from {ip} failed: {e}")
                continue

        return {}

    async def _async_seed_game_counts_from_peers_if_needed(self) -> None:
        """Async fallback to seed game counts from peers if local seeding failed.

        Jan 9, 2026: Cluster nodes don't have local canonical databases, so
        the synchronous seeding during __init__ returns empty. This method
        fetches game counts from the coordinator/peers during async startup,
        enabling proper underserved config prioritization on worker nodes.

        Without this, all configs appear to have 0 games and get the same
        maximum bootstrap boost (+100), which neutralizes the prioritization.
        """
        try:
            # Check if game counts were already seeded during __init__
            if self.selfplay_scheduler:
                existing_counts = self.selfplay_scheduler._get_game_counts_per_config()
                if existing_counts and len(existing_counts) >= 6:
                    # Already have game counts from local canonical DBs
                    logger.debug(
                        f"[P2P] Game counts already seeded ({len(existing_counts)} configs), "
                        "skipping peer fetch"
                    )
                    return

            # Fetch from peers/coordinator
            logger.info("[P2P] Local canonical DBs empty, fetching game counts from peers...")
            peer_counts = await self._fetch_game_counts_from_peers()

            if peer_counts and self.selfplay_scheduler:
                self.selfplay_scheduler.update_p2p_game_counts(peer_counts)
                logger.info(
                    f"[P2P] Seeded SelfplayScheduler with {len(peer_counts)} config game counts from peers"
                )
                # Log underserved configs for visibility
                for config_key, count in sorted(peer_counts.items(), key=lambda x: x[1]):
                    if count < 5000:
                        logger.info(f"[P2P] Underserved config (from peers): {config_key} = {count} games")
            else:
                logger.warning(
                    "[P2P] Could not fetch game counts from peers - "
                    "bootstrap prioritization may not work correctly"
                )

        except Exception as e:  # noqa: BLE001
            logger.warning(f"[P2P] Async game count seeding failed: {e}")

    async def _game_count_refresh_loop(self) -> None:
        """Periodically refresh game counts from coordinator.

        Jan 9, 2026: Cluster nodes need to periodically refresh game counts
        as games are generated and consolidated. This ensures the scheduler
        always has accurate game counts for prioritization decisions.

        Interval: 5 minutes (300 seconds)
        """
        REFRESH_INTERVAL = 300  # 5 minutes
        await asyncio.sleep(60)  # Initial delay to let cluster stabilize

        while True:
            try:
                # Skip if this node has local canonical DBs (coordinator)
                local_counts = await asyncio.to_thread(self._seed_selfplay_scheduler_game_counts_sync)
                if local_counts and len(local_counts) >= 6:
                    # Has local DBs - update from local
                    if self.selfplay_scheduler:
                        self.selfplay_scheduler.update_p2p_game_counts(local_counts)
                        logger.debug(f"[P2P] Refreshed game counts from local DBs ({len(local_counts)} configs)")
                else:
                    # Fetch from peers
                    peer_counts = await self._fetch_game_counts_from_peers()
                    if peer_counts and self.selfplay_scheduler:
                        self.selfplay_scheduler.update_p2p_game_counts(peer_counts)
                        logger.debug(f"[P2P] Refreshed game counts from peers ({len(peer_counts)} configs)")

            except Exception as e:  # noqa: BLE001
                logger.debug(f"[P2P] Game count refresh failed: {e}")

            await asyncio.sleep(REFRESH_INTERVAL)

    def _find_dbs_to_merge_sync(self, selfplay_dir: Path, main_db_path: Path) -> list[tuple[Path, int]]:
        """Find databases that need merging synchronously.

        IMPORTANT: This is a blocking operation. Call via asyncio.to_thread() from async code.
        Added Dec 2025 to fix P2P orchestrator CPU spikes from blocking file I/O in async loops.
        """
        dbs_to_merge = []
        for db_path in selfplay_dir.glob("**/games.db"):
            if ".tmp" in str(db_path) or db_path == main_db_path:
                continue
            try:
                with safe_db_connection(db_path, timeout=5) as conn:
                    count = conn.execute("SELECT COUNT(*) FROM games").fetchone()[0]
                    if count > 0:
                        dbs_to_merge.append((db_path, count))
            except (KeyError, IndexError, AttributeError, sqlite3.Error):
                pass
        return dbs_to_merge

    def _run_subprocess_sync(self, cmd: list, timeout: int = 10) -> tuple[int, str, str]:
        """Run subprocess synchronously.

        IMPORTANT: This is a blocking operation. Call via asyncio.to_thread() from async code.
        Added Dec 2025 to fix P2P orchestrator CPU spikes from blocking subprocess in async loops.

        Returns: (return_code, stdout, stderr)
        """
        import subprocess
        try:
            result = subprocess.run(cmd, timeout=timeout, capture_output=True, text=True)
            return (result.returncode, result.stdout or "", result.stderr or "")
        except subprocess.TimeoutExpired:
            return (-1, "", "timeout")
        except (OSError, subprocess.SubprocessError) as e:
            return (-1, "", str(e))

    async def _run_subprocess_async(self, cmd: list, timeout: int = 10) -> tuple[int, str, str]:
        """Run subprocess asynchronously via thread pool.

        Jan 2026: Added for Phase 1 multi-core parallelization.
        Uses asyncio.to_thread() to avoid blocking the event loop.

        Returns: (return_code, stdout, stderr)
        """
        return await asyncio.to_thread(self._run_subprocess_sync, cmd, timeout)

    def _count_local_jobs(self) -> tuple[int, int]:
        """Count running selfplay and training jobs on this node."""
        def _pid_alive(pid: int) -> bool:
            try:
                os.kill(pid, 0)
                return True
            except ProcessLookupError:
                return False
            except PermissionError:
                return True
            except (AttributeError):
                return False

        # Primary source of truth: jobs we started and are tracking.
        selfplay_pids: set[str] = set()
        training_pids: set[str] = set()

        stale_job_ids: list[str] = []
        try:
            with self.jobs_lock:
                jobs_snapshot = list(self.local_jobs.items())
            for job_id, job in jobs_snapshot:
                if job.status != "running":
                    continue
                pid = int(job.pid or 0)
                if pid <= 0:
                    continue
                if not _pid_alive(pid):
                    stale_job_ids.append(job_id)
                    continue
                if job.job_type in (JobType.SELFPLAY, JobType.GPU_SELFPLAY, JobType.HYBRID_SELFPLAY, JobType.CPU_SELFPLAY, JobType.GUMBEL_SELFPLAY):
                    selfplay_pids.add(str(pid))
                elif job.job_type == JobType.TRAINING:
                    training_pids.add(str(pid))

            if stale_job_ids:
                with self.jobs_lock:
                    for job_id in stale_job_ids:
                        self.local_jobs.pop(job_id, None)
        except (ValueError, AttributeError):
            pass

        # Secondary check: best-effort process scan for untracked jobs (e.g. manual runs).
        # IMPORTANT: never return (0,0) just because `pgrep` is missing or fails;
        # that can cause the leader to spawn runaway selfplay processes until disk fills.
        try:
            import shutil

            if shutil.which("pgrep"):
                # Jan 12, 2026: Helper to filter out non-Python processes
                # SSH processes and shell wrappers (zsh, bash) with "selfplay" in their args
                # were being counted as local jobs - only count actual Python processes
                def _get_excluded_pids() -> set[str]:
                    """Get PIDs of SSH and shell processes (to exclude from local job counts)."""
                    excluded_pids: set[str] = set()
                    # Exclude SSH processes (dispatchers to remote nodes)
                    for pattern in ("^ssh", "ssh "):
                        try:
                            out = subprocess.run(
                                ["pgrep", "-f", pattern],
                                capture_output=True,
                                text=True,
                                timeout=5,
                            )
                            if out.returncode == 0 and out.stdout.strip():
                                excluded_pids.update(out.stdout.strip().split())
                        except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError):
                            pass
                    # Exclude shell processes (Claude wrappers that contain "selfplay" in args)
                    for shell_pattern in ("/bin/zsh", "/bin/bash", "/bin/sh"):
                        try:
                            out = subprocess.run(
                                ["pgrep", "-f", shell_pattern],
                                capture_output=True,
                                text=True,
                                timeout=5,
                            )
                            if out.returncode == 0 and out.stdout.strip():
                                excluded_pids.update(out.stdout.strip().split())
                        except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError):
                            pass
                    return excluded_pids

                excluded_pids = _get_excluded_pids()

                # December 2025: Added selfplay.py pattern - the current unified selfplay entry point
                # December 2025: Added gumbel_selfplay and SelfplayRunner patterns for module invocations
                for pattern in (
                    "selfplay.py",
                    "run_self_play_soak.py",
                    "run_gpu_selfplay.py",
                    "run_hybrid_selfplay.py",
                    "gumbel_selfplay",  # screen session name
                    "SelfplayRunner",   # class-based invocation
                    "selfplay_runner",  # module invocation
                    "-m app.training.selfplay",  # module mode
                ):
                    out = subprocess.run(
                        ["pgrep", "-f", pattern],
                        capture_output=True,
                        text=True,
                        timeout=5,
                    )
                    if out.returncode == 0 and out.stdout.strip():
                        # Jan 12, 2026: Filter out excluded PIDs (SSH, shells) - not local jobs
                        pids = [p for p in out.stdout.strip().split() if p and p not in excluded_pids]
                        selfplay_pids.update(pids)

                for pattern in ("train_", "train.py", "-m app.training.train"):
                    out = subprocess.run(
                        ["pgrep", "-f", pattern],
                        capture_output=True,
                        text=True,
                        timeout=5,
                    )
                    if out.returncode == 0 and out.stdout.strip():
                        # Jan 12, 2026: Filter out excluded PIDs (SSH, shells)
                        pids = [p for p in out.stdout.strip().split() if p and p not in excluded_pids]
                        training_pids.update(pids)
        except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError, KeyError, IndexError, AttributeError, ImportError):
            pass

        return len(selfplay_pids), len(training_pids)

    def _get_max_selfplay_slots_for_node(self) -> int:
        """Get maximum selfplay slots based on GPU capability.

        Jan 2, 2026: Added for slot-based capacity management.
        This allows work queue claiming to coexist with legacy selfplay processes.

        The slot count is based on GPU type since different GPUs can handle
        different numbers of concurrent selfplay processes effectively.

        Returns:
            Maximum number of selfplay slots for this node.
        """
        import os

        # Check environment variable first (allows manual override)
        env_slots = os.environ.get("RINGRIFT_MAX_SELFPLAY_SLOTS")
        if env_slots:
            try:
                return int(env_slots)
            except ValueError:
                pass

        # Compute based on GPU name
        gpu_name = getattr(self.self_info, "gpu_name", "") or ""
        gpu_name_lower = gpu_name.lower()

        # High-end GPUs get more slots
        if "gh200" in gpu_name_lower or "h100" in gpu_name_lower:
            return 16
        elif "a100" in gpu_name_lower:
            return 12
        elif "5090" in gpu_name_lower or "4090" in gpu_name_lower:
            return 8
        elif "3090" in gpu_name_lower or "a40" in gpu_name_lower or "l40" in gpu_name_lower:
            return 6
        elif "4060" in gpu_name_lower or "3060" in gpu_name_lower:
            return 3
        elif self.self_info.has_gpu:
            return 4  # Default for other GPUs
        else:
            return 2  # CPU-only nodes

    def _cleanup_stale_processes(self) -> int:
        """Kill processes that have been running too long.

        Scans for known process patterns (tournaments, gauntlets, selfplay)
        and kills any that exceed their maximum runtime threshold.

        Returns:
            Number of processes killed.
        """
        import shutil

        if not shutil.which("pgrep") or not shutil.which("ps"):
            return 0

        killed_count = 0
        time.time()

        # Map patterns to their max runtimes
        # December 2025: Added selfplay.py - the current unified selfplay entry point
        pattern_max_runtime = {
            "run_model_elo_tournament.py": MAX_TOURNAMENT_RUNTIME,
            "run_gauntlet.py": MAX_GAUNTLET_RUNTIME,
            "selfplay.py": MAX_SELFPLAY_RUNTIME,  # Unified selfplay script
            "run_self_play_soak.py": MAX_SELFPLAY_RUNTIME,
            "run_gpu_selfplay.py": MAX_SELFPLAY_RUNTIME,
            "run_hybrid_selfplay.py": MAX_SELFPLAY_RUNTIME,
            "train_nnue.py": MAX_TRAINING_RUNTIME,
            "train.py": MAX_TRAINING_RUNTIME,
        }

        for pattern, max_runtime in pattern_max_runtime.items():
            try:
                # Get PIDs matching the pattern
                pgrep_result = subprocess.run(
                    ["pgrep", "-f", pattern],
                    capture_output=True,
                    text=True,
                    timeout=5,
                )
                if pgrep_result.returncode != 0 or not pgrep_result.stdout.strip():
                    continue

                pids = [p.strip() for p in pgrep_result.stdout.strip().split() if p.strip()]

                for pid in pids:
                    try:
                        # Get process start time using ps
                        ps_result = subprocess.run(
                            ["ps", "-o", "etimes=", "-p", pid],
                            capture_output=True,
                            text=True,
                            timeout=5,
                        )
                        if ps_result.returncode != 0:
                            continue

                        elapsed_seconds = int(ps_result.stdout.strip())

                        if elapsed_seconds > max_runtime:
                            # Process has exceeded max runtime - kill it
                            logger.warning(
                                f"Killing stale process {pid} ({pattern}): "
                                f"running for {elapsed_seconds/3600:.1f}h, max={max_runtime/3600:.1f}h"
                            )
                            subprocess.run(
                                ["kill", "-9", pid],
                                capture_output=True,
                                timeout=5,
                            )
                            killed_count += 1

                            # Send alert
                            if hasattr(self, 'notifier') and self.notifier:
                                asyncio.create_task(
                                    self.notifier.send(
                                        title="Stale Process Killed",
                                        message=f"Killed {pattern} (PID {pid}) after {elapsed_seconds/3600:.1f} hours",
                                        level="warning",
                                        node_id=self.node_id,
                                    )
                                )

                    except (ValueError, subprocess.TimeoutExpired):
                        continue

            except Exception as e:  # noqa: BLE001
                logger.debug(f"Error checking pattern {pattern}: {e}")
                continue

        if killed_count > 0:
            logger.info(f"Stale process cleanup: killed {killed_count} processes")

        return killed_count

    # ============================================
    # Phase 2: Distributed Data Sync Methods
    # ============================================

    def _collect_local_data_manifest(self) -> NodeDataManifest:
        """Collect manifest of all data files on this node.

        REFACTORED (Dec 2025): Delegates to SyncPlanner.collect_local_manifest().
        See scripts/p2p/managers/sync_planner.py for implementation.

        Scans the data directory for:
        - selfplay/ - Game replay files (.jsonl, .db)
        - models/ - Trained model files (.pt, .onnx)
        - training/ - Training data files (.npz)
        - games/ - Synced game databases (.db)

        Uses get_data_directory() to support both disk and ramdrive storage.
        """
        # Phase 2A: Delegate to SyncPlanner (Dec 2025)
        # This eliminates ~150 lines of duplicate code
        return self.sync_planner.collect_local_manifest(use_cache=False)

    # NOTE: _collect_local_data_manifest_legacy() removed Dec 27, 2025
    # (150 LOC dead code - was never called, SyncPlanner.collect_local_manifest used instead)
    # NOTE: _compute_file_hash() removed Dec 28, 2025
    # (12 LOC dead code - zero callers, orphaned legacy method)

    def _request_peer_manifest_sync(self, peer_id: str) -> NodeDataManifest | None:
        """Synchronous wrapper for requesting peer manifest.

        Used by SyncPlanner which expects a sync callback.
        Runs the async version in a new event loop.

        Args:
            peer_id: The peer's node ID to request from

        Returns:
            NodeDataManifest or None if request failed
        """
        # Look up peer info
        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        peer_info = self._peer_snapshot.get_snapshot().get(peer_id)

        if not peer_info:
            logger.debug(f"Peer {peer_id} not found in peers dict")
            return None

        # Run async version in event loop
        try:
            loop = asyncio.get_running_loop()
            # If we're in an async context, use run_coroutine_threadsafe
            import concurrent.futures
            future = asyncio.run_coroutine_threadsafe(
                self._request_peer_manifest(peer_info), loop
            )
            return future.result(timeout=15)
        except RuntimeError:
            # No running loop - use asyncio.run
            try:
                return asyncio.run(self._request_peer_manifest(peer_info))
            except Exception as e:  # noqa: BLE001
                logger.debug(f"Failed to request manifest from {peer_id}: {e}")
                return None
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Failed to request manifest from {peer_id}: {e}")
            return None

    async def _request_peer_manifest(self, peer_info: NodeInfo) -> NodeDataManifest | None:
        """Request data manifest from a peer node."""
        try:
            # Keep manifest requests snappy: these are advisory and should not
            # stall leader loops or external callers (e.g. the improvement
            # daemon). Prefer faster failure and rely on periodic retries.
            timeout = ClientTimeout(total=10, sock_connect=3, sock_read=7)
            async with get_client_session(timeout) as session:
                for url in self._urls_for_peer(peer_info, "/data_manifest"):
                    try:
                        async with session.get(url, headers=self._auth_headers()) as resp:
                            if resp.status != 200:
                                continue
                            data = await resp.json()
                        return NodeDataManifest.from_dict((data or {}).get("manifest", {}))
                    except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
                        continue
        except Exception as e:  # noqa: BLE001
            logger.error(f"requesting manifest from {peer_info.node_id}: {e}")
        return None

    # =========================================================================
    # Manifest Cache Methods - MOVED to SyncPlanner (Dec 2025)
    # =========================================================================
    # The following methods were moved to scripts/p2p/managers/sync_planner.py:
    # - get_manifest_cache_path() - disk cache path
    # - save_manifest_to_cache() - persist manifest
    # - load_manifest_from_cache() - load cached manifest
    # - collect_local_manifest_cached() - collect with disk caching
    # Access via: self.sync_planner.<method_name>()

    async def _collect_cluster_manifest(self) -> ClusterDataManifest:
        """Leader-only: Collect manifests from all peers and build cluster view."""
        cluster_manifest = ClusterDataManifest(
            collected_at=time.time(),
        )

        # Collect from self
        local_manifest = await asyncio.to_thread(self._collect_local_data_manifest)
        with self.manifest_lock:
            self.local_data_manifest = local_manifest
        cluster_manifest.node_manifests[self.node_id] = local_manifest

        # Collect from peers in parallel.
        #
        # Only probe peers that are currently alive and not retired; terminated
        # or long-dead nodes should not stall manifest collection. NAT-blocked
        # peers can't accept inbound /data_manifest, so they are excluded too.
        # Jan 2026: Use lock-free PeerSnapshot for read-only access
        peers_snapshot = self._peer_snapshot.get_snapshot()
        peers = [
            p
            for p in peers_snapshot.values()
            if p.is_alive()
            and not bool(getattr(p, "retired", False))
            and not bool(getattr(p, "nat_blocked", False))
        ]

        tasks = [self._request_peer_manifest(peer) for peer in peers]
        # December 2025: Add timeout to prevent hang if peers are unresponsive
        # Individual requests have 10s timeout, but aggregate needs overall limit
        # to prevent blocking leader loop. 45s covers ~30 peers with some slack.
        try:
            results = await asyncio.wait_for(
                asyncio.gather(*tasks, return_exceptions=True),
                timeout=45.0
            )
        except asyncio.TimeoutError:
            logger.warning(
                f"[ManifestCollection] Timed out after 45s collecting from {len(peers)} peers. "
                "Proceeding with partial data."
            )
            results = []  # Proceed with only local manifest

        for peer, result in zip(peers, results, strict=False):
            if isinstance(result, NodeDataManifest):
                cluster_manifest.node_manifests[peer.node_id] = result

        # Compute cluster-wide statistics
        cluster_manifest.total_nodes = len(cluster_manifest.node_manifests)

        all_files: set[str] = set()
        for node_id, node_manifest in cluster_manifest.node_manifests.items():
            cluster_manifest.total_files += node_manifest.total_files
            cluster_manifest.total_size_bytes += node_manifest.total_size_bytes
            cluster_manifest.total_selfplay_games += node_manifest.selfplay_games
            cluster_manifest.files_by_node[node_id] = node_manifest.total_files

            for file_info in node_manifest.files:
                all_files.add(file_info.path)

        cluster_manifest.unique_files = all_files

        # Find files missing from nodes (for sync planning)
        for file_path in all_files:
            nodes_with_file = []
            nodes_without_file = []
            for node_id, node_manifest in cluster_manifest.node_manifests.items():
                file_paths = {f.path for f in node_manifest.files}
                if file_path in file_paths:
                    nodes_with_file.append(node_id)
                else:
                    nodes_without_file.append(node_id)

            if nodes_without_file:
                cluster_manifest.missing_from_nodes[file_path] = nodes_without_file

        # Collect external storage metadata (OWC drive, S3 bucket)
        # Jan 2026: Added for unified cluster data visibility
        try:
            external_storage = await self._collect_external_storage_metadata()
            cluster_manifest.external_storage = external_storage
        except Exception as e:
            logger.debug(f"[ManifestCollection] External storage scan skipped: {e}")

        logger.info(f"Cluster manifest: {cluster_manifest.total_nodes} nodes, "
              f"{len(cluster_manifest.unique_files)} unique files, "
              f"{cluster_manifest.total_selfplay_games} total games")

        return cluster_manifest

    async def _collect_external_storage_metadata(self) -> ExternalStorageManifest:
        """Collect metadata from external storage sources (OWC drive, S3 bucket).

        Jan 2026: Added for unified cluster data visibility.

        Returns:
            ExternalStorageManifest with OWC and S3 metadata.
        """
        from scripts.p2p.models import ExternalStorageManifest

        external = ExternalStorageManifest(collected_at=time.time())

        # Collect OWC drive metadata (if accessible)
        try:
            owc_metadata = await self._scan_owc_metadata()
            if owc_metadata:
                external.owc_available = True
                external.owc_games_by_config = owc_metadata.get("games_by_config", {})
                external.owc_total_games = owc_metadata.get("total_games", 0)
                external.owc_total_size_bytes = owc_metadata.get("total_size_bytes", 0)
                external.owc_last_scan = time.time()
            else:
                external.owc_scan_error = "OWC scan returned no data"
        except Exception as e:
            external.owc_scan_error = str(e)
            logger.warning(f"[ExternalStorage] OWC scan failed: {e}")

        # Collect S3 bucket metadata (if configured)
        try:
            s3_metadata = await self._scan_s3_metadata()
            if s3_metadata:
                external.s3_available = True
                external.s3_games_by_config = s3_metadata.get("games_by_config", {})
                external.s3_total_games = s3_metadata.get("total_games", 0)
                external.s3_total_size_bytes = s3_metadata.get("total_size_bytes", 0)
                external.s3_bucket = s3_metadata.get("bucket", "")
                external.s3_last_scan = time.time()
            else:
                external.s3_scan_error = "S3 scan returned no data (check boto3 and credentials)"
        except Exception as e:
            external.s3_scan_error = str(e)
            logger.warning(f"[ExternalStorage] S3 scan failed: {e}")

        return external

    async def _scan_owc_metadata(self) -> dict | None:
        """Scan OWC external drive for game data metadata.

        Jan 2026: OWC drive is mounted on mac-studio at /Volumes/RingRift-Data.

        Returns:
            Dict with games_by_config, total_games, total_size_bytes, or None if unavailable.
        """
        import os
        import socket

        # Load OWC paths from config if available, otherwise use defaults
        owc_paths = []
        try:
            from app.config.cluster_config import load_cluster_config
            config = load_cluster_config()
            sync_cfg = config.get_raw_section("sync_routing")
            for storage in sync_cfg.get("allowed_external_storage", []):
                if storage.get("host") == "mac-studio":
                    owc_paths.append(storage.get("path"))
        except Exception:
            pass

        # Fallback to hardcoded paths if config unavailable
        if not owc_paths:
            owc_paths = [
                "/Volumes/RingRift-Data",
                "/Volumes/OWC",
            ]

        # Check if running on mac-studio (OWC is local)
        hostname = socket.gethostname().lower()
        is_mac_studio = "mac-studio" in hostname or hostname == "mac-studio"

        if is_mac_studio:
            # Direct local access
            for owc_path in owc_paths:
                if os.path.exists(owc_path):
                    return await asyncio.to_thread(
                        self._scan_owc_local, owc_path
                    )
            return None

        # Remote access via SSH to mac-studio (get IP from config)
        mac_studio_host = "mac-studio"  # Default hostname
        try:
            from app.config.cluster_config import load_cluster_config
            config = load_cluster_config()
            mac_studio_cfg = config.hosts_raw.get("mac-studio", {})
            # Prefer Tailscale IP for reliability
            mac_studio_host = (
                mac_studio_cfg.get("tailscale_ip")
                or mac_studio_cfg.get("ssh_host")
                or "mac-studio"
            )
        except Exception:
            pass  # Fall back to hostname

        try:
            return await self._scan_owc_remote(mac_studio_host, owc_paths[0])
        except Exception as e:
            logger.warning(f"[OWC] Remote scan failed to {mac_studio_host}: {e}")
            return None

    def _scan_owc_local(self, base_path: str) -> dict:
        """Scan OWC drive locally for game databases.

        Returns dict with games_by_config, total_games, total_size_bytes.
        """
        import os
        import sqlite3
        from pathlib import Path

        games_by_config: dict[str, int] = {}
        total_games = 0
        total_size_bytes = 0

        # Look for game databases in standard locations
        data_paths = [
            Path(base_path) / "data" / "games",
            Path(base_path) / "games",
            Path(base_path) / "selfplay",
        ]

        for data_path in data_paths:
            if not data_path.exists():
                continue

            for db_file in data_path.glob("**/*.db"):
                try:
                    total_size_bytes += db_file.stat().st_size

                    # Extract config from filename or query DB
                    config_key = self._extract_config_from_path(db_file)
                    if not config_key:
                        continue

                    # Quick game count query
                    conn = sqlite3.connect(str(db_file), timeout=5.0)
                    try:
                        cursor = conn.execute(
                            "SELECT COUNT(*) FROM games WHERE status = 'completed'"
                        )
                        count = cursor.fetchone()[0]
                        games_by_config[config_key] = (
                            games_by_config.get(config_key, 0) + count
                        )
                        total_games += count
                    except sqlite3.OperationalError:
                        # Table doesn't exist or different schema
                        pass
                    finally:
                        conn.close()
                except Exception:
                    continue

        return {
            "games_by_config": games_by_config,
            "total_games": total_games,
            "total_size_bytes": total_size_bytes,
        }

    def _extract_config_from_path(self, db_path: Path) -> str | None:
        """Extract board_type and num_players from database path.

        Expected patterns:
        - canonical_hex8_2p.db
        - hex8_2p_selfplay.db
        - games_square8_4p.db
        """
        import re

        filename = db_path.stem.lower()

        # Try common patterns
        patterns = [
            r"(hex8|hexagonal|square8|square19)_(\d)p",
            r"canonical_(hex8|hexagonal|square8|square19)_(\d)p",
        ]

        for pattern in patterns:
            match = re.search(pattern, filename)
            if match:
                board_type = match.group(1)
                num_players = match.group(2)
                return f"{board_type}_{num_players}p"

        return None

    async def _scan_owc_remote(self, host: str, owc_path: str) -> dict | None:
        """Scan OWC drive via SSH to mac-studio.

        Returns dict with games_by_config, total_games, total_size_bytes.
        """
        # Use a simple SSH command to get file listing and sizes
        ssh_cmd = f"""
        cd {owc_path} 2>/dev/null && find . -name "*.db" -type f -exec stat -f '%z %N' {{}} \\; 2>/dev/null | head -100
        """

        try:
            proc = await asyncio.create_subprocess_exec(
                "ssh", "-o", "ConnectTimeout=5", "-o", "BatchMode=yes",
                host, "bash", "-c", ssh_cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
            stdout, _ = await asyncio.wait_for(proc.communicate(), timeout=15.0)

            if proc.returncode != 0:
                return None

            # Parse output: "size path"
            games_by_config: dict[str, int] = {}
            total_size_bytes = 0
            total_games = 0

            for line in stdout.decode().strip().split("\n"):
                if not line:
                    continue
                parts = line.split(maxsplit=1)
                if len(parts) != 2:
                    continue
                size_str, path = parts
                try:
                    total_size_bytes += int(size_str)
                except ValueError:
                    continue

                # Extract config from path
                from pathlib import Path
                config_key = self._extract_config_from_path(Path(path))
                if config_key:
                    # Estimate 100 games per DB file as placeholder
                    # (accurate count would require opening each DB)
                    games_by_config[config_key] = games_by_config.get(config_key, 0) + 100
                    total_games += 100

            return {
                "games_by_config": games_by_config,
                "total_games": total_games,
                "total_size_bytes": total_size_bytes,
            }
        except (asyncio.TimeoutError, OSError):
            return None

    def _get_s3_bucket_from_config(self) -> str | None:
        """Get S3 bucket name from config or environment.

        Priority:
        1. RINGRIFT_S3_BUCKET environment variable (backward compat)
        2. sync_routing.s3.bucket from distributed_hosts.yaml
        3. Default: ringrift-models-20251214

        Returns:
            S3 bucket name or None if S3 is disabled.
        """
        import os

        # Priority 1: Environment variable
        s3_bucket = os.environ.get("RINGRIFT_S3_BUCKET")
        if s3_bucket:
            return s3_bucket

        # Priority 2: Load from YAML config
        try:
            from app.config.cluster_config import load_cluster_config
            config = load_cluster_config()
            s3_cfg = config.get_raw_section("sync_routing").get("s3", {})
            if not s3_cfg.get("enabled", True):
                logger.info("[S3] S3 disabled in config")
                return None
            bucket = s3_cfg.get("bucket")
            if bucket:
                return bucket
        except Exception as e:
            logger.debug(f"[S3] Could not load config from YAML: {e}")

        # Priority 3: Default bucket
        return "ringrift-models-20251214"

    async def _scan_s3_metadata(self) -> dict | None:
        """Scan S3 bucket for game data metadata.

        Jan 2026: Uses boto3 if available, with caching to avoid repeated API calls.

        Returns:
            Dict with games_by_config, total_games, total_size_bytes, bucket, or None.
        """
        # Get S3 bucket from config (env var or YAML)
        s3_bucket = self._get_s3_bucket_from_config()
        if not s3_bucket:
            logger.info("[S3] S3 scanning disabled - no bucket configured")
            return None

        # Check for cached result (cache for 1 hour)
        cache_key = "_s3_metadata_cache"
        cache_time_key = "_s3_metadata_cache_time"

        cached = getattr(self, cache_key, None)
        cached_time = getattr(self, cache_time_key, 0.0)

        if cached and (time.time() - cached_time) < 3600:  # 1 hour cache
            return cached

        try:
            import boto3
        except ImportError:
            logger.warning("[S3] boto3 not installed. Install with: pip install boto3")
            return None

        try:
            s3 = boto3.client("s3")

            games_by_config: dict[str, int] = {}
            total_games = 0
            total_size_bytes = 0

            # List objects in bucket (limit to games/ prefix)
            paginator = s3.get_paginator("list_objects_v2")
            for prefix in ["data/games/", "games/", "selfplay/"]:
                try:
                    for page in paginator.paginate(Bucket=s3_bucket, Prefix=prefix):
                        for obj in page.get("Contents", []):
                            key = obj["Key"]
                            size = obj["Size"]

                            if not key.endswith(".db"):
                                continue

                            total_size_bytes += size

                            # Extract config from key
                            from pathlib import Path
                            config_key = self._extract_config_from_path(Path(key))
                            if config_key:
                                # Estimate based on file size (rough: 1 game = 10KB)
                                est_games = max(1, size // 10000)
                                games_by_config[config_key] = (
                                    games_by_config.get(config_key, 0) + est_games
                                )
                                total_games += est_games
                except Exception:
                    continue

            result = {
                "games_by_config": games_by_config,
                "total_games": total_games,
                "total_size_bytes": total_size_bytes,
                "bucket": s3_bucket,
            }

            # Cache result
            setattr(self, cache_key, result)
            setattr(self, cache_time_key, time.time())

            return result

        except Exception as e:
            logger.warning(f"[S3] Scan failed: {e}")
            return None

    # ============================================
    # Phase 2: P2P Rsync Coordination Methods
    # ============================================

    # NOTE: _generate_sync_plan() removed Dec 2025 (61 LOC dead code).
    # Use self.sync_planner.generate_sync_plan(cluster_manifest) instead.

    async def _execute_sync_plan(self) -> None:
        """Leader executes the sync plan by dispatching jobs to nodes.

        Delegates to SyncPlanner.execute_sync_plan() with _request_node_sync as callback.
        Dec 2025: Refactored to delegate to SyncPlanner for consolidated logic.
        """
        if not self.current_sync_plan:
            return

        # Delegate to SyncPlanner with our network request callback
        result = await self.sync_planner.execute_sync_plan(
            plan=self.current_sync_plan,
            execute_job_callback_async=self._request_node_sync,
        )

        # Update local state from SyncPlanner result
        with self.sync_lock:
            self.last_sync_time = time.time()

        if not result.get("success", False):
            logger.warning(f"Sync plan execution issue: {result.get('error', 'unknown')}")

    async def _request_node_sync(self, job: DataSyncJob) -> bool:
        """Request a target node to pull files from a source node."""
        target_peer = self.peers.get(job.target_node)
        if job.target_node == self.node_id:
            target_peer = self.self_info

        source_peer = self.peers.get(job.source_node)
        if job.source_node == self.node_id:
            source_peer = self.self_info

        if not target_peer or not source_peer:
            job.status = "failed"
            job.error_message = "Source or target peer not found"
            return False

        job.status = "running"
        job.started_at = time.time()

        try:
            # Local target: execute the pull directly (no HTTP round-trip).
            if job.target_node == self.node_id:
                result = await self._handle_sync_pull_request(
                    source_host=source_peer.host,
                    source_port=source_peer.port,
                    source_reported_host=(getattr(source_peer, "reported_host", "") or None),
                    source_reported_port=(getattr(source_peer, "reported_port", 0) or None),
                    source_node_id=job.source_node,
                    files=job.files,
                )
            else:
                payload = {
                    "job_id": job.job_id,
                    # Back-compat: target will prefer source_node_id lookup.
                    "source_host": source_peer.host,
                    "source_port": source_peer.port,
                    "source_node_id": job.source_node,
                    "files": job.files,
                }
                rh = (getattr(source_peer, "reported_host", "") or "").strip()
                rp = int(getattr(source_peer, "reported_port", 0) or 0)
                if rh and rp and (rh != source_peer.host or rp != source_peer.port):
                    payload["source_reported_host"] = rh
                    payload["source_reported_port"] = rp

                timeout = ClientTimeout(total=600)
                async with get_client_session(timeout) as session:
                    result = None
                    last_err: str | None = None
                    for url in self._urls_for_peer(target_peer, "/sync/pull"):
                        try:
                            async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                                if resp.status != 200:
                                    last_err = f"http_{resp.status}"
                                    continue
                                result = await resp.json()
                                break
                        except Exception as e:  # noqa: BLE001
                            last_err = str(e)
                            continue
                    if result is None:
                        job.status = "failed"
                        job.error_message = last_err or "sync_pull_failed"
                        # Note: SyncPlanner tracks jobs_failed count
                        return False

            ok = bool(result.get("success"))
            job.status = "completed" if ok else "failed"
            job.completed_at = time.time()
            job.bytes_transferred = int(result.get("bytes_transferred", 0) or 0)
            job.files_completed = int(result.get("files_completed", 0) or 0)
            if not ok:
                job.error_message = str(result.get("error") or "Unknown error")

            # Note: SyncPlanner tracks jobs_completed/jobs_failed counts

            if ok:
                logger.info(f"Sync job {job.job_id[:8]} completed: {job.source_node} -> {job.target_node}")
            else:
                logger.info(f"Sync job {job.job_id[:8]} failed: {job.error_message}")

            return ok

        except Exception as e:  # noqa: BLE001
            job.status = "failed"
            job.error_message = str(e)
            job.completed_at = time.time()
            # Note: SyncPlanner tracks jobs_failed count
            logger.info(f"Sync job {job.job_id[:8]} failed: {e}")
            return False

    async def _handle_sync_pull_request(
        self,
        source_host: str,
        source_port: int,
        source_node_id: str,
        files: list[str],
        source_reported_host: str | None = None,
        source_reported_port: int | None = None,
    ) -> dict[str, Any]:
        """
        Handle incoming request to pull files from a source node.
        Pulls files over the P2P HTTP channel to avoid SSH/rsync dependencies.
        Uses get_data_directory() to support both disk and ramdrive storage.
        """
        # Check disk capacity before pulling files
        has_capacity, disk_percent = check_disk_has_capacity()
        if not has_capacity:
            return {
                "success": False,
                "error": f"Disk full ({disk_percent:.1f}% >= {MAX_DISK_USAGE_PERCENT}%)",
                "disk_percent": disk_percent,
                "bytes_transferred": 0,
                "files_completed": 0,
            }

        data_dir = self.get_data_directory()
        data_dir.mkdir(parents=True, exist_ok=True)

        bytes_transferred = 0
        files_completed = 0
        errors: list[str] = []

        # Multi-path sources: prefer observed endpoint but allow a self-reported
        # endpoint (e.g. Tailscale) when the public route fails.
        candidate_sources: list[tuple[str, int]] = []
        seen_sources: set[tuple[str, int]] = set()

        def _add_source(host: str | None, port: int | None) -> None:
            if not host:
                return
            h = str(host).strip()
            if not h:
                return
            try:
                p = int(port or 0)
            except (ValueError):
                return
            if p <= 0:
                return
            key = (h, p)
            if key in seen_sources:
                return
            seen_sources.add(key)
            candidate_sources.append(key)

        _add_source(source_host, source_port)
        _add_source(source_reported_host, source_reported_port)

        timeout = ClientTimeout(total=None, sock_connect=HTTP_CONNECT_TIMEOUT, sock_read=600)

        async with get_client_session(timeout) as session:
            for rel_path in files:
                rel_path = (rel_path or "").lstrip("/")
                if not rel_path:
                    errors.append("empty_path")
                    continue

                # Security: keep all writes within ai-service/data.
                dest_path = (data_dir / rel_path)
                try:
                    data_root = data_dir.resolve()
                    dest_resolved = dest_path.resolve()
                    dest_resolved.relative_to(data_root)
                except (AttributeError):
                    errors.append(f"invalid_path:{rel_path}")
                    continue

                dest_path.parent.mkdir(parents=True, exist_ok=True)
                tmp_path = dest_path.with_name(dest_path.name + ".partial")

                last_err: str | None = None
                success = False

                for host, base_port in candidate_sources:
                    # Back-compat: if caller passed an SSH-like port (22), try DEFAULT_PORT too.
                    ports_to_try: list[int] = []
                    try:
                        ports_to_try.append(int(base_port))
                    except (ValueError, AttributeError):
                        ports_to_try.append(DEFAULT_PORT)
                    if DEFAULT_PORT not in ports_to_try:
                        ports_to_try.append(DEFAULT_PORT)

                    for port in ports_to_try:
                        url = f"http://{host}:{port}/sync/file"
                        try:
                            async with session.get(
                                url,
                                params={"path": rel_path},
                                headers=self._auth_headers(),
                            ) as resp:
                                if resp.status != 200:
                                    text = ""
                                    try:
                                        text = (await resp.text())[:200]
                                    except (KeyError, IndexError, AttributeError):
                                        text = ""
                                    last_err = f"{resp.status} {text}".strip()
                                    continue

                                with open(tmp_path, "wb") as out_f:
                                    async for chunk in resp.content.iter_chunked(1024 * 1024):
                                        out_f.write(chunk)
                                        bytes_transferred += len(chunk)

                                tmp_path.replace(dest_path)
                                files_completed += 1
                                success = True
                                break

                        except Exception as e:  # noqa: BLE001
                            last_err = str(e)
                            continue
                    if success:
                        break

                if not success:
                    errors.append(f"{rel_path}: {last_err or 'download_failed'}")
                    try:
                        if tmp_path.exists():
                            tmp_path.unlink()
                    except (OSError, AttributeError):
                        pass

        if errors:
            return {
                "success": False,
                "files_completed": files_completed,
                "bytes_transferred": bytes_transferred,
                "error": "; ".join(errors[:5]),
            }

        return {
            "success": True,
            "files_completed": files_completed,
            "bytes_transferred": bytes_transferred,
        }

    async def start_cluster_sync(self) -> dict[str, Any]:
        """
        Leader initiates a full cluster data sync.
        Returns status of the sync operation.
        """
        if not self._is_leader():
            return {"success": False, "error": "Not the leader"}

        # First, collect fresh manifests
        logger.info("Collecting cluster manifest for sync...")
        self.cluster_data_manifest = await self._collect_cluster_manifest()

        # Generate sync plan (using SyncPlanner manager for consolidated logic)
        self.current_sync_plan = self.sync_planner.generate_sync_plan(self.cluster_data_manifest)
        if not self.current_sync_plan:
            return {"success": True, "message": "No sync needed, all nodes in sync"}

        # Execute the plan
        await self._execute_sync_plan()

        return {
            "success": True,
            "plan_id": self.current_sync_plan.plan_id,
            "total_jobs": len(self.current_sync_plan.sync_jobs),
            "jobs_completed": self.current_sync_plan.jobs_completed,
            "jobs_failed": self.current_sync_plan.jobs_failed,
            "status": self.current_sync_plan.status,
        }

    # ============================================
    # NodeSelector Wrapper Methods REMOVED (Dec 2025)
    # All call sites now use self.node_selector.* directly
    # ============================================

    async def _dispatch_gauntlet_to_cpu_node(
        self,
        config_key: str,
        model_id: str,
        baseline_id: str,
        games_per_side: int = 4
    ) -> dict[str, Any] | None:
        """Dispatch gauntlet evaluation to a CPU-rich node.

        This ensures gauntlets run on Vast instances with high CPU count
        rather than blocking GPU-rich nodes like GH200/H100.
        """
        cpu_node = self.node_selector.get_best_cpu_node_for_gauntlet()
        if not cpu_node:
            logger.info("No CPU node available for gauntlet, running locally")
            return None

        # If we're already the best CPU node, return None to run locally
        if cpu_node.node_id == self.node_id:
            return None

        try:
            # Dispatch to the CPU node's gauntlet endpoint
            payload = {
                "config_key": config_key,
                "model_id": model_id,
                "baseline_id": baseline_id,
                "games_per_side": games_per_side,
            }

            timeout = ClientTimeout(total=120)
            async with get_client_session(timeout) as session:
                for url in self._urls_for_peer(cpu_node, "/gauntlet/quick-eval"):
                    try:
                        async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                            if resp.status == 200:
                                result = await resp.json()
                                logger.info(f"Gauntlet dispatched to {cpu_node.node_id} "
                                      f"(cpu_power={cpu_node.cpu_power_score()})")
                                return result
                    except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
                        continue

            logger.error(f"Failed to dispatch gauntlet to {cpu_node.node_id}")
            return None
        except Exception as e:  # noqa: BLE001
            logger.error(f"dispatching gauntlet: {e}")
            return None

    def _should_sync_to_node(self, node: NodeInfo) -> bool:
        """Check if we should sync data TO this node based on disk space."""
        # Don't sync to nodes with critical disk usage
        if node.disk_percent >= DISK_CRITICAL_THRESHOLD:
            logger.info(f"Skipping sync to {node.node_id}: disk critical ({node.disk_percent:.1f}%)")
            return False
        # Warn but allow sync to nodes with warning-level disk
        if node.disk_percent >= DISK_WARNING_THRESHOLD:
            logger.warning(f"{node.node_id} disk at {node.disk_percent:.1f}%")
        return True

    def _should_cleanup_source(self, node: NodeInfo) -> bool:
        """Check if source node needs disk cleanup after sync."""
        return node.disk_percent >= DISK_CLEANUP_THRESHOLD

    async def _cleanup_synced_files(self, node_id: str, files: list[str]) -> bool:
        """Delete synced files from source node to free disk space.

        Only called after successful sync to training nodes.
        """
        with self.peers_lock:
            node = self.peers.get(node_id)
        if not node or not node.is_alive():
            return False

        try:
            if getattr(node, "nat_blocked", False):
                cmd_id = await self._enqueue_relay_command_for_peer(
                    node,
                    "cleanup_files",
                    {"files": list(files or []), "reason": "post_sync_cleanup"},
                )
                if cmd_id:
                    logger.info(f"Enqueued relay cleanup_files for {node_id} ({len(files)} files)")
                    return True
                logger.info(f"Relay queue full for {node_id}; skipping cleanup_files enqueue")
                return False

            timeout = ClientTimeout(total=60)
            async with get_client_session(timeout) as session:
                last_err: str | None = None
                for url in self._urls_for_peer(node, "/cleanup/files"):
                    try:
                        async with session.post(
                            url,
                            json={"files": files, "reason": "post_sync_cleanup"},
                            headers=self._auth_headers(),
                        ) as resp:
                            if resp.status != 200:
                                last_err = f"http_{resp.status}"
                                continue
                            result = await resp.json()
                            freed_bytes = result.get("freed_bytes", 0)
                            logger.info(f"Cleanup on {node_id}: freed {freed_bytes / 1e6:.1f}MB")
                            return True
                    except Exception as e:  # noqa: BLE001
                        last_err = str(e)
                        continue
                if last_err:
                    logger.info(f"Cleanup files request failed on {node_id}: {last_err}")
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to cleanup files on {node_id}: {e}")
        return False

    async def _sync_selfplay_to_training_nodes(self) -> dict[str, Any]:
        """Sync selfplay data to training primary nodes.

        December 2025: Delegated to SyncPlanner.sync_selfplay_to_training_nodes()
        """
        if not self._is_leader():
            return {"success": False, "error": "Not the leader"}

        # Use stale manifest if available, otherwise will be collected fresh
        manifest = self.cluster_data_manifest
        if (time.time() - self.last_manifest_collection > self.manifest_collection_interval
                or not manifest):
            manifest = None  # Will be collected by SyncPlanner

        result = await self.sync_planner.sync_selfplay_to_training_nodes(
            get_training_nodes=self.node_selector.get_training_primary_nodes,
            should_sync_to_node=self._should_sync_to_node,
            should_cleanup_source=self._should_cleanup_source,
            collect_manifest=self._collect_cluster_manifest,
            execute_sync_job=self._request_node_sync,
            cleanup_synced_files=self._cleanup_synced_files,
            get_sync_router=self._get_sync_router,
            cluster_manifest=manifest,
        )

        # Update orchestrator state
        if result.get("success"):
            self.last_training_sync_time = time.time()
            # Refresh manifest after sync
            if not manifest:
                # Dec 2025: Add 5-minute timeout for manifest collection
                try:
                    self.cluster_data_manifest = await asyncio.wait_for(
                        self._collect_cluster_manifest(),
                        timeout=300.0  # 5 minutes max
                    )
                    self.last_manifest_collection = time.time()
                except asyncio.TimeoutError:
                    logger.warning("Post-sync manifest collection timed out after 5 minutes")

        return result

    async def _execute_pending_sync_jobs(self):
        """Execute all pending sync jobs."""
        with self.sync_lock:
            pending_jobs = [
                job for job in self.active_sync_jobs.values()
                if job.status == "pending"
            ]

        for job in pending_jobs:
            try:
                success = await self._request_node_sync(job)
                if success:
                    job.status = "completed"
                    job.completed_at = time.time()
                else:
                    job.status = "failed"
            except Exception as e:  # noqa: BLE001
                logger.info(f"Sync job {job.job_id} failed: {e}")
                job.status = "failed"
                job.error_message = str(e)

    # NOTE: _training_sync_loop() removed Dec 2025 (72 LOC).
    # Now runs via LoopManager as TrainingSyncLoop.
    # See scripts/p2p/loops/training_sync_loop.py for implementation.
    # The loop calls self._sync_selfplay_to_training_nodes() via callback.

    async def _force_ip_refresh_all_sources(self) -> int:
        """Force immediate refresh of IPs from all CLI sources (Tailscale, Vast, AWS).

        Called when network partition is detected to aggressively discover
        alternative paths to reach peers.

        Returns:
            Total number of IPs updated across all sources
        """
        if not HAS_DYNAMIC_REGISTRY or get_registry is None:
            return 0

        registry = get_registry()
        total_updated = 0

        logger.info("Force-refreshing all IP sources for partition recovery...")

        # Refresh Tailscale first (most likely to help in partition)
        try:
            # Reset rate limit to force immediate check
            registry._last_tailscale_check = 0
            updated = await registry.update_tailscale_ips()
            if updated > 0:
                logger.info(f"Tailscale refresh: {updated} IPs updated")
                total_updated += updated
        except Exception as e:  # noqa: BLE001
            logger.info(f"Tailscale refresh error: {e}")

        # Refresh Vast IPs
        try:
            registry._last_vast_check = 0
            updated = await registry.update_vast_ips()
            if updated > 0:
                logger.info(f"Vast refresh: {updated} IPs updated")
                total_updated += updated
        except Exception as e:  # noqa: BLE001
            logger.info(f"Vast refresh error: {e}")

        # Refresh AWS IPs
        try:
            registry._last_aws_check = 0
            updated = await registry.update_aws_ips()
            if updated > 0:
                logger.info(f"AWS refresh: {updated} IPs updated")
                total_updated += updated
        except Exception as e:  # noqa: BLE001
            logger.info(f"AWS refresh error: {e}")

        if total_updated > 0:
            logger.info(f"Force refresh complete: {total_updated} total IPs updated")
        return total_updated

    async def _vast_ip_update_loop(self):
        """Background loop to periodically refresh Vast instance connection info.

        Uses VAST_API_KEY when available, otherwise falls back to the `vastai`
        CLI if installed (see DynamicHostRegistry.update_vast_ips).
        """
        if not HAS_DYNAMIC_REGISTRY:
            return

        logger.info("Vast IP update loop started")
        registry = get_registry()

        while self.running:
            try:
                await asyncio.sleep(300)  # Check every 5 minutes

                updated = await registry.update_vast_ips()
                if updated > 0:
                    logger.info(f"Updated {updated} Vast instance IPs from API")

            except asyncio.CancelledError:
                break
            except Exception as e:  # noqa: BLE001
                logger.info(f"Vast IP update loop error: {e}")
                await asyncio.sleep(60)

    async def _aws_ip_update_loop(self):
        """Background loop to periodically refresh AWS instance connection info.

        Uses the `aws` CLI (see DynamicHostRegistry.update_aws_ips). No-op when
        no AWS instances are configured in distributed_hosts.yaml properties.
        """
        if not HAS_DYNAMIC_REGISTRY:
            return

        logger.info("AWS IP update loop started")
        registry = get_registry()

        while self.running:
            try:
                await asyncio.sleep(300)  # Check every 5 minutes

                updated = await registry.update_aws_ips()
                if updated > 0:
                    logger.info(f"Updated {updated} AWS instance IPs via CLI")

            except asyncio.CancelledError:
                break
            except Exception as e:  # noqa: BLE001
                logger.info(f"AWS IP update loop error: {e}")
                await asyncio.sleep(60)

    async def _tailscale_ip_update_loop(self):
        """Background loop to discover and update Tailscale IPs for cluster nodes.

        Uses `tailscale status --json` to discover mesh network peers.
        Tailscale provides reliable connectivity even when public IPs change.
        """
        if not HAS_DYNAMIC_REGISTRY:
            return

        logger.info("Tailscale IP update loop started")
        registry = get_registry()

        while self.running:
            try:
                await asyncio.sleep(120)  # Check every 2 minutes

                updated = await registry.update_tailscale_ips()
                if updated > 0:
                    logger.info(f"Updated {updated} node Tailscale IPs")

            except asyncio.CancelledError:
                break
            except Exception as e:  # noqa: BLE001
                logger.info(f"Tailscale IP update loop error: {e}")
                await asyncio.sleep(60)

    async def _tailscale_peer_recovery_loop(self):
        """Proactively discover and connect to all Tailscale nodes.

        .. deprecated::
            December 2025 - This method is deprecated and now runs via LoopManager
            as TailscalePeerDiscoveryLoop. See scripts/p2p/loops/network_loops.py.
            This inline version is kept for fallback compatibility but is no longer
            invoked by default. Will be removed Q2 2026.

        LEARNED LESSONS: Cross-cloud nodes (Lambda, Vast, Hetzner) can lose
        connectivity intermittently. This loop ensures all nodes stay in the
        P2P network by:
        1. Running `tailscale status --json` to find all online nodes
        2. Attempting to connect to nodes not in the peer list
        3. Retrying dead/stale nodes more aggressively
        """
        import json
        import subprocess

        logger.info("Tailscale peer recovery loop started (interval=120s)")

        # Patterns for compute nodes we want in the P2P network
        COMPUTE_PATTERNS = [
            "lambda-", "vast-", "gh200", "h100", "a100", "a10",
            "192-222-", "aws-",  # Lambda public IPs and AWS nodes
        ]

        while self.running:
            try:
                await asyncio.sleep(120)  # Every 2 minutes

                # Phase 30: All nodes participate in discovery (not just leader)
                # This ensures isolated nodes can rejoin the cluster
                # Rate limit non-leaders to every 5 minutes (3 loops)
                is_leader = self.role == NodeRole.LEADER
                if not is_leader:
                    # Non-leaders do discovery less frequently
                    loop_count = getattr(self, "_ts_recovery_loop_count", 0) + 1
                    self._ts_recovery_loop_count = loop_count
                    if loop_count % 3 != 0:  # Every 3rd iteration = 6 minutes
                        # Unless we're isolated (few peers)
                        with self.peers_lock:
                            alive_count = sum(1 for p in self.peers.values() if p.is_alive())
                        if alive_count >= MIN_CONNECTED_PEERS:
                            continue  # Skip if we have enough peers

                # Get current peer node_ids
                current_peers = set()
                with self.peers_lock:
                    current_peers = {p.node_id for p in self.peers.values()}

                # Get Tailscale peers (Jan 2026: use async helper to avoid blocking event loop)
                try:
                    returncode, stdout, stderr = await self._run_subprocess_async(
                        ["tailscale", "status", "--json"], timeout=10
                    )
                    if returncode != 0:
                        continue
                    ts_data = json.loads(stdout)
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Tailscale status failed: {e}")
                    continue

                # Find compute nodes not in P2P network
                missing_nodes = []
                for _peer_id, peer_info in ts_data.get("Peer", {}).items():
                    hostname = peer_info.get("HostName", "").lower()
                    online = peer_info.get("Online", False)
                    ts_ips = peer_info.get("TailscaleIPs", [])

                    if not online or not ts_ips:
                        continue

                    # Check if this is a compute node
                    is_compute = any(pat in hostname for pat in COMPUTE_PATTERNS)
                    if not is_compute:
                        continue

                    # Check if already in P2P network
                    if hostname in current_peers or hostname.replace("-", "_") in current_peers:
                        continue

                    # Also check by IP prefix
                    ip = ts_ips[0] if ts_ips else ""
                    ip_based_id = ip.replace(".", "-")
                    if ip_based_id in current_peers:
                        continue

                    missing_nodes.append((hostname, ip))

                if missing_nodes:
                    logger.info(f"Tailscale peer recovery: {len(missing_nodes)} compute nodes not in P2P network")
                    for hostname, ip in missing_nodes[:10]:  # Limit to 10 per cycle
                        logger.info(f"  Attempting to connect: {hostname} ({ip})")
                        try:
                            # Try to establish connection via heartbeat
                            url = f"http://{ip}:{DEFAULT_PORT}/health"
                            timeout = ClientTimeout(total=10)
                            async with get_client_session(timeout) as session:
                                async with session.get(url) as resp:
                                    if resp.status == 200:
                                        data = await resp.json()
                                        node_id = data.get("node_id", hostname)
                                        logger.info(f"  Connected to {node_id}, sending join request")
                                        # Send heartbeat to register
                                        await self._send_heartbeat_to_peer(ip, DEFAULT_PORT)
                        except Exception as e:  # noqa: BLE001
                            logger.debug(f"  Failed to connect to {hostname}: {e}")

            except asyncio.CancelledError:
                break
            except Exception as e:  # noqa: BLE001
                logger.warning(f"Tailscale peer recovery loop error: {e}")
                await asyncio.sleep(60)

    async def _discover_tailscale_peers(self):
        """One-shot Tailscale peer discovery for bootstrap fallback.

        Phase 30: Called when bootstrap from seeds fails. Discovers peers
        via `tailscale status --json` and attempts to connect.

        Jan 2026: Uses async subprocess to avoid blocking event loop.
        """
        import json

        logger.info("Running one-shot Tailscale peer discovery...")

        try:
            returncode, stdout, stderr = await self._run_subprocess_async(
                ["tailscale", "status", "--json"], timeout=10
            )
            if returncode != 0:
                logger.warning(f"Tailscale status failed: {stderr}")
                return

            ts_data = json.loads(stdout)
            peers = ts_data.get("Peer", {})

            # Get current peer node_ids
            current_peers = set()
            with self.peers_lock:
                current_peers = {p.node_id for p in self.peers.values()}

            # Patterns for compute nodes
            COMPUTE_PATTERNS = [
                "lambda-", "vast-", "gh200", "h100", "a100", "a10",
                "nebius-", "runpod-", "vultr-", "hetzner-",
            ]

            discovered = 0
            for peer_info in peers.values():
                hostname = peer_info.get("HostName", "").lower()
                is_compute = any(pat in hostname for pat in COMPUTE_PATTERNS)
                if not is_compute:
                    continue

                # Get IP from TailscaleIPs (prefer IPv4)
                ts_ips = peer_info.get("TailscaleIPs", [])
                ipv4s = [ip for ip in ts_ips if "." in ip]
                if not ipv4s:
                    continue
                ip = ipv4s[0]

                # Skip if we already know this IP
                known = False
                with self.peers_lock:
                    for p in self.peers.values():
                        if getattr(p, "tailscale_ip", None) == ip or p.host == ip:
                            known = True
                            break
                if known:
                    continue

                # Try to connect
                try:
                    async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=5)) as session:
                        async with session.get(f"http://{ip}:{DEFAULT_PORT}/status") as resp:
                            if resp.status == 200:
                                data = await resp.json()
                                node_id = data.get("node_id", hostname)
                                if node_id not in current_peers:
                                    logger.info(f"Discovered peer {node_id} via Tailscale at {ip}")
                                    await self._send_heartbeat_to_peer(ip, DEFAULT_PORT)
                                    discovered += 1
                except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError) as e:
                    # Debug log for Tailscale discovery - failures are normal for offline nodes
                    logger.debug(f"Tailscale discovery failed for {hostname}/{ip}: {type(e).__name__}")

            if discovered > 0:
                logger.info(f"Tailscale discovery: connected to {discovered} new peer(s)")
            else:
                logger.info("Tailscale discovery: no new peers found")

        except FileNotFoundError:
            logger.debug("Tailscale not installed on this node")
        except Exception as e:  # noqa: BLE001
            logger.warning(f"Tailscale discovery error: {e}")

    async def _reconnect_missing_tailscale_peers(self) -> int:
        """Force reconnect to peers online in Tailscale but missing from P2P mesh.

        January 2026 (Phase 1.1): Fixes peer discovery asymmetry where P2P shows
        5-7 peers while Tailscale shows 40 online. This method is called during
        startup after Tailscale discovery to ensure all reachable peers are connected.

        The root cause is that SWIM gossip and heartbeat loops may not discover all
        peers during initial bootstrap, especially when nodes start asynchronously.
        This method performs a targeted reconnection sweep.

        Jan 2026: Uses async subprocess to avoid blocking event loop.

        Returns:
            Number of peers successfully reconnected.
        """
        import json

        logger.info("[NetworkHealth] Running Tailscale-to-P2P reconnection sweep...")

        try:
            # Get Tailscale status (Jan 2026: use async helper to avoid blocking)
            returncode, stdout, stderr = await self._run_subprocess_async(
                ["tailscale", "status", "--json"], timeout=10
            )
            if returncode != 0:
                logger.warning(f"Tailscale status failed: {stderr}")
                return 0

            ts_data = json.loads(stdout)
            ts_peers = ts_data.get("Peer", {})

            # Build map of Tailscale IPs to online status
            ts_online: dict[str, bool] = {}
            for peer_info in ts_peers.values():
                ts_ips = peer_info.get("TailscaleIPs", [])
                is_online = peer_info.get("Online", False)
                for ip in ts_ips:
                    if "." in ip:  # IPv4
                        ts_online[ip] = is_online

            # Get config hosts for IP-to-node mapping
            config_hosts = self._load_distributed_hosts().get("hosts", {})
            ip_to_node = {
                h.get("tailscale_ip"): (name, h)
                for name, h in config_hosts.items()
                if h.get("tailscale_ip") and h.get("p2p_enabled", True)
            }

            # Get current P2P peer IDs
            p2p_peer_ids = set()
            with self.peers_lock:
                for peer_id, peer_info in self.peers.items():
                    is_alive = getattr(peer_info, "is_alive", lambda: True)
                    if (callable(is_alive) and is_alive()) or (not callable(is_alive) and is_alive):
                        p2p_peer_ids.add(peer_id)

            # Find and reconnect missing peers
            reconnected = 0
            attempted = 0

            for ts_ip, is_online in ts_online.items():
                if not is_online:
                    continue

                if ts_ip not in ip_to_node:
                    continue

                node_name, node_config = ip_to_node[ts_ip]

                # Skip if already connected in P2P
                if node_name in p2p_peer_ids:
                    continue

                attempted += 1
                port = node_config.get("p2p_port", DEFAULT_PORT)

                try:
                    # Try to reconnect via heartbeat
                    success = await self._reconnect_discovered_peer(node_name, ts_ip, port)
                    if success:
                        reconnected += 1
                        logger.debug(f"[NetworkHealth] Reconnected {node_name}")
                except (aiohttp.ClientError, asyncio.TimeoutError, OSError) as e:
                    logger.debug(f"[NetworkHealth] Failed to reconnect {node_name}: {e}")

            if reconnected > 0:
                logger.info(
                    f"[NetworkHealth] Reconnection sweep complete: "
                    f"{reconnected}/{attempted} peers reconnected"
                )
            elif attempted > 0:
                logger.warning(
                    f"[NetworkHealth] Reconnection sweep: "
                    f"0/{attempted} peers reconnected (check network connectivity)"
                )
            else:
                logger.debug("[NetworkHealth] No missing peers to reconnect")

            # Emit metric for observability
            try:
                from app.coordination.event_router import safe_emit_event
                safe_emit_event(
                    "P2P_DISCOVERY_GAP",
                    {
                        "tailscale_online": sum(1 for v in ts_online.values() if v),
                        "p2p_connected": len(p2p_peer_ids),
                        "gap": attempted,
                        "reconnected": reconnected,
                        "node_id": self.node_id,
                    },
                    source="p2p_orchestrator",
                )
            except ImportError:
                pass

            return reconnected

        except FileNotFoundError:
            logger.debug("[NetworkHealth] Tailscale not installed")
            return 0
        except json.JSONDecodeError as e:
            logger.warning(f"[NetworkHealth] Tailscale status JSON parse error: {e}")
            return 0
        except subprocess.TimeoutExpired:
            logger.warning("[NetworkHealth] Tailscale status command timed out")
            return 0
        except Exception as e:  # noqa: BLE001
            logger.warning(f"[NetworkHealth] Reconnection sweep error: {e}")
            return 0

    async def _convert_jsonl_to_db(self, data_dir: Path, games_dir: Path) -> int:
        """Convert JSONL selfplay files to SQLite DB format for training.

        This enables the training pipeline to access games stored in JSONL format.
        Converted files are tracked to avoid re-processing.

        Features:
        - Chunked reading to handle large files without memory issues
        - Limited files per cycle to avoid blocking the event loop
        - Spawns external converter for large backlogs

        Returns:
            Number of games converted.
        """
        # Skip if disabled via environment variable (prevents blocking event loop on startup)
        if os.environ.get("RINGRIFT_SKIP_JSONL_CONVERSION", "").lower() in ("1", "true", "yes"):
            logger.debug("[P2POrchestrator] JSONLDB conversion skipped via RINGRIFT_SKIP_JSONL_CONVERSION")
            return 0

        import json
        import sqlite3

        # Configuration
        MAX_FILES_PER_CYCLE = 50  # Process max 50 files per cycle to avoid blocking
        CHUNK_SIZE = 500  # Lines to read at a time
        LARGE_BACKLOG_THRESHOLD = 200  # Spawn external converter if more files

        # Track converted files to avoid re-processing
        converted_marker_file = data_dir / ".jsonl_converted"
        converted_files: set = set()
        if converted_marker_file.exists():
            with contextlib.suppress(Exception):
                converted_files = set(converted_marker_file.read_text().strip().split("\n"))

        total_converted = 0
        newly_converted = []
        selfplay_dir = data_dir / "selfplay"

        if not selfplay_dir.exists():
            return 0

        # Find all JSONL files in selfplay subdirectories
        jsonl_files = list(selfplay_dir.rglob("*.jsonl"))
        if not jsonl_files:
            return 0

        # Filter to unconverted files and sort by size (smallest first for quick wins)
        unconverted_files = []
        for jsonl_file in jsonl_files:
            try:
                file_size = jsonl_file.stat().st_size
                if file_size < 100:
                    continue
                file_key = str(jsonl_file.relative_to(data_dir))
                if file_key not in converted_files:
                    unconverted_files.append((jsonl_file, file_size, file_key))
            except (AttributeError):
                continue

        if not unconverted_files:
            return 0

        # Sort by size (smallest first) and limit per cycle
        unconverted_files.sort(key=lambda x: x[1])
        files_this_cycle = unconverted_files[:MAX_FILES_PER_CYCLE]

        # If large backlog, spawn external converter in background
        if len(unconverted_files) > LARGE_BACKLOG_THRESHOLD:
            logger.info(f"Large JSONL backlog ({len(unconverted_files)} files), spawning background converter")
            converter_script = Path(self._get_ai_service_path()) / "scripts" / "chunked_jsonl_converter.py"
            if converter_script.exists():
                try:
                    import subprocess
                    subprocess.Popen(
                        ["python3", str(converter_script),
                         "--input-dir", str(selfplay_dir),
                         "--output-dir", str(games_dir),
                         "--workers", "2",
                         "--chunk-size", "500"],
                        stdout=open("/tmp/chunked_converter.log", "a"),
                        stderr=subprocess.STDOUT,
                        cwd=str(Path(self._get_ai_service_path())),
                    )
                except Exception as e:  # noqa: BLE001
                    logger.error(f"Failed to spawn background converter: {e}")

        # Group files by board type
        board_type_files: dict[str, list[tuple[Path, str]]] = {}
        for jsonl_file, _file_size, file_key in files_this_cycle:
            path_str = str(jsonl_file).lower()
            if "hex" in path_str:
                if "4p" in path_str:
                    board_key = "hexagonal_4p"
                elif "3p" in path_str:
                    board_key = "hexagonal_3p"
                else:
                    board_key = "hexagonal_2p"
            elif "square19" in path_str or "sq19" in path_str:
                if "4p" in path_str:
                    board_key = "square19_4p"
                elif "3p" in path_str:
                    board_key = "square19_3p"
                else:
                    board_key = "square19_2p"
            else:
                if "4p" in path_str:
                    board_key = "square8_4p"
                elif "3p" in path_str:
                    board_key = "square8_3p"
                else:
                    board_key = "square8_2p"

            if board_key not in board_type_files:
                board_type_files[board_key] = []
            board_type_files[board_key].append((jsonl_file, file_key))

        # Helper function to convert one board type's files to DB - runs in thread pool
        def _convert_board_type_to_db(
            board_key: str,
            files: list[tuple[Path, str]],
            games_dir: Path,
            chunk_size: int,
        ) -> tuple[int, list[str]]:
            """Convert one board type's JSONL files to SQLite DB.

            Runs in thread pool via asyncio.to_thread() to avoid blocking event loop.

            Returns:
                (games_added, newly_converted_file_keys)
            """
            db_path = games_dir / f"jsonl_converted_{board_key}.db"
            conn = None
            games_added = 0
            converted = []

            try:
                conn = sqlite3.connect(str(db_path), timeout=30.0)
                conn.execute("PRAGMA journal_mode=WAL")
                conn.execute("PRAGMA synchronous=NORMAL")
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS games (
                        game_id TEXT PRIMARY KEY,
                        board_type TEXT,
                        num_players INTEGER,
                        winner INTEGER,
                        move_count INTEGER,
                        game_status TEXT,
                        victory_type TEXT,
                        created_at TEXT,
                        source TEXT,
                        metadata_json TEXT
                    )
                """)
                conn.commit()

                for jsonl_file, file_key in files:
                    try:
                        # Read file in chunks to avoid memory issues
                        chunk_buffer = []
                        with open_jsonl_file(jsonl_file) as f:
                            for line_num, line in enumerate(f, 1):
                                line = line.strip()
                                if not line:
                                    continue
                                try:
                                    record = json.loads(line)
                                    game_id = f"{jsonl_file.stem}_{record.get('game_id', line_num)}"
                                    chunk_buffer.append((
                                        game_id,
                                        record.get("board_type", board_key.split("_")[0]),
                                        record.get("num_players", int(board_key[-2]) if board_key[-2].isdigit() else 2),
                                        record.get("winner", 0),
                                        record.get("move_count", 0),
                                        record.get("status", "completed"),
                                        record.get("victory_type", "unknown"),
                                        record.get("timestamp", ""),
                                        f"jsonl:{jsonl_file.name}",
                                        json.dumps(record),
                                    ))

                                    # Flush chunk when buffer is full
                                    if len(chunk_buffer) >= chunk_size:
                                        conn.executemany("""
                                            INSERT OR IGNORE INTO games
                                            (game_id, board_type, num_players, winner, move_count,
                                             game_status, victory_type, created_at, source, metadata_json)
                                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                                        """, chunk_buffer)
                                        games_added += len(chunk_buffer)
                                        chunk_buffer = []

                                except (json.JSONDecodeError, Exception):
                                    continue

                        # Flush remaining records
                        if chunk_buffer:
                            conn.executemany("""
                                INSERT OR IGNORE INTO games
                                (game_id, board_type, num_players, winner, move_count,
                                 game_status, victory_type, created_at, source, metadata_json)
                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                            """, chunk_buffer)
                            games_added += len(chunk_buffer)

                        converted.append(file_key)
                    except Exception as e:  # noqa: BLE001
                        logger.error(f"converting {jsonl_file.name}: {e}")
                        continue

                conn.commit()

                if games_added > 0:
                    logger.info(f"Converted {games_added} games to {db_path.name}")

            except Exception as e:  # noqa: BLE001
                logger.error(f"creating DB for {board_key}: {e}")
            finally:
                if conn:
                    conn.close()

            return games_added, converted

        # Convert each board type to a consolidated DB (chunked reading)
        # Jan 4, 2026: Run blocking SQLite ops in thread pool to avoid blocking event loop
        for board_key, files in board_type_files.items():
            if not files:
                continue

            games_added, converted = await asyncio.to_thread(
                _convert_board_type_to_db,
                board_key,
                files,
                games_dir,
                CHUNK_SIZE,
            )
            total_converted += games_added
            newly_converted.extend(converted)

        # Update converted files marker
        if newly_converted:
            try:
                all_converted = converted_files | set(newly_converted)
                converted_marker_file.write_text("\n".join(sorted(all_converted)))
            except (AttributeError):
                pass

        if total_converted > 0:
            logger.info(f"JSONL conversion complete: {total_converted} total games converted")

        return total_converted

    async def _convert_jsonl_to_npz_for_training(self, data_dir: Path, training_dir: Path) -> int:
        """Convert JSONL selfplay files directly to NPZ for training.

        This bypasses the DB intermediate step and creates training-ready NPZ files
        directly from JSONL. Called periodically when JSONL backlog exists.

        Returns:
            Number of NPZ files created.
        """
        # Skip if disabled via environment variable (prevents blocking event loop on startup)
        if os.environ.get("RINGRIFT_SKIP_JSONL_CONVERSION", "").lower() in ("1", "true", "yes"):
            logger.debug("[P2POrchestrator] JSONLNPZ conversion skipped via RINGRIFT_SKIP_JSONL_CONVERSION")
            return 0

        import json as json_module
        import subprocess

        # Configuration
        JSONL_THRESHOLD_GAMES = 50  # Only convert when > 50 games accumulated
        MAX_CONVERSIONS_PER_CYCLE = 3  # Limit conversions per cycle

        selfplay_dir = data_dir / "selfplay"
        canonical_dir = selfplay_dir / "canonical"

        # Track converted files
        npz_marker_file = data_dir / ".jsonl_to_npz_converted"
        converted_files: set = set()
        if npz_marker_file.exists():
            with contextlib.suppress(Exception):
                converted_files = set(npz_marker_file.read_text().strip().split("\n"))

        conversions_done = 0
        newly_converted = []

        # Board configs to check
        board_configs = [
            ("square8", 2), ("square8", 3), ("square8", 4),
            ("square19", 2), ("square19", 3), ("square19", 4),
            ("hex8", 2), ("hex8", 3), ("hex8", 4),
            ("hexagonal", 2), ("hexagonal", 3), ("hexagonal", 4),
        ]

        for board_type, num_players in board_configs:
            if conversions_done >= MAX_CONVERSIONS_PER_CYCLE:
                break

            config_key = f"{board_type}_{num_players}p"

            # Skip if already converted recently
            if config_key in converted_files:
                continue

            # Find JSONL files for this config
            jsonl_files = []
            search_dirs = [canonical_dir, selfplay_dir, data_dir / "games"]

            for search_dir in search_dirs:
                if not search_dir.exists():
                    continue
                for jsonl_file in search_dir.rglob("*.jsonl"):
                    if jsonl_file.stat().st_size < 100:
                        continue
                    jsonl_files.append(jsonl_file)

            if not jsonl_files:
                continue

            # Count games matching this config (quick check - just count lines with board type)
            game_count = 0
            valid_files = []
            for jsonl_file in jsonl_files:
                try:
                    with open_jsonl_file(jsonl_file) as f:
                        for line in f:
                            if not line.strip():
                                continue
                            try:
                                game = json_module.loads(line)
                                game_board = game.get("board_type", "")
                                game_players = game.get("num_players", 0)
                                has_moves = "moves" in game and len(game.get("moves", [])) > 0

                                # Check if matches config
                                board_match = board_type in game_board.lower() or game_board.lower() in board_type
                                if board_type == "hexagonal":
                                    board_match = "hex" in game_board.lower()

                                if board_match and game_players == num_players and has_moves:
                                    game_count += 1
                                    if jsonl_file not in valid_files:
                                        valid_files.append(jsonl_file)
                            except json_module.JSONDecodeError:
                                continue
                except (AttributeError):
                    continue

            if game_count < JSONL_THRESHOLD_GAMES:
                continue

            if not valid_files:
                continue

            # Convert to NPZ using jsonl_to_npz.py
            output_npz = training_dir / f"jsonl_auto_{config_key}_{int(time.time())}.npz"
            converter_script = Path(self._get_ai_service_path()) / "scripts" / "jsonl_to_npz.py"

            if not converter_script.exists():
                logger.info(f"JSONLNPZ converter not found: {converter_script}")
                continue

            cmd = [
                sys.executable, str(converter_script),
                "--output", str(output_npz),
                "--board-type", board_type,
                "--num-players", str(num_players),
                "--sample-every", "5",
                "--max-games", "100",
            ]
            for vf in valid_files[:10]:  # Limit input files
                cmd.extend(["--input", str(vf)])

            env = os.environ.copy()
            env["PYTHONPATH"] = str(Path(self._get_ai_service_path()))

            # Jan 2026: Use asyncio.to_thread() to avoid blocking event loop
            # This can take up to 600 seconds - must not block
            def _run_conversion():
                return subprocess.run(
                    cmd, capture_output=True, text=True, timeout=600, env=env,
                    cwd=str(Path(self._get_ai_service_path()))
                )

            try:
                logger.info(f"Converting {game_count} {config_key} JSONL games to NPZ...")
                result = await asyncio.to_thread(_run_conversion)
                if result.returncode == 0 and output_npz.exists():
                    logger.info(f"Created {output_npz.name} from JSONL")
                    conversions_done += 1
                    newly_converted.append(config_key)
                else:
                    logger.info(f"JSONLNPZ conversion failed for {config_key}: {result.stderr[:200] if result.stderr else 'no error'}")
            except subprocess.TimeoutExpired:
                logger.info(f"JSONLNPZ conversion timeout for {config_key}")
            except Exception as e:  # noqa: BLE001
                logger.info(f"JSONLNPZ conversion error for {config_key}: {e}")

        # Update marker file
        if newly_converted:
            try:
                all_converted = converted_files | set(newly_converted)
                npz_marker_file.write_text("\n".join(sorted(all_converted)))
            except (AttributeError):
                pass

        return conversions_done

    # NOTE: _data_management_loop_DEPRECATED() removed Dec 28, 2025 (~180 LOC).
    # See scripts/p2p/loops/data_loops.py::DataManagementLoop for replacement.

    # NOTE: _model_sync_loop_DEPRECATED() removed Dec 28, 2025 (~143 LOC).
    # See scripts/p2p/loops/data_loops.py::ModelSyncLoop for replacement.

    async def _consolidate_selfplay_data(self):
        """Consolidate siloed job databases AND JSONL files into training DB.

        LEARNED LESSONS: Selfplay jobs write to job-specific databases for isolation.
        GPU selfplay jobs write JSONL files for efficiency.
        These need to be periodically merged into the training DB for:
        1. Training triggers to see accurate game counts
        2. Cross-node data sync to work correctly
        3. Training scripts to find all available data

        Runs every ~5 minutes (every 5th job check cycle) to avoid overhead.
        """
        # Only run every 5th cycle (~5 minutes with JOB_CHECK_INTERVAL=60)
        cycle_counter = getattr(self, "_consolidation_cycle", 0)
        self._consolidation_cycle = cycle_counter + 1
        if cycle_counter % 5 != 0:
            return

        try:
            import sqlite3
            import subprocess
            from pathlib import Path

            data_dir = Path(self._get_ai_service_path()) / "data"
            selfplay_dir = data_dir / "selfplay"
            games_dir = data_dir / "games"
            main_db_path = games_dir / "selfplay.db"
            jsonl_db_path = games_dir / "jsonl_aggregated.db"

            if not selfplay_dir.exists():
                return

            env = os.environ.copy()
            env["PYTHONPATH"] = str(Path(self._get_ai_service_path()))

            # --- PART 1: Aggregate JSONL files (GPU selfplay output) ---
            jsonl_files = list(selfplay_dir.glob("**/games.jsonl"))
            # Filter to files > 1KB and modified in last 24 hours
            recent_jsonl = []
            for jf in jsonl_files:
                try:
                    if jf.stat().st_size > 1024:
                        recent_jsonl.append(jf)
                except (AttributeError):
                    pass

            if recent_jsonl:
                total_lines = 0
                for jf in recent_jsonl[:20]:  # Sample first 20
                    try:
                        with open(jf) as f:
                            total_lines += sum(1 for _ in f)
                    except (OSError):
                        pass

                if total_lines > 100:  # Only run if there's meaningful data
                    aggregate_script = Path(self._get_ai_service_path()) / "scripts" / "aggregate_jsonl_to_db.py"
                    if aggregate_script.exists() and not getattr(self, "_jsonl_aggregation_running", False):
                        self._jsonl_aggregation_running = True
                        logger.info(f"JSONL aggregation: ~{total_lines * len(recent_jsonl) // 20} games in {len(recent_jsonl)} files")
                        cmd = [
                            sys.executable, str(aggregate_script),
                            "--input-dir", str(selfplay_dir),
                            "--output-db", str(jsonl_db_path),
                        ]
                        proc = subprocess.Popen(
                            cmd,
                            env=env,
                            stdout=open("/tmp/jsonl_aggregate.log", "w"),
                            stderr=subprocess.STDOUT,
                            cwd=str(Path(self._get_ai_service_path())),
                        )
                        logger.info(f"Started JSONL aggregation (PID: {proc.pid})")
                        # Reset flag after ~10 minutes
                        asyncio.get_running_loop().call_later(
                            600, lambda: setattr(self, "_jsonl_aggregation_running", False)
                        )

            # --- PART 1b: Export NPZ from aggregated DB for training ---
            # Only run if we have a decent sized aggregated DB and not already exporting
            # LEARNED LESSONS: NPZ export is CPU-intensive. Route to high-CPU nodes
            # (vast nodes have 256-512 CPUs vs lambda's 64) to free GPU nodes for training.
            if jsonl_db_path.exists() and not getattr(self, "_npz_export_running", False):
                try:
                    # Use asyncio.to_thread to avoid blocking event loop (fix Dec 2025)
                    game_count = await asyncio.to_thread(self._get_db_game_count_sync, jsonl_db_path)

                    # Only export if we have enough games and it's been a while
                    training_dir = data_dir / "training"
                    training_dir.mkdir(exist_ok=True)
                    npz_output = training_dir / "auto_training_sq8_2p.npz"

                    # Check if existing NPZ is stale (older than 1 hour) or small
                    should_export = False
                    if not npz_output.exists():
                        should_export = game_count > 500
                    else:
                        npz_age_hours = (time.time() - npz_output.stat().st_mtime) / 3600
                        npz_size_mb = npz_output.stat().st_size / (1024 * 1024)
                        should_export = game_count > 1000 and (npz_age_hours > 1 or npz_size_mb < 1)

                    if should_export:
                        self._npz_export_running = True

                        # Find best CPU node for export (prefer vast nodes)
                        # Phase 2B: Direct call to NodeSelector
                        cpu_nodes = self.node_selector.get_cpu_primary_nodes(count=3)
                        export_node = None
                        for node in cpu_nodes:
                            # Skip nodes that are already very loaded
                            if node.get_load_score() < 80 and node.cpu_percent < 90:
                                export_node = node
                                break

                        if export_node and export_node.node_id != self.node_id:
                            # Dispatch to high-CPU node
                            logger.info(f"Dispatching NPZ export ({game_count} games) to {export_node.node_id} "
                                  f"(cpu_power={export_node.cpu_power_score()}, cpus={export_node.cpu_count})")
                            asyncio.create_task(self._dispatch_export_job(
                                node=export_node,
                                input_path=str(jsonl_db_path),
                                output_path=str(npz_output),
                                board_type="square8",
                                num_players=2,
                                encoder_version="v3",
                                max_games=5000,
                                is_jsonl=False,
                            ))
                        else:
                            # Fall back to local export if no suitable CPU node
                            export_script = Path(self._get_ai_service_path()) / "scripts" / "export_replay_dataset.py"
                            if export_script.exists():
                                logger.info(f"Starting local NPZ export ({game_count} games) -> {npz_output}")
                                cmd = [
                                    sys.executable, str(export_script),
                                    "--db", str(jsonl_db_path),
                                    "--board-type", "square8",
                                    "--num-players", "2",
                                    "--output", str(npz_output),
                                    "--encoder-version", "v3",
                                    "--max-games", "5000",
                                ]
                                subprocess.Popen(
                                    cmd,
                                    env=env,
                                    stdout=open("/tmp/npz_export.log", "w"),
                                    stderr=subprocess.STDOUT,
                                    cwd=str(Path(self._get_ai_service_path())),
                                )

                        # Reset flag after 30 minutes (export is slow)
                        asyncio.get_running_loop().call_later(
                            1800, lambda: setattr(self, "_npz_export_running", False)
                        )
                except Exception as e:  # noqa: BLE001
                    logger.info(f"NPZ export check error: {e}")

            # --- PART 2: Merge job DBs (CPU selfplay output) ---
            # Use asyncio.to_thread to avoid blocking event loop (fix Dec 2025)
            dbs_to_merge = await asyncio.to_thread(
                self._find_dbs_to_merge_sync, selfplay_dir, main_db_path
            )

            if dbs_to_merge:
                total_games = sum(c for _, c in dbs_to_merge)
                logger.info(f"DB consolidation: {len(dbs_to_merge)} DBs with {total_games} games to merge")

                # Use merge script if available
                merge_script = Path(self._get_ai_service_path()) / "scripts" / "merge_game_dbs.py"
                if merge_script.exists():
                    # Build command with all DBs
                    cmd = [
                        sys.executable, str(merge_script),  # Use venv Python
                        "--output", str(main_db_path),
                        "--dedupe-by-game-id",
                    ]
                    for db_path, _ in dbs_to_merge[:50]:  # Limit to 50 DBs at a time
                        cmd.extend(["--db", str(db_path)])

                    # Run merge in background
                    proc = subprocess.Popen(
                        cmd,
                        env=env,
                        stdout=subprocess.DEVNULL,
                        stderr=subprocess.DEVNULL,
                        cwd=str(Path(self._get_ai_service_path())),
                    )
                    logger.info(f"Started DB merge (PID: {proc.pid})")

        except Exception as e:  # noqa: BLE001
            logger.info(f"Data consolidation error: {e}")

    async def _start_auto_training(self, data_path: str):
        """Start automatic training job on local node."""
        try:
            run_dir = os.path.join(self._get_ai_service_path(), "models", f"auto_train_{int(time.time())}")
            Path(run_dir).mkdir(parents=True, exist_ok=True)

            cmd = [
                sys.executable,  # Use venv Python
                self._get_script_path("run_nn_training_baseline.py"),
                "--board", "square8",
                "--num-players", "2",
                "--run-dir", run_dir,
                "--data-path", data_path,
                "--epochs", "20",  # Jan 2026: Reduced from 50 to prevent overfitting (patience=7 will early stop)
                "--model-version", "v3",
            ]

            env = os.environ.copy()
            env["PYTHONPATH"] = self._get_ai_service_path()

            subprocess.Popen(
                cmd,
                stdout=open(f"{run_dir}/training.log", "w"),
                stderr=subprocess.STDOUT,
                env=env,
                cwd=self._get_ai_service_path(),
            )
            logger.info(f"Started auto-training job in {run_dir}")
            self.self_info.training_jobs += 1

        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to start auto-training: {e}")

    async def _request_data_from_peers(self):
        """Sync training data (NPZ files) from peers with large datasets.

        This ensures training data is distributed across the cluster so any
        GPU node can run training jobs.
        """
        try:
            # Check disk capacity before requesting data
            has_capacity, disk_pct = check_disk_has_capacity(DISK_CRITICAL_THRESHOLD)
            if not has_capacity:
                if self.verbose:
                    logger.info(f"Skipping data sync request: disk at {disk_pct:.1f}% (limit {DISK_CRITICAL_THRESHOLD}%)")
                return

            # Only sync if we're a GPU node (training capable)
            if not self.self_info.is_gpu_node():
                return

            # Rate limit: only sync every 10 minutes
            last_sync = getattr(self, "_last_training_data_sync", 0)
            if time.time() - last_sync < 600:
                return

            # Load hosts config to get SSH details
            if not HAS_HOSTS_FOR_SYNC:
                return

            hosts = load_remote_hosts()
            if not hosts:
                return

            local_training_dir = Path(self._get_ai_service_path()) / "data" / "training"
            local_training_dir.mkdir(parents=True, exist_ok=True)

            # Calculate local training data size
            local_training_mb = sum(
                f.stat().st_size for f in local_training_dir.glob("*.npz")
            ) / (1024 * 1024) if local_training_dir.exists() else 0

            # Find peers with more training data
            synced_from = []
            for host_name, host_config in hosts.items():
                if host_name == self.node_id:
                    continue

                # Skip hosts without SSH access
                ssh_host = host_config.ssh_host
                if not ssh_host:
                    continue

                # Check if host is alive via P2P
                with self.peers_lock:
                    peer = self.peers.get(host_name)
                if not peer or not peer.is_alive():
                    continue

                # Get remote training data size via SSH
                # Jan 2026: Use asyncio.to_thread() to avoid blocking event loop
                try:
                    ssh_user = getattr(host_config, 'ssh_user', 'ubuntu')
                    remote_path = getattr(host_config, 'ringrift_path', '/home/ubuntu/ringrift')
                    if remote_path.startswith('~'):
                        remote_path = remote_path.replace('~', f'/home/{ssh_user}')

                    # Check remote training data size
                    cmd = [
                        "ssh", "-o", "ConnectTimeout=10", "-o", "StrictHostKeyChecking=no",
                        f"{ssh_user}@{ssh_host}",
                        f"du -sb {remote_path}/ai-service/data/training/*.npz 2>/dev/null | awk '{{sum+=$1}}END{{print sum}}'"
                    ]
                    returncode, stdout, _stderr = await self._run_subprocess_async(cmd, timeout=30)
                    remote_size = int(stdout.strip() or 0) if returncode == 0 else 0
                    remote_mb = remote_size / (1024 * 1024)

                    # Sync if remote has significantly more data (>20MB more)
                    if remote_mb > local_training_mb + 20:
                        logger.info(f"Syncing training data from {host_name}: {remote_mb:.1f}MB -> local {local_training_mb:.1f}MB")

                        # Use rsync to sync NPZ files
                        rsync_cmd = [
                            "rsync", "-avz", "--progress",
                            "-e", "ssh -o ConnectTimeout=30 -o StrictHostKeyChecking=no",
                            f"{ssh_user}@{ssh_host}:{remote_path}/ai-service/data/training/*.npz",
                            str(local_training_dir) + "/"
                        ]

                        sync_returncode, _sync_stdout, sync_stderr = await self._run_subprocess_async(
                            rsync_cmd, timeout=300
                        )

                        if sync_returncode == 0:
                            synced_from.append(host_name)
                            logger.info(f"Successfully synced training data from {host_name}")
                        else:
                            logger.error(f"Failed to sync from {host_name}: {sync_stderr[:200]}")

                except subprocess.TimeoutExpired:
                    logger.info(f"Timeout checking training data on {host_name}")
                except Exception as e:  # noqa: BLE001
                    logger.error(f"syncing from {host_name}: {e}")

            if synced_from:
                # Recalculate local size after sync
                new_local_mb = sum(
                    f.stat().st_size for f in local_training_dir.glob("*.npz")
                ) / (1024 * 1024)
                logger.info(f"Training data sync complete: {local_training_mb:.1f}MB -> {new_local_mb:.1f}MB")

            self._last_training_data_sync = time.time()

        except Exception as e:  # noqa: BLE001
            logger.info(f"Data sync request error: {e}")
            import traceback
            traceback.print_exc()

    # ============================================
    # Git Auto-Update Methods (async - Jan 19, 2026)
    # All git operations run in thread pool to avoid blocking event loop
    # ============================================

    async def _get_local_git_commit(self) -> str | None:
        """Get the current local git commit hash (async)."""
        try:
            result = await async_subprocess_run(
                self._git_cmd("rev-parse", "HEAD"),
                cwd=self.ringrift_path,
                timeout=10
            )
            if result.returncode == 0:
                return result.stdout.strip()
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to get local git commit: {e}")
        return None

    async def _get_local_git_branch(self) -> str | None:
        """Get the current local git branch name (async)."""
        try:
            result = await async_subprocess_run(
                self._git_cmd("rev-parse", "--abbrev-ref", "HEAD"),
                cwd=self.ringrift_path,
                timeout=10
            )
            if result.returncode == 0:
                return result.stdout.strip()
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to get local git branch: {e}")
        return None

    async def _get_remote_git_commit(self) -> str | None:
        """Fetch and get the remote branch's latest commit hash (async)."""
        try:
            # First fetch to update remote refs
            fetch_result = await async_subprocess_run(
                self._git_cmd("fetch", GIT_REMOTE_NAME, GIT_BRANCH_NAME),
                cwd=self.ringrift_path,
                timeout=60
            )
            if fetch_result.returncode != 0:
                logger.info(f"Git fetch failed: {fetch_result.stderr}")
                return None

            # Get remote branch commit
            result = await async_subprocess_run(
                self._git_cmd("rev-parse", f"{GIT_REMOTE_NAME}/{GIT_BRANCH_NAME}"),
                cwd=self.ringrift_path,
                timeout=10
            )
            if result.returncode == 0:
                return result.stdout.strip()
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to get remote git commit: {e}")
        return None

    async def _check_for_updates(self) -> tuple[bool, str | None, str | None]:
        """Check if there are updates available from GitHub (async).

        Returns: (has_updates, local_commit, remote_commit)
        """
        # Run both git queries in parallel
        local_commit, remote_commit = await asyncio.gather(
            self._get_local_git_commit(),
            self._get_remote_git_commit(),
        )

        if not local_commit or not remote_commit:
            return False, local_commit, remote_commit

        has_updates = local_commit != remote_commit
        return has_updates, local_commit, remote_commit

    async def _get_commits_behind(self, local_commit: str, remote_commit: str) -> int:
        """Get the number of commits the local branch is behind remote (async)."""
        try:
            result = await async_subprocess_run(
                self._git_cmd("rev-list", "--count", f"{local_commit}..{remote_commit}"),
                cwd=self.ringrift_path,
                timeout=10
            )
            if result.returncode == 0:
                return int(result.stdout.strip())
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to count commits behind: {e}")
        return 0

    async def _check_local_changes(self) -> bool:
        """Check if there are uncommitted local changes (async).

        Notes:
        - Ignore untracked files by default. Cluster nodes often accumulate local
          artifacts (logs, data, env backups) that should not block git updates.
        - Still blocks on tracked/staged modifications to avoid stomping on
          local hotfixes.
        """
        try:
            result = await async_subprocess_run(
                self._git_cmd("status", "--porcelain", "--untracked-files=no"),
                cwd=self.ringrift_path,
                timeout=10
            )
            if result.returncode == 0:
                # If there's output, there are uncommitted changes
                return bool(result.stdout.strip())
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to check local changes: {e}")
        return True  # Assume changes exist on error (safer)

    async def _stop_all_local_jobs(self) -> int:
        """Stop all local jobs gracefully before update.

        Returns: Number of jobs stopped
        """
        stopped = 0
        with self.jobs_lock:
            for job_id, job in list(self.local_jobs.items()):
                try:
                    if job.pid > 0:
                        os.kill(job.pid, signal.SIGTERM)
                        logger.info(f"Sent SIGTERM to job {job_id} (PID {job.pid})")
                        stopped += 1
                        job.status = "stopping"
                except ProcessLookupError:
                    # Process already gone
                    job.status = "stopped"
                except Exception as e:  # noqa: BLE001
                    logger.error(f"Failed to stop job {job_id}: {e}")

        # Wait for processes to terminate gracefully
        # GPU games can take 1-10 minutes, so use a longer timeout (Dec 2025 fix)
        grace_period = int(os.environ.get("RINGRIFT_JOB_GRACE_PERIOD", "60"))
        if stopped > 0:
            await asyncio.sleep(grace_period)

            # Force kill any remaining
            with self.jobs_lock:
                for job_id, job in list(self.local_jobs.items()):
                    if job.status == "stopping" and job.pid > 0:
                        try:
                            os.kill(job.pid, signal.SIGKILL)
                            logger.info(f"Force killed job {job_id}")
                        except OSError:
                            pass  # Process already dead
                        job.status = "stopped"

        return stopped

    async def _perform_git_update(self) -> tuple[bool, str]:
        """Perform git pull to update the codebase (async).

        Returns: (success, message)
        """
        # Check for local changes (async)
        if await self._check_local_changes():
            return False, "Local changes detected. Cannot auto-update. Please commit or stash changes."

        # Stop jobs if configured
        if GRACEFUL_SHUTDOWN_BEFORE_UPDATE:
            stopped = await self._stop_all_local_jobs()
            if stopped > 0:
                logger.info(f"Stopped {stopped} jobs before update")

        try:
            # Perform git pull (async - Jan 19, 2026)
            result = await async_subprocess_run(
                self._git_cmd("pull", GIT_REMOTE_NAME, GIT_BRANCH_NAME),
                cwd=self.ringrift_path,
                timeout=120
            )

            if result.returncode != 0:
                return False, f"Git pull failed: {result.stderr}"

            logger.info(f"Git pull successful: {result.stdout}")
            return True, result.stdout

        except subprocess.TimeoutExpired:
            return False, "Git pull timed out"
        except Exception as e:  # noqa: BLE001
            return False, f"Git pull error: {e}"

    async def _restart_orchestrator(self):
        """Restart the orchestrator process after update."""
        logger.info("Restarting orchestrator to apply updates...")

        # Save state before restart
        self._save_state()

        # Get current script path and arguments
        script_path = Path(__file__).resolve()
        args = sys.argv[1:]

        # Schedule restart
        await asyncio.sleep(2)

        # Use exec to replace current process
        os.execv(sys.executable, [sys.executable, str(script_path), *args])

    async def _git_update_loop(self):
        """Background loop to periodically check for and apply updates.

        Jan 2026: Uses asyncio.to_thread() for git operations to avoid blocking.
        """
        if not AUTO_UPDATE_ENABLED:
            logger.info("Auto-update disabled")
            return

        logger.info(f"Git auto-update loop started (interval: {GIT_UPDATE_CHECK_INTERVAL}s)")

        while self.running:
            try:
                await asyncio.sleep(GIT_UPDATE_CHECK_INTERVAL)

                if not self.running:
                    break

                # Check for updates (Jan 19, 2026: methods are now async)
                has_updates, local_commit, remote_commit = await self._check_for_updates()

                if has_updates and local_commit and remote_commit:
                    commits_behind = await self._get_commits_behind(local_commit, remote_commit)
                    logger.info(f"Update available: {commits_behind} commits behind")
                    logger.info(f"Local:  {local_commit[:8]}")
                    logger.info(f"Remote: {remote_commit[:8]}")

                    # Perform update
                    success, message = await self._perform_git_update()

                    if success:
                        logger.info("Update successful, restarting...")
                        await self._restart_orchestrator()
                    else:
                        logger.info(f"Update failed: {message}")

            except asyncio.CancelledError:
                break
            except Exception as e:  # noqa: BLE001
                logger.info(f"Git update loop error: {e}")
                await asyncio.sleep(60)  # Wait before retry on error

    # ============================================
    # HTTP API Handlers
    # ============================================

    async def handle_heartbeat(self, request: web.Request) -> web.Response:
        """Handle heartbeat from peer node."""
        try:
            data = await request.json()
            incoming_voters = data.get("voter_node_ids") or data.get("voters") or None
            if incoming_voters:
                voters_list: list[str] = []
                if isinstance(incoming_voters, list):
                    voters_list = [str(v).strip() for v in incoming_voters if str(v).strip()]
                elif isinstance(incoming_voters, str):
                    voters_list = [t.strip() for t in incoming_voters.split(",") if t.strip()]
                if voters_list:
                    self._maybe_adopt_voter_node_ids(voters_list, source="learned")
            peer_info = NodeInfo.from_dict(data)
            # Ignore self-heartbeats so NAT detection + leader election aren't
            # distorted when COORDINATOR_URL includes this node's own endpoint(s).
            if peer_info.node_id == self.node_id:
                self._update_self_info()
                payload = self.self_info.to_dict()
                voter_node_ids = list(getattr(self, "voter_node_ids", []) or [])
                if voter_node_ids:
                    payload["voter_node_ids"] = voter_node_ids
                    payload["voter_quorum_size"] = int(getattr(self, "voter_quorum_size", 0) or 0)
                    payload["voter_config_source"] = str(getattr(self, "voter_config_source", "") or "")
                return web.json_response(payload)
            # Receiving any inbound heartbeat implies we're reachable inbound.
            self.last_inbound_heartbeat = time.time()
            # Preserve the node's self-reported endpoint for multi-path retries.
            if not peer_info.reported_host:
                peer_info.reported_host = peer_info.host
            if not peer_info.reported_port:
                peer_info.reported_port = peer_info.port
            peer_info.last_heartbeat = time.time()
            # Prefer the remote socket address over self-reported host so that
            # nodes behind overlays (e.g., Tailscale) use a reachable address.
            forwarded_for = (
                request.headers.get("X-Forwarded-For")
                or request.headers.get("X-Real-IP")
                or request.headers.get("CF-Connecting-IP")
            )
            if forwarded_for:
                peer_info.host = forwarded_for.split(",")[0].strip()
            elif request.remote:
                peer_info.host = request.remote

            # Preserve local reachability diagnostics: a peer can be "alive" (it can
            # send us heartbeats) while still being unreachable for inbound HTTP
            # (e.g. NAT/firewall). Our outbound heartbeat failures track that.
            # Use AsyncLockWrapper to avoid blocking event loop under high load
            async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                existing = self.peers.get(peer_info.node_id)
                # Dec 2025: Track first-contact for HOST_ONLINE emission
                is_first_contact = existing is None
                if existing:
                    # Dec 29, 2025: Merge alternate IPs for peer deduplication
                    all_ips = set(existing.alternate_ips) if existing.alternate_ips else set()
                    if existing.host:
                        all_ips.add(existing.host)
                    if peer_info.host and peer_info.host != existing.host:
                        all_ips.add(peer_info.host)
                    all_ips.discard("")
                    # Remove primary host from alternates
                    all_ips.discard(peer_info.host)
                    peer_info.alternate_ips = all_ips
                    peer_info.consecutive_failures = int(getattr(existing, "consecutive_failures", 0) or 0)
                    peer_info.last_failure_time = float(getattr(existing, "last_failure_time", 0.0) or 0.0)
                    # Sticky NAT/relay routing with recovery:
                    # - Receiving a direct heartbeat does NOT imply the peer is reachable inbound.
                    # - If a peer has ever registered via /relay/heartbeat, preserve nat_blocked
                    #   and relay_via so leaders can continue routing commands through the relay hub.
                    # - BUT: allow recovery after NAT_BLOCKED_RECOVERY_TIMEOUT if peer becomes reachable
                    existing_nat_blocked = getattr(existing, "nat_blocked", False)
                    existing_nat_blocked_since = float(getattr(existing, "nat_blocked_since", 0.0) or 0.0)
                    existing_last_nat_probe = float(getattr(existing, "last_nat_probe", 0.0) or 0.0)

                    if existing_nat_blocked and not getattr(peer_info, "nat_blocked", False):
                        # Peer was NAT-blocked, incoming says not blocked - preserve unless recovery triggered
                        peer_info.nat_blocked = True
                        peer_info.nat_blocked_since = existing_nat_blocked_since or time.time()
                        peer_info.last_nat_probe = existing_last_nat_probe
                    elif peer_info.nat_blocked and not existing_nat_blocked:
                        # Peer newly marked as NAT-blocked - record timestamp
                        peer_info.nat_blocked_since = time.time()
                    elif existing_nat_blocked and peer_info.nat_blocked:
                        # Both agree NAT-blocked - preserve original timestamp
                        peer_info.nat_blocked_since = existing_nat_blocked_since or time.time()
                        peer_info.last_nat_probe = existing_last_nat_probe

                    if (getattr(existing, "relay_via", "") or "") and not (getattr(peer_info, "relay_via", "") or ""):
                        peer_info.relay_via = str(getattr(existing, "relay_via", "") or "")
                    # Preserve retirement state across updates.
                    if getattr(existing, "retired", False):
                        peer_info.retired = True
                        peer_info.retired_at = float(getattr(existing, "retired_at", 0.0) or 0.0)

                # STABILITY FIX: Correct stale leader role claims
                # If a peer claims to be leader but we have an elected leader that's different,
                # downgrade their role to follower to prevent split-brain confusion.
                # This prevents stale leader info from propagating through gossip.
                if peer_info.role == NodeRole.LEADER and peer_info.node_id != self.node_id:
                    actual_leader = self.leader_id
                    if actual_leader and actual_leader != peer_info.node_id:
                        # Peer claims leader but we have a different elected leader
                        peer_info.role = NodeRole.FOLLOWER

                # Dec 2025: Leader discovery from peer heartbeats
                # If we don't have a leader but peer reports one, consider adopting it
                # Jan 3, 2026: CRITICAL FIX - Handle case where peer reports US as leader
                peer_leader = getattr(peer_info, "leader_id", "") or ""
                if peer_leader and not self.leader_id:
                    # CRITICAL FIX (Jan 3, 2026): Check if peer reports US as leader
                    # This fixes the leader self-recognition desync bug where leader_id
                    # gets set to self.node_id but role stays FOLLOWER.
                    if peer_leader == self.node_id:
                        # Peer thinks WE are leader - verify with lease before accepting
                        if self._is_leader_lease_valid():
                            self._set_leader(self.node_id, reason=f"gossip_self_leader_discovery_via_{peer_info.node_id}")
                            logger.info(f"Discovered we are leader from heartbeat via {peer_info.node_id}")
                        else:
                            logger.debug(
                                f"Peer {peer_info.node_id} reports us as leader but lease invalid; "
                                f"lease_expires={self.leader_lease_expires}, now={time.time()}"
                            )
                    else:
                        # Peer reports someone else as leader - adopt if valid
                        potential_leader = self.peers.get(peer_leader)
                        if potential_leader and potential_leader.is_alive() and potential_leader.role == NodeRole.LEADER:
                            self._set_leader(peer_leader, reason=f"gossip_adopt_from_{peer_info.node_id}")
                            logger.info(f"Adopted leader {peer_leader} from heartbeat via {peer_info.node_id}")

                # Dec 30, 2025: Auto-populate capabilities from hardware if empty
                # This handles nodes that have coordinator flag set incorrectly
                if not getattr(peer_info, "capabilities", None):
                    has_gpu = getattr(peer_info, "has_gpu", False)
                    memory_gb = int(getattr(peer_info, "memory_gb", 0) or 0)
                    if has_gpu or memory_gb > 0:
                        inferred_caps = self._infer_capabilities_from_hardware(
                            has_gpu=has_gpu,
                            memory_gb=memory_gb,
                            gpu_name=getattr(peer_info, "gpu_name", "") or "",
                        )
                        peer_info.capabilities = inferred_caps
                        logger.info(
                            f"[P2P] Inferred capabilities for {peer_info.node_id}: {inferred_caps} "
                            f"(has_gpu={has_gpu}, memory_gb={memory_gb})"
                        )

                self.peers[peer_info.node_id] = peer_info

            # Dec 2025: Emit HOST_ONLINE for first-contact peers
            # This enables SelfplayScheduler, SyncRouter, and DataPipelineOrchestrator
            # to detect newly available nodes for work distribution
            # Dec 30, 2025: Use capabilities from heartbeat instead of computing minimal list
            # This preserves the full capability set (selfplay, training, cmaes, large_boards)
            if is_first_contact:
                # Prefer capabilities from peer_info (advertised by the peer itself)
                capabilities = getattr(peer_info, "capabilities", None) or []
                if not capabilities:
                    # Fallback: compute minimal caps if peer didn't advertise any
                    if getattr(peer_info, "has_gpu", False):
                        gpu_type = getattr(peer_info, "gpu_type", "") or "gpu"
                        capabilities = [gpu_type]
                    else:
                        capabilities = ["cpu"]
                await self._emit_host_online(peer_info.node_id, capabilities)
                logger.info(f"First-contact peer registered: {peer_info.node_id} (caps: {capabilities})")

            # Jan 12, 2026: Sync to lock-free snapshot after peer update
            self._sync_peer_snapshot()

            # Return our info
            self._update_self_info()
            payload = self.self_info.to_dict()
            voter_node_ids = list(getattr(self, "voter_node_ids", []) or [])
            if voter_node_ids:
                payload["voter_node_ids"] = voter_node_ids
                payload["voter_quorum_size"] = int(getattr(self, "voter_quorum_size", 0) or 0)
                payload["voter_config_source"] = str(getattr(self, "voter_config_source", "") or "")
            return web.json_response(payload)
        except json.JSONDecodeError as e:
            logger.warning(f"[heartbeat] JSON parse error from {request.remote}: {e}")
            return web.json_response({"error": "invalid_json", "detail": str(e)}, status=400)
        except KeyError as e:
            logger.warning(f"[heartbeat] Missing required field from {request.remote}: {e}")
            return web.json_response({"error": "missing_field", "field": str(e)}, status=400)
        except ValueError as e:
            logger.warning(f"[heartbeat] Validation error from {request.remote}: {e}")
            return web.json_response({"error": "validation_error", "detail": str(e)}, status=400)
        except Exception as e:  # noqa: BLE001
            logger.error(f"[heartbeat] Unexpected error from {request.remote}: {type(e).__name__}: {e}")
            return web.json_response({"error": "internal_error", "type": type(e).__name__}, status=500)

    @with_request_timeout(30.0)
    async def handle_status(self, request: web.Request) -> web.Response:
        """Return cluster status.

        Query parameters:
            alive_only: If "true" (default), only show alive peers. Set to "false" to include dead/stale peers.
            include_stale_jobs: If "false" (default), dead peers show 0 jobs. Set to "true" to show stale job counts.

        December 30, 2025: Made non-blocking with timeout-based lock acquisition.
        If locks can't be acquired within 2 seconds, returns partial status with
        "unavailable" markers instead of blocking indefinitely.

        Jan 12, 2026: Changed to non-blocking self_info update - schedules background
        refresh and returns immediately with cached data. This prevents 15s+ timeouts
        on macOS where resource detection is slow.

        Jan 16, 2026: Added @with_request_timeout(30.0) decorator to prevent overall
        handler timeout. Individual metric timeouts are 2s, but other operations
        (voter health, partition status, etc.) can hang without protection.
        """
        # Jan 12, 2026: Non-blocking mode - schedule background refresh, use cached data
        try:
            asyncio.create_task(self._update_self_info_async())
        except Exception:
            pass  # Fire-and-forget, don't block on errors

        # Parse query parameters for filtering
        alive_only = request.query.get("alive_only", "true").lower() != "false"
        include_stale_jobs = request.query.get("include_stale_jobs", "false").lower() == "true"

        # Jan 12, 2026: Lock-free peer snapshot using copy-on-write pattern
        # PeerSnapshot.get_snapshot() returns instantly without acquiring any lock.
        # The snapshot is updated atomically whenever peers are modified.
        # This eliminates the 6+ second timeouts that occurred under load.
        snapshot_dict = self._peer_snapshot.get_snapshot()
        peers_snapshot: list = list(snapshot_dict.values())

        conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])
        effective_leader = self._get_leader_peer()

        now = time.time()
        peers: dict[str, Any] = {}
        for node_id, info in ((p.node_id, p) for p in peers_snapshot):
            is_alive = info.is_alive()

            # Skip dead peers if alive_only is set
            if alive_only and not is_alive:
                continue

            d = info.to_dict()
            d["endpoint_conflict"] = self._endpoint_key(info) in conflict_keys
            d["leader_eligible"] = self._is_leader_eligible(info, conflict_keys, require_alive=False)

            # Add explicit alive status and staleness info
            d["is_alive"] = is_alive
            last_hb = float(getattr(info, "last_heartbeat", 0.0) or 0.0)
            d["seconds_since_heartbeat"] = int(now - last_hb) if last_hb > 0 else -1

            # Zero out job counts for dead peers unless explicitly requested
            if not is_alive and not include_stale_jobs:
                d["selfplay_jobs"] = 0
                d["training_jobs"] = 0
                d["active_job_count"] = 0

            peers[node_id] = d

        # Jan 5, 2026 (Session 17.28): Build all_peers dict with ALL peers regardless of alive status
        # This is required for remote job dispatch which needs to know about all configured nodes
        all_peers: dict[str, Any] = {}
        for peer in peers_snapshot:
            all_peers[peer.node_id] = {
                "node_id": peer.node_id,
                "host": getattr(peer, "host", None),
                "port": getattr(peer, "port", 8770),
                "role": peer.role.value if hasattr(peer.role, "value") else str(peer.role),
                "capabilities": getattr(peer, "capabilities", []),
                "load_score": getattr(peer, "load_score", 0.0),
                "status": "alive" if peer.is_alive() else "dead",
                "is_alive": peer.is_alive(),
                "last_heartbeat": float(getattr(peer, "last_heartbeat", 0.0) or 0.0),
            }

        # Convenience diagnostics: reported leaders vs eligible leaders.
        leaders_reported = sorted(
            [p.node_id for p in peers_snapshot if p.role == NodeRole.LEADER and p.is_alive()]
        )
        leaders_eligible = sorted(
            [
                p.node_id
                for p in peers_snapshot
                if p.role == NodeRole.LEADER and self._is_leader_eligible(p, conflict_keys)
            ]
        )

        # Jan 12, 2026: Lock-free job snapshot access
        # Uses JobSnapshot copy-on-write pattern - no lock needed for reads.
        # Previous lock-based code removed (was causing 6+ second timeouts).
        jobs = self._job_snapshot.get_snapshot()

        # Get improvement cycle manager status
        improvement_status = None
        if self.improvement_cycle_manager:
            try:
                improvement_status = self.improvement_cycle_manager.get_status()
            except Exception as e:  # noqa: BLE001
                improvement_status = {"error": str(e)}

        # Get diversity metrics (delegated to SelfplayScheduler)
        # December 27, 2025: Added try-except to prevent 500 errors on memory-constrained nodes
        try:
            diversity_metrics = self.selfplay_scheduler.get_diversity_metrics()
        except Exception as e:  # noqa: BLE001
            diversity_metrics = {"error": str(e)}

        voter_ids = list(getattr(self, "voter_node_ids", []) or [])
        # Jan 2, 2026: Use _count_alive_voters() to check IP:port matches
        voters_alive = self._count_alive_voters()

        # Get P2P sync metrics (with error handling for new features)
        # December 27, 2025: Wrapped all metric calls to prevent cascading 500 errors
        # December 31, 2025: Added asyncio.wait_for() timeouts to prevent /status hangs
        # January 12, 2026: CRITICAL FIX - Run all metric calls in PARALLEL instead of sequential
        # Previous sequential approach took up to 24 seconds (10 calls  2s timeout each).
        # Parallel approach takes at most 2 seconds (max of all timeouts).
        p2p_sync_metrics = getattr(self, "_p2p_sync_metrics", {})
        _STATUS_TIMEOUT = 5.0  # seconds for each blocking call (Jan 16, 2026: increased from 2.0)

        # Define all metric gathering tasks
        async def _safe_metric(name: str, func: callable) -> tuple[str, dict]:
            """Wrapper to safely gather a metric with timeout and error handling."""
            try:
                result = await asyncio.wait_for(
                    asyncio.to_thread(func),
                    timeout=_STATUS_TIMEOUT,
                )
                return name, result
            except asyncio.TimeoutError:
                logger.warning(f"handle_status: {name} timed out")
                return name, {"error": "timeout"}
            except Exception as e:  # noqa: BLE001
                return name, {"error": str(e)}

        # Jan 19, 2026: Run ALL metric gathering calls in PARALLEL
        # Previously some metrics (swim_raft, partition, background_loops, voter_health)
        # were awaited sequentially after this gather, adding up to 20s latency.
        # Now everything runs in parallel for <5s total latency.
        def _get_loop_manager_status():
            loop_manager = self._get_loop_manager()
            if loop_manager is not None:
                return loop_manager.get_all_status()
            return {"error": "LoopManager not initialized"}

        metric_tasks = [
            _safe_metric("gossip_metrics", self._get_gossip_metrics_summary),
            _safe_metric("distributed_training", self._get_distributed_training_summary),
            _safe_metric("cluster_elo", self._get_cluster_elo_summary),
            _safe_metric("node_recovery", self._get_node_recovery_metrics),
            _safe_metric("leader_consensus", self._get_cluster_leader_consensus),
            _safe_metric("peer_reputation", self._get_cluster_peer_reputation),
            _safe_metric("sync_intervals", self._get_sync_interval_summary),
            _safe_metric("tournament_scheduling", self._get_distributed_tournament_summary),
            _safe_metric("data_dedup", self._get_dedup_summary),
            # Jan 19, 2026: Added these to parallel gather (were sequential before)
            _safe_metric("swim_raft", self._get_swim_raft_status),
            _safe_metric("partition", self.get_partition_status),
            _safe_metric("background_loops", _get_loop_manager_status),
            _safe_metric("voter_health", self._check_voter_health),
        ]

        # Gather all results in parallel
        metric_results = await asyncio.gather(*metric_tasks, return_exceptions=True)

        # Extract results into named variables
        metrics_dict = {}
        for result in metric_results:
            if isinstance(result, tuple) and len(result) == 2:
                name, value = result
                metrics_dict[name] = value
            elif isinstance(result, Exception):
                logger.warning(f"handle_status: metric task failed with {result}")

        gossip_metrics = metrics_dict.get("gossip_metrics", {"error": "not_collected"})
        distributed_training = metrics_dict.get("distributed_training", {"error": "not_collected"})
        cluster_elo = metrics_dict.get("cluster_elo", {"error": "not_collected"})
        node_recovery = metrics_dict.get("node_recovery", {"error": "not_collected"})
        leader_consensus = metrics_dict.get("leader_consensus", {"error": "not_collected"})
        peer_reputation = metrics_dict.get("peer_reputation", {"error": "not_collected"})
        sync_intervals = metrics_dict.get("sync_intervals", {"error": "not_collected"})
        tournament_scheduling = metrics_dict.get("tournament_scheduling", {"error": "not_collected"})
        data_dedup = metrics_dict.get("data_dedup", {"error": "not_collected"})
        # Jan 19, 2026: These were previously sequential - now parallel
        swim_raft_status = metrics_dict.get("swim_raft", {"error": "not_collected"})
        partition_status = metrics_dict.get("partition", {"error": "not_collected"})
        background_loops = metrics_dict.get("background_loops", {"error": "not_collected"})
        voter_health = metrics_dict.get("voter_health", {"error": "not_collected"})

        # Jan 3, 2026: Transport latency stats for diagnosing slow transports
        transport_latency: dict = {}
        try:
            from scripts.p2p.transport_cascade import get_transport_cascade
            cascade = get_transport_cascade()
            transport_latency = cascade.get_transport_latency_summary()
        except ImportError:
            transport_latency = {"available": False, "reason": "import_error"}
        except Exception as e:  # noqa: BLE001
            transport_latency = {"available": False, "error": str(e)}

        # Dec 2025: Get event subscription status for health monitoring
        event_subscriptions = getattr(self, "_event_subscription_status", {
            "daemon_events": False,
            "feedback_signals": False,
            "manager_events": False,
            "all_healthy": False,
            "timestamp": 0,
        })

        # Jan 19, 2026: partition_status and background_loops now computed in parallel gather above

        # Jan 1, 2026: Work queue status for monitoring (Phase 4B fix)
        work_queue_size = 0
        active_jobs_count = 0
        selfplay_jobs_count = 0
        try:
            from app.coordination.work_queue import get_work_queue
            wq = get_work_queue()
            if wq is not None and hasattr(wq, 'get_queue_status'):
                wq_status = wq.get_queue_status()
                work_queue_size = wq_status.get('total_items', 0)
        except Exception:  # noqa: BLE001
            pass  # Fall back to 0

        # Count jobs directly from local_jobs
        if isinstance(jobs, dict) and "error" not in jobs:
            for job_data in jobs.values():
                if isinstance(job_data, dict):
                    status = job_data.get("status", "")
                    job_type = job_data.get("job_type", "")
                    if status in ("running", "claimed"):
                        active_jobs_count += 1
                    if job_type == "selfplay" and status in ("running", "claimed"):
                        selfplay_jobs_count += 1

        # Jan 1, 2026: Aggregate cluster-wide selfplay jobs from peers
        cluster_selfplay_jobs = selfplay_jobs_count  # Start with local count
        cluster_training_jobs = 0
        for peer_node_id, peer_data in peers.items():
            if isinstance(peer_data, dict):
                cluster_selfplay_jobs += int(peer_data.get("selfplay_jobs", 0) or 0)
                cluster_training_jobs += int(peer_data.get("training_jobs", 0) or 0)

        # Jan 19, 2026: voter_health now computed in parallel gather above

        return web.json_response({
            "node_id": self.node_id,
            "role": self.role.value,
            "leader_id": self.leader_id,
            "effective_leader_id": (effective_leader.node_id if effective_leader else None),
            # Jan 1, 2026: Provisional leadership status
            "is_provisional_leader": self.role == NodeRole.PROVISIONAL_LEADER,
            "provisional_claimed_at": getattr(self, "_provisional_leader_claimed_at", 0.0) or 0.0,
            "provisional_acks": len(getattr(self, "_provisional_leader_acks", set()) or set()),
            "provisional_challengers": len(getattr(self, "_provisional_leader_challengers", {}) or {}),
            "fallback_leader_since": getattr(self, "_fallback_leader_since", 0.0) or 0.0,
            "fallback_leader_reason": getattr(self, "_fallback_leader_reason", "") or "",
            "leaders_reported": leaders_reported,
            "leaders_eligible": leaders_eligible,
            "voter_node_ids": voter_ids,
            "voter_quorum_size": int(getattr(self, "voter_quorum_size", 0) or 0),
            "voters_alive": voters_alive,
            "voter_quorum_ok": self._has_voter_quorum(),
            # Jan 20, 2026: Voter config sync - version and hash for drift detection
            "voter_config_version": self._get_voter_config_version(),
            "voter_config_hash": self._get_voter_config_hash(),
            # Jan 2, 2026: Detailed voter health for monitoring
            # Jan 16, 2026: Now pre-computed with timeout protection
            "voter_health": voter_health,
            "self": self.self_info.to_dict(),
            "peers": peers,
            "all_peers": all_peers,  # Jan 5, 2026: All peers regardless of alive status for job dispatch
            "local_jobs": jobs,
            "alive_peers": len([p for p in self.peers.values() if p.is_alive()]),
            "improvement_cycle_manager": improvement_status,
            "diversity_metrics": diversity_metrics,
            "gossip_metrics": gossip_metrics,
            "p2p_sync_metrics": p2p_sync_metrics,
            "distributed_training": distributed_training,
            "cluster_elo": cluster_elo,
            "node_recovery": node_recovery,
            "leader_consensus": leader_consensus,
            "peer_reputation": peer_reputation,
            "sync_intervals": sync_intervals,
            "tournament_scheduling": tournament_scheduling,
            "data_dedup": data_dedup,
            "swim_raft": swim_raft_status,
            "transport_latency": transport_latency,  # Jan 3, 2026: Per-transport latency metrics
            "event_subscriptions": event_subscriptions,
            "partition": partition_status,
            "background_loops": background_loops,
            # December 30, 2025: Cluster observability for debugging idle nodes
            "cluster_observability": self._get_cluster_observability(),
            # Session 17.41 (Jan 6, 2026): Fallback mechanism status for partition debugging
            "fallback_status": self._get_fallback_status(),
            # December 30, 2025: Lock acquisition status for debugging
            "_lock_status": {
                "peers_lock_acquired": peers_snapshot is not None,
                "jobs_lock_acquired": "error" not in jobs,
            },
            # Jan 1, 2026: Explicit work queue and job counts (Phase 4B fix)
            "work_queue_size": work_queue_size,
            "active_jobs": active_jobs_count,
            "selfplay_jobs": cluster_selfplay_jobs,  # Cluster-wide aggregated
            "training_jobs": cluster_training_jobs,  # Cluster-wide aggregated
            "local_selfplay_jobs": selfplay_jobs_count,  # This node only
            # Jan 2, 2026: Dual-stack IPv4/IPv6 network info
            "network": {
                "advertise_host": self.advertise_host,
                "advertise_host_family": "ipv6" if ":" in (self.advertise_host or "") else "ipv4",
                "alternate_ips": list(getattr(self, "alternate_ips", set()) or set()),
                "alternate_ipv4_count": sum(1 for ip in getattr(self, "alternate_ips", set()) or set() if ":" not in ip),
                "alternate_ipv6_count": sum(1 for ip in getattr(self, "alternate_ips", set()) or set() if ":" in ip),
            },
            # Jan 3, 2026: Leadership consistency metrics for monitoring desync issues
            # This enables detection of the leader self-recognition bug where leader_id
            # is set correctly but role doesn't match.
            "leadership_consistency": self._get_leadership_consistency_metrics(),
            "is_leader": self._is_leader(),  # Explicit field for quick checks
            # Jan 13, 2026: Config version for drift detection (P2P Cluster Stability Plan Phase 1)
            "config_version": self._get_config_version(),
            # Jan 13, 2026: Unified data summary across all sources (LOCAL, CLUSTER, S3, OWC)
            "data_summary": self._get_data_summary_cached(),
            # Jan 20, 2026: Adaptive dead peer cooldown stats
            "cooldown_stats": self._get_cooldown_stats(),
        })

    @with_request_timeout(10.0)
    async def handle_progress(self, request: web.Request) -> web.Response:
        """GET /progress - Return Elo progress report for demonstrating iterative improvement.

        January 16, 2026: Added to provide visibility into NN strength improvement.

        Query parameters:
            config: Optional config filter (e.g., "hex8_2p")
            days: Lookback period in days (default: 30)

        Returns JSON with:
            - configs: Per-config progress data (starting_elo, current_elo, delta, iterations)
            - overall: Summary stats (total_iterations, configs_improving, avg_elo_gain)
            - generated_at: Timestamp
        """
        config_filter = request.query.get("config")
        try:
            days = float(request.query.get("days", "30"))
        except ValueError:
            days = 30.0

        try:
            # Import progress report logic
            import sys
            sys.path.insert(0, str(Path(__file__).parent.parent))
            from scripts.elo_progress_report import get_full_report
            from dataclasses import asdict

            report = get_full_report(days=days, config_filter=config_filter)

            # Convert to JSON-serializable dict
            data = {
                "configs": {k: asdict(v) for k, v in report.configs.items()},
                "overall": asdict(report.overall),
                "generated_at": report.generated_at,
            }

            return web.json_response(data)

        except ImportError as e:
            logger.warning(f"[handle_progress] Import error: {e}")
            return web.json_response({
                "error": "progress_report_unavailable",
                "detail": str(e),
            }, status=500)
        except Exception as e:  # noqa: BLE001
            logger.error(f"[handle_progress] Error generating progress report: {e}")
            return web.json_response({
                "error": "internal_error",
                "detail": str(e),
            }, status=500)

    # -------------------------------------------------------------------------
    # Peer Health Handlers - EXTRACTED to scripts/p2p/handlers/status.py
    # January 2026 - P2P Modularization Phase 6a
    # Provides: handle_peer_health, handle_external_work
    # See StatusHandlersMixin
    # -------------------------------------------------------------------------

    # Work Queue Handlers moved to scripts/p2p/handlers/work_queue.py
    # Inherited from WorkQueueHandlersMixin: handle_work_*, handle_populator_status

    # Election Handlers moved to scripts/p2p/handlers/election.py
    # Inherited from ElectionHandlersMixin: handle_election, handle_lease_request,
    # handle_voter_grant_status, handle_election_reset, handle_election_force_leader

    # ============================================================
    # SERF INTEGRATION - Battle-tested membership/failure detection
    # ============================================================

    async def handle_serf_event(self, request: web.Request) -> web.Response:
        """POST /serf/event - Receive events from Serf event handler.

        SERF INTEGRATION: HashiCorp Serf provides battle-tested SWIM gossip
        for membership and failure detection. This endpoint receives events
        from the serf_event_handler.py script.

        Event types:
        - member-join: New node joined the cluster
        - member-leave: Node gracefully left
        - member-failed: Node failed (detected by SWIM)
        - member-update: Node tags changed
        - member-reap: Failed node was reaped from membership list
        - user: Custom user event (training-complete, model-promoted, etc.)

        Request body:
        {
            "event_type": "member-join",
            "timestamp": "2025-12-26T...",
            "payload": { event-specific data }
        }
        """
        try:
            if self.auth_token and not self._is_request_authorized(request):
                return web.json_response({"error": "unauthorized"}, status=401)

            data = await request.json()
            event_type = data.get("event_type", "")
            timestamp = data.get("timestamp", "")
            payload = data.get("payload", {})

            logger.info(f"Serf event received: {event_type} at {timestamp}")

            # Process based on event type
            if event_type == "member-join":
                await self._handle_serf_member_join(payload.get("members", []))
            elif event_type == "member-leave":
                await self._handle_serf_member_leave(payload.get("members", []))
            elif event_type == "member-failed":
                await self._handle_serf_member_failed(payload.get("members", []))
            elif event_type == "member-update":
                await self._handle_serf_member_update(payload.get("members", []))
            elif event_type == "member-reap":
                await self._handle_serf_member_reap(payload.get("members", []))
            elif event_type == "user":
                await self._handle_serf_user_event(payload)
            else:
                logger.warning(f"Unknown Serf event type: {event_type}")

            return web.json_response({
                "status": "processed",
                "event_type": event_type,
                "node_id": self.node_id,
            })

        except Exception as e:  # noqa: BLE001
            logger.error(f"Error processing Serf event: {e}")
            return web.json_response({"error": str(e)}, status=500)

    async def _handle_serf_member_join(self, members: list) -> None:
        """Handle Serf member-join events.

        When Serf detects new members, update our peer list and mark them alive.
        This is more reliable than our custom gossip because Serf uses SWIM
        with indirect probing.
        """
        for member in members:
            node_name = member.get("name", "")
            addr = member.get("addr", "")
            tags = member.get("tags", {})

            if not node_name or node_name == self.node_id:
                continue

            logger.info(f"Serf: member joined: {node_name} @ {addr}")

            # Update peer state
            now = time.time()
            if node_name not in self.peers:
                # Parse addr to get host:port (format: "ip:port")
                host, port = (addr.rsplit(":", 1) + ["8770"])[:2] if ":" in addr else (addr, "8770")
                try:
                    port_int = int(port)
                except ValueError:
                    port_int = 8770
                self.peers[node_name] = NodeInfo(
                    node_id=node_name,
                    host=host or "unknown",
                    port=port_int,
                    last_heartbeat=now,
                )
            else:
                peer = self.peers[node_name]
                if isinstance(peer, NodeInfo):
                    peer.last_heartbeat = now
                    if addr:
                        host, _ = (addr.rsplit(":", 1) + [""])[:2] if ":" in addr else (addr, "")
                        if host:
                            peer.host = host

            # Extract tags into peer info (store as capability hints)
            # Note: Serf tags are for reference only, NodeInfo uses capabilities list

        # C2 fix: Sync peer snapshot after Serf member join updates
        if members:
            self._sync_peer_snapshot()

    async def _handle_serf_member_leave(self, members: list) -> None:
        """Handle Serf member-leave events (graceful departure)."""
        for member in members:
            node_name = member.get("name", "")

            if not node_name or node_name == self.node_id:
                continue

            logger.info(f"Serf: member left gracefully: {node_name}")

            if node_name in self.peers:
                peer = self.peers[node_name]
                if isinstance(peer, NodeInfo):
                    # Mark as retired (NodeInfo equivalent of "left")
                    peer.retired = True
                    peer.retired_at = time.time()
                    # Jan 20, 2026: Use adaptive cooldown manager
                    if self._cooldown_manager:
                        self._cooldown_manager.record_death(node_name)
                    else:
                        self._dead_peer_timestamps[node_name] = time.time()

        # C2 fix: Sync peer snapshot after Serf member leave updates
        if members:
            self._sync_peer_snapshot()

    async def _handle_serf_member_failed(self, members: list) -> None:
        """Handle Serf member-failed events (SWIM failure detection).

        SWIM's failure detection is more reliable than our custom ping/pong
        because it uses indirect probing through multiple peers.
        """
        for member in members:
            node_name = member.get("name", "")
            addr = member.get("addr", "")

            if not node_name or node_name == self.node_id:
                continue

            logger.warning(f"Serf: member FAILED (SWIM detected): {node_name} @ {addr}")

            if node_name in self.peers:
                peer = self.peers[node_name]
                if isinstance(peer, NodeInfo):
                    # Mark with consecutive failures (triggers dead/suspect state)
                    peer.consecutive_failures += 1
                    peer.last_failure_time = time.time()

            # If the failed node was leader, trigger election
            if node_name == self.leader_id:
                logger.warning(f"Leader {node_name} failed (Serf detected) - triggering election")
                # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                self._set_leader(None, reason="serf_leader_failed", save_state=True)
                self.election_in_progress = False  # Allow new election

        # C2 fix: Sync peer snapshot after Serf member failed updates
        if members:
            self._sync_peer_snapshot()

    async def _handle_serf_member_update(self, members: list) -> None:
        """Handle Serf member-update events (tag changes)."""
        for member in members:
            node_name = member.get("name", "")
            tags = member.get("tags", {})

            if not node_name or node_name == self.node_id:
                continue

            logger.info(f"Serf: member updated: {node_name}")

            if node_name in self.peers:
                peer = self.peers[node_name]
                if isinstance(peer, NodeInfo):
                    peer.last_heartbeat = time.time()
                    # Tags can update capabilities if structured appropriately
                    if "capabilities" in tags and isinstance(tags["capabilities"], list):
                        peer.capabilities = tags["capabilities"]

    async def _handle_serf_member_reap(self, members: list) -> None:
        """Handle Serf member-reap events (failed nodes removed from list)."""
        for member in members:
            node_name = member.get("name", "")

            if not node_name or node_name == self.node_id:
                continue

            logger.info(f"Serf: member reaped (final cleanup): {node_name}")

            # Mark as retired (reaped means permanently gone)
            if node_name in self.peers:
                peer = self.peers[node_name]
                if isinstance(peer, NodeInfo):
                    peer.retired = True
                    peer.retired_at = time.time()
                    # Jan 20, 2026: Use adaptive cooldown manager
                    if self._cooldown_manager:
                        self._cooldown_manager.record_death(node_name)
                    else:
                        self._dead_peer_timestamps[node_name] = time.time()

    async def _handle_serf_user_event(self, payload: dict) -> None:
        """Handle Serf user events (custom RingRift events).

        User events include:
        - training-complete: Training job finished
        - model-promoted: Model was promoted to canonical
        - selfplay-started: Selfplay jobs started on a node
        - node-status: Periodic node status broadcast
        """
        event_name = payload.get("name", "")
        event_payload = payload.get("payload", {})
        ltime = payload.get("ltime", "0")

        logger.info(f"Serf user event: {event_name} (ltime={ltime})")

        if event_name == "training-complete":
            config_key = event_payload.get("config_key", "")
            model_path = event_payload.get("model_path", "")
            metrics = event_payload.get("metrics", {})
            logger.info(f"Training complete via Serf: {config_key} -> {model_path}")
            # Could trigger evaluation here

        elif event_name == "model-promoted":
            config_key = event_payload.get("config_key", "")
            model_path = event_payload.get("model_path", "")
            elo_gain = event_payload.get("elo_gain", 0)
            logger.info(f"Model promoted via Serf: {config_key} (+{elo_gain} Elo)")
            # Could trigger model distribution here

        elif event_name == "selfplay-started":
            node = event_payload.get("node", "")
            config_key = event_payload.get("config_key", "")
            job_count = event_payload.get("job_count", 1)
            logger.info(f"Selfplay started via Serf: {node} running {config_key} x{job_count}")

        elif event_name == "node-status":
            # Status updates from nodes - could merge with gossip state
            node_id = event_payload.get("node_id", "")
            if node_id and node_id in self.peers:
                # Update peer with status info
                status_fields = ["gpu_util", "gpu_mem_used", "cpu_percent", "memory_percent"]
                for field in status_fields:
                    if field in event_payload:
                        self.peers[node_id][field] = event_payload[field]

    # ============================================================
    # SWIM Native Integration - swim-p2p membership status
    # ============================================================

    async def handle_swim_status(self, request: web.Request) -> web.Response:
        """GET /swim/status - Return SWIM membership protocol status.

        SWIM provides leaderless gossip-based membership with:
        - O(1) bandwidth per node (constant message complexity)
        - <5 second failure detection (vs 60+ seconds with heartbeats)
        - Suspicion mechanism to reduce false positives
        """
        try:
            if not self._swim_manager:
                return web.json_response({
                    "status": "disabled",
                    "reason": "swim-p2p not installed or SWIM adapter not available",
                    "node_id": self.node_id,
                    "fallback": "http_heartbeats",
                })

            summary = self._swim_manager.get_membership_summary()
            alive_peers = self._swim_manager.get_alive_peers() if self._swim_started else []

            return web.json_response({
                "status": "enabled" if self._swim_started else "initialized",
                "node_id": self.node_id,
                "swim": summary,
                "alive_peers": alive_peers,
                "peer_count": len(alive_peers),
            })

        except Exception as e:  # noqa: BLE001
            logger.error(f"Error getting SWIM status: {e}")
            return web.json_response({
                "status": "error",
                "error": str(e),
                "node_id": self.node_id,
            }, status=500)

    async def _swim_membership_loop(self) -> None:
        """Background task that integrates SWIM membership with P2P peer tracking.

        This task:
        1. Starts the SWIM manager if available
        2. Periodically syncs SWIM membership with our peers dict
        3. Uses SWIM failure detection to mark peers as failed faster
        """
        if not self._swim_manager:
            logger.info("SWIM membership loop: disabled (swim-p2p not available)")
            return

        try:
            # Start SWIM manager
            started = await self._swim_manager.start()
            if not started:
                logger.warning("SWIM membership loop: failed to start SWIM manager")
                return

            self._swim_started = True
            logger.info("SWIM membership loop: started successfully")

            # Sync SWIM membership with our peer tracking every 10 seconds
            while self.running:
                try:
                    alive_peers = self._swim_manager.get_alive_peers()

                    # Update peers from SWIM detection
                    now = time.time()
                    with self.peers_lock:
                        for peer_id in alive_peers:
                            # Jan 7, 2026 Session 17.43: Filter SWIM protocol entries (IP:7947 format)
                            # SWIM peer IDs like "100.126.21.102:7947" should NOT be added to self.peers
                            # They pollute VoterHealth, Elo sync, and other peer iteration points
                            # Proper peers are discovered via HTTP gossip with node names or IP:8770
                            if ":" in peer_id:
                                _, port_str = peer_id.rsplit(":", 1)
                                if port_str == "7947":
                                    # Skip SWIM-format peer IDs - they'll be discovered via HTTP gossip
                                    continue

                            if peer_id not in self.peers:
                                # New peer detected by SWIM - convert to HTTP format
                                # peer_id format is typically "host:port" from SWIM
                                host, port_str = (peer_id.rsplit(":", 1) + ["8770"])[:2] if ":" in peer_id else (peer_id, "8770")
                                try:
                                    port_int = int(port_str)
                                except ValueError:
                                    port_int = 8770  # Use P2P port, not SWIM port
                                # SWIM detection creates a minimal NodeInfo; HTTP handshake fills details
                                self.peers[peer_id] = NodeInfo(
                                    node_id=peer_id,
                                    host=host or "unknown",
                                    port=8770,  # P2P API port (SWIM uses 7947)
                                    last_heartbeat=now,
                                )
                            else:
                                # Update existing peer's heartbeat
                                peer = self.peers[peer_id]
                                if isinstance(peer, NodeInfo):
                                    peer.last_heartbeat = now

                except Exception as e:  # noqa: BLE001
                    logger.warning(f"SWIM sync error: {e}")

                # Dec 30, 2025: Try deferred Raft initialization after peers discovered
                # Raft needs peer addresses which aren't available at startup
                try:
                    from scripts.p2p.constants import RAFT_ENABLED
                    if (
                        RAFT_ENABLED
                        and not getattr(self, "_raft_initialized", False)
                        and hasattr(self, "try_deferred_raft_init")
                    ):
                        self.try_deferred_raft_init()
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Deferred Raft init attempt: {e}")

                await asyncio.sleep(10)  # Sync every 10 seconds

        except asyncio.CancelledError:
            logger.info("SWIM membership loop: cancelled")
            raise
        except Exception as e:  # noqa: BLE001
            logger.error(f"SWIM membership loop error: {e}", exc_info=True)
        finally:
            if self._swim_manager and self._swim_started:
                try:
                    await self._swim_manager.stop()
                except Exception as e:  # noqa: BLE001
                    logger.warning(f"Error stopping SWIM manager: {e}")
                self._swim_started = False

    async def handle_coordinator(self, request: web.Request) -> web.Response:
        """Handle coordinator announcement from new leader.

        LEARNED LESSONS - Only accept leadership from higher-priority nodes (Bully algorithm).
        Also handles lease-based leadership updates.
        """
        try:
            self._update_self_info()
            data = await request.json()
            new_leader_raw = data.get("leader_id")
            if not new_leader_raw:
                return web.json_response(
                    {"accepted": False, "reason": "missing_leader_id"},
                    status=400,
                )
            new_leader = str(new_leader_raw)
            lease_id = data.get("lease_id", "")
            lease_expires = data.get("lease_expires", 0)
            is_renewal = data.get("lease_renewal", False)
            incoming_voters = data.get("voter_node_ids") or data.get("voters") or None
            if incoming_voters:
                voters_list: list[str] = []
                if isinstance(incoming_voters, list):
                    voters_list = [str(v).strip() for v in incoming_voters if str(v).strip()]
                elif isinstance(incoming_voters, str):
                    voters_list = [t.strip() for t in incoming_voters.split(",") if t.strip()]
                if voters_list:
                    self._maybe_adopt_voter_node_ids(voters_list, source="learned")

            voters = list(getattr(self, "voter_node_ids", []) or [])
            if voters and new_leader not in voters:
                return web.json_response(
                    {"accepted": False, "reason": "leader_not_voter", "voters": voters},
                    status=403,
                )

            # Voter-side safety: if we've granted a still-valid lease to a different leader,
            # do not accept a conflicting coordinator announcement. This prevents a voter
            # from "following" a non-quorum leader during transient partitions.
            if voters and self.node_id in voters:
                grant_leader = str(getattr(self, "voter_grant_leader_id", "") or "")
                grant_expires = float(getattr(self, "voter_grant_expires", 0.0) or 0.0)
                if grant_leader and grant_expires > time.time() and grant_leader != new_leader:
                    return web.json_response(
                        {
                            "accepted": False,
                            "reason": "voter_lease_conflict",
                            "granted_to": grant_leader,
                            "granted_until": grant_expires,
                        },
                        status=409,
                    )

            # If quorum gating is not configured, fall back to bully ordering
            # (lexicographically highest node_id wins).
            if not voters and self.role == NodeRole.LEADER and new_leader < self.node_id:
                # Exception: accept if our lease has expired
                if self.leader_lease_expires > 0 and time.time() >= self.leader_lease_expires:
                    logger.info(f"Our lease expired, accepting leader: {new_leader}")
                else:
                    logger.info(f"Rejecting leader announcement from lower-priority node: {new_leader} < {self.node_id}")
                    return web.json_response({"accepted": False, "reason": "lower_priority"})

            # Reject leadership from nodes that are not directly reachable / uniquely addressable.
            if new_leader != self.node_id:
                with self.peers_lock:
                    peer = self.peers.get(new_leader)
                    peers_snapshot = [p for p in self.peers.values() if p.node_id != self.node_id]
                if peer:
                    conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])
                    if not self._is_leader_eligible(peer, conflict_keys, require_alive=False):
                        return web.json_response({"accepted": False, "reason": "leader_ineligible"})

            if is_renewal and new_leader == self.leader_id:
                self.leader_lease_expires = lease_expires
                self.leader_lease_id = lease_id
                return web.json_response({"accepted": True})

            logger.info(f"Accepting leader announcement: {new_leader}")
            # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
            self._set_leader(new_leader, reason="accept_coordinator_announcement", save_state=True)
            self.leader_lease_id = lease_id
            self.leader_lease_expires = lease_expires if lease_expires else time.time() + LEADER_LEASE_DURATION

            return web.json_response({"accepted": True})
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=400)

    # -------------------------------------------------------------------------
    # Job Management Handlers - EXTRACTED to scripts/p2p/handlers/jobs_api.py
    # January 2026 - P2P Modularization Phase 7a
    # Provides: handle_start_job, handle_stop_job, handle_job_kill,
    #           handle_cleanup, handle_restart_stuck_jobs, handle_cleanup_files
    # See JobsApiHandlersMixin
    # -------------------------------------------------------------------------

    # NOTE: Peer admin handlers (handle_purge_retired_peers, handle_purge_stale_peers,
    # handle_admin_unretire, handle_admin_reset_node_jobs) moved to AdminHandlersMixin
    # in scripts/p2p/handlers/admin.py (Dec 28, 2025)

    async def handle_process_kill(self, request: web.Request) -> web.Response:
        """Kill processes matching a pattern on this node.

        Jan 21, 2026: Added to fix zombie process accumulation.
        This endpoint enables remote cleanup of stuck/zombie processes.

        Request JSON:
            pattern: str - Process pattern to match (e.g., "selfplay", "gpu_selfplay")
            signal: str - Signal to send (default: "SIGKILL")

        Returns:
            JSON with killed count and details
        """
        import shutil

        try:
            data = await request.json()
            pattern = data.get("pattern", "")
            signal_name = data.get("signal", "SIGKILL").upper()

            if not pattern:
                return web.json_response(
                    {"error": "missing pattern", "killed": 0},
                    status=400,
                )

            # Validate signal
            signal_map = {
                "SIGTERM": "-15",
                "SIGKILL": "-9",
                "SIGHUP": "-1",
            }
            signal_flag = signal_map.get(signal_name, "-9")

            # Safety: only allow certain patterns to prevent accidents
            allowed_patterns = [
                "selfplay", "gpu_selfplay", "gumbel", "train", "gauntlet",
                "tournament", "export", "run_self_play", "run_gpu_selfplay",
                "run_hybrid_selfplay", "policy_only", "nnue",
            ]
            if not any(allowed in pattern.lower() for allowed in allowed_patterns):
                return web.json_response(
                    {"error": f"pattern not allowed: {pattern}", "killed": 0},
                    status=403,
                )

            if not shutil.which("pgrep") or not shutil.which("pkill"):
                return web.json_response(
                    {"error": "pgrep/pkill not available", "killed": 0},
                    status=500,
                )

            # Count matching processes first
            try:
                pgrep_result = await asyncio.to_thread(
                    subprocess.run,
                    ["pgrep", "-f", pattern],
                    capture_output=True,
                    text=True,
                    timeout=10,
                )
                if pgrep_result.returncode == 0 and pgrep_result.stdout.strip():
                    pids = [p.strip() for p in pgrep_result.stdout.strip().split() if p.strip()]
                    pid_count = len(pids)
                else:
                    pid_count = 0
                    pids = []
            except subprocess.TimeoutExpired:
                return web.json_response(
                    {"error": "pgrep timeout", "killed": 0},
                    status=500,
                )

            if pid_count == 0:
                return web.json_response({
                    "killed": 0,
                    "pattern": pattern,
                    "message": "no matching processes",
                })

            # Kill matching processes
            try:
                pkill_result = await asyncio.to_thread(
                    subprocess.run,
                    ["pkill", signal_flag, "-f", pattern],
                    capture_output=True,
                    text=True,
                    timeout=15,
                )
                # pkill returns 0 if processes were killed, 1 if none matched
                killed = pid_count if pkill_result.returncode == 0 else 0
            except subprocess.TimeoutExpired:
                return web.json_response(
                    {"error": "pkill timeout", "killed": 0},
                    status=500,
                )

            logger.info(
                f"[ProcessKill] Killed {killed} processes matching '{pattern}' "
                f"with {signal_name} (pids: {pids[:10]}{'...' if len(pids) > 10 else ''})"
            )

            return web.json_response({
                "killed": killed,
                "pattern": pattern,
                "signal": signal_name,
                "pids": pids[:20],  # Limit returned PIDs
            })

        except json.JSONDecodeError:
            return web.json_response(
                {"error": "invalid JSON", "killed": 0},
                status=400,
            )
        except Exception as e:  # noqa: BLE001
            logger.error(f"[ProcessKill] Error: {e}")
            return web.json_response(
                {"error": str(e), "killed": 0},
                status=500,
            )

    async def handle_training_sync(self, request: web.Request) -> web.Response:
        """Manually trigger sync of selfplay data to training nodes.

        Leader-only: Syncs selfplay data to the top GPU nodes for training.
        """
        try:
            result = await self._sync_selfplay_to_training_nodes()
            return web.json_response(result)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=500)

    # -------------------------------------------------------------------------
    # Cluster Node Handlers - EXTRACTED to scripts/p2p/handlers/cluster_nodes.py
    # January 2026 - P2P Modularization Phase 7b
    # Provides: handle_gpu_rankings, handle_probe_vast_nodes
    # See ClusterNodeHandlersMixin
    # -------------------------------------------------------------------------

    # -------------------------------------------------------------------------
    # Status & Health Handlers - EXTRACTED to scripts/p2p/handlers/status.py
    # January 2026 - P2P Modularization Phase 6a
    # Provides: handle_health, handle_game_counts, handle_refresh_game_counts,
    #           handle_loop_restart, handle_restart_stopped_loops, handle_loops_status,
    #           handle_circuit_breaker_status, handle_transport_stats, handle_dispatch_stats
    # See StatusHandlersMixin
    # -------------------------------------------------------------------------


    # Gauntlet Handlers moved to scripts/p2p/handlers/gauntlet.py
    # Inherited from GauntletHandlersMixin: handle_gauntlet_execute, handle_gauntlet_status,
    # handle_gauntlet_quick_eval, _execute_gauntlet_batch, _execute_single_gauntlet_game,
    # _run_gauntlet_game_sync

    # Admin/Git Handlers moved to scripts/p2p/handlers/admin.py
    # Inherited from AdminHandlersMixin: handle_git_status, handle_git_update, handle_admin_restart

    # Manifest handlers moved to scripts/p2p/handlers/manifest.py (Dec 28, 2025 - Phase 8)
    # Inherited from ManifestHandlersMixin: handle_data_manifest, handle_cluster_data_manifest, handle_refresh_manifest

    # CMA-ES Handlers moved to scripts/p2p/handlers/cmaes.py
    # Inherited from CMAESHandlersMixin:
    # - handle_cmaes_start, handle_cmaes_evaluate
    # - handle_cmaes_status, handle_cmaes_result

    async def _run_distributed_cmaes(self, job_id: str):
        """Main coordinator loop for distributed CMA-ES.

        Integrates with CMA-ES algorithm to optimize heuristic weights.
        Distributes candidate evaluation across GPU workers in the cluster.
        """
        try:
            state = self.distributed_cmaes_state.get(job_id)
            if not state:
                return

            logger.info(f"CMA-ES coordinator started for job {job_id}")
            logger.info(f"Config: {state.generations} gens, pop={state.population_size}, {state.games_per_eval} games/eval")

            # Try to import CMA-ES library
            try:
                import cma
                import numpy as np
            except ImportError:
                logger.info("CMA-ES requires: pip install cma numpy")
                state.status = "error: cma not installed"
                return

            # Default heuristic weights to optimize
            weight_names = [
                "material_weight", "ring_count_weight", "stack_height_weight",
                "center_control_weight", "territory_weight", "mobility_weight",
                "line_potential_weight", "defensive_weight",
            ]
            default_weights = {
                "material_weight": 1.0, "ring_count_weight": 0.5,
                "stack_height_weight": 0.3, "center_control_weight": 0.4,
                "territory_weight": 0.8, "mobility_weight": 0.2,
                "line_potential_weight": 0.6, "defensive_weight": 0.3,
            }

            # Convert to vector for CMA-ES
            x0 = np.array([default_weights[n] for n in weight_names])

            # Initialize CMA-ES
            es = cma.CMAEvolutionStrategy(x0, 0.5, {
                'popsize': state.population_size,
                'maxiter': state.generations,
                'bounds': [0, 2],  # Weights between 0 and 2
            })

            state.current_generation = 0

            while not es.stop() and state.status == "running":
                state.current_generation += 1
                state.last_update = time.time()

                # Get candidate solutions
                solutions = es.ask()

                # Distribute evaluations across workers
                fitness_results = {}
                pending_evals = {}

                for idx, sol in enumerate(solutions):
                    weights = {name: float(sol[i]) for i, name in enumerate(weight_names)}

                    # Round-robin assign to workers
                    if state.worker_nodes:
                        worker_idx = idx % len(state.worker_nodes)
                        worker_id = state.worker_nodes[worker_idx]

                        # Send evaluation request to worker
                        eval_id = f"{job_id}_gen{state.current_generation}_idx{idx}"
                        pending_evals[eval_id] = idx

                        try:
                            with self.peers_lock:
                                worker = self.peers.get(worker_id)
                            if worker:
                                timeout = ClientTimeout(total=300)
                                async with get_client_session(timeout) as session:
                                    url = self._url_for_peer(worker, "/cmaes/evaluate")
                                    await session.post(url, json={
                                        "job_id": job_id,
                                        "weights": weights,
                                        "generation": state.current_generation,
                                        "individual_idx": idx,
                                        "games_per_eval": state.games_per_eval,
                                        "board_type": state.board_type,
                                        "num_players": state.num_players,
                                    }, headers=self._auth_headers())
                        except Exception as e:  # noqa: BLE001
                            logger.error(f"Failed to send eval to {worker_id}: {e}")
                            # Fall back to local evaluation
                            fitness = await self._evaluate_cmaes_weights_local(
                                weights, state.games_per_eval, state.board_type, state.num_players
                            )
                            fitness_results[idx] = fitness

                # Wait for results with timeout
                wait_start = time.time()
                len(solutions) - len(fitness_results)
                while len(fitness_results) < len(solutions) and (time.time() - wait_start) < 300:
                    await asyncio.sleep(1)
                    state.last_update = time.time()

                    # Check for results that came in via /cmaes/result endpoint
                    # Results are stored in state.pending_results by handle_cmaes_result
                    for idx in range(len(solutions)):
                        if idx in fitness_results:
                            continue
                        result_key = f"{state.current_generation}_{idx}"
                        if result_key in state.pending_results:
                            fitness_results[idx] = state.pending_results[result_key]
                            del state.pending_results[result_key]  # Clean up

                    # Progress logging every 30 seconds
                    elapsed = time.time() - wait_start
                    if int(elapsed) % 30 == 0 and elapsed > 1:
                        received = len(fitness_results)
                        logger.info(f"Gen {state.current_generation}: {received}/{len(solutions)} results received ({elapsed:.0f}s elapsed)")

                # Fill in any missing results with default fitness
                fitnesses = []
                for idx in range(len(solutions)):
                    fitness = fitness_results.get(idx, 0.5)  # Default to 0.5 if no result
                    fitnesses.append(-fitness)  # CMA-ES minimizes, so negate

                # Update CMA-ES
                es.tell(solutions, fitnesses)

                # Track best
                best_idx = np.argmin(fitnesses)
                if -fitnesses[best_idx] > state.best_fitness:
                    state.best_fitness = -fitnesses[best_idx]
                    state.best_weights = {name: float(solutions[best_idx][i]) for i, name in enumerate(weight_names)}

                logger.info(f"Gen {state.current_generation}: best_fitness={state.best_fitness:.4f}")

            state.status = "completed"
            logger.info(f"CMA-ES job {job_id} completed: best_fitness={state.best_fitness:.4f}")
            logger.info(f"Best weights: {state.best_weights}")

            # Feed CMA-ES results back to improvement cycle manager
            if self.improvement_cycle_manager and state.best_weights:
                try:
                    agent_id = self.improvement_cycle_manager.handle_cmaes_complete(
                        state.board_type, state.num_players, state.best_weights
                    )
                    logger.info(f"CMA-ES weights registered as agent: {agent_id}")
                    self.diversity_metrics["cmaes_triggers"] += 1

                    # Save weights to file for future use
                    weights_file = Path(self._get_ai_service_path()) / "data" / "cmaes" / f"best_weights_{state.board_type}_{state.num_players}p.json"
                    weights_file.parent.mkdir(parents=True, exist_ok=True)
                    import json as json_mod
                    with open(weights_file, "w") as f:
                        json_mod.dump({
                            "weights": state.best_weights,
                            "fitness": state.best_fitness,
                            "job_id": job_id,
                            "generation": state.current_generation,
                            "timestamp": time.time(),
                        }, f, indent=2)
                    logger.info(f"Saved CMA-ES weights to {weights_file}")

                    # Propagate new weights to selfplay jobs
                    asyncio.create_task(self._propagate_cmaes_weights(
                        state.board_type, state.num_players, state.best_weights
                    ))
                except Exception as e:  # noqa: BLE001
                    logger.error(f"Failed to register CMA-ES weights: {e}")

        except Exception as e:  # noqa: BLE001
            import traceback
            logger.info(f"CMA-ES coordinator error: {e}")
            traceback.print_exc()
            if job_id in self.distributed_cmaes_state:
                self.distributed_cmaes_state[job_id].status = f"error: {e}"

    async def _evaluate_cmaes_weights_local(
        self, weights: dict, num_games: int, board_type: str, num_players: int
    ) -> float:
        """Evaluate weights locally by running selfplay games."""
        try:
            sem = getattr(self, "_cmaes_eval_semaphore", None)
            if sem is None:
                sem = asyncio.Semaphore(1)

            async with sem:
                # Run selfplay subprocess to evaluate weights
                import json as json_mod
                import tempfile

                with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                    json_mod.dump(weights, f)
                    weights_file = f.name

                ai_service_path = str(Path(self._get_ai_service_path()))
                cmd = [
                    sys.executable, "-c", f"""
import sys
sys.path.insert(0, '{ai_service_path}')
from app.game_engine import GameEngine
from app.ai.heuristic_ai import HeuristicAI
from app.models import AIConfig, BoardType, GameStatus
from app.training.generate_data import create_initial_state
import json

weights = json.load(open('{weights_file}'))
board_type = BoardType('{board_type}')
wins = 0
total = {num_games}

for i in range(total):
    state = create_initial_state(board_type, num_players={num_players})
    engine = GameEngine()

    # Candidate with custom weights vs baseline
    config_candidate = AIConfig(difficulty=5, randomness=0.1, think_time=500, custom_weights=weights)
    config_baseline = AIConfig(difficulty=5, randomness=0.1, think_time=500)

    ai_candidate = HeuristicAI(1, config_candidate)
    ai_baseline = HeuristicAI(2, config_baseline)

    move_count = 0
    while state.game_status == GameStatus.ACTIVE and move_count < 300:
        current_ai = ai_candidate if state.current_player == 1 else ai_baseline
        move = current_ai.select_move(state)
        if move is None:
            break
        state = engine.apply_move(state, move)
        move_count += 1

    if state.winner == 1:
        wins += 1
    elif state.winner is None:
        wins += 0.5  # Draw counts as half

print(wins / total)
"""
                ]

                proc = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                    env={**os.environ, "PYTHONPATH": ai_service_path},
                )
                stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=300)

                # Clean up temp file
                os.unlink(weights_file)

                if proc.returncode == 0:
                    return float(stdout.decode().strip())
                else:
                    logger.info(f"Local eval error: {stderr.decode()}")
                    return 0.5

        except Exception as e:  # noqa: BLE001
            logger.info(f"Local CMA-ES evaluation error: {e}")
            return 0.5

    async def _evaluate_cmaes_weights(
        self, job_id: str, weights: dict, generation: int, individual_idx: int,
        games_per_eval: int = 5, board_type: str = "square8", num_players: int = 2
    ):
        """Evaluate weights locally and report result to coordinator."""
        try:
            # Run local evaluation using passed parameters (workers don't have state)
            fitness = await self._evaluate_cmaes_weights_local(
                weights, games_per_eval, board_type, num_players
            )

            logger.info(f"Completed local CMA-ES evaluation: job={job_id}, gen={generation}, idx={individual_idx}, fitness={fitness:.4f}")

            # If we're not the coordinator, report result back
            if self.role != NodeRole.LEADER and self.leader_id:
                with self.peers_lock:
                    leader = self.peers.get(self.leader_id)
                if leader:
                    try:
                        timeout = ClientTimeout(total=30)
                        async with get_client_session(timeout) as session:
                            url = self._url_for_peer(leader, "/cmaes/result")
                            await session.post(url, json={
                                "job_id": job_id,
                                "generation": generation,
                                "individual_idx": individual_idx,
                                "fitness": fitness,
                                "weights": weights,
                                "worker_id": self.node_id,
                            }, headers=self._auth_headers())
                    except Exception as e:  # noqa: BLE001
                        logger.error(f"Failed to report CMA-ES result to leader: {e}")

        except Exception as e:  # noqa: BLE001
            logger.info(f"CMA-ES evaluation error: {e}")

    # Tournament Handlers moved to scripts/p2p/handlers/tournament.py
    # Inherited from TournamentHandlersMixin:
    # - handle_tournament_start, handle_tournament_match
    # - handle_tournament_status, handle_tournament_result

    # -------------------------------------------------------------------------
    # Evaluation Play Handlers - EXTRACTED to scripts/p2p/handlers/evaluation_play.py
    # January 2026 - P2P Modularization Phase 5a
    # Provides: handle_play_elo_match, _play_elo_match_sync, _save_tournament_game_for_training
    # See EvaluationPlayHandlersMixin
    # -------------------------------------------------------------------------

    # -------------------------------------------------------------------------
    # SSH Tournament Handlers - EXTRACTED to scripts/p2p/handlers/ssh_tournament.py
    # Provides: handle_ssh_tournament_start, handle_ssh_tournament_status,
    #           handle_ssh_tournament_cancel, _monitor_ssh_tournament_process
    # See SSHTournamentHandlersMixin
    # -------------------------------------------------------------------------

    # NOTE: _run_distributed_tournament() removed Dec 27, 2025 (~9 LOC)
    # Use self.job_manager.run_distributed_tournament() directly.

    async def _send_match_to_worker(self, job_id: str, worker_id: str, match: dict):
        """Send a match to a worker node."""
        try:
            with self.peers_lock:
                worker = self.peers.get(worker_id)
            if not worker:
                return

            timeout = ClientTimeout(total=10)
            async with get_client_session(timeout) as session:
                url = self._url_for_peer(worker, "/tournament/match")
                await session.post(url, json={"job_id": job_id, "match": match}, headers=self._auth_headers())
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to send match to worker {worker_id}: {e}")

    async def _play_tournament_match(self, job_id: str, match_info: dict) -> dict | None:
        """Play a tournament match locally using subprocess selfplay.

        Dec 28, 2025: Fixed to accept both field name conventions:
        - agent1/agent2 (from tournament handler)
        - player1_model/player2_model (from JobManager)

        Returns:
            Match result dict or None on error
        """
        try:
            import json as json_module
            import sys

            # Support both naming conventions
            agent1 = match_info.get("agent1") or match_info.get("player1_model")
            agent2 = match_info.get("agent2") or match_info.get("player2_model")
            game_num = match_info.get("game_num", 0)
            board_type = match_info.get("board_type", "square8")
            num_players = match_info.get("num_players", 2)

            logger.info(f"Playing tournament match: {agent1} vs {agent2} (game {game_num})")

            # Build the subprocess command to run a single game
            # Agent IDs map to model paths or heuristic configurations
            # Dec 28, 2025: Fixed imports and game playing logic
            game_script = f"""
import sys
sys.path.insert(0, '{self._get_ai_service_path()}')
from app.training.initial_state import create_initial_state
from app.rules import get_rules_engine
from app.ai.heuristic_ai import HeuristicAI
from app.ai.random_ai import RandomAI
from app.models import AIConfig
import json
import os

# Skip shadow contracts for performance
os.environ['RINGRIFT_SKIP_SHADOW_CONTRACTS'] = 'true'

def load_agent(agent_id: str, player_idx: int, board_type: str, num_players: int):
    '''Load agent by ID - supports random, heuristic, or model paths.'''
    # Dec 29, 2025: Added difficulty=5 (medium) as required by AIConfig
    config = AIConfig(board_type=board_type, num_players=num_players, difficulty=5)
    if agent_id == 'random':
        return RandomAI(player_idx, config=config)
    elif agent_id == 'heuristic':
        return HeuristicAI(player_idx, config=config)
    elif agent_id.startswith('heuristic:'):
        # Parse weights from agent ID: "heuristic:w1,w2,w3,..."
        weight_str = agent_id.split(':')[1]
        weights = [float(w) for w in weight_str.split(',')]
        weight_names = [
            "material_weight", "ring_count_weight", "stack_height_weight",
            "center_control_weight", "territory_weight", "mobility_weight",
            "line_potential_weight", "defensive_weight",
        ]
        weight_dict = dict(zip(weight_names, weights))
        config.heuristic_weights = weight_dict
        return HeuristicAI(player_idx, config=config)
    elif agent_id.startswith('model:') or agent_id.startswith('canonical_'):
        # Neural network model - for now, fall back to heuristic
        # TODO: Load actual neural network models
        return HeuristicAI(player_idx, config=config)
    else:
        # Default heuristic agent
        return HeuristicAI(player_idx, config=config)

# Initialize game state and engine
engine = get_rules_engine(skip_shadow_contracts=True)
state = create_initial_state(board_type='{board_type}', num_players={num_players})
agents = [
    load_agent('{agent1}', 0, '{board_type}', {num_players}),
    load_agent('{agent2}', 1, '{board_type}', {num_players}),
]

# Play until completion
max_moves = 10000
move_count = 0
while not state.game_over and move_count < max_moves:
    current_player = state.current_player_index
    agent = agents[current_player]
    move = agent.select_move(state)
    if move is None:
        break
    state = engine.apply_move(state, move)
    move_count += 1

# Get result
winner_idx = None
victory_type = 'unknown'
if state.game_over:
    # Find winner from scores
    scores = state.player_scores
    if scores:
        max_score = max(scores)
        if scores.count(max_score) == 1:
            winner_idx = scores.index(max_score)

# Map winner index to agent ID
winner_agent = None
if winner_idx == 0:
    winner_agent = '{agent1}'
elif winner_idx == 1:
    winner_agent = '{agent2}'

result = {{
    'agent1': '{agent1}',
    'agent2': '{agent2}',
    'winner': winner_agent,
    'winner_idx': winner_idx,
    'victory_type': victory_type,
    'move_count': move_count,
    'game_num': {game_num},
}}
print(json.dumps(result))
"""
            # Run the game in subprocess
            cmd = [sys.executable, "-c", game_script]
            env = os.environ.copy()
            env["PYTHONPATH"] = self._get_ai_service_path()
            env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

            proc = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env,
            )

            stdout, stderr = await asyncio.wait_for(
                proc.communicate(),
                timeout=300  # 5 minute timeout per game
            )

            if proc.returncode != 0:
                logger.info(f"Tournament match subprocess error: {stderr.decode()}")
                result = {
                    "agent1": agent1,
                    "agent2": agent2,
                    "winner": None,
                    "error": stderr.decode()[:200],
                    "game_num": game_num,
                }
            else:
                # Parse result from stdout
                output_lines = stdout.decode().strip().split('\n')
                result_line = output_lines[-1] if output_lines else '{}'
                result = json_module.loads(result_line)

            logger.info(f"Match result: {agent1} vs {agent2} -> winner={result.get('winner')}")

            # Report result back to coordinator (leader)
            if self.role != NodeRole.LEADER and self.leader_id:
                with self.peers_lock:
                    leader = self.peers.get(self.leader_id)
                if leader:
                    try:
                        timeout = ClientTimeout(total=10)
                        async with get_client_session(timeout) as session:
                            url = self._url_for_peer(leader, "/tournament/result")
                            await session.post(url, json={
                                "job_id": job_id,
                                "result": result,
                                "worker_id": self.node_id,
                            }, headers=self._auth_headers())
                    except Exception as e:  # noqa: BLE001
                        logger.error(f"Failed to report tournament result to leader: {e}")
            else:
                # We are the leader, update state directly
                if job_id in self.distributed_tournament_state:
                    state = self.distributed_tournament_state[job_id]
                    state.results.append(result)
                    state.completed_matches += 1
                    state.last_update = time.time()

            # Dec 28, 2025: Return result for synchronous handler usage
            return result

        except asyncio.TimeoutError:
            logger.info(f"Tournament match timed out: {match_info}")
            return None
        except Exception as e:  # noqa: BLE001
            logger.info(f"Tournament match error: {e}")
            return None

    # NOTE: _calculate_tournament_ratings removed Dec 27, 2025 (dead code, never called)
    # Elo rating calculation is now handled in JobManager.run_distributed_tournament()

    # =========================================================================
    # NOTE: Improvement handlers moved to scripts/p2p/handlers/improvement.py (Dec 28, 2025 - Phase 8)
    # Inherited from ImprovementHandlersMixin:
    # - handle_improvement_start, handle_improvement_status, handle_improvement_phase_complete
    # - handle_improvement_cycles_status, handle_improvement_cycles_leaderboard
    # - handle_improvement_training_complete, handle_improvement_evaluation_complete
    # =========================================================================

    # =========================================================================
    # NOTE: Sync handlers (handle_sync_*) extracted to SyncHandlersMixin
    # See: scripts/p2p/handlers/sync.py (Dec 28, 2025 - Phase 8)
    # Removed: handle_sync_start, handle_sync_status, handle_sync_push,
    #          handle_sync_receipt, handle_sync_receipts_status, handle_sync_pull,
    #          handle_sync_file, handle_sync_job_update (~625 LOC)
    # =========================================================================

    # -------------------------------------------------------------------------
    # Event Management Handlers - EXTRACTED to scripts/p2p/handlers/event_management.py
    # January 2026 - P2P Modularization Phase 5b
    # Provides: handle_subscriptions
    # See EventManagementHandlersMixin
    # -------------------------------------------------------------------------

    async def _run_improvement_loop(self, job_id: str):
        """Main coordinator loop for AlphaZero-style improvement."""
        try:
            state = self.improvement_loop_state.get(job_id)
            if not state:
                return

            logger.info(f"Improvement loop coordinator started for job {job_id}")

            while state.current_iteration < state.max_iterations and state.status == "running":
                state.current_iteration += 1
                logger.info(f"Improvement iteration {state.current_iteration}/{state.max_iterations}")

                # Phase 1: Selfplay
                state.phase = "selfplay"
                state.selfplay_progress = {}
                await self.job_manager.run_distributed_selfplay(job_id)

                # Phase 2: Export training data
                state.phase = "export"
                await self.job_manager.export_training_data(job_id)

                # Phase 3: Training
                state.phase = "train"
                await self.job_manager.run_training(job_id)

                # Phase 4: Evaluation
                state.phase = "evaluate"
                await self._run_evaluation(job_id)

                # Phase 5: Promote if better
                state.phase = "promote"
                await self._promote_model_if_better(job_id)

                state.last_update = time.time()

            state.status = "completed"
            state.phase = "idle"
            logger.info(f"Improvement loop {job_id} completed after {state.current_iteration} iterations")

        except Exception as e:  # noqa: BLE001
            logger.info(f"Improvement loop error: {e}")
            if job_id in self.improvement_loop_state:
                self.improvement_loop_state[job_id].status = f"error: {e}"

    # NOTE: The following methods were removed Dec 27, 2025 (call sites updated to use job_manager directly):
    # - _run_distributed_selfplay() -> self.job_manager.run_distributed_selfplay()
    # - _export_training_data() -> self.job_manager.export_training_data()
    # - _run_training() -> self.job_manager.run_training()
    # - _run_local_selfplay() -> self.job_manager.run_local_selfplay() (removed Dec 2025)
    # - _run_local_training() -> self.job_manager.run_local_training() (removed Dec 2025)

    # ============================================
    # Phase 3: Training Pipeline Integration Methods
    # ============================================

    # NOTE: _check_training_readiness() removed Dec 2025 (95 LOC).
    # Use self.training_coordinator.check_training_readiness() instead.
    # See scripts/p2p/managers/training_coordinator.py for implementation.

    # NOTE: _find_running_training_job() and _find_resumable_training_job() removed Dec 2025 (29 LOC).
    # Use self.training_coordinator.find_running_training_job() and
    # self.training_coordinator.find_resumable_training_job() instead.

    # NOTE: _dispatch_training_job() removed Dec 2025 (9 LOC).
    # Use self.training_coordinator.dispatch_training_job() directly.

    async def _check_and_trigger_training(self):
        """Periodic check for training readiness (leader only)."""
        if self.role != NodeRole.LEADER:
            return

        # Phase 2.4 (Dec 29, 2025): Skip training dispatch in partition readonly mode
        if self.is_partition_readonly():
            logger.debug("[P2P] Skipping training check: partition readonly mode")
            return

        current_time = time.time()
        if current_time - self.last_training_check < self.training_check_interval:
            return

        self.last_training_check = current_time

        # Get jobs that should be started (delegated to TrainingCoordinator manager)
        jobs_to_start = self.training_coordinator.check_training_readiness()

        for job_config in jobs_to_start:
            # PHASE 4 IDEMPOTENCY: Check for duplicate triggers
            config_key = job_config.get("config_key", "")
            game_count = job_config.get("total_games", 0)
            can_proceed, trigger_hash = self._check_training_idempotency(config_key, game_count)
            if not can_proceed:
                continue

            logger.info(f"Auto-triggering {job_config['job_type']} training for {config_key} ({game_count} games)")
            await self.training_coordinator.dispatch_training_job(job_config)
            self._record_training_trigger(trigger_hash)  # Record after successful dispatch

    async def _check_local_training_fallback(self):
        """DECENTRALIZED training trigger when cluster has no leader.

        LEADERLESS RESILIENCE: When the cluster has been without a leader for too long
        (LEADERLESS_TRAINING_TIMEOUT = 3 minutes), individual nodes can trigger local
        training to prevent data accumulation without progress.

        This makes the system more resilient to leader election failures while avoiding
        duplicate training by:
        1. Only triggering after a brief leaderless period (3 minutes)
        2. Using random jitter so nodes don't all train simultaneously
        3. Only training on local data (no cluster-wide coordination needed)
        4. Using reasonable cooldowns between fallback training runs
        """
        # Skip if we ARE the leader or have a known leader
        if self.role == NodeRole.LEADER or self.leader_id:
            self.last_leader_seen = time.time()  # Update leader seen time
            return

        current_time = time.time()
        leaderless_duration = current_time - self.last_leader_seen

        # Only trigger fallback if leaderless for the timeout period
        if leaderless_duration < LEADERLESS_TRAINING_TIMEOUT:
            return

        # Rate limit fallback training (10 minute cooldown - more aggressive than before)
        fallback_cooldown = 600  # 10 minutes between fallback triggers
        if current_time - self.last_local_training_fallback < fallback_cooldown:
            return

        # Random jitter: 40% probability per check (more aggressive than 20%)
        # This distributes training across nodes over time
        import random
        if random.random() > 0.4:
            return

        # Check if we have a GPU (training needs GPU)
        if not getattr(self.self_info, "has_gpu", False):
            return

        # Check local data manifest (use cached version for speed)
        local_manifest = getattr(self, "local_data_manifest", None)
        if not local_manifest:
            # Try to load from cache or collect if we don't have one
            try:
                local_manifest = self.sync_planner.collect_local_manifest_cached(max_cache_age=600)
                with self.manifest_lock:
                    self.local_data_manifest = local_manifest
            except (AttributeError):
                return

        # Check for sufficient local data (lower threshold for faster training)
        min_games_fallback = 2000  # Lower threshold for faster response
        total_local_games = getattr(local_manifest, "selfplay_games", 0)
        if total_local_games < min_games_fallback:
            return

        # Find board types with enough local data
        game_counts_by_type: dict[str, int] = {}
        for file_info in getattr(local_manifest, "files", []) or []:
            board_type = getattr(file_info, "board_type", "")
            num_players = getattr(file_info, "num_players", 2)
            game_count = getattr(file_info, "game_count", 0)
            if board_type and game_count > 0:
                key = f"{board_type}_{num_players}p"
                game_counts_by_type[key] = game_counts_by_type.get(key, 0) + game_count

        # Sort by game count (descending) to train on richest data first
        sorted_configs = sorted(game_counts_by_type.items(), key=lambda x: x[1], reverse=True)

        # Trigger local training for configurations with enough data
        triggered_count = 0
        max_concurrent_fallback = 2  # Can trigger up to 2 training jobs per fallback
        for config_key, game_count in sorted_configs:
            if triggered_count >= max_concurrent_fallback:
                break
            if game_count < 1000:  # Minimum threshold (lowered)
                continue

            # Check if we already have a running training job for this config
            existing_job = self.training_coordinator.find_running_training_job("nnue", config_key)
            if existing_job:
                continue

            # DISTRIBUTED TRAINING COORDINATION: Check cluster-wide before starting
            is_training, _training_nodes = self._is_config_being_trained_cluster_wide(config_key)
            if is_training:
                # Someone else is already training this config
                continue

            # Use distributed slot claiming to avoid race conditions
            if not self._should_claim_training_slot(config_key):
                continue

            # Parse board type and player count
            parts = config_key.split("_")
            if len(parts) < 2:
                continue
            board_type = parts[0]
            num_players = int(parts[1].replace("p", ""))

            # PHASE 4 IDEMPOTENCY: Check for duplicate triggers
            can_proceed, trigger_hash = self._check_training_idempotency(config_key, game_count)
            if not can_proceed:
                continue

            logger.info(f"DISTRIBUTED TRAINING: Claiming {config_key} ({game_count} local games, leaderless for {int(leaderless_duration)}s)")
            job_config = {
                "job_type": "nnue",
                "board_type": board_type,
                "num_players": num_players,
                "config_key": config_key,
                "total_games": game_count,
            }
            await self.training_coordinator.dispatch_training_job(job_config)
            self._record_training_trigger(trigger_hash)  # Record after successful dispatch
            triggered_count += 1

        if triggered_count > 0:
            self.last_local_training_fallback = current_time
            logger.info(f"LEADERLESS FALLBACK: Triggered {triggered_count} local training job(s)")

    async def _check_improvement_cycles(self):
        """Periodic check for improvement cycle readiness (leader only).

        This integrates with the ImprovementCycleManager to:
        1. Check if any cycles need training based on data thresholds
        2. Trigger export/training jobs for ready cycles
        3. Run evaluations and update Elo ratings
        4. Schedule CMA-ES optimization when needed
        5. Schedule diverse tournaments for AI calibration
        """
        if self.role != NodeRole.LEADER:
            return

        if not self.improvement_cycle_manager:
            return

        current_time = time.time()
        if current_time - self.last_improvement_cycle_check < self.improvement_cycle_check_interval:
            return

        self.last_improvement_cycle_check = current_time

        # Check which cycles are ready for training
        training_ready = self.improvement_cycle_manager.check_training_needed()

        # Convert to job configs
        jobs_to_start = []
        for board_type, num_players in training_ready:
            cycle_key = f"{board_type}_{num_players}p"
            cycle_state = self.improvement_cycle_manager.state.cycles.get(cycle_key)
            if cycle_state and self.improvement_cycle_manager.trigger_training(board_type, num_players):
                jobs_to_start.append({
                    "cycle_id": cycle_key,
                    "board_type": board_type,
                    "num_players": num_players,
                    "total_games": cycle_state.games_since_last_training,
                    "iteration": cycle_state.current_iteration + 1,
                })

        # Also check for CMA-ES optimization opportunities
        cmaes_ready = self.improvement_cycle_manager.check_cmaes_needed()
        for board_type, num_players in cmaes_ready:
            # Trigger distributed CMA-ES
            logger.info(f"CMA-ES optimization ready for {board_type}_{num_players}p")
            asyncio.create_task(self._trigger_auto_cmaes(board_type, num_players))

        # Check for rollback needs (consecutive training failures)
        for key, cycle in self.improvement_cycle_manager.state.cycles.items():
            if not cycle.pending_training and not cycle.pending_evaluation:
                should_rollback, reason = self.improvement_cycle_manager.check_rollback_needed(
                    cycle.board_type, cycle.num_players
                )
                if should_rollback:
                    logger.info(f"ROLLBACK NEEDED for {key}: {reason}")
                    if self.improvement_cycle_manager.execute_rollback(cycle.board_type, cycle.num_players):
                        self.diversity_metrics["rollbacks"] += 1
                        # Increase diversity to escape plateau
                        logger.info(f"Increasing diversity to escape training plateau for {key}")

        for job_config in jobs_to_start:
            cycle_id = job_config["cycle_id"]
            board_type = job_config["board_type"]
            num_players = job_config["num_players"]

            logger.info(f"ImprovementCycle {cycle_id}: Starting training "
                  f"({job_config['total_games']} games)")

            # Find GPU worker for training
            gpu_worker = None
            candidates: list[NodeInfo] = []
            with self.peers_lock:
                candidates.extend([p for p in self.peers.values() if p.is_gpu_node() and p.is_healthy()])
            if self.self_info.is_gpu_node() and self.self_info.is_healthy():
                candidates.append(self.self_info)
            if candidates:
                candidates.sort(
                    key=lambda p: (-p.gpu_power_score(), p.get_load_score(), str(p.node_id))
                )
                gpu_worker = candidates[0]

            if not gpu_worker:
                logger.info(f"ImprovementCycle {cycle_id}: No GPU worker available, deferring")
                self.improvement_cycle_manager.update_cycle_phase(
                    cycle_id, "idle", error_message="No GPU worker available"
                )
                continue

            # Create training job
            job_id = f"cycle_{cycle_id}_{int(time.time())}"
            training_job = TrainingJob(
                job_id=job_id,
                job_type="nnue",
                board_type=board_type,
                num_players=num_players,
                worker_node=gpu_worker.node_id,
                epochs=job_config.get("epochs", 100),
                batch_size=job_config.get("batch_size", 4096),
                learning_rate=job_config.get("learning_rate", 0.001),
                data_games_count=job_config.get("total_games", 0),
            )

            with self.training_lock:
                self.training_jobs[job_id] = training_job

            # Update cycle state
            self.improvement_cycle_manager.update_cycle_phase(
                cycle_id, "training", training_job_id=job_id
            )

            # Dispatch training to worker
            await self._dispatch_improvement_training(training_job, cycle_id)

    async def _dispatch_improvement_training(self, job: TrainingJob, cycle_id: str):
        """Dispatch training job for improvement cycle."""
        try:
            # Find the worker node
            worker_node = None
            if job.worker_node == self.node_id:
                worker_node = self.self_info
            else:
                with self.peers_lock:
                    worker_node = self.peers.get(job.worker_node)

            if not worker_node:
                logger.info(f"ImprovementCycle {cycle_id}: Worker {job.worker_node} not found")
                self.improvement_cycle_manager.update_cycle_phase(
                    cycle_id, "idle", error_message=f"Worker {job.worker_node} not found"
                )
                return

            # Build training payload
            payload = {
                "job_id": job.job_id,
                "cycle_id": cycle_id,
                "board_type": job.board_type,
                "num_players": job.num_players,
                "epochs": job.epochs,
                "batch_size": job.batch_size,
                "learning_rate": job.learning_rate,
            }

            # Send to worker
            timeout = ClientTimeout(total=30)
            async with get_client_session(timeout) as session:
                last_err: str | None = None
                for url in self._urls_for_peer(worker_node, "/training/nnue/start"):
                    try:
                        async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                            if resp.status != 200:
                                last_err = f"http_{resp.status}"
                                continue
                            result = await resp.json()
                        if result.get("success"):
                            job.status = "running"
                            job.started_at = time.time()
                            logger.info(f"ImprovementCycle {cycle_id}: Training started on {worker_node.node_id}")
                            return
                        self.improvement_cycle_manager.update_cycle_phase(
                            cycle_id, "idle", error_message=result.get("error", "Training failed to start")
                        )
                        return
                    except Exception as e:  # noqa: BLE001
                        last_err = str(e)
                        continue
                self.improvement_cycle_manager.update_cycle_phase(
                    cycle_id, "idle", error_message=last_err or "dispatch_failed"
                )

        except Exception as e:  # noqa: BLE001
            logger.info(f"ImprovementCycle {cycle_id}: Training dispatch failed: {e}")
            self.improvement_cycle_manager.update_cycle_phase(
                cycle_id, "idle", error_message=str(e)
            )

    # NOTE: Training control handlers moved to TrainingControlHandlersMixin (Jan 2026 - P2P Modularization Phase 3a)
    # Includes: handle_training_start, handle_training_status, handle_training_progress, handle_training_update,
    #           handle_training_trigger, handle_training_trigger_decision, handle_training_trigger_configs, handle_nnue_start

    async def _trigger_auto_cmaes(self, board_type: str, num_players: int):
        """Automatically trigger CMA-ES optimization for a configuration.

        Called by improvement cycle manager when optimization is due.
        """
        try:
            job_id = f"auto_cmaes_{board_type}_{num_players}p_{int(time.time())}"
            logger.info(f"Auto-triggering CMA-ES: {job_id}")

            # Check for GPU workers
            gpu_workers = []
            with self.peers_lock:
                for peer in self.peers.values():
                    if peer.is_healthy() and peer.has_gpu and peer.node_id != self.node_id:
                        gpu_workers.append(peer)

            if self.self_info.has_gpu:
                gpu_workers.append(self.self_info)

            if len(gpu_workers) >= 2:
                # DISTRIBUTED MODE
                cmaes_job_id = f"cmaes_auto_{job_id}"
                state = DistributedCMAESState(
                    job_id=cmaes_job_id,
                    board_type=board_type,
                    num_players=num_players,
                    generations=100,
                    population_size=max(32, len(gpu_workers) * 8),
                    games_per_eval=100,
                    status="running",
                    started_at=time.time(),
                    last_update=time.time(),
                    worker_nodes=[w.node_id for w in gpu_workers],
                )
                self.distributed_cmaes_state[cmaes_job_id] = state
                asyncio.create_task(self._run_distributed_cmaes(cmaes_job_id))
                logger.info(f"Started distributed CMA-ES with {len(gpu_workers)} workers")
            else:
                # LOCAL MODE - use GPU CMA-ES script
                output_dir = os.path.join(
                    self._get_ai_service_path(), "data", "cmaes",
                    f"{board_type}_{num_players}p_auto_{int(time.time())}"
                )
                os.makedirs(output_dir, exist_ok=True)

                cmd = [
                    sys.executable,
                    os.path.join(self._get_ai_service_path(), "scripts", "run_gpu_cmaes.py"),
                    "--board", board_type,
                    "--num-players", str(num_players),
                    "--generations", "100",
                    "--population-size", "32",
                    "--games-per-eval", "100",
                    "--max-moves", "10000",
                    "--output-dir", output_dir,
                    "--multi-gpu",
                ]

                env = os.environ.copy()
                env["PYTHONPATH"] = self._get_ai_service_path()
                env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

                proc = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                    env=env,
                )
                logger.info(f"Started local CMA-ES optimization (PID {proc.pid})")

        except Exception as e:  # noqa: BLE001
            logger.info(f"Auto CMA-ES trigger failed: {e}")

    async def handle_cmaes_start_auto(self, request: web.Request) -> web.Response:
        """Handle CMA-ES optimization start request.

        Uses distributed GPU CMA-ES across all cluster GPU nodes for maximum throughput.
        Falls back to local GPU CMA-ES if no remote workers available.
        """
        try:
            data = await request.json()
            job_id = data.get("job_id")
            board_type = data.get("board_type", "square8")
            num_players = data.get("num_players", 2)

            # Check for available GPU workers in the cluster
            gpu_workers = []
            with self.peers_lock:
                for peer in self.peers.values():
                    if peer.is_healthy() and peer.has_gpu and peer.node_id != self.node_id:
                        gpu_workers.append(peer)

            # Include self if we have GPU
            if self.self_info.has_gpu:
                gpu_workers.append(self.self_info)

            if len(gpu_workers) >= 2:
                # DISTRIBUTED MODE: Use P2P distributed CMA-ES across cluster
                logger.info(f"Starting DISTRIBUTED GPU CMA-ES with {len(gpu_workers)} workers")

                # Create distributed CMA-ES state
                cmaes_job_id = f"cmaes_auto_{job_id}_{int(time.time())}"
                state = DistributedCMAESState(
                    job_id=cmaes_job_id,
                    board_type=board_type,
                    num_players=num_players,
                    generations=100,  # More generations for better optimization
                    population_size=max(32, len(gpu_workers) * 8),  # Scale with workers
                    games_per_eval=100,  # More games for accurate fitness
                    status="running",
                    started_at=time.time(),
                    last_update=time.time(),
                    worker_nodes=[w.node_id for w in gpu_workers],
                )
                self.distributed_cmaes_state[cmaes_job_id] = state

                # Launch distributed coordinator task
                asyncio.create_task(self._run_distributed_cmaes(cmaes_job_id))

                # Track as training job
                with self.training_lock:
                    if job_id in self.training_jobs:
                        self.training_jobs[job_id].status = "running"
                        self.training_jobs[job_id].started_at = time.time()

                return web.json_response({
                    "success": True,
                    "mode": "distributed",
                    "job_id": cmaes_job_id,
                    "workers": [w.node_id for w in gpu_workers],
                })

            else:
                # LOCAL MODE: Run GPU CMA-ES on this node only
                logger.info("Starting LOCAL GPU CMA-ES (no remote workers available)")

                output_dir = os.path.join(
                    self._get_ai_service_path(), "data", "cmaes",
                    f"{board_type}_{num_players}p_auto_{int(time.time())}"
                )
                os.makedirs(output_dir, exist_ok=True)

                cmd = [
                    sys.executable,
                    os.path.join(self._get_ai_service_path(), "scripts", "run_gpu_cmaes.py"),
                    "--board", board_type,
                    "--num-players", str(num_players),
                    "--generations", "100",
                    "--population-size", "32",
                    "--games-per-eval", "100",
                    "--max-moves", "10000",
                    "--output-dir", output_dir,
                    "--multi-gpu",
                ]

                env = os.environ.copy()
                env["PYTHONPATH"] = self._get_ai_service_path()
                env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

                proc = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                    env=env,
                )

                logger.info(f"Started local GPU CMA-ES (PID {proc.pid}) for job {job_id}")
                asyncio.create_task(self._monitor_training_process(job_id, proc, output_dir))

                return web.json_response({
                    "success": True,
                    "mode": "local",
                    "pid": proc.pid,
                })

        except Exception as e:  # noqa: BLE001
            return web.json_response({"success": False, "error": str(e)})

    def _get_training_timeout(self, job_id: str) -> int:
        """Get dynamic timeout based on job configuration.

        Returns timeout in seconds based on board type and model complexity:
        - square19: 6 hours (large board, 361 cells)
        - hexagonal: 5 hours (469 cells)
        - square8/hex8: 2 hours (small boards)
        Default: 3 hours if job not found
        """
        with self.training_lock:
            job = self.training_jobs.get(job_id)
            if not job:
                return 10800  # 3 hours default

            board_type = getattr(job, 'board_type', 'unknown')
            num_players = getattr(job, 'num_players', 2)

            # Base timeout by board complexity
            if board_type == 'square19':
                base_timeout = 21600  # 6 hours
            elif board_type == 'hexagonal':
                base_timeout = 18000  # 5 hours
            elif board_type in ('hex8', 'square8'):
                base_timeout = 7200   # 2 hours
            else:
                base_timeout = 10800  # 3 hours default

            # Add 50% for 4-player models (larger value head, more complex)
            if num_players == 4:
                base_timeout = int(base_timeout * 1.5)
            elif num_players == 3:
                base_timeout = int(base_timeout * 1.25)

            return base_timeout

    async def _monitor_training_process(self, job_id: str, proc, output_path: str):
        """Monitor training subprocess and report completion to leader."""
        try:
            timeout = self._get_training_timeout(job_id)
            _stdout, stderr = await asyncio.wait_for(
                proc.communicate(),
                timeout=timeout
            )

            success = proc.returncode == 0

            # Report to leader with retry logic
            if self.leader_id and self.leader_id != self.node_id:
                leader = self.peers.get(self.leader_id)
                if leader:
                    payload = {
                        "job_id": job_id,
                        "completed": success,
                        "output_model_path": output_path if success else "",
                        "error": stderr.decode()[:500] if not success else "",
                    }
                    # Retry with exponential backoff (3 attempts: 5s, 10s, 20s)
                    max_retries = 3
                    base_delay = 5.0
                    for attempt in range(max_retries):
                        try:
                            http_timeout = ClientTimeout(total=30)
                            async with get_client_session(http_timeout) as session:
                                url = self._url_for_peer(leader, "/training/update")
                                resp = await session.post(url, json=payload, headers=self._auth_headers())
                                if resp.status < 400:
                                    logger.info(f"Training completion reported to leader (attempt {attempt + 1})")
                                    break
                                else:
                                    logger.warning(f"Leader returned {resp.status}, retrying...")
                        except Exception as e:  # noqa: BLE001
                            delay = base_delay * (2 ** attempt)
                            if attempt < max_retries - 1:
                                logger.warning(f"Failed to report training completion (attempt {attempt + 1}): {e}, retrying in {delay}s")
                                await asyncio.sleep(delay)
                            else:
                                logger.error(f"Failed to report training completion after {max_retries} attempts: {e}")
            else:
                # We are the leader, update directly
                with self.training_lock:
                    job = self.training_jobs.get(job_id)
                    if job:
                        if success:
                            job.status = "completed"
                            job.completed_at = time.time()
                            job.output_model_path = output_path
                            # LEARNED LESSONS - Schedule tournament to compare new model against baseline
                            asyncio.create_task(self._schedule_model_comparison(job, output_path))
                            # Update improvement cycle manager with training completion
                            if self.improvement_cycle_manager:
                                self.improvement_cycle_manager.handle_training_complete(
                                    job.board_type, job.num_players,
                                    output_path, job.data_games_count or 0
                                )
                            # PFSP: Add trained model to opponent pool for diverse selfplay
                            config_key = f"{job.board_type}_{job.num_players}p"
                            if HAS_PFSP and config_key in self.pfsp_pools:
                                try:
                                    model_id = Path(output_path).stem
                                    self.pfsp_pools[config_key].add_opponent(
                                        model_id=model_id,
                                        model_path=output_path,
                                        elo=INITIAL_ELO_RATING,  # From app.config.thresholds
                                        win_rate=0.5,
                                    )
                                    logger.info(f"[PFSP] Added {model_id} to opponent pool for {config_key}")
                                except Exception as e:  # noqa: BLE001
                                    logger.error(f"[PFSP] Error adding model to pool: {e}")
                            # CMA-ES: Check for Elo plateau and trigger auto-tuning
                            asyncio.create_task(self._check_cmaes_auto_tuning(config_key))
                        else:
                            job.status = "failed"
                            job.error_message = stderr.decode()[:500]
                        job.completed_at = time.time()

            logger.info(f"Training job {job_id} {'completed' if success else 'failed'}")

        except asyncio.TimeoutError:
            logger.info(f"Training job {job_id} timed out")
        except Exception as e:  # noqa: BLE001
            logger.info(f"Training monitor error for {job_id}: {e}")

    async def _monitor_gpu_selfplay_and_validate(
        self,
        job_id: str,
        proc: subprocess.Popen,
        output_dir: Path,
        board_type: str,
        num_players: int,
    ) -> None:
        """Monitor GPU selfplay completion and run CPU validation.

        When GPU selfplay completes, this runs import_gpu_selfplay_to_db.py to:
        1. Replay each game with CPU GameEngine
        2. Validate all moves against legal move lists
        3. Discard games with invalid moves
        4. Store only validated games in canonical DB format

        This ensures GPU-generated games are safe for training.
        """
        try:
            # Wait for GPU selfplay to complete (with timeout)
            return_code = await asyncio.wait_for(
                asyncio.to_thread(proc.wait),
                timeout=7200,  # 2 hour max
            )

            # Update job status
            with self.jobs_lock:
                job = self.local_jobs.get(job_id)
                if job:
                    job.status = "completed" if return_code == 0 else "failed"
                    job.completed_at = time.time()

            if return_code != 0:
                logger.info(f"GPU selfplay job {job_id} failed (exit code {return_code})")
                return

            # Find the generated JSONL file
            jsonl_files = list(output_dir.glob("*.jsonl"))
            if not jsonl_files:
                logger.warning(f"GPU selfplay job {job_id}: No JSONL output found")
                with self.jobs_lock:
                    job = self.local_jobs.get(job_id)
                    if job:
                        job.status = "failed"
                        job.completed_at = time.time()
                        job.error_message = "missing_jsonl_output"
                self._record_gpu_job_result(success=False)
                self._update_gpu_job_count(-1)
                return

            input_jsonl = jsonl_files[0]
            try:
                if input_jsonl.stat().st_size == 0:
                    logger.warning(f"GPU selfplay job {job_id}: JSONL output is empty ({input_jsonl})")
                    with self.jobs_lock:
                        job = self.local_jobs.get(job_id)
                        if job:
                            job.status = "failed"
                            job.completed_at = time.time()
                            job.error_message = "empty_jsonl_output"
                    self._record_gpu_job_result(success=False)
                    self._update_gpu_job_count(-1)
                    return
            except OSError as e:
                logger.warning(f"GPU selfplay job {job_id}: Failed to stat JSONL output ({input_jsonl}): {e}")
                with self.jobs_lock:
                    job = self.local_jobs.get(job_id)
                    if job:
                        job.status = "failed"
                        job.completed_at = time.time()
                        job.error_message = "jsonl_stat_failed"
                self._record_gpu_job_result(success=False)
                self._update_gpu_job_count(-1)
                return
            validated_db = output_dir / "validated_games.db"

            logger.info(f"GPU selfplay job {job_id} completed, running CPU validation...")

            # Run CPU validation import
            validate_cmd = [
                sys.executable,  # Use venv Python
                self._get_script_path("import_gpu_selfplay_to_db.py"),
                "--input", str(input_jsonl),
                "--output", str(validated_db),
            ]

            env = os.environ.copy()
            env["PYTHONPATH"] = self._get_ai_service_path()

            validate_proc = await asyncio.create_subprocess_exec(
                *validate_cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env,
                cwd=self.ringrift_path,
            )

            stdout, stderr = await asyncio.wait_for(
                validate_proc.communicate(),
                timeout=1800,  # 30 min validation timeout
            )

            if validate_proc.returncode == 0:
                # Parse validation results from output
                output_text = stdout.decode()
                imported = 0
                failed = 0
                for line in output_text.split("\n"):
                    if "Successfully imported:" in line:
                        imported = int(line.split(":")[-1].strip())
                    elif "Failed:" in line:
                        failed = int(line.split(":")[-1].strip())

                validation_rate = imported / (imported + failed) * 100 if (imported + failed) > 0 else 0

                logger.info(f"GPU selfplay {job_id} CPU validation complete:")
                logger.info(f"  Valid games: {imported}, Invalid: {failed}, Validation rate: {validation_rate:.1f}%")

                # Track validation metrics for diversity reporting
                if hasattr(self, 'diversity_metrics'):
                    if "gpu_validation_stats" not in self.diversity_metrics:
                        self.diversity_metrics["gpu_validation_stats"] = {
                            "total_generated": 0,
                            "total_validated": 0,
                            "total_failed": 0,
                        }
                    self.diversity_metrics["gpu_validation_stats"]["total_generated"] += imported + failed
                    self.diversity_metrics["gpu_validation_stats"]["total_validated"] += imported
                    self.diversity_metrics["gpu_validation_stats"]["total_failed"] += failed

                # Record validation rate metric for observability
                self.record_metric(
                    "validation_rate",
                    validation_rate,
                    board_type=board_type,
                    num_players=num_players,
                    metadata={
                        "job_id": job_id,
                        "imported": imported,
                        "failed": failed,
                    },
                )

                # Auto-import to canonical database if validation rate is high enough
                if validation_rate >= 95 and imported > 0:
                    asyncio.create_task(self._import_gpu_selfplay_to_canonical(
                        validated_db, board_type, num_players, imported
                    ))
                    # Jan 7, 2026: Track GPU job success
                    self._record_gpu_job_result(success=True)
                    self._update_gpu_job_count(-1)
                elif validation_rate < 95:
                    logger.info(f"WARNING: GPU selfplay validation rate {validation_rate:.1f}% is below 95%")
                    logger.info("  This indicates potential GPU/CPU rule divergence")
                    logger.info("  Skipping auto-import to canonical database")
                    # Jan 7, 2026: Low validation rate is a failure
                    self._record_gpu_job_result(success=False)
                    self._update_gpu_job_count(-1)
                    # Alert on low validation rate
                    asyncio.create_task(self.notifier.send(
                        title="Low GPU Validation Rate",
                        message=f"GPU selfplay validation rate {validation_rate:.1f}% is below 95% threshold",
                        level="warning",
                        fields={
                            "Config": f"{board_type}_{num_players}p",
                            "Valid": str(imported),
                            "Invalid": str(failed),
                            "Rate": f"{validation_rate:.1f}%",
                        },
                        node_id=self.node_id,
                    ))

            else:
                logger.info(f"GPU selfplay {job_id} CPU validation failed:")
                logger.info(f"  {stderr.decode()[:500]}")
                self._record_gpu_job_result(success=False)
                self._update_gpu_job_count(-1)

        except asyncio.TimeoutError:
            logger.info(f"GPU selfplay job {job_id} timed out")
            with self.jobs_lock:
                job = self.local_jobs.get(job_id)
                if job:
                    job.status = "failed"
            self._record_gpu_job_result(success=False)
            self._update_gpu_job_count(-1)
        except Exception as e:  # noqa: BLE001
            logger.info(f"GPU selfplay monitor error for {job_id}: {e}")
            with self.jobs_lock:
                job = self.local_jobs.get(job_id)
                if job:
                    job.status = "failed"
            self._record_gpu_job_result(success=False)
            self._update_gpu_job_count(-1)

    async def _monitor_selfplay_process(
        self,
        job_id: str,
        proc: subprocess.Popen,
        output_dir: Path,
        board_type: str,
        num_players: int,
        job_type_str: str = "selfplay",
    ) -> None:
        """Monitor a selfplay subprocess and update job status on completion.

        Dec 31, 2025: Added to fix missing process monitoring for SELFPLAY
        and CPU_SELFPLAY jobs. Previously, these jobs were spawned but never
        monitored, causing them to remain in "running" status indefinitely.

        This function:
        1. Waits for the subprocess to complete (with 2-hour timeout)
        2. Updates job status to "completed" or "failed"
        3. Logs completion/failure with details
        4. Emits TASK_COMPLETED or TASK_FAILED events for pipeline coordination
        """
        try:
            # Wait for process to complete (with timeout)
            return_code = await asyncio.wait_for(
                asyncio.to_thread(proc.wait),
                timeout=7200,  # 2 hour max
            )

            duration = 0.0
            with self.jobs_lock:
                job = self.local_jobs.get(job_id)
                if job:
                    duration = time.time() - job.started_at
                    if return_code == 0:
                        job.status = "completed"
                        job.completed_at = time.time()
                        logger.info(
                            f"Selfplay job {job_id} completed successfully "
                            f"(duration: {duration:.1f}s)"
                        )
                    else:
                        job.status = "failed"
                        job.completed_at = time.time()
                        # Try to get error message from run.log
                        error_msg = f"exit_code={return_code}"
                        log_file = output_dir / "run.log"
                        if log_file.exists():
                            try:
                                # Get last 500 chars of log for error context
                                content = log_file.read_text(encoding='utf-8', errors='replace')
                                if content:
                                    error_msg = content[-500:].strip()
                            except OSError:
                                pass
                        job.error_message = error_msg
                        logger.warning(
                            f"Selfplay job {job_id} failed (exit code {return_code}): "
                            f"{error_msg[:200]}..."
                        )

            # Emit task events for pipeline coordination
            try:
                from app.coordination.data_events import DataEventType, emit_data_event
                config_key = f"{board_type}_{num_players}p"
                if return_code == 0:
                    emit_data_event(DataEventType.TASK_COMPLETED, {
                        "task_id": job_id,
                        "task_type": job_type_str,
                        "config_key": config_key,
                        "board_type": board_type,
                        "num_players": num_players,
                        "duration_seconds": duration,
                        "node_id": self.node_id,
                    })
                else:
                    emit_data_event(DataEventType.TASK_FAILED, {
                        "task_id": job_id,
                        "task_type": job_type_str,
                        "config_key": config_key,
                        "board_type": board_type,
                        "num_players": num_players,
                        "error": f"exit_code={return_code}",
                        "node_id": self.node_id,
                    })
            except ImportError:
                pass  # Event system not available

        except asyncio.TimeoutError:
            logger.warning(f"Selfplay job {job_id} timed out after 2 hours")
            with self.jobs_lock:
                job = self.local_jobs.get(job_id)
                if job:
                    job.status = "timeout"
                    job.completed_at = time.time()
                    job.error_message = "timeout_2_hours"
            # Kill the process
            try:
                proc.terminate()
                await asyncio.sleep(5)
                if proc.poll() is None:
                    proc.kill()
            except OSError:
                pass

        except Exception as e:  # noqa: BLE001
            logger.error(f"Selfplay process monitor error for {job_id}: {e}")
            with self.jobs_lock:
                job = self.local_jobs.get(job_id)
                if job:
                    job.status = "error"
                    job.completed_at = time.time()
                    job.error_message = str(e)

    async def _schedule_model_comparison(self, job: TrainingJob, new_model_path: str):
        """Schedule a tournament to compare new model against current baseline.

        LEARNED LESSONS - After training, automatically run tournament to:
        1. Compare new model against current best baseline
        2. Update Elo ratings
        3. Promote to best baseline if new model wins
        """
        try:
            config_key = f"{job.board_type}_{job.num_players}p"
            logger.info(f"Scheduling model comparison tournament for {config_key}")

            # Find current baseline model
            baseline_dir = Path(self._get_ai_service_path()) / "models" / job.job_type
            baseline_pattern = f"{job.board_type}_{job.num_players}p_best*"

            baseline_model = None
            for f in baseline_dir.glob(baseline_pattern):
                baseline_model = str(f)
                break

            if not baseline_model:
                # No baseline - this model becomes baseline
                logger.info(f"No baseline found for {config_key}, new model becomes baseline")
                await self._promote_to_baseline(new_model_path, job.board_type, job.num_players, job.job_type)
                return

            # Schedule tournament via SSH tournament system
            tournament_id = f"autoeval_{config_key}_{int(time.time())}"

            # Use existing SSH tournament infrastructure
            with self.ssh_tournament_lock:
                self.ssh_tournament_runs[tournament_id] = SSHTournamentRun(
                    tournament_id=tournament_id,
                    board_type=job.board_type,
                    num_players=job.num_players,
                    status="pending",
                    started_at=time.time(),
                )

            # Start tournament in background
            tournament_config = {
                "tournament_id": tournament_id,
                "board_type": job.board_type,
                "num_players": job.num_players,
                "model_a": new_model_path,
                "model_b": baseline_model,
                "games_per_matchup": 50,
            }
            asyncio.create_task(self._run_model_comparison_tournament(tournament_config))

        except Exception as e:  # noqa: BLE001
            logger.info(f"Model comparison scheduling error: {e}")

    async def _run_model_comparison_tournament(self, config: dict):
        """Run a model comparison tournament and update baseline if new model wins."""
        tournament_id = config["tournament_id"]
        try:
            logger.info(f"Running model comparison tournament {tournament_id}")

            results_dir = Path(self._get_ai_service_path()) / "results" / "tournaments"
            results_dir.mkdir(parents=True, exist_ok=True)

            cmd = [
                sys.executable,
                os.path.join(self._get_ai_service_path(), "scripts", "run_tournament.py"),
                "--player1", f"nn:{config['model_a']}",
                "--player2", f"nn:{config['model_b']}",
                "--board", config["board_type"],
                "--num-players", str(config["num_players"]),
                "--games", str(config["games_per_matchup"]),
                "--output", str(results_dir / f"{tournament_id}.json"),
            ]

            env = os.environ.copy()
            env["PYTHONPATH"] = self._get_ai_service_path()
            env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

            proc = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env,
            )

            _stdout, _stderr = await asyncio.wait_for(proc.communicate(), timeout=3600)

            if proc.returncode == 0:
                results_file = results_dir / f"{tournament_id}.json"
                if results_file.exists():
                    import json as json_module
                    results = json_module.loads(results_file.read_text())
                    new_model_wins = results.get("player1_wins", 0)
                    baseline_wins = results.get("player2_wins", 0)
                    total_games = new_model_wins + baseline_wins

                    win_rate = new_model_wins / total_games if total_games > 0 else 0.5
                    logger.info(f"Tournament {tournament_id}: new model win rate = {win_rate:.1%}")

                    promoted = win_rate >= 0.55
                    if promoted:
                        logger.info("New model beats baseline! Promoting to best baseline.")
                        await self._promote_to_baseline(
                            config["model_a"], config["board_type"],
                            config["num_players"], "nnue" if "nnue" in config["model_a"].lower() else "cmaes"
                        )

                    # Update improvement cycle manager with tournament result
                    await self._handle_tournament_completion(
                        tournament_id,
                        config["board_type"],
                        config["num_players"],
                        config["model_a"],
                        config["model_b"],
                        win_rate,
                        promoted,
                    )

            with self.ssh_tournament_lock:
                if tournament_id in self.ssh_tournament_runs:
                    self.ssh_tournament_runs[tournament_id].status = "completed"
                    self.ssh_tournament_runs[tournament_id].completed_at = time.time()

        except Exception as e:  # noqa: BLE001
            logger.info(f"Tournament {tournament_id} error: {e}")
            with self.ssh_tournament_lock:
                if tournament_id in self.ssh_tournament_runs:
                    self.ssh_tournament_runs[tournament_id].status = "failed"
                    self.ssh_tournament_runs[tournament_id].error = str(e)

    async def _promote_to_baseline(self, model_path: str, board_type: str, num_players: int, model_type: str):
        """Promote a model to the best baseline for its board type."""
        try:
            import shutil
            baseline_dir = Path(self._get_ai_service_path()) / "models" / model_type
            baseline_dir.mkdir(parents=True, exist_ok=True)

            baseline_path = baseline_dir / f"{board_type}_{num_players}p_best.pt"
            if baseline_path.exists():
                backup_path = baseline_dir / f"{board_type}_{num_players}p_prev_{int(time.time())}.pt"
                shutil.copy2(baseline_path, backup_path)
                logger.info(f"Backed up previous baseline to {backup_path}")

            shutil.copy2(model_path, baseline_path)
            logger.info(f"Promoted {model_path} to baseline at {baseline_path}")

            # Dec 2025: Emit MODEL_PROMOTED event for coordination layer
            # Enables: model distribution, model selector hot-reload, temperature adjustment
            config_key = f"{board_type}_{num_players}p"
            model_id = Path(model_path).name
            await self._emit_model_promoted(
                model_id=model_id,
                config_key=config_key,
                elo=0.0,  # Elo not available in this context
                elo_gain=0.0,
                source="p2p_orchestrator._promote_to_baseline",
            )

        except Exception as e:  # noqa: BLE001
            logger.info(f"Baseline promotion error: {e}")

    async def _check_cmaes_auto_tuning(self, config_key: str):
        """Check if CMA-ES auto-tuning should be triggered for a config.

        Monitors Elo progression and triggers hyperparameter optimization
        when the model's improvement plateaus.
        """
        if not HAS_PFSP or config_key not in self.cmaes_auto_tuners:
            return

        try:
            # Get current Elo from unified database
            from app.tournament import get_elo_database
            db = get_elo_database()

            parts = config_key.rsplit("_", 1)
            board_type = parts[0]
            num_players = int(parts[1].replace("p", ""))

            # Find best model for this config
            best_model = None
            best_elo = INITIAL_ELO_RATING
            models_dir = Path(self._get_ai_service_path()) / "models" / "nnue"
            pattern = f"nnue_{board_type}_{num_players}p*.pt"

            for model_path in models_dir.glob(pattern):
                model_id = model_path.stem
                elo = db.get_elo(model_id)
                if elo and elo > best_elo:
                    best_elo = elo
                    best_model = model_id

            if not best_model:
                return

            # Check for plateau
            auto_tuner = self.cmaes_auto_tuners[config_key]
            self.last_cmaes_elo.get(config_key, INITIAL_ELO_RATING)

            # Record Elo history for plateau detection
            should_tune = auto_tuner.check_plateau(best_elo)
            self.last_cmaes_elo[config_key] = best_elo

            if should_tune:
                logger.info(f"[CMA-ES] Elo plateau detected for {config_key} (Elo: {best_elo:.0f})")
                logger.info("[CMA-ES] Triggering auto hyperparameter optimization...")

                # Trigger CMA-ES via existing distributed infrastructure
                await self._trigger_auto_cmaes(board_type, num_players)

        except Exception as e:  # noqa: BLE001
            logger.info(f"[CMA-ES] Auto-tuning check error for {config_key}: {e}")

    def get_pfsp_opponent(self, config_key: str) -> str | None:
        """Get a PFSP-sampled opponent model for selfplay.

        Returns path to an opponent model sampled from the PFSP pool,
        weighted by difficulty (harder opponents sampled more frequently).
        """
        if not HAS_PFSP or config_key not in self.pfsp_pools:
            return None

        try:
            pool = self.pfsp_pools[config_key]
            opponent = pool.sample_opponent()
            if opponent:
                return opponent.model_path
        except Exception as e:  # noqa: BLE001
            logger.error(f"[PFSP] Error sampling opponent: {e}")
        return None

    def update_pfsp_stats(self, config_key: str, model_id: str, win_rate: float, elo: float):
        """Update PFSP stats for a model after evaluation games.

        Called after tournament/evaluation to update opponent difficulty metrics.
        """
        if not HAS_PFSP or config_key not in self.pfsp_pools:
            return

        try:
            self.pfsp_pools[config_key].update_stats(model_id, win_rate=win_rate, elo=elo)
            logger.info(f"[PFSP] Updated stats for {model_id}: win_rate={win_rate:.2f}, elo={elo:.0f}")
        except Exception as e:  # noqa: BLE001
            logger.error(f"[PFSP] Error updating stats: {e}")

    async def _handle_tournament_completion(
        self,
        tournament_id: str,
        board_type: str,
        num_players: int,
        new_model: str,
        baseline_model: str,
        win_rate: float,
        promoted: bool,
    ):
        """Handle tournament completion - update cycle state and trigger next iteration.

        This closes the feedback loop by:
        1. Updating improvement cycle manager with evaluation result
        2. Recording result to unified Elo database
        3. Updating diversity metrics
        4. Boosting selfplay for this config if model was promoted
        """
        try:
            # 1. Update improvement cycle manager
            if self.improvement_cycle_manager:
                self.improvement_cycle_manager.handle_evaluation_complete(
                    board_type, num_players, win_rate, new_model
                )
                logger.info(f"Updated improvement cycle for {board_type}_{num_players}p")

            # 2. Record to unified Elo database
            try:
                from app.tournament import get_elo_database
                db = get_elo_database()
                # Rankings: 0 = winner, 1 = loser
                rankings = [0, 1] if win_rate > 0.5 else [1, 0]
                db.record_match_and_update(
                    participant_ids=[new_model, baseline_model],
                    rankings=rankings,
                    board_type=board_type,
                    num_players=num_players,
                    tournament_id=tournament_id,
                )
                logger.info("Recorded tournament result to unified Elo DB")

                # Trigger Elo sync to propagate to cluster
                if HAS_ELO_SYNC and self.elo_sync_manager:
                    asyncio.create_task(self._trigger_elo_sync_after_matches(1))
            except Exception as e:  # noqa: BLE001
                logger.info(f"Elo database update failed (non-fatal): {e}")

            # 3. Update diversity metrics
            if hasattr(self, 'diversity_metrics'):
                self.diversity_metrics["tournament_runs"] = self.diversity_metrics.get("tournament_runs", 0) + 1
                if promoted:
                    self.diversity_metrics["promotions"] = self.diversity_metrics.get("promotions", 0) + 1

            # 4. Record metrics for observability
            self.record_metric(
                "tournament_win_rate",
                win_rate,
                board_type=board_type,
                num_players=num_players,
                metadata={
                    "new_model": new_model,
                    "baseline_model": baseline_model,
                    "promoted": promoted,
                    "tournament_id": tournament_id,
                },
            )

            # 5. Boost selfplay for this config if promoted (more data for next iteration)
            if promoted:
                asyncio.create_task(self._boost_selfplay_for_config(board_type, num_players))
                # Alert on successful promotion
                asyncio.create_task(self.notifier.send(
                    title="Model Promoted",
                    message=f"New model promoted for {board_type}_{num_players}p with {win_rate*100:.1f}% win rate",
                    level="info",
                    fields={"Model": new_model, "Win Rate": f"{win_rate*100:.1f}%"},
                    node_id=self.node_id,
                ))
            elif win_rate < 0.5:
                # Alert on failed promotion (new model lost)
                asyncio.create_task(self.notifier.send(
                    title="Model Promotion Failed",
                    message=f"New model failed tournament for {board_type}_{num_players}p with only {win_rate*100:.1f}% win rate",
                    level="warning",
                    fields={
                        "Model": new_model,
                        "Win Rate": f"{win_rate*100:.1f}%",
                        "Baseline": baseline_model,
                    },
                    node_id=self.node_id,
                ))

        except Exception as e:  # noqa: BLE001
            logger.info(f"Tournament completion handler error: {e}")
            asyncio.create_task(self.notifier.send(
                title="Tournament Handler Error",
                message=str(e),
                level="error",
                node_id=self.node_id,
            ))

    async def _boost_selfplay_for_config(self, board_type: str, num_players: int):
        """Temporarily boost selfplay for a configuration after model promotion.

        This accelerates data generation for the next training iteration.
        """
        try:
            config_key = f"{board_type}_{num_players}p"
            logger.info(f"Boosting selfplay for {config_key} after promotion")

            # Schedule additional selfplay jobs for this configuration
            # This will be picked up by the next job scheduling cycle
            if hasattr(self, 'selfplay_boost_configs'):
                self.selfplay_boost_configs[config_key] = {
                    "boost_until": time.time() + 3600,  # Boost for 1 hour
                    "multiplier": 1.5,  # 50% more jobs
                }
            else:
                self.selfplay_boost_configs = {
                    config_key: {
                        "boost_until": time.time() + 3600,
                        "multiplier": 1.5,
                    }
                }

        except Exception as e:  # noqa: BLE001
            logger.info(f"Selfplay boost error: {e}")

    async def _propagate_cmaes_weights(
        self, board_type: str, num_players: int, weights: dict[str, float]
    ):
        """Propagate new CMA-ES weights to selfplay workers.

        After CMA-ES optimization finds better weights, this:
        1. Saves weights to shared config file
        2. Restarts selfplay jobs for this config with new weights
        """
        try:
            config_key = f"{board_type}_{num_players}p"
            logger.info(f"Propagating CMA-ES weights for {config_key}")

            # 1. Save to shared heuristic weights config
            config_path = Path(self._get_ai_service_path()) / "config" / "heuristic_weights.json"
            config_path.parent.mkdir(parents=True, exist_ok=True)

            import json as json_mod
            existing = {}
            if config_path.exists():
                with contextlib.suppress(Exception):
                    existing = json_mod.loads(config_path.read_text())

            existing[config_key] = {
                "weights": weights,
                "updated_at": time.time(),
            }
            config_path.write_text(json_mod.dumps(existing, indent=2))
            logger.info(f"Updated heuristic_weights.json with {config_key} weights")

            # 2. Track config for weight-aware selfplay scheduling
            if not hasattr(self, 'cmaes_weight_configs'):
                self.cmaes_weight_configs = {}

            self.cmaes_weight_configs[config_key] = {
                "weights": weights,
                "updated_at": time.time(),
            }

            # 3. Stop existing selfplay jobs for this config (they'll restart with new weights)
            jobs_to_stop = []
            with self.jobs_lock:
                for job_id, job in self.local_jobs.items():
                    if (job.job_type in (JobType.SELFPLAY, JobType.GPU_SELFPLAY, JobType.HYBRID_SELFPLAY, JobType.CPU_SELFPLAY, JobType.GUMBEL_SELFPLAY)
                        and getattr(job, 'board_type', None) == board_type
                        and getattr(job, 'num_players', None) == num_players
                        and job.status == "running"):
                        jobs_to_stop.append(job_id)

            for job_id in jobs_to_stop:
                await self._stop_local_job(job_id)
                logger.info(f"Stopped selfplay job {job_id} for weight update")

            # 4. Boost selfplay to generate data with new weights
            asyncio.create_task(self._boost_selfplay_for_config(board_type, num_players))

            logger.info(f"Weight propagation complete for {config_key}")

        except Exception as e:  # noqa: BLE001
            logger.info(f"CMA-ES weight propagation error: {e}")

    async def _stop_local_job(self, job_id: str):
        """Stop a local job by job ID."""
        try:
            with self.jobs_lock:
                job = self.local_jobs.get(job_id)
                if job and hasattr(job, 'process') and job.process:
                    job.process.terminate()
                    job.status = "stopped"
        except Exception as e:  # noqa: BLE001
            logger.error(f"stopping job {job_id}: {e}")

    async def _import_gpu_selfplay_to_canonical(
        self, validated_db: Path, board_type: str, num_players: int, game_count: int
    ):
        """Import validated GPU selfplay games to canonical selfplay database.

        After GPU selfplay games pass CPU validation (>=95% validation rate),
        this merges them into the canonical selfplay database for training.
        """
        try:
            # Determine canonical DB path
            canonical_db = Path(self._get_ai_service_path()) / "data" / "games" / "selfplay.db"
            if not canonical_db.parent.exists():
                canonical_db.parent.mkdir(parents=True, exist_ok=True)

            logger.info(f"Auto-importing {game_count} validated GPU games to canonical DB...")

            # Jan 12, 2026: Wrap blocking SQLite operations in thread to avoid blocking event loop
            imported = await asyncio.to_thread(
                self._import_gpu_selfplay_sync, validated_db, canonical_db
            )

            logger.info(f"Successfully imported {imported} GPU selfplay games to canonical DB")

            # Update cluster data manifest to reflect new games
            config_key = f"{board_type}_{num_players}p"
            if hasattr(self, 'cluster_data_manifest') and self.cluster_data_manifest and config_key in self.cluster_data_manifest.by_board_type:
                self.cluster_data_manifest.by_board_type[config_key]["total_games"] = (
                    self.cluster_data_manifest.by_board_type[config_key].get("total_games", 0) + imported
                )

            # Notify improvement cycle manager of new games
            if self.improvement_cycle_manager and imported > 0:
                self.improvement_cycle_manager.record_games(board_type, num_players, imported)

        except Exception as e:  # noqa: BLE001
            logger.info(f"GPU selfplay import error: {e}")
            import traceback
            traceback.print_exc()

    def _import_gpu_selfplay_sync(self, validated_db: Path, canonical_db: Path) -> int:
        """Synchronous helper for _import_gpu_selfplay_to_canonical().

        Jan 12, 2026: Extracted to allow wrapping in asyncio.to_thread().
        """
        import sqlite3

        # Phase 3.4 Dec 29, 2025: Use context managers to prevent connection leaks
        with safe_db_connection(validated_db) as src_conn, \
             safe_db_connection(canonical_db) as dst_conn:

                # Ensure destination tables exist
                dst_conn.execute("""
                    CREATE TABLE IF NOT EXISTS games (
                        game_id TEXT PRIMARY KEY,
                        board_type TEXT NOT NULL,
                        num_players INTEGER NOT NULL,
                        winner INTEGER,
                        move_count INTEGER,
                        game_time_ms INTEGER,
                        created_at REAL,
                        source TEXT DEFAULT 'selfplay'
                    )
                """)
                dst_conn.execute("""
                    CREATE TABLE IF NOT EXISTS moves (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        game_id TEXT NOT NULL,
                        move_number INTEGER NOT NULL,
                        player INTEGER NOT NULL,
                        move_type TEXT NOT NULL,
                        from_pos TEXT,
                        to_pos TEXT,
                        direction TEXT,
                        captured_pos TEXT,
                        state_before TEXT,
                        policy_probs TEXT,
                        value_est REAL,
                        FOREIGN KEY (game_id) REFERENCES games(game_id)
                    )
                """)
                dst_conn.execute("""
                    CREATE INDEX IF NOT EXISTS idx_moves_game_id ON moves(game_id)
                """)
                dst_conn.commit()

                # Check source schema and copy games
                src_cursor = src_conn.cursor()
                src_cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                src_tables = {row[0] for row in src_cursor.fetchall()}

                imported = 0
                if "games" in src_tables:
                    # Get existing game IDs in destination to avoid duplicates
                    dst_cursor = dst_conn.cursor()
                    dst_cursor.execute("SELECT game_id FROM games")
                    existing_ids = {row[0] for row in dst_cursor.fetchall()}

                    # Copy games that don't already exist
                    src_cursor.execute("SELECT * FROM games")
                    src_columns = [desc[0] for desc in src_cursor.description]

                    for row in src_cursor.fetchall():
                        game_id_idx = src_columns.index("game_id") if "game_id" in src_columns else 0
                        game_id = row[game_id_idx]

                        if game_id in existing_ids:
                            continue

                        # Insert game with proper column mapping
                        placeholders = ", ".join(["?"] * len(row))
                        columns = ", ".join(src_columns)
                        try:
                            dst_conn.execute(
                                f"INSERT OR IGNORE INTO games ({columns}) VALUES ({placeholders})",
                                row
                            )
                            imported += 1
                        except (AttributeError):
                            continue

                    # Copy moves for new games
                    if "moves" in src_tables and imported > 0:
                        src_cursor.execute("SELECT * FROM moves")
                        move_columns = [desc[0] for desc in src_cursor.description]
                        move_placeholders = ", ".join(["?"] * len(move_columns))
                        move_col_str = ", ".join(move_columns)

                        for row in src_cursor.fetchall():
                            game_id_idx = move_columns.index("game_id") if "game_id" in move_columns else 1
                            game_id = row[game_id_idx]
                            if game_id not in existing_ids:
                                try:
                                    dst_conn.execute(
                                        f"INSERT OR IGNORE INTO moves ({move_col_str}) VALUES ({move_placeholders})",
                                        row
                                    )
                                except (AttributeError):
                                    continue

                    dst_conn.commit()

        return imported

    # =========================================================================

    # =========================================================================
    # NOTE: Improvement Cycle handlers moved to ImprovementHandlersMixin
    # See: scripts/p2p/handlers/improvement.py (Dec 28, 2025 - Phase 8)
    # =========================================================================


    # handle_metrics moved to MetricsHandlersMixin (Jan 2026 - P2P Modularization Phase 4b)
    # handle_metrics_prometheus moved to MetricsHandlersMixin (Jan 2026 - P2P Modularization)
    # handle_improvement_training_complete and handle_improvement_evaluation_complete
    # moved to ImprovementHandlersMixin (Dec 28, 2025 - Phase 8)

    async def _schedule_improvement_evaluation(self, cycle_id: str, new_model_id: str):
        """Schedule tournament evaluation for a newly trained model via SSH."""
        if not self.improvement_cycle_manager:
            return
        try:
            cycle = self.improvement_cycle_manager.state.cycles.get(cycle_id)
            if not cycle:
                return

            config = cycle.config
            best_model_id = cycle.best_model_id or f"baseline_{config.board_type}_{config.num_players}p"

            logger.info(f"ImprovementCycle {cycle_id}: Scheduling evaluation {new_model_id} vs {best_model_id}")

            self.improvement_cycle_manager.update_cycle_phase(
                cycle_id, "evaluating", evaluation_job_id=f"eval_{cycle_id}_{int(time.time())}"
            )

            # Run SSH tournament evaluation
            eval_result = await self._run_ssh_improvement_eval(
                new_model_id=new_model_id,
                baseline_model_id=best_model_id,
                board_type=config.board_type,
                num_players=config.num_players,
                games=config.evaluation_games,
            )

            if eval_result.get("success"):
                new_model_wins = eval_result.get("new_model_wins", 0)
                baseline_wins = eval_result.get("baseline_wins", 0)
                draws = eval_result.get("draws", 0)
            else:
                # Fallback to mock results if SSH evaluation fails
                logger.info(f"ImprovementCycle {cycle_id}: SSH evaluation failed, using fallback")
                import random
                total_games = config.evaluation_games
                new_model_wins = random.randint(int(total_games * 0.4), int(total_games * 0.6))
                draws = random.randint(0, int(total_games * 0.1))
                baseline_wins = total_games - new_model_wins - draws

            self.improvement_cycle_manager.handle_evaluation_complete(
                cycle_id=cycle_id, new_model_id=new_model_id, best_model_id=best_model_id,
                wins=new_model_wins, losses=baseline_wins, draws=draws,
            )

        except Exception as e:  # noqa: BLE001
            logger.info(f"ImprovementCycle {cycle_id}: Evaluation scheduling failed: {e}")
            if self.improvement_cycle_manager:
                self.improvement_cycle_manager.update_cycle_phase(cycle_id, "idle", error_message=str(e))

    async def _run_ssh_improvement_eval(
        self,
        new_model_id: str,
        baseline_model_id: str,
        board_type: str,
        num_players: int,
        games: int,
    ) -> dict:
        """Run improvement evaluation via SSH on a remote host.

        Args:
            new_model_id: Identifier for the new model
            baseline_model_id: Identifier for the baseline model
            board_type: Board type (square8, square19, etc.)
            num_players: Number of players
            games: Number of games to play

        Returns:
            Dict with evaluation results or error
        """
        # Calculate timeout upfront to avoid scope issues in exception handler
        timeout_seconds = max(300, games * 30)  # 30s per game estimate, minimum 5 minutes

        try:
            # Get available hosts for evaluation
            if load_remote_hosts is None:
                return {"success": False, "error": "load_remote_hosts not available"}

            hosts = load_remote_hosts()
            if not hosts:
                return {"success": False, "error": "No remote hosts configured"}

            # Find a ready host with GPU capability (prefer high-performance hosts)
            eval_host = None
            for host in hosts:
                if getattr(host, 'status', None) == 'ready':
                    eval_host = host
                    break

            if not eval_host:
                # Try any host
                eval_host = hosts[0] if hosts else None

            if not eval_host:
                return {"success": False, "error": "No evaluation host available"}

            ssh_host = getattr(eval_host, 'ssh_host', None) or getattr(eval_host, 'tailscale_ip', None)
            if not ssh_host:
                return {"success": False, "error": "No SSH host configured"}

            ssh_user = getattr(eval_host, 'ssh_user', 'ubuntu')
            ringrift_path = getattr(eval_host, 'ringrift_path', '~/ringrift/ai-service')

            # Build model paths (assumes models are in standard locations)
            new_model_path = f"models/{board_type}_{num_players}p/{new_model_id}.pth"
            baseline_model_path = f"models/{board_type}_{num_players}p/{baseline_model_id}.pth"

            # Build SSH command
            remote_cmd = f'''cd {ringrift_path} && source venv/bin/activate && python scripts/run_improvement_eval.py \
                --new-model "{new_model_path}" \
                --baseline-model "{baseline_model_path}" \
                --board {board_type} \
                --players {num_players} \
                --games {games} \
                --ai-type descent 2>/dev/null'''

            logger.info(f"Running SSH evaluation on {eval_host.name}: {new_model_id} vs {baseline_model_id}")

            proc = await asyncio.create_subprocess_exec(
                "ssh",
                "-o", "ConnectTimeout=30",
                "-o", "BatchMode=yes",
                "-o", "StrictHostKeyChecking=no",
                f"{ssh_user}@{ssh_host}",
                remote_cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )

            stdout, stderr = await asyncio.wait_for(
                proc.communicate(),
                timeout=timeout_seconds
            )

            if proc.returncode != 0:
                stderr_text = stderr.decode()[:500] if stderr else ""
                logger.info(f"SSH evaluation failed on {eval_host.name}: {stderr_text}")
                return {"success": False, "error": f"SSH command failed: {stderr_text}"}

            # Parse JSON result from stdout
            stdout_text = stdout.decode().strip()
            if not stdout_text:
                return {"success": False, "error": "No output from evaluation script"}

            result = json.loads(stdout_text)
            logger.info(f"SSH evaluation complete: {result.get('new_model_wins', 0)}-{result.get('baseline_wins', 0)}-{result.get('draws', 0)}")
            return result

        except asyncio.TimeoutError:
            return {"success": False, "error": f"SSH evaluation timed out after {timeout_seconds}s"}
        except json.JSONDecodeError as e:
            return {"success": False, "error": f"Failed to parse evaluation result: {e}"}
        except Exception as e:  # noqa: BLE001
            return {"success": False, "error": str(e)}

    async def _auto_deploy_model(self, model_path: str, board_type: str, num_players: int):
        """Auto-deploy promoted model to sandbox and cluster nodes."""
        try:
            import subprocess
            logger.info(f"Auto-deploying model: {model_path}")

            # Build command args
            cmd_args = [
                sys.executable, "scripts/auto_deploy_models.py",
                "--model-path", model_path,
                "--board-type", board_type,
                "--num-players", str(num_players),
                "--skip-eval",  # Already evaluated
            ]
            if self._is_leader():
                cmd_args.append("--sync-cluster")

            # Run deployment script
            result = await asyncio.to_thread(
                subprocess.run,
                cmd_args,
                capture_output=True,
                text=True,
                timeout=300,
                cwd=str(Path(__file__).parent.parent),
            )

            if result.returncode == 0:
                logger.info(f"Model deployed successfully: {model_path}")
            else:
                logger.info(f"Model deployment failed: {result.stderr}")

        except Exception as e:  # noqa: BLE001
            logger.info(f"Auto-deploy error: {e}")

    # Canonical Pipeline Integration (for pipeline_orchestrator.py)
    # =========================================================================

    async def handle_pipeline_start(self, request: web.Request) -> web.Response:
        """POST /pipeline/start - Start a canonical pipeline phase."""
        try:
            if not self._is_leader() and request.query.get("local") != "1":
                return await self._proxy_to_leader(request)
            if not self._is_leader():
                return web.json_response({"success": False, "error": "Only leader can start pipeline phases",
                                         "leader_id": self.leader_id}, status=403)
            data = await request.json()
            phase = data.get("phase")
            board_type = data.get("board_type", "square8")
            num_players = data.get("num_players", 2)

            if phase == "canonical_selfplay":
                result = await self._start_canonical_selfplay_pipeline(
                    board_type,
                    num_players,
                    data.get("games_per_node", 500),
                    data.get("seed", 0),
                    include_gpu_nodes=bool(data.get("include_gpu_nodes", False)),
                )
            elif phase == "parity_validation":
                result = await self._start_parity_validation_pipeline(
                    board_type, num_players, data.get("db_paths"))
            elif phase == "npz_export":
                result = await self._start_npz_export_pipeline(
                    board_type, num_players, data.get("output_dir", "data/training"))
            else:
                return web.json_response({"success": False,
                    "error": f"Unknown phase: {phase}. Supported: canonical_selfplay, parity_validation, npz_export"}, status=400)
            return web.json_response(result)
        except Exception as e:  # noqa: BLE001
            logger.info(f"Pipeline start error: {e}")
            return web.json_response({"success": False, "error": str(e)}, status=500)

    # handle_pipeline_status moved to MetricsHandlersMixin (Jan 2026 - P2P Modularization Phase 4b)

    async def _start_canonical_selfplay_pipeline(
        self,
        board_type: str,
        num_players: int,
        games_per_node: int,
        seed: int,
        include_gpu_nodes: bool = False,
    ) -> dict[str, Any]:
        """Start canonical selfplay on healthy nodes in the cluster.

        Canonical selfplay is CPU-bound. By default, prefer CPU-only nodes so GPU
        machines remain available for GPU-utilizing tasks (training/hybrid selfplay).
        """
        job_id = f"pipeline-selfplay-{int(time.time())}"
        healthy_nodes: list[tuple[str, NodeInfo]] = []
        with self.peers_lock:
            for peer_id, peer in self.peers.items():
                if peer.is_alive() and peer.is_healthy():
                    healthy_nodes.append((peer_id, peer))
        if self.self_info.is_healthy():
            healthy_nodes.append((self.node_id, self.self_info))

        if not include_gpu_nodes:
            cpu_nodes = [(nid, n) for nid, n in healthy_nodes if n.is_cpu_only_node()]
            if cpu_nodes:
                healthy_nodes = cpu_nodes

        # Load-balance: least-loaded nodes first.
        healthy_nodes.sort(key=lambda pair: pair[1].get_load_score())

        if not healthy_nodes:
            return {"success": False, "error": "No healthy nodes available"}

        logger.info(f"Starting canonical selfplay pipeline: {len(healthy_nodes)} nodes, {games_per_node} games/node")
        dispatched = 0
        for i, (node_id, node) in enumerate(healthy_nodes):
            node_seed = seed + i * 10000 + hash(node_id) % 10000
            if node_id == self.node_id:
                asyncio.create_task(self._run_local_canonical_selfplay(
                    f"{job_id}-{node_id}", board_type, num_players, games_per_node, node_seed))
                dispatched += 1
            else:
                try:
                    if getattr(node, "nat_blocked", False):
                        payload = {
                            "job_id": f"{job_id}-{node_id}",
                            "board_type": board_type,
                            "num_players": num_players,
                            "num_games": games_per_node,
                            "seed": node_seed,
                        }
                        cmd_id = await self._enqueue_relay_command_for_peer(node, "canonical_selfplay", payload)
                        if cmd_id:
                            dispatched += 1
                        else:
                            logger.info(f"Relay queue full; skipping canonical selfplay enqueue for {node_id}")
                    else:
                        payload = {
                            "job_id": f"{job_id}-{node_id}",
                            "board_type": board_type,
                            "num_players": num_players,
                            "num_games": games_per_node,
                            "seed": node_seed,
                        }
                        async with get_client_session(ClientTimeout(total=30)) as session:
                            for url in self._urls_for_peer(node, "/pipeline/selfplay_worker"):
                                try:
                                    async with session.post(url, json=payload, headers=self._get_auth_headers()) as resp:
                                        if resp.status == 200:
                                            dispatched += 1
                                            break
                                except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
                                    continue
                except Exception as e:  # noqa: BLE001
                    logger.error(f"Failed to dispatch selfplay to {node_id}: {e}")

        self._pipeline_status = {"job_id": job_id, "phase": "canonical_selfplay", "status": "running",
            "dispatched_count": dispatched, "total_nodes": len(healthy_nodes),
            "board_type": board_type, "num_players": num_players,
            "games_per_node": games_per_node, "started_at": time.time()}
        return {"success": True, "job_id": job_id, "dispatched_count": dispatched, "total_nodes": len(healthy_nodes)}

    async def _run_local_canonical_selfplay(self, job_id: str, board_type: str, num_players: int,
                                            num_games: int, seed: int):
        """Run canonical selfplay locally."""
        try:
            db_file = os.path.join(self._get_ai_service_path(), "data", "games",
                                   f"canonical_{board_type}_{num_players}p_{self.node_id}.db")
            log_file = os.path.join(self._get_ai_service_path(), "logs", "selfplay",
                                    f"canonical_{job_id}.jsonl")
            os.makedirs(os.path.dirname(db_file), exist_ok=True)
            os.makedirs(os.path.dirname(log_file), exist_ok=True)

            cmd = [sys.executable, os.path.join(self._get_ai_service_path(), "scripts", "run_self_play_soak.py"),
                "--num-games", str(num_games), "--board-type", board_type, "--num-players", str(num_players),
                "--max-moves", "10000",  # LEARNED LESSONS - Avoid draws due to move limit
                "--difficulty-band", "light", "--seed", str(seed), "--log-jsonl", log_file, "--record-db", db_file]
            env = os.environ.copy()
            env["PYTHONPATH"] = self._get_ai_service_path()
            env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

            logger.info(f"Starting canonical selfplay job {job_id}: {num_games} games -> {db_file}")
            proc = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE,
                                                        stderr=asyncio.subprocess.PIPE, env=env)
            _stdout, stderr = await proc.communicate()
            if proc.returncode == 0:
                logger.info(f"Canonical selfplay job {job_id} completed successfully")
            else:
                logger.info(f"Canonical selfplay job {job_id} failed: {stderr.decode()[:500]}")
        except Exception as e:  # noqa: BLE001
            logger.info(f"Canonical selfplay job {job_id} error: {e}")

    async def _start_parity_validation_pipeline(self, board_type: str, num_players: int,
                                                db_paths: list[str] | None) -> dict[str, Any]:
        """Start parity validation on the leader node."""
        job_id = f"pipeline-parity-{int(time.time())}"
        asyncio.create_task(self._run_parity_validation(job_id, board_type, num_players, db_paths))
        self._pipeline_status = {"job_id": job_id, "phase": "parity_validation", "status": "running",
                                "board_type": board_type, "num_players": num_players, "started_at": time.time()}
        return {"success": True, "job_id": job_id, "message": "Parity validation started"}

    async def _run_parity_validation(self, job_id: str, board_type: str, num_players: int,
                                     db_paths: list[str] | None):
        """Run parity validation."""
        try:
            if not db_paths:
                import glob
                db_paths = glob.glob(os.path.join(self._get_ai_service_path(), "data", "games",
                                                  f"canonical_{board_type}_{num_players}p_*.db"))
            if not db_paths:
                self._pipeline_status["status"] = "failed"
                self._pipeline_status["error"] = "No databases found"
                return

            output_json = os.path.join(self._get_ai_service_path(), "data", f"parity_validation_{job_id}.json")
            cmd = [sys.executable, os.path.join(self._get_ai_service_path(), "scripts", "run_parity_validation.py"),
                "--databases", *db_paths, "--mode", "canonical", "--output-json", output_json, "--progress-every", "100"]
            env = os.environ.copy()
            env["PYTHONPATH"] = self._get_ai_service_path()
            env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

            logger.info(f"Starting parity validation job {job_id}: {len(db_paths)} databases")
            proc = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE,
                                                        stderr=asyncio.subprocess.PIPE, env=env)
            _stdout, stderr = await proc.communicate()
            if proc.returncode == 0:
                logger.info(f"Parity validation job {job_id} completed successfully")
                self._pipeline_status["status"] = "completed"
                if os.path.exists(output_json):
                    with open(output_json) as f:
                        self._pipeline_status["results"] = json.load(f)
            else:
                logger.info(f"Parity validation job {job_id} failed: {stderr.decode()[:500]}")
                self._pipeline_status["status"] = "failed"
                self._pipeline_status["error"] = stderr.decode()[:500]
        except Exception as e:  # noqa: BLE001
            logger.info(f"Parity validation job {job_id} error: {e}")
            self._pipeline_status["status"] = "failed"
            self._pipeline_status["error"] = str(e)

    async def _start_npz_export_pipeline(self, board_type: str, num_players: int,
                                         output_dir: str) -> dict[str, Any]:
        """Start NPZ export on the leader node."""
        job_id = f"pipeline-npz-{int(time.time())}"
        asyncio.create_task(self._run_npz_export(job_id, board_type, num_players, output_dir))
        self._pipeline_status = {"job_id": job_id, "phase": "npz_export", "status": "running",
                                "board_type": board_type, "num_players": num_players,
                                "output_dir": output_dir, "started_at": time.time()}
        return {"success": True, "job_id": job_id, "message": "NPZ export started"}

    async def _run_npz_export(self, job_id: str, board_type: str, num_players: int, output_dir: str):
        """Run NPZ export."""
        try:
            import glob
            db_paths = glob.glob(os.path.join(self._get_ai_service_path(), "data", "games",
                                              f"canonical_{board_type}_{num_players}p_*.db"))
            if not db_paths:
                self._pipeline_status["status"] = "failed"
                self._pipeline_status["error"] = "No databases found"
                return

            full_output_dir = os.path.join(self._get_ai_service_path(), output_dir)
            os.makedirs(full_output_dir, exist_ok=True)
            output_file = os.path.join(full_output_dir, f"canonical_{board_type}_{num_players}p_{job_id}.npz")

            cmd = [sys.executable, os.path.join(self._get_ai_service_path(), "scripts", "export_replay_dataset.py"),
                "--databases", *db_paths, "--output", output_file, "--board-type", board_type,
                "--num-players", str(num_players)]
            env = os.environ.copy()
            env["PYTHONPATH"] = self._get_ai_service_path()

            logger.info(f"Starting NPZ export job {job_id}: {len(db_paths)} databases -> {output_file}")
            proc = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE,
                                                        stderr=asyncio.subprocess.PIPE, env=env)
            _stdout, stderr = await proc.communicate()
            if proc.returncode == 0:
                logger.info(f"NPZ export job {job_id} completed successfully")
                self._pipeline_status["status"] = "completed"
                self._pipeline_status["output_file"] = output_file
            else:
                logger.info(f"NPZ export job {job_id} failed: {stderr.decode()[:500]}")
                self._pipeline_status["status"] = "failed"
                self._pipeline_status["error"] = stderr.decode()[:500]
        except Exception as e:  # noqa: BLE001
            logger.info(f"NPZ export job {job_id} error: {e}")
            self._pipeline_status["status"] = "failed"
            self._pipeline_status["error"] = str(e)

    def _get_auth_headers(self) -> dict[str, str]:
        """Get authentication headers for peer requests."""
        return {"Authorization": f"Bearer {self.auth_token}"} if self.auth_token else {}

    # =========================================================================
    # Phase 4: REST API for External Job Submission and Dashboard
    # =========================================================================


    # NOTE: Cluster API handlers moved to ClusterApiHandlersMixin (Jan 2026 - P2P Modularization)
    # - handle_api_cluster_status, handle_api_cluster_git_update
    # See scripts/p2p/handlers/cluster_api.py for implementation.

    # NOTE: Dashboard handlers moved to DashboardHandlersMixin (Jan 2026 - P2P Modularization Phase 2a)
    # - handle_root, handle_dashboard, handle_work_queue_dashboard, handle_resource_optimizer
    # See scripts/p2p/handlers/dashboard.py for implementation.

    # handle_api_selfplay_stats moved to SelfplayHandlersMixin (Jan 2026 - P2P Modularization)
    # handle_api_elo_leaderboard moved to EloAnalyticsHandlersMixin (Jan 2026 - P2P Modularization Phase 4a)

    # handle_elo_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)
    # handle_nodes_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    async def _get_victory_type_stats(self) -> dict[tuple[str, int, str], int]:
        """Aggregate victory types from recent game data.

        Returns dict mapping (board_type, num_players, victory_type) -> count.
        Caches results for 5 minutes to avoid excessive I/O.
        """
        import json
        from collections import defaultdict

        cache_key = "_victory_stats_cache"
        cache_time_key = "_victory_stats_cache_time"
        cache_ttl = 300  # 5 minutes

        # Check cache
        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        # Skip JSONL scanning during startup grace period
        if self._is_in_startup_grace_period():
            return {}

        stats: dict[tuple[str, int, str], int] = defaultdict(int)

        # Scan recent game files (last 24 hours)
        ai_root = Path(self._get_ai_service_path())
        data_dirs = [
            ai_root / "data" / "games" / "daemon_sync",
            ai_root / "data" / "selfplay",
        ]

        cutoff_time = now - 86400  # 24 hours ago

        for data_dir in data_dirs:
            if not data_dir.exists():
                continue
            for jsonl_path in data_dir.rglob("*.jsonl"):
                try:
                    # Skip files older than 24h
                    if jsonl_path.stat().st_mtime < cutoff_time:
                        continue
                    with open_jsonl_file(jsonl_path) as f:
                        for line in f:
                            try:
                                game = json.loads(line)
                                board_type = game.get("board_type", "unknown")
                                num_players = game.get("num_players", 0)
                                victory_type = game.get("victory_type", "unknown")
                                if victory_type and victory_type != "unknown":
                                    stats[(board_type, num_players, victory_type)] += 1
                            except json.JSONDecodeError:
                                continue
                except (json.JSONDecodeError, AttributeError):
                    continue

        # Update cache
        setattr(self, cache_key, dict(stats))
        setattr(self, cache_time_key, now)

        return dict(stats)

    async def _get_game_analytics_cached(self) -> dict[str, Any]:
        """Get game analytics with caching (5 min TTL)."""
        import json
        from collections import defaultdict

        cache_key = "_game_analytics_cache"
        cache_time_key = "_game_analytics_cache_time"
        cache_ttl = 300

        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        # Skip JSONL scanning during startup grace period
        if self._is_in_startup_grace_period():
            return {"configs": {}}

        hours = 24
        cutoff = now - (hours * 3600)

        ai_root = Path(self._get_ai_service_path())
        data_dirs = [
            ai_root / "data" / "games" / "daemon_sync",
            ai_root / "data" / "selfplay",
        ]

        game_lengths: dict[str, list[int]] = defaultdict(list)
        games_by_hour: dict[str, dict[int, int]] = defaultdict(lambda: defaultdict(int))
        opening_moves: dict[str, dict[str, int]] = defaultdict(lambda: defaultdict(int))

        for data_dir in data_dirs:
            if not data_dir.exists():
                continue
            for jsonl_path in data_dir.rglob("*.jsonl"):
                try:
                    if jsonl_path.stat().st_mtime < cutoff:
                        continue
                    with open_jsonl_file(jsonl_path) as f:
                        for line in f:
                            try:
                                game = json.loads(line)
                                board_type = game.get("board_type", "unknown")
                                num_players = game.get("num_players", 0)
                                config = f"{board_type}_{num_players}p"

                                length = game.get("length", 0)
                                if length > 0:
                                    game_lengths[config].append(length)

                                hour_bucket = int(jsonl_path.stat().st_mtime // 3600)
                                games_by_hour[config][hour_bucket] += 1

                                moves = game.get("moves", [])
                                if moves and len(moves) >= 1:
                                    first_move = str(moves[0].get("action", ""))[:20]
                                    if first_move:
                                        opening_moves[config][first_move] += 1
                            except json.JSONDecodeError:
                                continue
                except (json.JSONDecodeError, ValueError, AttributeError):
                    continue

        analytics = {"configs": {}}
        for config in set(list(game_lengths.keys()) + list(games_by_hour.keys())):
            lengths = game_lengths.get(config, [])
            hourly = games_by_hour.get(config, {})
            openings = opening_moves.get(config, {})
            throughput = sum(hourly.values()) / max(len(hourly), 1) if hourly else 0

            analytics["configs"][config] = {
                "avg_length": round(sum(lengths) / len(lengths), 1) if lengths else 0,
                "throughput_per_hour": round(throughput, 1),
                "opening_diversity": len(openings),
            }

        setattr(self, cache_key, analytics)
        setattr(self, cache_time_key, now)
        return analytics

    async def _get_training_metrics_cached(self) -> dict[str, Any]:
        """Get training metrics with caching (2 min TTL)."""
        import re

        cache_key = "_training_metrics_cache"
        cache_time_key = "_training_metrics_cache_time"
        cache_ttl = 120

        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        ai_root = Path(self._get_ai_service_path())
        logs_dir = ai_root / "logs" / "training"

        metrics = {"configs": {}}

        if logs_dir.exists():
            log_files = sorted(logs_dir.glob("*.log"), key=lambda f: f.stat().st_mtime, reverse=True)[:10]

            for log_file in log_files:
                try:
                    content = log_file.read_text()
                    config_match = re.search(r"(square\d+|hexagonal|hex)_(\d+)p", log_file.name)
                    if not config_match:
                        continue
                    config = f"{config_match.group(1)}_{config_match.group(2)}p"

                    loss_pattern = re.compile(r"[Ee]poch\s+(\d+).*?loss[=:]\s*([\d.]+)")
                    epochs = []
                    for match in loss_pattern.finditer(content):
                        epochs.append({
                            "epoch": int(match.group(1)),
                            "loss": float(match.group(2)),
                        })

                    if epochs:
                        metrics["configs"][config] = {
                            "latest_loss": epochs[-1]["loss"],
                            "latest_epoch": epochs[-1]["epoch"],
                        }
                except (OSError, ValueError, KeyError, IndexError, AttributeError):
                    continue

        setattr(self, cache_key, metrics)
        setattr(self, cache_time_key, now)
        return metrics

    async def _get_holdout_metrics_cached(self) -> dict[str, Any]:
        """Get holdout validation metrics with caching (5 min TTL)."""
        import sqlite3

        cache_key = "_holdout_metrics_cache"
        cache_time_key = "_holdout_metrics_cache_time"
        cache_ttl = 300

        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        ai_root = Path(self._get_ai_service_path())
        db_path = ai_root / "data" / "holdouts" / "holdout_validation.db"

        metrics = {"configs": {}, "evaluations": [], "summary": {}}

        if not db_path.exists():
            setattr(self, cache_key, metrics)
            setattr(self, cache_time_key, now)
            return metrics

        try:
            # Phase 3.4 Dec 29, 2025: Use context manager to prevent connection leaks
            with safe_db_connection(db_path) as conn:
                conn.row_factory = sqlite3.Row
                cursor = conn.cursor()

                # Get holdout game counts by config
                cursor.execute("""
                    SELECT board_type, num_players, COUNT(*) as game_count, SUM(num_positions) as total_positions
                    FROM holdout_games
                    GROUP BY board_type, num_players
                """)
                for row in cursor.fetchall():
                    config = f"{row['board_type']}_{row['num_players']}p"
                    metrics["configs"][config] = {
                        "holdout_games": row["game_count"],
                        "holdout_positions": row["total_positions"] or 0,
                    }

                # Get latest evaluations per config
                cursor.execute("""
                    SELECT model_path, board_type, num_players, holdout_loss, holdout_accuracy,
                           train_loss, num_samples, evaluated_at, overfit_gap
                    FROM evaluations
                    WHERE id IN (
                        SELECT MAX(id) FROM evaluations
                        GROUP BY board_type, num_players
                    )
                    ORDER BY evaluated_at DESC
                """)
                for row in cursor.fetchall():
                    config = f"{row['board_type']}_{row['num_players']}p"
                    eval_data = {
                        "config": config,
                        "model": row["model_path"],
                        "holdout_loss": row["holdout_loss"],
                        "holdout_accuracy": row["holdout_accuracy"],
                        "train_loss": row["train_loss"],
                        "overfit_gap": row["overfit_gap"],
                        "num_samples": row["num_samples"],
                        "evaluated_at": row["evaluated_at"],
                    }
                    metrics["evaluations"].append(eval_data)
                    # Update config metrics
                    if config in metrics["configs"]:
                        metrics["configs"][config].update({
                            "holdout_loss": row["holdout_loss"],
                            "holdout_accuracy": row["holdout_accuracy"],
                            "overfit_gap": row["overfit_gap"],
                        })

                # Get summary stats
                cursor.execute("SELECT COUNT(*) FROM holdout_games")
                metrics["summary"]["total_holdout_games"] = cursor.fetchone()[0]
                cursor.execute("SELECT COUNT(*) FROM evaluations")
                metrics["summary"]["total_evaluations"] = cursor.fetchone()[0]
        except (sqlite3.Error, OSError, KeyError, IndexError, TypeError):
            pass

        setattr(self, cache_key, metrics)
        setattr(self, cache_time_key, now)
        return metrics

    async def _get_mcts_stats_cached(self) -> dict[str, Any]:
        """Get MCTS search statistics with caching (2 min TTL)."""
        import json
        import re

        cache_key = "_mcts_stats_cache"
        cache_time_key = "_mcts_stats_cache_time"
        cache_ttl = 120

        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        # Skip JSONL scanning during startup grace period
        if self._is_in_startup_grace_period():
            return {"configs": {}, "summary": {}}

        ai_root = Path(self._get_ai_service_path())
        stats = {"configs": {}, "summary": {}}

        # Parse selfplay logs for MCTS stats
        logs_dir = ai_root / "logs" / "selfplay"
        if logs_dir.exists():
            log_files = sorted(logs_dir.glob("*.log"), key=lambda f: f.stat().st_mtime, reverse=True)[:20]

            nodes_per_move = []
            depth_stats = []
            time_per_move = []

            for log_file in log_files:
                try:
                    content = log_file.read_text(errors='ignore')
                    # Parse MCTS stats patterns (nodes visited, search depth, time)
                    # Pattern: "nodes: 1234" or "nodes_visited: 1234"
                    for match in re.finditer(r'nodes[_\s]*(?:visited)?[:\s]*(\d+)', content, re.I):
                        nodes_per_move.append(int(match.group(1)))
                    # Pattern: "depth: 12" or "search_depth: 12"
                    for match in re.finditer(r'(?:search_)?depth[:\s]*(\d+)', content, re.I):
                        depth_stats.append(int(match.group(1)))
                    # Pattern: "time: 0.123s" or "move_time: 123ms"
                    for match in re.finditer(r'(?:move_)?time[:\s]*([\d.]+)\s*(?:s|ms)?', content, re.I):
                        time_per_move.append(float(match.group(1)))
                except (ValueError, KeyError, IndexError, AttributeError):
                    continue

            if nodes_per_move:
                stats["summary"]["avg_nodes_per_move"] = sum(nodes_per_move) / len(nodes_per_move)
                stats["summary"]["max_nodes_per_move"] = max(nodes_per_move)
            if depth_stats:
                stats["summary"]["avg_search_depth"] = sum(depth_stats) / len(depth_stats)
                stats["summary"]["max_search_depth"] = max(depth_stats)
            if time_per_move:
                stats["summary"]["avg_time_per_move"] = sum(time_per_move) / len(time_per_move)

        # Also check game JSONL files for MCTS metadata
        data_dirs = [
            ai_root / "data" / "games" / "daemon_sync",
            ai_root / "data" / "selfplay",
        ]
        cutoff = now - 3600  # Last hour

        for data_dir in data_dirs:
            if not data_dir.exists():
                continue
            for jsonl_path in data_dir.rglob("*.jsonl"):
                try:
                    if jsonl_path.stat().st_mtime < cutoff:
                        continue
                    with open_jsonl_file(jsonl_path) as f:
                        for line in f:
                            try:
                                game = json.loads(line)
                                board_type = game.get("board_type", "unknown")
                                num_players = game.get("num_players", 0)
                                config = f"{board_type}_{num_players}p"

                                # Check for MCTS metadata in game
                                mcts_data = game.get("mcts_stats", {})
                                if mcts_data:
                                    if config not in stats["configs"]:
                                        stats["configs"][config] = {
                                            "nodes_samples": [],
                                            "depth_samples": [],
                                        }
                                    if "avg_nodes" in mcts_data:
                                        stats["configs"][config]["nodes_samples"].append(mcts_data["avg_nodes"])
                                    if "avg_depth" in mcts_data:
                                        stats["configs"][config]["depth_samples"].append(mcts_data["avg_depth"])
                            except json.JSONDecodeError:
                                continue
                except (json.JSONDecodeError, AttributeError):
                    continue

        # Compute per-config averages
        for _config, data in stats["configs"].items():
            if data.get("nodes_samples"):
                data["avg_nodes"] = sum(data["nodes_samples"]) / len(data["nodes_samples"])
            if data.get("depth_samples"):
                data["avg_depth"] = sum(data["depth_samples"]) / len(data["depth_samples"])
            # Clean up sample lists
            data.pop("nodes_samples", None)
            data.pop("depth_samples", None)

        setattr(self, cache_key, stats)
        setattr(self, cache_time_key, now)
        return stats

    # =========================================================================
    # Feature 1: Tournament Matchup Analysis
    # =========================================================================

    async def _get_matchup_matrix_cached(self) -> dict[str, Any]:
        """Get head-to-head matchup statistics with caching (5 min TTL)."""
        import sqlite3
        from collections import defaultdict

        cache_key = "_matchup_matrix_cache"
        cache_time_key = "_matchup_matrix_cache_time"
        cache_ttl = 300

        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        ai_root = Path(self._get_ai_service_path())
        db_path = ai_root / "data" / "unified_elo.db"

        matrix = {"matchups": [], "models": [], "configs": {}}

        if not db_path.exists():
            setattr(self, cache_key, matrix)
            setattr(self, cache_time_key, now)
            return matrix

        try:
            # Phase 3.4 Dec 29, 2025: Use context manager to prevent connection leaks
            with safe_db_connection(db_path) as conn:
                conn.row_factory = sqlite3.Row

                # Get all match history
                rows = conn.execute("""
                    SELECT participant_a, participant_b, winner, board_type, num_players,
                           game_length, duration_sec, timestamp
                    FROM match_history
                    WHERE timestamp > ?
                    ORDER BY timestamp DESC
                    LIMIT 10000
                """, (now - 86400 * 7,)).fetchall()  # Last 7 days

                # Build matchup stats
                h2h: dict[str, dict[str, dict[str, int]]] = defaultdict(lambda: defaultdict(lambda: {"wins": 0, "losses": 0, "draws": 0}))
                models = set()
                config_stats = defaultdict(lambda: {"total_matches": 0, "avg_game_length": [], "avg_duration": []})

                for row in rows:
                    a = row["participant_a"]
                    b = row["participant_b"]
                    winner = row["winner"]
                    config = f"{row['board_type']}_{row['num_players']}p"

                    if a and b:
                        models.add(a)
                        models.add(b)

                        if winner == a:
                            h2h[a][b]["wins"] += 1
                            h2h[b][a]["losses"] += 1
                        elif winner == b:
                            h2h[b][a]["wins"] += 1
                            h2h[a][b]["losses"] += 1
                        else:
                            h2h[a][b]["draws"] += 1
                            h2h[b][a]["draws"] += 1

                        config_stats[config]["total_matches"] += 1
                        if row["game_length"]:
                            config_stats[config]["avg_game_length"].append(row["game_length"])
                        if row["duration_sec"]:
                            config_stats[config]["avg_duration"].append(row["duration_sec"])

                # Convert to matchup list
                matchups = []
                for model_a in sorted(models):
                    for model_b in sorted(models):
                        if model_a < model_b:  # Avoid duplicates
                            stats = h2h[model_a][model_b]
                            total = stats["wins"] + stats["losses"] + stats["draws"]
                            if total > 0:
                                matchups.append({
                                    "model_a": model_a,
                                    "model_b": model_b,
                                    "a_wins": stats["wins"],
                                    "b_wins": stats["losses"],
                                    "draws": stats["draws"],
                                    "total": total,
                                    "a_win_rate": round(stats["wins"] / total, 3) if total > 0 else 0,
                                })

                # Compute config averages
                for _config, data in config_stats.items():
                    if data["avg_game_length"]:
                        data["avg_game_length"] = round(sum(data["avg_game_length"]) / len(data["avg_game_length"]), 1)
                    else:
                        data["avg_game_length"] = 0
                    if data["avg_duration"]:
                        data["avg_duration"] = round(sum(data["avg_duration"]) / len(data["avg_duration"]), 2)
                    else:
                        data["avg_duration"] = 0

                matrix["matchups"] = matchups
                matrix["models"] = sorted(models)
                matrix["configs"] = dict(config_stats)
                matrix["total_matches"] = sum(c["total_matches"] for c in config_stats.values())
        except (sqlite3.Error, OSError, KeyError, ValueError, TypeError):
            pass

        setattr(self, cache_key, matrix)
        setattr(self, cache_time_key, now)
        return matrix

    # =========================================================================
    # Feature 2: Model Lineage Tracking
    # =========================================================================

    async def _get_model_lineage_cached(self) -> dict[str, Any]:
        """Get model lineage and ancestry with caching (10 min TTL)."""
        import re

        cache_key = "_model_lineage_cache"
        cache_time_key = "_model_lineage_cache_time"
        cache_ttl = 600

        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        ai_root = Path(self._get_ai_service_path())
        models_dir = ai_root / "models"

        lineage = {"models": [], "generations": {}, "configs": {}}

        if not models_dir.exists():
            setattr(self, cache_key, lineage)
            setattr(self, cache_time_key, now)
            return lineage

        try:
            # Discover all models
            model_files = list(models_dir.glob("**/*.pt")) + list(models_dir.glob("**/*.pth"))

            for model_path in model_files:
                model_name = model_path.stem
                model_stat = model_path.stat()

                # Parse model name for lineage info
                # Common patterns:
                #   - square8_2p_v5_gen12, nnue_square8_2p_epoch50
                #   - ringrift_best_sq8_2p, ringrift_best_sq19_2p
                #   - hex_3p_nn_baseline, ringrift_best_hex_2p
                # Handle both full names (square8, hexagonal) and abbreviations (sq8, hex)
                config_match = re.search(
                    r"(square\d+|sq\d+|hexagonal|hex)[\W_]*(\d+)p",
                    model_name,
                    re.I
                )
                gen_match = re.search(r"gen(\d+)|v(\d+)|epoch(\d+)", model_name, re.I)

                if config_match:
                    board = config_match.group(1).lower()
                    players = config_match.group(2)
                    # Normalize board names (only transform abbreviations, not full names)
                    if board.startswith("sq") and not board.startswith("square"):
                        # sq8 -> square8, sq19 -> square19
                        board = f"square{board[2:]}"
                    elif board == "hex":
                        board = "hexagonal"
                    config = f"{board}_{players}p"
                else:
                    config = "unknown"
                generation = int(gen_match.group(1) or gen_match.group(2) or gen_match.group(3) or 0) if gen_match else 0

                model_info = {
                    "name": model_name,
                    "path": str(model_path.relative_to(ai_root)),
                    "config": config,
                    "generation": generation,
                    "size_mb": round(model_stat.st_size / 1024 / 1024, 2),
                    "created_at": model_stat.st_mtime,
                    "age_hours": round((now - model_stat.st_mtime) / 3600, 1),
                }
                lineage["models"].append(model_info)

                # Track generations per config
                if config not in lineage["generations"]:
                    lineage["generations"][config] = []
                lineage["generations"][config].append(model_info)

            # Sort models by generation within each config
            for config in lineage["generations"]:
                lineage["generations"][config].sort(key=lambda m: m["generation"])

            # Summary per config
            for config, models in lineage["generations"].items():
                lineage["configs"][config] = {
                    "total_models": len(models),
                    "latest_generation": max(m["generation"] for m in models) if models else 0,
                    "latest_model": models[-1]["name"] if models else None,
                    "total_size_mb": round(sum(m["size_mb"] for m in models), 1),
                }

            lineage["total_models"] = len(lineage["models"])

        except (sqlite3.Error, OSError, KeyError, ValueError, TypeError):
            pass

        setattr(self, cache_key, lineage)
        setattr(self, cache_time_key, now)
        return lineage

    # =========================================================================
    # Feature 3: Data Quality Metrics
    # =========================================================================

    async def _get_data_quality_cached(self) -> dict[str, Any]:
        """Get data quality metrics with caching (5 min TTL)."""
        import json
        from collections import defaultdict

        cache_key = "_data_quality_cache"
        cache_time_key = "_data_quality_cache_time"
        cache_ttl = 300

        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        # Skip JSONL scanning during startup grace period
        if self._is_in_startup_grace_period():
            return {"configs": {}, "issues": [], "summary": {}}

        ai_root = Path(self._get_ai_service_path())
        quality = {"configs": {}, "issues": [], "summary": {}}

        data_dirs = [
            ai_root / "data" / "games" / "daemon_sync",
            ai_root / "data" / "selfplay",
        ]
        cutoff = now - 86400  # Last 24 hours

        try:
            config_stats = defaultdict(lambda: {
                "total_games": 0,
                "game_lengths": [],
                "short_games": 0,  # < 10 moves
                "long_games": 0,   # > 500 moves
                "stalemates": 0,
                "unique_openings": set(),
                "player_wins": defaultdict(int),
                "parse_errors": 0,
            })

            for data_dir in data_dirs:
                if not data_dir.exists():
                    continue
                for jsonl_path in data_dir.rglob("*.jsonl"):
                    try:
                        if jsonl_path.stat().st_mtime < cutoff:
                            continue
                        with open_jsonl_file(jsonl_path) as f:
                            for line in f:
                                try:
                                    game = json.loads(line)
                                    board_type = game.get("board_type", "unknown")
                                    num_players = game.get("num_players", 0)
                                    config = f"{board_type}_{num_players}p"

                                    stats = config_stats[config]
                                    stats["total_games"] += 1

                                    length = game.get("length", 0)
                                    if length > 0:
                                        stats["game_lengths"].append(length)
                                        if length < 10:
                                            stats["short_games"] += 1
                                        elif length > 500:
                                            stats["long_games"] += 1

                                    victory_type = game.get("victory_type", "")
                                    if victory_type == "stalemate":
                                        stats["stalemates"] += 1

                                    # Track opening diversity
                                    moves = game.get("moves", [])
                                    if moves and len(moves) >= 2:
                                        opening = str(moves[0].get("action", ""))[:15] + "-" + str(moves[1].get("action", ""))[:15]
                                        stats["unique_openings"].add(opening)

                                    # Track winner distribution
                                    winner = game.get("winner")
                                    if winner is not None:
                                        stats["player_wins"][winner] += 1

                                except json.JSONDecodeError:
                                    config_stats["unknown"]["parse_errors"] += 1
                    except (OSError, ValueError, KeyError):
                        continue

            # Convert to quality metrics
            issues = []
            for config, stats in config_stats.items():
                total = stats["total_games"]
                if total == 0:
                    continue

                lengths = stats["game_lengths"]
                avg_length = sum(lengths) / len(lengths) if lengths else 0
                length_std = (sum((length - avg_length) ** 2 for length in lengths) / len(lengths)) ** 0.5 if len(lengths) > 1 else 0

                short_rate = stats["short_games"] / total
                long_rate = stats["long_games"] / total
                stalemate_rate = stats["stalemates"] / total
                opening_diversity = len(stats["unique_openings"])

                # Detect issues
                if short_rate > 0.1:
                    issues.append({"config": config, "issue": "high_short_game_rate", "value": round(short_rate * 100, 1), "severity": "warning"})
                if stalemate_rate > 0.3:
                    issues.append({"config": config, "issue": "high_stalemate_rate", "value": round(stalemate_rate * 100, 1), "severity": "warning"})
                if opening_diversity < 5 and total > 50:
                    issues.append({"config": config, "issue": "low_opening_diversity", "value": opening_diversity, "severity": "warning"})

                # Check for player bias
                wins = stats["player_wins"]
                if len(wins) >= 2 and total > 20:
                    max_win_rate = max(wins.values()) / total
                    if max_win_rate > 0.7:
                        issues.append({"config": config, "issue": "player_bias", "value": round(max_win_rate * 100, 1), "severity": "info"})

                quality["configs"][config] = {
                    "total_games": total,
                    "avg_length": round(avg_length, 1),
                    "length_std": round(length_std, 1),
                    "short_game_rate": round(short_rate * 100, 1),
                    "long_game_rate": round(long_rate * 100, 1),
                    "stalemate_rate": round(stalemate_rate * 100, 1),
                    "opening_diversity": opening_diversity,
                    "parse_errors": stats["parse_errors"],
                }

            quality["issues"] = issues
            quality["summary"] = {
                "total_configs": len(quality["configs"]),
                "total_issues": len(issues),
                "critical_issues": len([i for i in issues if i["severity"] == "critical"]),
                "warning_issues": len([i for i in issues if i["severity"] == "warning"]),
            }

        except (OSError, ValueError, KeyError, TypeError):
            pass

        setattr(self, cache_key, quality)
        setattr(self, cache_time_key, now)
        return quality

    # =========================================================================
    # Feature 4: Training Efficiency Dashboard
    # =========================================================================

    async def _get_training_efficiency_cached(self) -> dict[str, Any]:
        """Get training efficiency metrics with caching (5 min TTL)."""
        import re
        import sqlite3

        cache_key = "_training_efficiency_cache"
        cache_time_key = "_training_efficiency_cache_time"
        cache_ttl = 300

        now = time.time()
        if hasattr(self, cache_key) and hasattr(self, cache_time_key) and now - getattr(self, cache_time_key) < cache_ttl:
            return getattr(self, cache_key)

        ai_root = Path(self._get_ai_service_path())
        efficiency = {"configs": {}, "summary": {}, "cost_tracking": {}}

        try:
            # Get Elo history to track improvements
            db_path = ai_root / "data" / "unified_elo.db"
            elo_history = {}

            if db_path.exists():
                # Phase 3.4 Dec 29, 2025: Use context manager to prevent connection leaks
                with safe_db_connection(db_path) as conn:
                    rows = conn.execute("""
                        SELECT board_type, num_players, participant_id, rating, timestamp
                        FROM rating_history
                        WHERE timestamp > ?
                        ORDER BY timestamp ASC
                    """, (now - 86400 * 7,)).fetchall()  # Last 7 days

                    for row in rows:
                        config = f"{row[0]}_{row[1]}p"
                        if config not in elo_history:
                            elo_history[config] = {"ratings": [], "timestamps": []}
                        elo_history[config]["ratings"].append(row[3])
                        elo_history[config]["timestamps"].append(row[4])

            # Parse training logs for GPU hours
            logs_dir = ai_root / "logs" / "training"
            gpu_hours_per_config = {}

            if logs_dir.exists():
                for log_file in logs_dir.glob("*.log"):
                    try:
                        content = log_file.read_text(errors='ignore')
                        config_match = re.search(r"(square\d+|hex\w*)_(\d+)p", log_file.name)
                        if not config_match:
                            continue
                        config = f"{config_match.group(1)}_{config_match.group(2)}p"

                        # Extract training duration
                        duration_match = re.search(r"(?:total[_\s]?time|duration)[:\s]*([\d.]+)\s*(?:s|sec|min|h)", content, re.I)
                        if duration_match:
                            duration = float(duration_match.group(1))
                            # Assume hours if > 100, else assume minutes
                            if duration > 100:
                                duration = duration / 3600  # seconds to hours
                            elif duration < 24:
                                duration = duration / 60  # minutes to hours

                            if config not in gpu_hours_per_config:
                                gpu_hours_per_config[config] = 0
                            gpu_hours_per_config[config] += duration
                    except (ValueError, KeyError, IndexError, AttributeError):
                        continue

            # Calculate efficiency metrics per config
            for config in set(list(elo_history.keys()) + list(gpu_hours_per_config.keys())):
                elo_data = elo_history.get(config, {"ratings": [], "timestamps": []})
                gpu_hours = gpu_hours_per_config.get(config, 0)

                if elo_data["ratings"]:
                    initial_elo = elo_data["ratings"][0] if elo_data["ratings"] else INITIAL_ELO_RATING
                    current_elo = elo_data["ratings"][-1] if elo_data["ratings"] else INITIAL_ELO_RATING
                    elo_gain = current_elo - initial_elo
                else:
                    initial_elo = current_elo = INITIAL_ELO_RATING
                    elo_gain = 0

                # Elo per GPU hour
                elo_per_hour = elo_gain / gpu_hours if gpu_hours > 0 else 0

                # Estimated cost (assuming $2/GPU-hour average)
                estimated_cost = gpu_hours * 2.0

                efficiency["configs"][config] = {
                    "gpu_hours": round(gpu_hours, 2),
                    "initial_elo": round(initial_elo, 1),
                    "current_elo": round(current_elo, 1),
                    "elo_gain": round(elo_gain, 1),
                    "elo_per_gpu_hour": round(elo_per_hour, 2),
                    "estimated_cost_usd": round(estimated_cost, 2),
                    "cost_per_elo_point": round(estimated_cost / max(elo_gain, 1), 2) if elo_gain > 0 else None,
                }

            # Summary
            total_gpu_hours = sum(c.get("gpu_hours", 0) for c in efficiency["configs"].values())
            total_elo_gain = sum(c.get("elo_gain", 0) for c in efficiency["configs"].values())
            total_cost = sum(c.get("estimated_cost_usd", 0) for c in efficiency["configs"].values())

            efficiency["summary"] = {
                "total_gpu_hours": round(total_gpu_hours, 2),
                "total_elo_gain": round(total_elo_gain, 1),
                "total_estimated_cost_usd": round(total_cost, 2),
                "overall_elo_per_gpu_hour": round(total_elo_gain / max(total_gpu_hours, 1), 2),
            }

        except (sqlite3.Error, OSError, ValueError, KeyError, TypeError):
            pass

        setattr(self, cache_key, efficiency)
        setattr(self, cache_time_key, now)
        return efficiency

    # =========================================================================
    # Feature 5: Automated Model Rollback
    # =========================================================================

    async def _check_rollback_conditions(self) -> dict[str, Any]:
        """Check if any models should be rolled back based on metrics."""
        rollback_status = {"candidates": [], "recent_rollbacks": [], "config_status": {}}

        try:
            # Get holdout metrics for overfitting detection
            holdout = await self._get_holdout_metrics_cached()

            # Get Elo data for regression detection
            ai_root = Path(self._get_ai_service_path())
            db_path = ai_root / "data" / "unified_elo.db"

            elo_data = {}
            if db_path.exists():
                import sqlite3
                # Phase 3.4 Dec 29, 2025: Use context manager to prevent connection leaks
                with safe_db_connection(db_path) as conn:
                    rows = conn.execute("""
                        SELECT board_type, num_players, participant_id, rating, timestamp
                        FROM rating_history
                        ORDER BY timestamp DESC
                        LIMIT 1000
                    """).fetchall()

                    for row in rows:
                        config = f"{row[0]}_{row[1]}p"
                        if config not in elo_data:
                            elo_data[config] = []
                        elo_data[config].append({"model": row[2], "rating": row[3], "timestamp": row[4]})

            # Check each config for rollback conditions
            for config, holdout_data in holdout.get("configs", {}).items():
                status = {"config": config, "rollback_recommended": False, "reasons": []}

                # Check 1: Overfitting (overfit_gap > 0.15)
                overfit_gap = holdout_data.get("overfit_gap", 0)
                if overfit_gap and overfit_gap > 0.15:
                    status["rollback_recommended"] = True
                    status["reasons"].append(f"Overfitting detected: gap={overfit_gap:.3f}")

                # Check 2: Low holdout accuracy (< 60%)
                holdout_acc = holdout_data.get("holdout_accuracy", 1.0)
                if holdout_acc and holdout_acc < 0.6:
                    status["rollback_recommended"] = True
                    status["reasons"].append(f"Low holdout accuracy: {holdout_acc*100:.1f}%")

                # Check 3: Elo regression (dropped > 50 points recently)
                if config in elo_data and len(elo_data[config]) >= 2:
                    recent = elo_data[config][0]["rating"]
                    previous = max(e["rating"] for e in elo_data[config][:10])
                    if previous - recent > 50:
                        status["rollback_recommended"] = True
                        status["reasons"].append(f"Elo regression: {previous:.0f} -> {recent:.0f}")

                rollback_status["config_status"][config] = status
                if status["rollback_recommended"]:
                    rollback_status["candidates"].append(status)

            # Load recent rollback history if exists
            rollback_log = ai_root / "logs" / "rollbacks.json"
            if rollback_log.exists():
                import json
                with contextlib.suppress(json.JSONDecodeError, OSError, KeyError, IndexError):
                    rollback_status["recent_rollbacks"] = json.loads(rollback_log.read_text())[-10:]

        except (sqlite3.Error, OSError, ValueError, KeyError, TypeError):
            pass

        return rollback_status

    async def _execute_rollback(self, config: str, dry_run: bool = False) -> dict[str, Any]:
        """Execute a rollback for the given config by restoring previous model.

        Args:
            config: Config string like "square8_2p"
            dry_run: If True, only simulate the rollback without making changes

        Returns:
            Dict with rollback results (success, message, details)
        """
        import json
        import shutil

        result = {
            "success": False,
            "config": config,
            "dry_run": dry_run,
            "message": "",
            "details": {},
        }

        try:
            ai_root = Path(self._get_ai_service_path())
            models_dir = ai_root / "models"
            archive_dir = models_dir / "archive"
            archive_dir.mkdir(parents=True, exist_ok=True)

            # Parse config to get board type and player count
            parts = config.rsplit("_", 1)
            if len(parts) != 2 or not parts[1].endswith("p"):
                result["message"] = f"Invalid config format: {config}"
                return result

            board = parts[0]
            players = parts[1][:-1]

            # Find the current best model alias
            # Common patterns: ringrift_best_sq8_2p, ringrift_best_square8_2p
            board_abbrev = board.replace("square", "sq").replace("hexagonal", "hex")
            best_patterns = [
                f"ringrift_best_{board_abbrev}_{players}p.pth",
                f"ringrift_best_{board}_{players}p.pth",
            ]

            current_best = None
            for pattern in best_patterns:
                candidate = models_dir / pattern
                if candidate.exists():
                    current_best = candidate
                    break

            if not current_best:
                result["message"] = f"No best model found for {config}"
                return result

            # Find previous checkpoints for this config
            checkpoint_dir = models_dir / "checkpoints"
            checkpoints = []
            if checkpoint_dir.exists():
                for ckpt in checkpoint_dir.glob(f"*{board_abbrev}*{players}p*.pth"):
                    try:
                        stat = ckpt.stat()
                        checkpoints.append({
                            "path": ckpt,
                            "mtime": stat.st_mtime,
                            "name": ckpt.name,
                        })
                    except (AttributeError):
                        continue

            # Also check archive for previous best models
            for archived in archive_dir.glob(f"*{board_abbrev}*{players}p*.pth"):
                try:
                    stat = archived.stat()
                    checkpoints.append({
                        "path": archived,
                        "mtime": stat.st_mtime,
                        "name": archived.name,
                    })
                except (AttributeError):
                    continue

            # Sort by modification time descending
            checkpoints.sort(key=lambda x: x["mtime"], reverse=True)

            # Filter out the current best model
            current_mtime = current_best.stat().st_mtime
            previous_checkpoints = [c for c in checkpoints if abs(c["mtime"] - current_mtime) > 60]

            if not previous_checkpoints:
                result["message"] = f"No previous checkpoints found for rollback of {config}"
                return result

            # Select the most recent previous checkpoint
            rollback_source = previous_checkpoints[0]

            result["details"] = {
                "current_model": current_best.name,
                "rollback_to": rollback_source["name"],
                "rollback_age_hours": round((time.time() - rollback_source["mtime"]) / 3600, 1),
                "available_checkpoints": len(previous_checkpoints),
            }

            if dry_run:
                result["success"] = True
                result["message"] = f"Dry run: Would rollback {current_best.name} to {rollback_source['name']}"
                return result

            # Archive the current model
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            archived_name = f"{current_best.stem}_archived_{timestamp}.pth"
            shutil.copy2(current_best, archive_dir / archived_name)

            # Restore the previous checkpoint
            shutil.copy2(rollback_source["path"], current_best)

            # Log the rollback
            rollback_log = ai_root / "logs" / "rollbacks.json"
            rollback_log.parent.mkdir(parents=True, exist_ok=True)

            rollback_entry = {
                "timestamp": time.strftime("%Y-%m-%dT%H:%M:%S"),
                "config": config,
                "previous_model": current_best.name,
                "rolled_back_to": rollback_source["name"],
                "archived_as": archived_name,
            }

            try:
                existing = json.loads(rollback_log.read_text()) if rollback_log.exists() else []
            except (json.JSONDecodeError, OSError, KeyError, IndexError, AttributeError):
                existing = []

            existing.append(rollback_entry)
            rollback_log.write_text(json.dumps(existing[-100:], indent=2))  # Keep last 100 rollbacks

            result["success"] = True
            result["message"] = f"Successfully rolled back {config} from {current_best.name} to {rollback_source['name']}"

            # Increment rollback counter
            self.diversity_metrics["rollbacks"] += 1

            # Send alert notification
            asyncio.create_task(self.notifier.send(
                title="Model Rollback Executed",
                message=f"Rolled back {config} from {current_best.name} to {rollback_source['name']}",
                level="warning",
                fields={
                    "Config": config,
                    "Previous": current_best.name,
                    "Restored": rollback_source["name"],
                    "Age": f"{result['details']['rollback_age_hours']:.1f}h",
                },
                node_id=self.node_id,
            ))

        except Exception as e:  # noqa: BLE001
            result["message"] = f"Rollback failed: {e!s}"

        return result

    async def _auto_rollback_check(self) -> list[dict[str, Any]]:
        """Automatically check and execute rollbacks for critical candidates.

        Returns list of executed rollbacks.
        """
        # Check if auto-rollback is enabled
        if os.environ.get("RINGRIFT_AUTO_ROLLBACK", "").lower() not in ("1", "true", "yes"):
            return []

        executed = []
        try:
            status = await self._check_rollback_conditions()
            for candidate in status.get("candidates", []):
                # Only auto-rollback if multiple serious conditions are met
                reasons = candidate.get("reasons", [])
                if len(reasons) >= 2 or any("Overfitting" in r for r in reasons):
                    config = candidate["config"]
                    result = await self._execute_rollback(config, dry_run=False)
                    executed.append(result)
                    if result["success"]:
                        logger.warning(f"[AUTO-ROLLBACK] Executed for {config}: {reasons}")
        except Exception as e:  # noqa: BLE001
            logger.error(f"[AUTO-ROLLBACK] Error: {e}")

        return executed

    # =========================================================================
    # Feature 6: Distributed Selfplay Autoscaling
    # =========================================================================

    async def _get_autoscaling_metrics(self) -> dict[str, Any]:
        """Get metrics for autoscaling decisions."""
        # Autoscaling thresholds tuned for 46-node cluster
        # These can be overridden via environment variables
        max_workers = int(os.environ.get("RINGRIFT_AUTOSCALE_MAX_WORKERS", "46"))
        min_workers = int(os.environ.get("RINGRIFT_AUTOSCALE_MIN_WORKERS", "2"))
        scale_up_threshold = int(os.environ.get("RINGRIFT_AUTOSCALE_SCALE_UP_GPH", "100"))
        scale_down_threshold = int(os.environ.get("RINGRIFT_AUTOSCALE_SCALE_DOWN_GPH", "500"))
        target_freshness = float(os.environ.get("RINGRIFT_AUTOSCALE_TARGET_FRESHNESS_HOURS", "2"))

        autoscale = {
            "current_state": {},
            "recommendations": [],
            "thresholds": {
                "scale_up_games_per_hour": scale_up_threshold,  # Scale up if below this
                "scale_down_games_per_hour": scale_down_threshold,  # Scale down if above this
                "max_workers": max_workers,
                "min_workers": min_workers,
                "target_data_freshness_hours": target_freshness,
            },
        }

        try:
            # Get current worker count
            with self.peers_lock:
                total_nodes = len(self.peers) + 1
                gpu_nodes = len([p for p in self.peers.values() if getattr(p, "has_gpu", False)])
                if self.self_info.has_gpu:
                    gpu_nodes += 1

            with self.jobs_lock:
                active_selfplay = len([j for j in self.local_jobs.values()
                                      if j.job_type in (JobType.SELFPLAY, JobType.GPU_SELFPLAY, JobType.HYBRID_SELFPLAY, JobType.CPU_SELFPLAY, JobType.GUMBEL_SELFPLAY)
                                      and j.status == "running"])

            autoscale["current_state"] = {
                "total_nodes": total_nodes,
                "gpu_nodes": gpu_nodes,
                "active_selfplay_jobs": active_selfplay,
            }

            # Get game generation throughput
            analytics = await self._get_game_analytics_cached()
            total_throughput = sum(c.get("throughput_per_hour", 0) for c in analytics.get("configs", {}).values())

            autoscale["current_state"]["games_per_hour"] = round(total_throughput, 1)

            # Get data freshness
            now = time.time()
            ai_root = Path(self._get_ai_service_path())
            selfplay_dir = ai_root / "data" / "selfplay"

            freshest_data = 0
            if selfplay_dir.exists():
                for jsonl in selfplay_dir.rglob("*.jsonl"):
                    try:
                        mtime = jsonl.stat().st_mtime
                        if mtime > freshest_data:
                            freshest_data = mtime
                    except (AttributeError):
                        continue

            data_age_hours = (now - freshest_data) / 3600 if freshest_data > 0 else 999
            autoscale["current_state"]["data_freshness_hours"] = round(data_age_hours, 2)

            # Generate recommendations
            thresholds = autoscale["thresholds"]

            if total_throughput < thresholds["scale_up_games_per_hour"] and total_nodes < thresholds["max_workers"]:
                autoscale["recommendations"].append({
                    "action": "scale_up",
                    "reason": f"Low throughput ({total_throughput:.0f} games/h < {thresholds['scale_up_games_per_hour']})",
                    "suggested_workers": min(total_nodes + 2, thresholds["max_workers"]),
                })

            if total_throughput > thresholds["scale_down_games_per_hour"] and total_nodes > thresholds["min_workers"]:
                autoscale["recommendations"].append({
                    "action": "scale_down",
                    "reason": f"High throughput ({total_throughput:.0f} games/h > {thresholds['scale_down_games_per_hour']})",
                    "suggested_workers": max(total_nodes - 1, thresholds["min_workers"]),
                })

            if data_age_hours > thresholds["target_data_freshness_hours"]:
                autoscale["recommendations"].append({
                    "action": "scale_up",
                    "reason": f"Stale data ({data_age_hours:.1f}h > {thresholds['target_data_freshness_hours']}h)",
                    "suggested_workers": min(total_nodes + 1, thresholds["max_workers"]),
                })

            # Cost optimization recommendation
            efficiency = await self._get_training_efficiency_cached()
            elo_per_hour = efficiency.get("summary", {}).get("overall_elo_per_gpu_hour", 0)
            if elo_per_hour < 1 and total_nodes > 2:
                autoscale["recommendations"].append({
                    "action": "optimize",
                    "reason": f"Low efficiency ({elo_per_hour:.2f} Elo/GPU-h) - consider reducing workers",
                    "suggested_workers": max(total_nodes - 1, thresholds["min_workers"]),
                })

        except (AttributeError, KeyError, ValueError, TypeError):
            pass

        return autoscale

    # handle_victory_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)
    # handle_elo_history moved to EloAnalyticsHandlersMixin (Jan 2026 - P2P Modularization Phase 4a)

    # Elo Sync Handlers moved to scripts/p2p/handlers/elo_sync.py
    # Inherited from EloSyncHandlersMixin:
    # - handle_elo_sync_status, handle_elo_sync_trigger
    # - handle_elo_sync_download, handle_elo_sync_upload
    # - _trigger_elo_sync_after_matches

    # NOTE: _elo_sync_loop() removed Dec 2025 (29 LOC).
    # Now runs via LoopManager as EloSyncLoop.
    # See scripts/p2p/loops/elo_sync_loop.py for implementation.

    # NOTE: _worker_pull_loop() removed Dec 2025 (85 LOC).
    # Now runs via LoopManager as WorkerPullLoop.
    # See scripts/p2p/loops/job_loops.py for implementation.
    # Helper methods _claim_work_from_leader, _execute_claimed_work, _report_work_result
    # are retained and passed as callbacks to WorkerPullLoop.

    async def _claim_work_from_leader(self, capabilities: list[str]) -> dict[str, Any] | None:
        """Claim work from the leader's work queue."""
        if not self.leader_id or self.leader_id == self.node_id:
            return None

        # Find leader peer
        with self.peers_lock:
            leader_peer = self.peers.get(self.leader_id)

        if not leader_peer:
            return None

        try:
            timeout = ClientTimeout(total=15)
            async with get_client_session(timeout) as session:
                caps_str = ",".join(capabilities)
                url = self._url_for_peer(leader_peer, f"/work/claim?node_id={self.node_id}&capabilities={caps_str}")
                async with session.get(url, headers=self._auth_headers()) as resp:
                    if resp.status == 200:
                        # Jan 10, 2026: Update timestamp on any leader response (not just work)
                        # This prevents false stall detection when queue is empty
                        self.last_work_from_leader = time.time()
                        data = await resp.json()
                        if data.get("status") == "claimed":
                            return data.get("work")
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Failed to claim work from leader: {e}")

        return None

    async def _claim_work_batch_from_leader(
        self, capabilities: list[str], max_items: int
    ) -> list[dict[str, Any]]:
        """Claim multiple work items from the leader's work queue.

        Session 17.34 (Jan 5, 2026): Added batch claiming to reduce HTTP round-trips
        and improve GPU utilization by +30-40%.

        Args:
            capabilities: List of work types this node can handle
            max_items: Maximum number of items to claim (capped at 10)

        Returns:
            List of work items, empty list if none available or error
        """
        if not self.leader_id or self.leader_id == self.node_id:
            return []

        # Find leader peer
        with self.peers_lock:
            leader_peer = self.peers.get(self.leader_id)

        if not leader_peer:
            return []

        try:
            timeout = ClientTimeout(total=20)  # Slightly longer for batch
            async with get_client_session(timeout) as session:
                caps_str = ",".join(capabilities)
                url = self._url_for_peer(
                    leader_peer,
                    f"/work/claim_batch?node_id={self.node_id}&capabilities={caps_str}&max_items={max_items}"
                )
                async with session.get(url, headers=self._auth_headers()) as resp:
                    if resp.status == 200:
                        # Jan 10, 2026: Update timestamp on any leader response (not just work)
                        # This prevents false stall detection when queue is empty
                        self.last_work_from_leader = time.time()
                        data = await resp.json()
                        if data.get("status") == "claimed" and data.get("items"):
                            return data.get("items", [])
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Failed to batch claim work from leader: {e}")

        return []

    async def _execute_claimed_work(self, work_item: dict[str, Any]) -> bool:
        """Execute a claimed work item locally."""
        work_type = work_item.get("work_type", "")
        config = work_item.get("config", {})
        work_id = work_item.get("work_id", "")

        # Track work execution via JobOrchestrationManager (Jan 2026)
        if hasattr(self, "job_orchestration") and self.job_orchestration:
            self.job_orchestration.record_work_executed(work_type)

        try:
            if work_type == "training":
                # Start training job - January 2026: Fixed to actually execute training
                # Previously this was a NO-OP that just logged and returned True
                board_type = config.get("board_type", "square8")
                num_players = config.get("num_players", 2)
                epochs = config.get("epochs", 20)  # Jan 2026: Reduced from 50 to prevent overfitting
                batch_size = config.get("batch_size", 256)
                learning_rate = config.get("learning_rate", 1e-3)
                model_version = config.get("model_version", "v5")

                # Build config key and model filename
                config_key = f"{board_type}_{num_players}p"
                model_filename = f"canonical_{config_key}_{model_version}.pth"

                # Build training command
                cmd = [
                    sys.executable, "-m", "app.training.train",
                    "--board-type", board_type,
                    "--num-players", str(num_players),
                    "--model-version", model_version,
                    "--epochs", str(epochs),
                    "--batch-size", str(batch_size),
                    "--learning-rate", str(learning_rate),
                    "--save-path", f"models/{model_filename}",
                    "--allow-stale-data",
                    "--max-data-age-hours", "168",  # 7 days tolerance
                ]

                logger.info(
                    f"Executing training work {work_id}: {config_key} with {model_version} "
                    f"(epochs={epochs}, batch={batch_size})"
                )

                # Run training as background subprocess
                async def _run_training_subprocess():
                    try:
                        proc = await asyncio.create_subprocess_exec(
                            *cmd,
                            stdout=asyncio.subprocess.PIPE,
                            stderr=asyncio.subprocess.STDOUT,
                            cwd=str(Path(__file__).parent.parent),  # ai-service directory
                        )
                        stdout, _ = await proc.communicate()
                        if proc.returncode == 0:
                            logger.info(
                                f"Training completed successfully: {config_key}/{model_version} "
                                f"(work_id={work_id})"
                            )
                            # Emit training completed event
                            try:
                                from app.distributed.data_events import DataEventType
                                from app.coordination.event_router import emit_event
                                emit_event(DataEventType.TRAINING_COMPLETED, {
                                    "config_key": config_key,
                                    "board_type": board_type,
                                    "num_players": num_players,
                                    "model_version": model_version,
                                    "model_path": f"models/{model_filename}",
                                    "work_id": work_id,
                                })
                            except ImportError:
                                pass
                        else:
                            output = stdout.decode()[:2000] if stdout else "no output"
                            logger.error(
                                f"Training failed: {config_key}/{model_version}: "
                                f"returncode={proc.returncode}, output={output}"
                            )
                    except Exception as e:
                        logger.exception(f"Training subprocess error for {config_key}: {e}")

                asyncio.create_task(_run_training_subprocess())
                return True

            elif work_type == "selfplay":
                # Jan 5, 2026: Check if this node should do selfplay
                # Prevents coordinator (mac-studio) from spawning selfplay despite claiming work
                if not _is_selfplay_enabled_for_node():
                    logger.info(
                        f"Skipping selfplay work {work_id}: selfplay_enabled=false for this node"
                    )
                    return True  # Return True to indicate "handled" (just skipped)

                # Start selfplay job
                board_type = config.get("board_type", "square8")
                num_players = config.get("num_players", 2)
                num_games = config.get("num_games", 500)
                engine_mode = config.get("engine_mode", "mixed")
                engine_extra_args = config.get("engine_extra_args")  # December 2025: for budget override
                selfplay_model_version = config.get("model_version", "v5")  # Jan 5, 2026: Architecture selection

                # Delegate to JobManager (Phase 2B refactoring, Dec 2025)
                asyncio.create_task(self.job_manager.run_gpu_selfplay_job(
                    job_id=f"pull-{work_id}",
                    board_type=board_type,
                    num_players=num_players,
                    num_games=num_games,
                    engine_mode=engine_mode,
                    engine_extra_args=engine_extra_args,
                    model_version=selfplay_model_version,  # Jan 5, 2026: Architecture selection
                ))

                # Track diversity metrics for monitoring (Phase 2B, Dec 2025)
                self.selfplay_scheduler.track_diversity({
                    "board_type": board_type,
                    "num_players": num_players,
                    "engine_mode": engine_mode,
                })
                return True

            elif work_type == "gpu_cmaes":
                # Start CMA-ES optimization
                logger.info(f"Executing GPU CMA-ES work: {config}")
                return True

            elif work_type == "tournament":
                # Start tournament evaluation
                # Jan 4, 2026: Fixed to actually execute tournament via JobManager
                # Previously this was a no-op that just logged and returned True
                from scripts.p2p.models import DistributedTournamentState

                board_type = config.get("board_type", "square8")
                num_players = config.get("num_players", 2)
                games_per_pairing = config.get("games", 2)
                job_id = f"tournament-{work_id}"

                # Discover models for this config
                config_key = f"{board_type}_{num_players}p"
                agent_ids = []
                try:
                    # Try to get models from model registry or filesystem
                    from app.models.discovery import discover_models
                    models = discover_models(
                        board_type=board_type,
                        num_players=num_players,
                        model_type="nn",
                    )
                    # Sort by modified time (most recent first) and take top 5
                    models.sort(key=lambda m: m.modified_at or 0, reverse=True)
                    agent_ids = [str(m.path) for m in models[:5]] if models else []
                except (ImportError, ValueError, AttributeError, RuntimeError):
                    pass

                # Fallback: use canonical model if available
                if len(agent_ids) < 2:
                    canonical_path = f"models/canonical_{config_key}.pth"
                    if Path(canonical_path).exists():
                        agent_ids = [canonical_path]
                    # Add heuristic baseline for comparison
                    agent_ids.append(f"heuristic:{config_key}")

                if len(agent_ids) < 2:
                    logger.warning(
                        f"Tournament {work_id}: Not enough agents for {config_key} "
                        f"(found {len(agent_ids)}), skipping"
                    )
                    return False

                logger.info(
                    f"Executing tournament work {work_id}: {board_type}/{num_players}p "
                    f"with {len(agent_ids)} agents"
                )

                # Create tournament state (matches /tournament/start handler pattern)
                pairings = []
                for i, a1 in enumerate(agent_ids):
                    for a2 in agent_ids[i + 1:]:
                        for game_num in range(games_per_pairing):
                            pairings.append({
                                "agent1": a1,
                                "agent2": a2,
                                "game_num": game_num,
                                "status": "pending",
                            })

                state = DistributedTournamentState(
                    job_id=job_id,
                    board_type=board_type,
                    num_players=num_players,
                    agent_ids=agent_ids,
                    games_per_pairing=games_per_pairing,
                    total_matches=len(pairings),
                    pending_matches=pairings,
                    status="running",
                    started_at=time.time(),
                    last_update=time.time(),
                )

                # Find available workers
                with self.peers_lock:
                    workers = [p.node_id for p in self.peers.values() if p.is_healthy()]
                state.worker_nodes = workers

                # Register state before running
                self.distributed_tournament_state[job_id] = state

                # Run tournament via job manager
                async def _run_tournament_task():
                    try:
                        await self.job_manager.run_distributed_tournament(job_id)
                    except Exception as e:
                        logger.exception(f"Tournament task failed for {job_id}: {e}")

                asyncio.create_task(_run_tournament_task())
                return True

            else:
                logger.warning(f"Unknown work type: {work_type}")
                return False

        except Exception as e:  # noqa: BLE001
            logger.error(f"Error executing work {work_id}: {e}")
            return False

    async def _report_work_result(self, work_item: dict[str, Any], success: bool) -> None:
        """Report work completion/failure to the leader."""
        if not self.leader_id or self.leader_id == self.node_id:
            return

        work_id = work_item.get("work_id", "")
        if not work_id:
            return

        with self.peers_lock:
            leader_peer = self.peers.get(self.leader_id)

        if not leader_peer:
            return

        try:
            timeout = ClientTimeout(total=15)
            async with get_client_session(timeout) as session:
                if success:
                    url = self._url_for_peer(leader_peer, "/work/complete")
                    payload = {"work_id": work_id, "result": {"node_id": self.node_id}}
                else:
                    url = self._url_for_peer(leader_peer, "/work/fail")
                    payload = {"work_id": work_id, "error": "execution_failed"}

                async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                    if resp.status == 200:
                        logger.debug(f"Reported work {work_id} result: {'success' if success else 'failed'}")
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Failed to report work result: {e}")

    # NOTE: _work_queue_maintenance_loop() removed Dec 2025 (42 LOC).
    # Now runs via LoopManager as WorkQueueMaintenanceLoop.
    # See scripts/p2p/loops/job_loops.py for implementation.

    # NOTE: _idle_detection_loop() removed Dec 2025 (128 LOC).
    # Now runs via LoopManager as IdleDetectionLoop.
    # See scripts/p2p/loops/job_loops.py for implementation.

    async def _handle_zombie_detected(self, peer, zombie_duration: float) -> None:
        """Handle detection of zombie/stuck selfplay processes on a node.

        Jan 2, 2026: Added as callback for IdleDetectionLoop's on_zombie_detected.
        When a node reports selfplay_jobs > 0 but gpu_util < 10% for extended
        time, the processes may be stuck (zombie). This handler kills them.

        Args:
            peer: NodeInfo or DiscoveredNode with zombie processes
            zombie_duration: How long the node has been in zombie state (seconds)
        """
        node_id = getattr(peer, "node_id", str(peer))
        logger.warning(
            f"Zombie processes detected on {node_id} for {zombie_duration:.0f}s, "
            "attempting to kill stale selfplay"
        )

        try:
            # Send kill command to the node's /process/kill endpoint
            url = self._url_for_peer(peer, "/process/kill")
            timeout = ClientTimeout(total=15)

            async with aiohttp.ClientSession(timeout=timeout) as session:
                # Kill all selfplay processes
                async with session.post(
                    url,
                    json={"pattern": "selfplay", "signal": "SIGTERM"},
                    headers=self._auth_headers(),
                ) as resp:
                    if resp.status == 200:
                        result = await resp.json()
                        killed = result.get("killed", 0)
                        logger.info(
                            f"Killed {killed} zombie selfplay processes on {node_id}"
                        )
                    else:
                        logger.warning(
                            f"Failed to kill zombies on {node_id}: HTTP {resp.status}"
                        )

        except asyncio.TimeoutError:
            logger.warning(f"Timeout killing zombie processes on {node_id}")
        except Exception as e:  # noqa: BLE001
            logger.warning(f"Error killing zombie processes on {node_id}: {e}")

    async def _auto_start_selfplay(self, peer, idle_duration: float):
        """Auto-start diverse hybrid selfplay on an idle node.

        Works with both NodeInfo (P2P peers) and DiscoveredNode (unified inventory).
        Uses diverse profiles for high-quality training data:
        - Multiple engine modes (gumbel-mcts, nnue-guided, policy-only, mcts)
        - Multiple board types (hex8, square8, square19)
        - Multiple player counts (2, 3, 4)
        - Multiple heuristic profiles (balanced, aggressive, territorial, defensive)
        """
        # Check for GPU - works with both NodeInfo and DiscoveredNode
        gpu_name = getattr(peer, "gpu_name", "") or ""

        # Don't auto-start on nodes that aren't GPU nodes
        has_gpu = bool(gpu_name) or getattr(peer, "has_gpu", False)
        is_gpu_node = getattr(peer, "is_gpu_node", lambda: has_gpu)()
        if not has_gpu and not is_gpu_node:
            return

        # GPU selfplay uses batch processing - scale based on GPU power
        # Jan 7, 2026: Reduced games_per_process to prevent 1-hour timeouts
        # GUMBEL MCTS with 800 budget takes ~2s per game, so max ~1800 games/hour
        # Using 500 games max with safety margin for larger boards
        if "GH200" in gpu_name.upper() or "H100" in gpu_name.upper() or "H200" in gpu_name.upper():
            num_processes = 4
            games_per_process = 500
            gpu_tier = "high"
        elif "A100" in gpu_name.upper() or "A40" in gpu_name.upper():
            num_processes = 3
            games_per_process = 300
            gpu_tier = "high"
        elif "4090" in gpu_name.upper() or "5090" in gpu_name.upper():
            num_processes = 3
            games_per_process = 300
            gpu_tier = "mid"
        elif "4080" in gpu_name.upper() or "5080" in gpu_name.upper() or "5070" in gpu_name.upper():
            num_processes = 2
            games_per_process = 200
            gpu_tier = "mid"
        elif "3090" in gpu_name.upper() or "4070" in gpu_name.upper():
            num_processes = 2
            games_per_process = 200
            gpu_tier = "mid"
        else:
            num_processes = 2
            games_per_process = 100
            gpu_tier = "low"

        logger.info(f"Auto-starting {num_processes} diverse selfplay processes on idle node {peer.node_id} "
                   f"(GPU={gpu_name}, tier={gpu_tier}, {games_per_process} games each, idle for {idle_duration:.0f}s)")

        # Send parallel requests to /selfplay/start endpoint
        try:
            url = self._url_for_peer(peer, "/selfplay/start")
            timeout = ClientTimeout(total=30)

            # Diverse profile configurations for high-quality training data
            # Each profile targets different aspects of game understanding
            # Dec 28, 2025: CRITICAL FIX - Changed "hex" to "hex8" in all profiles.
            # "hex" was being normalized to "hexagonal" (large board with 469 cells)
            # instead of "hex8" (small board with 61 cells), causing hex8_* configs
            # to receive no selfplay games for 52+ hours.
            DIVERSE_PROFILES = [
                # High-quality neural-guided profiles (50% of games)
                {
                    "engine_mode": "gumbel-mcts",
                    "board_type": "hex8",
                    "num_players": 2,
                    "profile": "balanced",
                    "weight": 0.18,
                    "description": "Gumbel MCTS 2P hex8 - highest quality",
                },
                {
                    "engine_mode": "policy-only",
                    "board_type": "hex8",
                    "num_players": 2,
                    "profile": "balanced",
                    "weight": 0.12,
                    "description": "Policy-only 2P hex8 - fast NN inference",
                },
                {
                    "engine_mode": "nnue-guided",
                    "board_type": "square8",
                    "num_players": 2,
                    "profile": "aggressive",
                    "weight": 0.08,
                    "description": "NNUE-guided 2P square - aggressive style",
                },
                {
                    "engine_mode": "gumbel-mcts",
                    "board_type": "square8",
                    "num_players": 3,
                    "profile": "balanced",
                    "weight": 0.06,
                    "description": "Gumbel MCTS 3P square - multiplayer strategy",
                },
                {
                    "engine_mode": "mcts",
                    "board_type": "hex8",
                    "num_players": 2,
                    "profile": "territorial",
                    "weight": 0.06,
                    "description": "MCTS 2P hex8 - territorial focus",
                },
                # MaxN/BRS multiplayer profiles (15% of games)
                # Benchmarks show: MaxN >> Descent in 3P/4P, MaxN  BRS
                {
                    "engine_mode": "maxn",
                    "board_type": "hex8",
                    "num_players": 3,
                    "profile": "balanced",
                    "weight": 0.05,
                    "description": "MaxN 3P hex8 - optimal multiplayer search",
                },
                {
                    "engine_mode": "maxn",
                    "board_type": "square8",
                    "num_players": 4,
                    "profile": "balanced",
                    "weight": 0.04,
                    "description": "MaxN 4P square - best for 4-player",
                },
                {
                    "engine_mode": "brs",
                    "board_type": "hex8",
                    "num_players": 3,
                    "profile": "aggressive",
                    "weight": 0.03,
                    "description": "BRS 3P hex8 - fast multiplayer search",
                },
                {
                    "engine_mode": "brs",
                    "board_type": "square8",
                    "num_players": 4,
                    "profile": "territorial",
                    "weight": 0.03,
                    "description": "BRS 4P square - territorial multiplayer",
                },
                # GPU-accelerated throughput profiles (25% of games)
                {
                    "engine_mode": "heuristic-only",
                    "board_type": "hex8",
                    "num_players": 2,
                    "profile": "balanced",
                    "weight": 0.10,
                    "description": "GPU heuristic 2P hex8 - fast throughput",
                },
                {
                    "engine_mode": "heuristic-only",
                    "board_type": "square8",
                    "num_players": 2,
                    "profile": "defensive",
                    "weight": 0.07,
                    "description": "GPU heuristic 2P square - defensive style",
                },
                {
                    "engine_mode": "heuristic-only",
                    "board_type": "hex8",
                    "num_players": 4,
                    "profile": "balanced",
                    "weight": 0.05,
                    "description": "GPU heuristic 4P hex8 - large multiplayer",
                },
                # Exploration profiles (10% of games)
                {
                    "engine_mode": "mixed",
                    "board_type": "square19",
                    "num_players": 2,
                    "profile": "balanced",
                    "weight": 0.04,
                    "description": "Mixed 2P large board - strategic depth",
                },
                {
                    "engine_mode": "nnue-guided",
                    "board_type": "hex8",
                    "num_players": 3,
                    "profile": "aggressive",
                    "weight": 0.04,
                    "description": "NNUE 3P hex8 - aggressive multiplayer",
                },
                {
                    "engine_mode": "policy-only",
                    "board_type": "square8",
                    "num_players": 4,
                    "profile": "territorial",
                    "weight": 0.05,
                    "description": "Policy 4P square - territory control",
                },
                # Dec 28, 2025: Added large board profiles (square19, hexagonal)
                # Uses lighter engines (heuristic, brs, maxn) for feasible throughput
                {
                    "engine_mode": "heuristic-only",
                    "board_type": "square19",
                    "num_players": 2,
                    "profile": "balanced",
                    "weight": 0.03,
                    "description": "Heuristic 2P square19 - fast large board",
                },
                {
                    "engine_mode": "heuristic-only",
                    "board_type": "hexagonal",
                    "num_players": 2,
                    "profile": "balanced",
                    "weight": 0.03,
                    "description": "Heuristic 2P hexagonal - fast large board",
                },
                {
                    "engine_mode": "brs",
                    "board_type": "square19",
                    "num_players": 3,
                    "profile": "balanced",
                    "weight": 0.02,
                    "description": "BRS 3P square19 - multiplayer large board",
                },
                {
                    "engine_mode": "brs",
                    "board_type": "hexagonal",
                    "num_players": 3,
                    "profile": "balanced",
                    "weight": 0.02,
                    "description": "BRS 3P hexagonal - multiplayer large board",
                },
                {
                    "engine_mode": "maxn",
                    "board_type": "square19",
                    "num_players": 4,
                    "profile": "balanced",
                    "weight": 0.02,
                    "description": "MaxN 4P square19 - high quality 4-player",
                },
                {
                    "engine_mode": "maxn",
                    "board_type": "hexagonal",
                    "num_players": 4,
                    "profile": "balanced",
                    "weight": 0.02,
                    "description": "MaxN 4P hexagonal - high quality 4-player",
                },
                # Jan 1, 2026: Added missing harness types for full diversity
                # Minimax (2P paranoid search), Descent (stochastic), Random (baseline)
                {
                    "engine_mode": "nn-minimax",
                    "board_type": "hex8",
                    "num_players": 2,
                    "profile": "balanced",
                    "weight": 0.02,
                    "description": "NN-Minimax 2P hex8 - deep alpha-beta search",
                },
                {
                    "engine_mode": "nn-minimax",
                    "board_type": "square8",
                    "num_players": 2,
                    "profile": "balanced",
                    "weight": 0.02,
                    "description": "NN-Minimax 2P square - tactical search",
                },
                {
                    "engine_mode": "nn-descent",
                    "board_type": "hex8",
                    "num_players": 2,
                    "profile": "balanced",
                    "weight": 0.01,
                    "description": "NN-Descent 2P hex8 - stochastic exploration",
                },
                {
                    "engine_mode": "nn-descent",
                    "board_type": "square8",
                    "num_players": 3,
                    "profile": "balanced",
                    "weight": 0.01,
                    "description": "NN-Descent 3P square - multiplayer descent",
                },
                {
                    "engine_mode": "random",
                    "board_type": "hex8",
                    "num_players": 2,
                    "profile": "balanced",
                    "weight": 0.005,
                    "description": "Random 2P hex8 - baseline diversity",
                },
                {
                    "engine_mode": "random",
                    "board_type": "square8",
                    "num_players": 4,
                    "profile": "balanced",
                    "weight": 0.005,
                    "description": "Random 4P square - multiplayer baseline",
                },
            ]

            # Select profiles based on weighted random sampling
            import random
            weights = [p["weight"] for p in DIVERSE_PROFILES]
            selected_profiles = random.choices(DIVERSE_PROFILES, weights=weights, k=num_processes)

            # Build job configs from selected profiles
            job_configs = []
            for i, profile in enumerate(selected_profiles):
                job_configs.append({
                    "board_type": profile["board_type"],
                    "num_players": profile["num_players"],
                    "num_games": games_per_process,
                    "engine_mode": profile["engine_mode"],
                    "heuristic_profile": profile["profile"],
                    "auto_assigned": True,
                    "reason": f"auto_idle_{profile['engine_mode']}_{profile['board_type']}_{profile['num_players']}p_{int(idle_duration)}s",
                })
                logger.debug(f"  Process {i}: {profile['description']}")

            # Dec 29, 2025: NAT-blocked node detection and work queue routing
            # NAT-blocked nodes can't receive inbound HTTP connections, so we
            # add work to the queue instead. WorkerPullLoop on the node will claim it.
            is_nat_blocked = getattr(peer, "nat_blocked", False)
            if is_nat_blocked:
                try:
                    from app.coordination.work_queue import WorkItem, WorkType, get_work_queue
                    wq = get_work_queue()
                    if wq is not None:
                        queued_count = 0
                        for cfg in job_configs:
                            work_item = WorkItem(
                                work_type=WorkType.SELFPLAY,
                                priority=50,  # Normal priority for auto-idle work
                                config={
                                    **cfg,
                                    "target_node": peer.node_id,  # Hint for WorkerPullLoop
                                    "target_node_expires_at": time.time() + 600,  # 10 min expiration
                                    "nat_blocked_dispatch": True,  # Mark as NAT-blocked dispatch
                                },
                                timeout_seconds=3600.0,  # 1 hour timeout
                            )
                            wq.add_work(work_item)
                            queued_count += 1

                        from collections import Counter
                        engine_counts = Counter(cfg["engine_mode"] for cfg in job_configs)
                        board_counts = Counter(cfg["board_type"] for cfg in job_configs)
                        profile_summary = ", ".join(f"{k}:{v}" for k, v in engine_counts.items())
                        board_summary = ", ".join(f"{k}:{v}" for k, v in board_counts.items())

                        logger.info(
                            f"NAT-blocked node {peer.node_id}: Queued {queued_count} selfplay jobs "
                            f"[engines: {profile_summary}] [boards: {board_summary}] "
                            f"(idle for {idle_duration:.0f}s, WorkerPullLoop will claim)"
                        )
                        return  # Success via queue path
                    else:
                        logger.warning(f"NAT-blocked node {peer.node_id}: Work queue unavailable, cannot dispatch")
                        return
                except Exception as e:
                    logger.warning(f"NAT-blocked node {peer.node_id}: Failed to queue work: {e}")
                    return

            # Non-NAT-blocked nodes: Direct HTTP push to /selfplay/start endpoint
            async def send_selfplay_request(session, payload):
                """Send a single selfplay start request."""
                async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                    if resp.status == 200:
                        return True
                    else:
                        body = await resp.text()
                        logger.warning(f"Failed selfplay request on {peer.node_id}: {resp.status} {body[:100]}")
                        return False

            async with get_client_session(timeout) as session:
                tasks = [send_selfplay_request(session, cfg) for cfg in job_configs]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                started = 0
                failed = 0
                for r in results:
                    if isinstance(r, Exception):
                        failed += 1
                        logger.debug(f"Selfplay request failed with exception: {r}")
                    elif r is True:
                        started += 1
                if failed > 0:
                    logger.warning(f"Auto-start on {peer.node_id}: {failed}/{len(results)} requests failed")

                # Log profile distribution
                from collections import Counter
                engine_counts = Counter(cfg["engine_mode"] for cfg in job_configs)
                board_counts = Counter(cfg["board_type"] for cfg in job_configs)
                profile_summary = ", ".join(f"{k}:{v}" for k, v in engine_counts.items())
                board_summary = ", ".join(f"{k}:{v}" for k, v in board_counts.items())

                logger.info(f"Auto-started {started}/{num_processes} diverse selfplay on {peer.node_id} "
                           f"[engines: {profile_summary}] [boards: {board_summary}]")

        except Exception as e:  # noqa: BLE001
            logger.warning(f"Auto-start request failed for {peer.node_id}: {e}")

    # =========================================================================
    # PREDICTIVE SCALING HELPERS (January 2026 Sprint 6)
    # Support methods for PredictiveScalingLoop - proactive job spawning
    # =========================================================================

    def _get_work_queue(self) -> Any:
        """Get work queue instance for WorkQueueMaintenanceLoop.

        This method wraps the global get_work_queue() function to make it
        accessible via OrchestratorContext.from_orchestrator().

        Returns:
            Work queue instance, or None if unavailable.

        Note:
            January 11, 2026: Added to fix "'NoneType' object is not callable"
            error in WorkQueueMaintenanceLoop. The OrchestratorContext was
            looking for this method but it didn't exist.
        """
        return get_work_queue()

    def _get_work_queue_depth(self) -> int:
        """Get current work queue depth for predictive scaling decisions.

        Returns the number of pending work items in the queue. Used by
        PredictiveScalingLoop to determine when to spawn preemptive jobs.

        Returns:
            Number of pending work items, or 0 if queue unavailable.
        """
        try:
            wq = get_work_queue()
            if wq is None:
                return 0
            return wq.get_pending_count()
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Failed to get work queue depth: {e}")
            return 0

    def _get_pending_jobs_for_node(self, node_id: str) -> int:
        """Get count of pending/running jobs assigned to a specific node.

        Used by PredictiveScalingLoop to skip nodes that already have
        work pending, avoiding over-allocation.

        Args:
            node_id: The node identifier to check.

        Returns:
            Number of pending/running jobs for this node.
        """
        try:
            if self.job_manager is None:
                return 0
            # Count jobs that are pending or running for this node
            jobs = self.job_manager.get_jobs_for_node(node_id)
            return len([j for j in jobs if j.get("status") in ("pending", "running", "claimed")])
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Failed to get pending jobs for {node_id}: {e}")
            return 0

    async def _spawn_preemptive_selfplay_job(self, peer_info: dict[str, Any]) -> bool:
        """Spawn a preemptive selfplay job on a node approaching idle.

        Called by PredictiveScalingLoop when it detects a node with low
        GPU utilization and no pending work. This spawns a job BEFORE
        the node becomes fully idle to minimize launch latency.

        Args:
            peer_info: Peer information dict with node_id, gpu_utilization, etc.

        Returns:
            True if job was successfully spawned, False otherwise.
        """
        try:
            node_id = peer_info.get("node_id", "unknown")
            logger.info(f"[PredictiveScaling] Spawning preemptive job on {node_id}")

            # Use selfplay scheduler to pick the best config for this node
            if self.selfplay_scheduler is None:
                logger.debug("[PredictiveScaling] No selfplay scheduler, cannot spawn")
                return False

            # Get node-specific job recommendation
            job_recommendation = await self.selfplay_scheduler.get_job_for_node(node_id)
            if job_recommendation is None:
                logger.debug(f"[PredictiveScaling] No job recommendation for {node_id}")
                return False

            # Dispatch the job
            board_type = job_recommendation.get("board_type", "hex8")
            num_players = job_recommendation.get("num_players", 2)
            num_games = job_recommendation.get("num_games", 100)

            # Use job manager for dispatch
            if self.job_manager is None:
                logger.debug("[PredictiveScaling] No job manager, cannot dispatch")
                return False

            job_id = f"preemptive-{node_id}-{int(time.time())}"
            result = await self.job_manager.dispatch_selfplay_job(
                node_id=node_id,
                job_id=job_id,
                board_type=board_type,
                num_players=num_players,
                num_games=num_games,
                preemptive=True,  # Mark as preemptive for tracking
                engine_mode="mixed",  # Jan 12, 2026: Enable harness diversity
            )

            if result.get("success"):
                logger.info(
                    f"[PredictiveScaling] Spawned preemptive job {job_id} on {node_id} "
                    f"({board_type}_{num_players}p, {num_games} games)"
                )
                return True
            else:
                logger.debug(f"[PredictiveScaling] Failed to spawn on {node_id}: {result.get('error')}")
                return False

        except Exception as e:  # noqa: BLE001
            logger.debug(f"[PredictiveScaling] Exception spawning preemptive job: {e}")
            return False

    # =========================================================================
    # Support methods for JobReassignmentLoop - orphaned job recovery (Sprint 6)
    # =========================================================================

    def _get_healthy_node_ids_for_reassignment(self) -> list[str]:
        """Get list of healthy node IDs that can accept reassigned jobs.

        Used by JobReassignmentLoop to find nodes for orphaned job reassignment.
        A healthy node is one that:
        - Is currently alive in the peer list
        - Has recent health check data
        - Is not overloaded (CPU < 90%, GPU mem < 95%)

        Returns:
            List of node IDs suitable for job reassignment.
        """
        try:
            healthy_nodes = []
            for node_id, peer_info in self.peers.items():
                # Skip nodes that are marked as offline or failing
                status = peer_info.get("status", "unknown")
                if status in ("offline", "failing", "retired"):
                    continue

                # Check basic health metrics
                cpu_percent = peer_info.get("cpu_percent", 0.0)
                gpu_mem_percent = peer_info.get("gpu_memory_percent", 0.0)

                # Skip overloaded nodes
                if cpu_percent > 90.0:
                    continue
                if gpu_mem_percent > 95.0:
                    continue

                # Skip nodes without recent heartbeat
                last_seen = peer_info.get("last_seen", 0.0)
                if time.time() - last_seen > 120:  # 2 minutes stale
                    continue

                healthy_nodes.append(node_id)

            return healthy_nodes

        except Exception as e:
            logger.debug(f"[JobReassignment] Error getting healthy nodes: {e}")
            return []

    # =========================================================================
    # AUTOMATION LOOPS (2024-12)
    # These loops enable hands-free cluster operation
    # =========================================================================

    # NOTE: _auto_scaling_loop() removed Dec 2025 (101 LOC).
    # Now runs via LoopManager as AutoScalingLoop.
    # See scripts/p2p/loops/coordination_loops.py for implementation.

    # NOTE: _predictive_monitoring_loop() removed Dec 2025 (~98 LOC).
    # Now runs via LoopManager as PredictiveMonitoringLoop.
    # See scripts/p2p/loops/resilience_loops.py for implementation.

    # NOTE: _self_healing_loop() removed Dec 2025 (~71 LOC).
    # Now runs via LoopManager as SelfHealingLoop.
    # See scripts/p2p/loops/resilience_loops.py for implementation.

    # NOTE: _job_reaper_loop() removed Dec 2025 (60 LOC).
    # Now runs via LoopManager as JobReaperLoop.
    # See scripts/p2p/loops/job_loops.py for implementation.

    # NOTE: _get_ssh_config_for_reaper() removed Dec 2025 (~23 LOC).
    # No callers existed - job reaper now uses cluster_config helpers.

    # NOTE: _validation_loop() removed Dec 2025 (92 LOC).
    # Now runs via LoopManager as ValidationLoop.
    # See scripts/p2p/loops/validation_loop.py for implementation.

    # NOTE: _queue_populator_loop() removed Dec 2025 (82 LOC).
    # Now runs via LoopManager as QueuePopulatorLoop.
    # See scripts/p2p/loops/queue_populator_loop.py for implementation.

    async def handle_games_analytics(self, request: web.Request) -> web.Response:
        """GET /games/analytics - Game statistics for dashboards.

        Returns aggregated game analytics including:
        - Average game length by config
        - Victory type distribution
        - Games per hour throughput
        - Opening move diversity
        """
        import json
        from collections import defaultdict

        try:
            # Skip JSONL scanning during startup grace period
            if self._is_in_startup_grace_period():
                return web.json_response({"configs": {}, "message": "Startup in progress"})

            hours = int(request.query.get("hours", "24"))
            cutoff = time.time() - (hours * 3600)

            ai_root = Path(self._get_ai_service_path())
            data_dirs = [
                ai_root / "data" / "games" / "daemon_sync",
                ai_root / "data" / "selfplay",
            ]

            # Aggregation containers
            game_lengths: dict[str, list[int]] = defaultdict(list)
            victory_types: dict[str, dict[str, int]] = defaultdict(lambda: defaultdict(int))
            games_by_hour: dict[str, dict[int, int]] = defaultdict(lambda: defaultdict(int))
            opening_moves: dict[str, dict[str, int]] = defaultdict(lambda: defaultdict(int))
            total_games = 0

            for data_dir in data_dirs:
                if not data_dir.exists():
                    continue
                for jsonl_path in data_dir.rglob("*.jsonl"):
                    try:
                        if jsonl_path.stat().st_mtime < cutoff:
                            continue
                        with open_jsonl_file(jsonl_path) as f:
                            for line in f:
                                try:
                                    game = json.loads(line)
                                    board_type = game.get("board_type", "unknown")
                                    num_players = game.get("num_players", 0)
                                    config = f"{board_type}_{num_players}p"

                                    # Game length
                                    length = game.get("length", 0)
                                    if length > 0:
                                        game_lengths[config].append(length)

                                    # Victory type
                                    vt = game.get("victory_type", "unknown")
                                    if vt:
                                        victory_types[config][vt] += 1

                                    # Games by hour (for throughput)
                                    moves = game.get("moves", [])
                                    if moves and len(moves) > 0:
                                        # Use first move timestamp or file mtime
                                        hour_bucket = int(jsonl_path.stat().st_mtime // 3600)
                                        games_by_hour[config][hour_bucket] += 1

                                    # Opening moves (first 3 moves)
                                    if moves and len(moves) >= 1:
                                        first_move = str(moves[0].get("action", ""))[:20]
                                        if first_move:
                                            opening_moves[config][first_move] += 1

                                    total_games += 1
                                except json.JSONDecodeError:
                                    continue
                    except (OSError, ValueError, KeyError):
                        continue

            # Build response
            analytics = {
                "period_hours": hours,
                "total_games": total_games,
                "configs": {}
            }

            for config in set(list(game_lengths.keys()) + list(victory_types.keys())):
                lengths = game_lengths.get(config, [])
                vt = dict(victory_types.get(config, {}))
                openings = dict(opening_moves.get(config, {}))

                # Calculate throughput (games/hour)
                hourly = games_by_hour.get(config, {})
                throughput = sum(hourly.values()) / max(len(hourly), 1) if hourly else 0

                analytics["configs"][config] = {
                    "games": len(lengths),
                    "avg_length": round(sum(lengths) / len(lengths), 1) if lengths else 0,
                    "min_length": min(lengths) if lengths else 0,
                    "max_length": max(lengths) if lengths else 0,
                    "victory_types": vt,
                    "throughput_per_hour": round(throughput, 1),
                    "opening_diversity": len(openings),
                    "top_openings": dict(sorted(openings.items(), key=lambda x: -x[1])[:5]),
                }

            return web.json_response(analytics)

        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)})

    async def handle_training_metrics(self, request: web.Request) -> web.Response:
        """GET /training/metrics - Training loss and accuracy metrics.

        Returns recent training metrics from log files.
        """
        import re

        try:
            ai_root = Path(self._get_ai_service_path())
            logs_dir = ai_root / "logs" / "training"

            metrics = {
                "configs": {},
                "latest_training": None,
            }

            if not logs_dir.exists():
                return web.json_response(metrics)

            # Find recent training logs
            log_files = sorted(logs_dir.glob("*.log"), key=lambda f: f.stat().st_mtime, reverse=True)[:10]

            for log_file in log_files:
                try:
                    content = log_file.read_text()

                    # Extract config from filename (e.g., train_square8_2p_20251214.log)
                    config_match = re.search(r"(square\d+|hexagonal|hex)_(\d+)p", log_file.name)
                    if not config_match:
                        continue
                    config = f"{config_match.group(1)}_{config_match.group(2)}p"

                    # Parse training metrics from log
                    # Look for patterns like: "Epoch 5: loss=0.423, policy_loss=0.312, value_loss=0.111"
                    loss_pattern = re.compile(
                        r"[Ee]poch\s+(\d+).*?loss[=:]\s*([\d.]+).*?"
                        r"(?:policy[_\s]?loss[=:]\s*([\d.]+))?.*?"
                        r"(?:value[_\s]?loss[=:]\s*([\d.]+))?"
                    )

                    epochs = []
                    for match in loss_pattern.finditer(content):
                        epoch = int(match.group(1))
                        total_loss = float(match.group(2))
                        policy_loss = float(match.group(3)) if match.group(3) else None
                        value_loss = float(match.group(4)) if match.group(4) else None
                        epochs.append({
                            "epoch": epoch,
                            "loss": total_loss,
                            "policy_loss": policy_loss,
                            "value_loss": value_loss,
                        })

                    if epochs:
                        metrics["configs"][config] = {
                            "log_file": log_file.name,
                            "epochs": epochs[-20:],  # Last 20 epochs
                            "latest_loss": epochs[-1]["loss"] if epochs else None,
                            "latest_epoch": epochs[-1]["epoch"] if epochs else None,
                        }
                        if not metrics["latest_training"]:
                            metrics["latest_training"] = {
                                "config": config,
                                "file": log_file.name,
                                "mtime": log_file.stat().st_mtime,
                            }

                except (OSError, ValueError, KeyError, json.JSONDecodeError):
                    continue

            return web.json_response(metrics)

        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)})

    async def handle_holdout_metrics(self, request: web.Request) -> web.Response:
        """GET /holdout/metrics - Holdout validation metrics.

        Returns holdout set statistics and evaluation results for overfitting detection.
        Supports optional query params:
            - config: Filter by config (e.g., square8_2p)
        """
        try:
            config_filter = request.query.get("config")
            metrics = await self._get_holdout_metrics_cached()

            if config_filter:
                # Filter to specific config
                filtered = {
                    "configs": {k: v for k, v in metrics.get("configs", {}).items() if k == config_filter},
                    "evaluations": [e for e in metrics.get("evaluations", []) if e.get("config") == config_filter],
                    "summary": metrics.get("summary", {}),
                }
                return web.json_response(filtered)

            return web.json_response(metrics)

        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)})

    # handle_holdout_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    async def handle_mcts_stats(self, request: web.Request) -> web.Response:
        """GET /mcts/stats - MCTS search statistics.

        Returns MCTS performance metrics including nodes/move, search depth, and timing.
        """
        try:
            stats = await self._get_mcts_stats_cached()
            return web.json_response(stats)

        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)})

    # handle_mcts_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    # =========================================================================
    # Feature Endpoints
    # =========================================================================

    async def handle_matchup_matrix(self, request: web.Request) -> web.Response:
        """GET /matchups/matrix - Head-to-head matchup statistics."""
        try:
            matrix = await self._get_matchup_matrix_cached()
            return web.json_response(matrix)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)})

    # handle_matchup_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    async def handle_model_lineage(self, request: web.Request) -> web.Response:
        """GET /models/lineage - Model ancestry and generation tracking."""
        try:
            lineage = await self._get_model_lineage_cached()
            return web.json_response(lineage)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)})

    # handle_model_lineage_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    async def handle_data_quality(self, request: web.Request) -> web.Response:
        """GET /data/quality - Data quality metrics and issue detection."""
        try:
            quality = await self._get_data_quality_cached()
            return web.json_response(quality)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)})

    # handle_data_quality_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)
    # handle_data_quality_issues() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    async def handle_training_efficiency(self, request: web.Request) -> web.Response:
        """GET /training/efficiency - Training efficiency and cost metrics."""
        try:
            efficiency = await self._get_training_efficiency_cached()
            return web.json_response(efficiency)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)})

    # handle_training_efficiency_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    # NOTE: Rollback handlers moved to RecoveryHandlersMixin (Jan 2026 - P2P Modularization Phase 2b)
    # - handle_rollback_status, handle_rollback_execute, handle_rollback_auto
    # - handle_rollback_candidates was already moved to TableHandlersMixin
    # See scripts/p2p/handlers/recovery.py for implementation.

    async def handle_autoscale_metrics(self, request: web.Request) -> web.Response:
        """GET /autoscale/metrics - Autoscaling metrics and recommendations."""
        try:
            metrics = await self._get_autoscaling_metrics()
            return web.json_response(metrics)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)})

    # handle_autoscale_recommendations() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)


    async def handle_resource_utilization_history(self, request: web.Request) -> web.Response:
        """GET /resource/history - Resource utilization history for graphing.

        Query params:
            node_id: Specific node (optional, defaults to cluster average)
            hours: Hours of history (default: 1)
        """
        try:
            if not HAS_NEW_COORDINATION:
                return web.json_response([])

            node_id = request.query.get("node_id")
            hours = float(request.query.get("hours", "1"))

            optimizer = get_resource_optimizer()
            history = optimizer.get_utilization_history(node_id=node_id, hours=hours)
            return web.json_response(history)
        except (ValueError, AttributeError):
            return web.json_response([])

    async def handle_webhook_test(self, request: web.Request) -> web.Response:
        """POST /webhook/test - Test webhook notification.

        Query params:
            level: debug/info/warning/error (default: info)
            message: Custom message (default: "Test notification")
        """
        try:
            level = request.query.get("level", "info")
            message = request.query.get("message", "Test notification from RingRift AI orchestrator")

            has_slack = bool(self.notifier.slack_webhook)
            has_discord = bool(self.notifier.discord_webhook)

            if not has_slack and not has_discord:
                return web.json_response({
                    "success": False,
                    "message": "No webhooks configured. Set RINGRIFT_SLACK_WEBHOOK and/or RINGRIFT_DISCORD_WEBHOOK",
                })

            await self.notifier.send(
                title="Webhook Test",
                message=message,
                level=level,
                fields={
                    "Node": self.node_id,
                    "Timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "Level": level.upper(),
                },
                node_id=self.node_id,
            )

            return web.json_response({
                "success": True,
                "message": f"Test notification sent to {'Slack' if has_slack else ''}{' and ' if has_slack and has_discord else ''}{'Discord' if has_discord else ''}",
                "level": level,
            })
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=500)

    async def handle_trends_summary(self, request: web.Request) -> web.Response:
        """GET /trends/summary - Get summary of metrics over time period.

        Query params:
            hours: Time period in hours (default: 24)
        """
        try:
            hours = float(request.query.get("hours", "24"))
            # Jan 12, 2026: Wrap blocking SQLite call to avoid blocking event loop
            summary = await asyncio.to_thread(self.get_metrics_summary, hours)
            return web.json_response(summary)
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=500)

    async def handle_trends_history(self, request: web.Request) -> web.Response:
        """GET /trends/history - Get historical metrics data.

        Query params:
            metric: Metric type (required) - e.g., "best_elo", "games_generated", "training_loss"
            hours: Time period in hours (default: 24)
            board: Board type filter (optional) - e.g., "square8"
            players: Number of players filter (optional) - e.g., 2
            limit: Max records to return (default: 1000)
        """
        try:
            metric_type = request.query.get("metric")
            if not metric_type:
                return web.json_response({"error": "Missing required parameter: metric"}, status=400)

            hours = float(request.query.get("hours", "24"))
            board_type = request.query.get("board")
            num_players = int(request.query.get("players")) if request.query.get("players") else None
            limit = int(request.query.get("limit", "1000"))

            # Jan 12, 2026: Wrap blocking SQLite call to avoid blocking event loop
            history = await asyncio.to_thread(
                self.get_metrics_history,
                metric_type=metric_type,
                board_type=board_type,
                num_players=num_players,
                hours=hours,
                limit=limit,
            )

            return web.json_response({
                "metric": metric_type,
                "period_hours": hours,
                "count": len(history),
                "data": history,
            })
        except Exception as e:  # noqa: BLE001
            return web.json_response({"error": str(e)}, status=500)

    # handle_trends_table() moved to TableHandlersMixin (Dec 28, 2025 - Phase 8)

    # ==================== A/B Testing Framework ====================

    def _calculate_ab_test_stats(self, test_id: str) -> dict[str, Any]:
        """Calculate statistical significance for an A/B test."""
        import math

        try:
            # Phase 3.4 Dec 29, 2025: Use context manager to prevent connection leaks
            with safe_db_connection(self.db_path) as conn:
                cursor = conn.cursor()

                # Get game results
                cursor.execute("""
                    SELECT model_a_result, model_a_score, model_b_score, game_length
                    FROM ab_test_games WHERE test_id = ?
                """, (test_id,))
                games = cursor.fetchall()

            if not games:
                return {
                    "games_played": 0,
                    "model_a_wins": 0,
                    "model_b_wins": 0,
                    "draws": 0,
                    "model_a_score": 0.0,
                    "model_b_score": 0.0,
                    "model_a_winrate": 0.0,
                    "model_b_winrate": 0.0,
                    "confidence": 0.0,
                    "likely_winner": None,
                    "statistically_significant": False,
                }

            # Count results
            model_a_wins = sum(1 for g in games if g[0] == "win")
            model_b_wins = sum(1 for g in games if g[0] == "loss")
            draws = sum(1 for g in games if g[0] == "draw")
            total = len(games)

            model_a_score = sum(g[1] for g in games)
            model_b_score = sum(g[2] for g in games)

            # Winrate (using score, e.g., 1 for win, 0.5 for draw, 0 for loss)
            model_a_winrate = model_a_score / total if total > 0 else 0.0
            model_b_winrate = model_b_score / total if total > 0 else 0.0

            # Wilson score confidence interval for statistical significance
            # Using normal approximation for simplicity
            def wilson_ci(wins: int, n: int, z: float = 1.96) -> tuple[float, float]:
                if n == 0:
                    return (0.0, 1.0)
                p = wins / n
                denominator = 1 + z * z / n
                center = (p + z * z / (2 * n)) / denominator
                spread = z * math.sqrt((p * (1 - p) + z * z / (4 * n)) / n) / denominator
                return (max(0, center - spread), min(1, center + spread))

            # Calculate confidence intervals
            a_lo, a_hi = wilson_ci(model_a_wins + draws // 2, total)
            b_lo, b_hi = wilson_ci(model_b_wins + draws // 2, total)

            # Determine if statistically significant (non-overlapping CIs)
            statistically_significant = a_hi < b_lo or b_hi < a_lo

            # Estimate confidence based on score difference and sample size
            if total > 0:
                score_diff = abs(model_a_winrate - model_b_winrate)
                # Rough confidence estimate (higher with more games and larger diff)
                confidence = min(0.99, 1 - math.exp(-total * score_diff * 2))
            else:
                confidence = 0.0

            # Determine likely winner
            likely_winner = None
            if model_a_winrate > model_b_winrate + 0.05:
                likely_winner = "model_a"
            elif model_b_winrate > model_a_winrate + 0.05:
                likely_winner = "model_b"

            avg_game_length = sum(g[3] for g in games if g[3]) / max(1, sum(1 for g in games if g[3]))

            return {
                "games_played": total,
                "model_a_wins": model_a_wins,
                "model_b_wins": model_b_wins,
                "draws": draws,
                "model_a_score": model_a_score,
                "model_b_score": model_b_score,
                "model_a_winrate": round(model_a_winrate, 4),
                "model_b_winrate": round(model_b_winrate, 4),
                "confidence": round(confidence, 4),
                "likely_winner": likely_winner,
                "statistically_significant": statistically_significant,
                "avg_game_length": round(avg_game_length, 1),
            }
        except Exception as e:  # noqa: BLE001
            return {"error": str(e)}

    # A/B Test handlers moved to scripts/p2p/handlers/abtest.py (Dec 28, 2025 - Phase 8)
    # Inherited from ABTestHandlersMixin:
    # - handle_abtest_create, handle_abtest_result, handle_abtest_status
    # - handle_abtest_list, handle_abtest_cancel, handle_abtest_run
    # Note: handle_abtest_table() was previously moved to TableHandlersMixin

    async def handle_api_training_status(self, request: web.Request) -> web.Response:
        """Get training pipeline status including NNUE, CMAES, and auto-promotion state.

        Returns daemon state for NNUE training, CMAES optimization, and model promotion.
        """
        try:
            from datetime import datetime

            ai_root = Path(self._get_ai_service_path())

            # Load daemon state (from continuous_improvement_daemon.py)
            daemon_state_path = ai_root / "logs" / "improvement_daemon" / "state.json"
            daemon_state = {}
            daemon_running = False
            daemon_pid = None
            daemon_uptime = 0

            # Check if daemon is running
            pid_file = ai_root / "logs" / "improvement_daemon" / "daemon.pid"
            if pid_file.exists():
                try:
                    daemon_pid = int(pid_file.read_text().strip())
                    # Check if process is running
                    import os
                    os.kill(daemon_pid, 0)  # Doesn't kill, just checks
                    daemon_running = True
                except (ValueError, ProcessLookupError, PermissionError):
                    daemon_running = False

            if daemon_state_path.exists():
                try:
                    daemon_state = json.loads(daemon_state_path.read_text())
                    # Calculate uptime if daemon is running
                    if daemon_running and daemon_state.get("started_at"):
                        started = datetime.fromisoformat(daemon_state["started_at"])
                        daemon_uptime = (datetime.now() - started).total_seconds()
                except (json.JSONDecodeError, ValueError, OSError):
                    pass

            # Load runtime overrides (promoted models)
            overrides_path = ai_root / "data" / "ladder_runtime_overrides.json"
            runtime_overrides = {}
            if overrides_path.exists():
                with contextlib.suppress(json.JSONDecodeError, ValueError, OSError):
                    runtime_overrides = json.loads(overrides_path.read_text())

            # Load auto-promotion log
            promotion_log_path = (
                ai_root / "runs" / "promotion" / "model_promotion_history.json"
                if (ai_root / "runs" / "promotion" / "model_promotion_history.json").exists()
                else (ai_root / "data" / "auto_promotion_log.json")
            )
            promotion_log = []
            if promotion_log_path.exists():
                try:
                    promotion_log = json.loads(promotion_log_path.read_text())
                    if isinstance(promotion_log, list):
                        promotion_log = promotion_log[-10:]  # Last 10 entries
                except (json.JSONDecodeError, ValueError, OSError):
                    pass

            # Check NNUE model timestamps
            nnue_models = {}
            nnue_dir = ai_root / "models" / "nnue"
            if nnue_dir.exists():
                for model_file in nnue_dir.glob("*.pt"):
                    if "_prev" not in model_file.name:
                        stat = model_file.stat()
                        nnue_models[model_file.stem] = {
                            "path": str(model_file),
                            "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                            "size_mb": round(stat.st_size / 1024 / 1024, 2),
                        }

            # Check trained heuristic profiles
            profiles_path = ai_root / "data" / "trained_heuristic_profiles.json"
            heuristic_profiles = {}
            if profiles_path.exists():
                try:
                    profiles_data = json.loads(profiles_path.read_text())
                    heuristic_profiles = {
                        "count": len(profiles_data),
                        "profiles": list(profiles_data.keys())[:20],
                    }
                except (json.JSONDecodeError, ValueError, OSError):
                    pass

            return web.json_response({
                "success": True,
                "daemon": {
                    "running": daemon_running,
                    "pid": daemon_pid,
                    "uptime_seconds": daemon_uptime,
                    "current_cycle": daemon_state.get("total_cycles", 0),
                    "last_cycle_at": daemon_state.get("last_cycle_at", ""),
                    "total_games_generated": daemon_state.get("total_games_generated", 0),
                    "total_training_runs": daemon_state.get("total_training_runs", 0),
                    "total_tournaments": daemon_state.get("total_tournaments", 0),
                    "total_auto_promotions": daemon_state.get("total_auto_promotions", 0),
                    "last_auto_promote_time": daemon_state.get("last_auto_promote_time", 0),
                    "consecutive_failures": daemon_state.get("consecutive_failures", 0),
                },
                "nnue": {
                    "state": "idle" if not daemon_state.get("nnue_state") else "active",
                    "models": list(nnue_models.keys()),
                    "model_details": nnue_models,
                    "per_config_state": daemon_state.get("nnue_state", {}),
                    "last_gate_result": daemon_state.get("last_nnue_gate_result", None),
                },
                "cmaes": {
                    "state": "idle" if not daemon_state.get("cmaes_state") else "active",
                    "profiles": heuristic_profiles.get("profiles", []) if heuristic_profiles else [],
                    "profile_count": heuristic_profiles.get("count", 0) if heuristic_profiles else 0,
                    "per_config_state": daemon_state.get("cmaes_state", {}),
                    "generations": sum(s.get("generations", 0) for s in daemon_state.get("cmaes_state", {}).values()),
                },
                "promotion": {
                    "runtime_overrides": runtime_overrides,
                    "recent_promotions": promotion_log,
                },
                "timestamp": time.time(),
            })

        except Exception as e:  # noqa: BLE001
            return web.json_response({"success": False, "error": str(e)}, status=500)

    # =========================================================================
    # NOTE: Canonical gate handlers moved to scripts/p2p/handlers/canonical_gate.py (Dec 28, 2025 - Phase 8)
    # Inherited from CanonicalGateHandlersMixin:
    # - _canonical_slug_for_board, _canonical_gate_paths, _tail_text_file, _canonical_gate_log_dir
    # - _monitor_canonical_gate_job
    # - handle_api_canonical_health, handle_api_canonical_jobs_list, handle_api_canonical_job_get
    # - handle_api_canonical_job_log, handle_api_canonical_logs_list, handle_api_canonical_log_tail
    # - handle_api_canonical_generate, handle_api_canonical_job_cancel
    # =========================================================================

    # =========================================================================
    # NOTE: Jobs API handlers moved to scripts/p2p/handlers/jobs_api.py (Dec 28, 2025 - Phase 8)
    # Inherited from JobsApiHandlersMixin:
    # - handle_api_jobs_list, handle_api_jobs_submit, handle_api_job_get, handle_api_job_cancel
    # - _get_job_type_enum (helper for lazy JobType import)
    # =========================================================================


    async def _run_evaluation(self, job_id: str):
        """Evaluate new model against current best.

        Runs evaluation games between the candidate model and the best model.
        Reports win rate for the candidate.
        """
        import json as json_module
        import sys

        state = self.improvement_loop_state.get(job_id)
        if not state:
            return

        logger.info(f"Running evaluation for job {job_id}, iteration {state.current_iteration}")

        getattr(state, 'candidate_model_path', None)

        # Number of evaluation games
        eval_games = 100

        eval_script = f"""
import sys
sys.path.insert(0, '{self._get_ai_service_path()}')
from app.game_engine import GameEngine
from app.agents.heuristic_agent import HeuristicAgent
import json

# Run evaluation games
candidate_wins = 0
best_wins = 0
draws = 0

for game_idx in range({eval_games}):
    engine = GameEngine(board_type='{state.board_type}', num_players={state.num_players})

    # Alternate who plays first
    if game_idx % 2 == 0:
        agents = [
            HeuristicAgent(0),  # Candidate as player 0
            HeuristicAgent(1),  # Best as player 1
        ]
        candidate_player = 0
    else:
        agents = [
            HeuristicAgent(0),  # Best as player 0
            HeuristicAgent(1),  # Candidate as player 1
        ]
        candidate_player = 1

    # Play game
    max_moves = 10000
    move_count = 0
    while not engine.is_game_over() and move_count < max_moves:
        current_player = engine.current_player
        agent = agents[current_player]
        legal_moves = engine.get_legal_moves()
        if not legal_moves:
            break
        move = agent.select_move(engine.get_state(), legal_moves)
        engine.apply_move(move)
        move_count += 1

    outcome = engine.get_outcome()
    winner = outcome.get('winner')

    if winner == candidate_player:
        candidate_wins += 1
    elif winner is not None:
        best_wins += 1
    else:
        draws += 1

# Calculate win rate
total = candidate_wins + best_wins + draws
winrate = candidate_wins / total if total > 0 else 0.5

print(json.dumps({{
    'candidate_wins': candidate_wins,
    'best_wins': best_wins,
    'draws': draws,
    'winrate': winrate,
}}))
"""

        cmd = [sys.executable, "-c", eval_script]
        env = os.environ.copy()
        env["PYTHONPATH"] = self._get_ai_service_path()
        env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

        try:
            proc = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env,
            )

            stdout, stderr = await asyncio.wait_for(
                proc.communicate(),
                timeout=3600  # 1 hour max
            )

            if proc.returncode == 0:
                output_lines = stdout.decode().strip().split('\n')
                result_line = output_lines[-1] if output_lines else '{}'
                result = json_module.loads(result_line)

                state.evaluation_winrate = result.get('winrate', 0.5)
                logger.info(f"Evaluation result: winrate={state.evaluation_winrate:.2%}")
                logger.info("  Candidate")
            else:
                logger.info(f"Evaluation failed: {stderr.decode()[:500]}")
                state.evaluation_winrate = 0.5

        except asyncio.TimeoutError:
            logger.info("Evaluation timed out")
            state.evaluation_winrate = 0.5
        except Exception as e:  # noqa: BLE001
            logger.info(f"Evaluation error: {e}")
            state.evaluation_winrate = 0.5

    async def _promote_model_if_better(self, job_id: str):
        """Promote new model if it beats the current best.

        Promotion threshold: candidate must win >= 55% of evaluation games.
        """
        state = self.improvement_loop_state.get(job_id)
        if not state:
            return

        PROMOTION_THRESHOLD = 0.55  # 55% win rate required

        winrate = getattr(state, 'evaluation_winrate', 0.5)
        candidate_path = getattr(state, 'candidate_model_path', None)

        logger.info(f"Checking model promotion for job {job_id}")
        logger.info("  Current")
        logger.info("  Candidate")
        logger.info("  Threshold")

        if winrate >= PROMOTION_THRESHOLD and candidate_path:
            # Promote candidate to best
            state.best_model_path = candidate_path
            state.best_winrate = winrate

            # Save best model to well-known location
            best_model_dir = os.path.join(
                self._get_ai_service_path(), "models", "best"
            )
            os.makedirs(best_model_dir, exist_ok=True)

            import shutil
            best_path = os.path.join(best_model_dir, f"{state.board_type}_{state.num_players}p.pt")
            if os.path.exists(candidate_path):
                shutil.copy2(candidate_path, best_path)
                logger.info(f"PROMOTED: New best model at {best_path}")
                logger.info(f"  Win rate: {winrate:.2%}")
            else:
                logger.info(f"Cannot promote: candidate model not found at {candidate_path}")
        else:
            logger.info(f"No promotion: candidate ({winrate:.2%}) below threshold ({PROMOTION_THRESHOLD:.0%})")

    # ============================================
    # Core Logic
    # ============================================

    def _update_self_info(self):
        """Update self info with current resource usage."""
        usage = self._get_resource_usage()
        selfplay, training = self._count_local_jobs()

        # NAT/relay detection: if we haven't received any inbound heartbeats for a
        # while (but we do know about other peers), assume we're not reachable
        # inbound and must poll a relay for commands.
        now = time.time()
        if self.known_peers or self.peers:
            last_inbound = self.last_inbound_heartbeat or self.start_time
            self.self_info.nat_blocked = (now - last_inbound) >= NAT_INBOUND_HEARTBEAT_STALE_SECONDS
        else:
            self.self_info.nat_blocked = False

        if not self.self_info.nat_blocked:
            self.self_info.relay_via = ""
        elif self.leader_id and self.leader_id != self.node_id:
            self.self_info.relay_via = self.leader_id

        self.self_info.cpu_percent = usage["cpu_percent"]
        self.self_info.memory_percent = usage["memory_percent"]
        self.self_info.disk_percent = usage["disk_percent"]
        self.self_info.gpu_percent = usage["gpu_percent"]
        self.self_info.gpu_memory_percent = usage["gpu_memory_percent"]
        self.self_info.selfplay_jobs = selfplay
        # Jan 2, 2026: Set max slots for slot-based work queue claiming
        self.self_info.max_selfplay_slots = self._get_max_selfplay_slots_for_node()
        self.self_info.training_jobs = training
        self.self_info.role = self.role
        self.self_info.last_heartbeat = time.time()
        # Dec 2025: Propagate leader_id in heartbeats for cluster-wide leader discovery
        self.self_info.leader_id = self.leader_id or ""

        # Detect external work (running outside P2P orchestrator tracking)
        external = self._detect_local_external_work()
        self.self_info.cmaes_running = external.get('cmaes_running', False)
        self.self_info.gauntlet_running = external.get('gauntlet_running', False)
        self.self_info.tournament_running = external.get('tournament_running', False)
        self.self_info.data_merge_running = external.get('data_merge_running', False)

        # Phase 6: Health broadcasting - additional health metrics
        self.self_info.nfs_accessible = self._check_nfs_accessible()
        self.self_info.code_version = self.build_version
        self.self_info.errors_last_hour = getattr(self, '_error_count_last_hour', 0)
        self.self_info.disk_free_gb = usage.get("disk_free_gb", 0.0)
        self.self_info.active_job_count = (
            selfplay + training +
            (1 if self.self_info.cmaes_running else 0) +
            (1 if self.self_info.gauntlet_running else 0) +
            (1 if self.self_info.tournament_running else 0)
        )

        # Report to unified resource optimizer for cluster-wide coordination
        if HAS_NEW_COORDINATION:
            try:
                optimizer = get_resource_optimizer()
                node_resources = NodeResources(
                    node_id=self.node_id,
                    cpu_percent=usage["cpu_percent"],
                    gpu_percent=usage["gpu_percent"],
                    memory_percent=usage["memory_percent"],
                    disk_percent=usage["disk_percent"],
                    gpu_memory_percent=usage["gpu_memory_percent"],
                    cpu_count=int(getattr(self.self_info, "cpu_count", 0) or 0),
                    memory_gb=float(getattr(self.self_info, "memory_gb", 0) or 0),
                    has_gpu=bool(getattr(self.self_info, "has_gpu", False)),
                    gpu_name=str(getattr(self.self_info, "gpu_name", "") or ""),
                    active_jobs=selfplay + training,
                    selfplay_jobs=selfplay,
                    training_jobs=training,
                    orchestrator="p2p_orchestrator",
                )
                optimizer.report_node_resources(node_resources)
            except (ValueError, KeyError, IndexError, AttributeError):
                pass  # Don't fail heartbeat if optimizer unavailable

        # December 2025: Emit NODE_CAPACITY_UPDATED for backpressure detection
        # Sprint 10 (Jan 3, 2026): Use unified emitter for consistent payloads
        # Throttled to every 30 seconds to avoid event spam
        now = time.time()
        last_emit = getattr(self, "_last_capacity_emit_time", 0)
        if now - last_emit >= 30:  # 30s throttle matches backpressure cooldown
            self._last_capacity_emit_time = now
            try:
                from app.distributed.data_events import emit_node_capacity_updated_sync

                available_slots = max(0, self._get_max_selfplay_jobs() - selfplay - training)
                emit_node_capacity_updated_sync(
                    node_id=self.node_id,
                    gpu_utilization=usage["gpu_percent"],
                    cpu_utilization=usage["cpu_percent"],
                    available_slots=available_slots,
                    reason="heartbeat",
                    source="p2p_orchestrator",
                    queue_depth=getattr(self, "_work_queue_depth", 0),
                )
            except (ImportError, RuntimeError, AttributeError):
                pass  # Event system not available or no event loop

        # Jan 12, 2026: Sync host/port when advertise_host changes
        # Root cause fix: self.self_info.host was never updated after init,
        # causing heartbeats to broadcast stale IPs to all peers.
        if self.self_info.host != self.advertise_host:
            old_host = self.self_info.host
            self.self_info.host = self.advertise_host
            logger.info(f"[P2P] Updated self.self_info.host: {old_host} -> {self.advertise_host}")
        if self.self_info.port != self.advertise_port:
            self.self_info.port = self.advertise_port

    def _set_advertise_host(self, new_host: str, reason: str = "") -> bool:
        """Atomically update advertise_host and self.self_info.host.

        Jan 12, 2026: Centralized setter to prevent host desync.

        Previously, multiple code paths could set advertise_host without
        updating self.self_info.host, causing heartbeats to broadcast stale IPs.
        This setter ensures both are always in sync.

        Args:
            new_host: New IP/hostname to advertise
            reason: Why the change is happening (for logging)

        Returns:
            True if host changed, False if no change needed
        """
        if not new_host or new_host == self.advertise_host:
            return False

        old_host = self.advertise_host
        self.advertise_host = new_host

        # CRITICAL: Update self.self_info snapshot immediately
        if hasattr(self, "self_info") and self.self_info:
            self.self_info.host = new_host
            self.self_info.last_heartbeat = time.time()

        reason_str = f" ({reason})" if reason else ""
        logger.info(f"[P2P] advertise_host changed: {old_host} -> {new_host}{reason_str}")
        return True

    async def _update_self_info_async(self, cache_ttl: float = 5.0):
        """Async version of _update_self_info() to avoid blocking event loop.

        Dec 30, 2025: Added to fix gossip latency issues on coordinator nodes.
        The sync version calls subprocess for resource detection which blocks
        the event loop. This async version uses asyncio.to_thread() for those
        blocking operations.

        Jan 12, 2026: Added caching to reduce health endpoint latency from 3-6s
        to <100ms for repeated requests. Resource metrics are cached for cache_ttl
        seconds (default 5s) since they don't change rapidly.

        Args:
            cache_ttl: How long to cache resource metrics (seconds). Default 5s.
        """
        import asyncio

        # Jan 12, 2026: Check cache to avoid expensive resource detection on every request
        now = time.time()
        cache_key = "_self_info_cache_time"
        last_update = getattr(self, cache_key, 0)
        if (now - last_update) < cache_ttl:
            # Cache hit - self_info already has recent data
            return

        # Run blocking operations in thread pool
        usage = await self._get_resource_usage_async()
        selfplay, training = await asyncio.to_thread(self._count_local_jobs)

        # NAT/relay detection (fast, no subprocess)
        now = time.time()
        if self.known_peers or self.peers:
            last_inbound = self.last_inbound_heartbeat or self.start_time
            self.self_info.nat_blocked = (now - last_inbound) >= NAT_INBOUND_HEARTBEAT_STALE_SECONDS
        else:
            self.self_info.nat_blocked = False

        if not self.self_info.nat_blocked:
            self.self_info.relay_via = ""
        elif self.leader_id and self.leader_id != self.node_id:
            self.self_info.relay_via = self.leader_id

        self.self_info.cpu_percent = usage["cpu_percent"]
        self.self_info.memory_percent = usage["memory_percent"]
        self.self_info.disk_percent = usage["disk_percent"]
        self.self_info.gpu_percent = usage["gpu_percent"]
        self.self_info.gpu_memory_percent = usage["gpu_memory_percent"]
        self.self_info.selfplay_jobs = selfplay
        # Jan 2, 2026: Set max slots for slot-based work queue claiming
        self.self_info.max_selfplay_slots = self._get_max_selfplay_slots_for_node()
        self.self_info.training_jobs = training
        self.self_info.role = self.role
        self.self_info.last_heartbeat = time.time()
        self.self_info.leader_id = self.leader_id or ""

        # Run blocking external work detection in thread pool
        external = await asyncio.to_thread(self._detect_local_external_work)
        self.self_info.cmaes_running = external.get('cmaes_running', False)
        self.self_info.gauntlet_running = external.get('gauntlet_running', False)
        self.self_info.tournament_running = external.get('tournament_running', False)
        self.self_info.data_merge_running = external.get('data_merge_running', False)

        # Health metrics (NFS check in thread pool as it can block)
        self.self_info.nfs_accessible = await asyncio.to_thread(self._check_nfs_accessible)
        self.self_info.code_version = self.build_version
        self.self_info.errors_last_hour = getattr(self, '_error_count_last_hour', 0)
        self.self_info.disk_free_gb = usage.get("disk_free_gb", 0.0)
        self.self_info.active_job_count = (
            selfplay + training +
            (1 if self.self_info.cmaes_running else 0) +
            (1 if self.self_info.gauntlet_running else 0) +
            (1 if self.self_info.tournament_running else 0)
        )

        # Report to resource optimizer (fast, in-memory)
        if HAS_NEW_COORDINATION:
            try:
                optimizer = get_resource_optimizer()
                node_resources = NodeResources(
                    node_id=self.node_id,
                    cpu_percent=usage["cpu_percent"],
                    gpu_percent=usage["gpu_percent"],
                    memory_percent=usage["memory_percent"],
                    disk_percent=usage["disk_percent"],
                    gpu_memory_percent=usage["gpu_memory_percent"],
                    cpu_count=int(getattr(self.self_info, "cpu_count", 0) or 0),
                    memory_gb=float(getattr(self.self_info, "memory_gb", 0) or 0),
                    has_gpu=bool(getattr(self.self_info, "has_gpu", False)),
                    gpu_name=str(getattr(self.self_info, "gpu_name", "") or ""),
                    active_jobs=selfplay + training,
                    selfplay_jobs=selfplay,
                    training_jobs=training,
                    orchestrator="p2p_orchestrator",
                )
                optimizer.report_node_resources(node_resources)
            except (ValueError, KeyError, IndexError, AttributeError):
                pass

        # NODE_CAPACITY_UPDATED event (throttled, fast)
        # Sprint 10 (Jan 3, 2026): Use unified emitter for consistent payloads
        last_emit = getattr(self, "_last_capacity_emit_time", 0)
        if now - last_emit >= 30:
            self._last_capacity_emit_time = now
            try:
                from app.distributed.data_events import emit_node_capacity_updated_sync

                available_slots = max(0, self._get_max_selfplay_jobs() - selfplay - training)
                emit_node_capacity_updated_sync(
                    node_id=self.node_id,
                    gpu_utilization=usage["gpu_percent"],
                    cpu_utilization=usage["cpu_percent"],
                    available_slots=available_slots,
                    reason="heartbeat_async",
                    source="p2p_orchestrator",
                    queue_depth=getattr(self, "_work_queue_depth", 0),
                )
            except (ImportError, RuntimeError, AttributeError):
                pass

        # Jan 12, 2026: Update cache timestamp after successful update
        setattr(self, "_self_info_cache_time", time.time())

    async def _send_heartbeat_to_peer(self, peer_host: str, peer_port: int, scheme: str = "http", timeout: int = 15) -> NodeInfo | None:
        """Send heartbeat to a peer and return their info.

        Args:
            peer_host: Target peer hostname or IP
            peer_port: Target peer port
            scheme: HTTP or HTTPS scheme
            timeout: Request timeout in seconds (default 15, use smaller for voter heartbeats)

        Dec 2025: Added SSH fallback via HybridTransport when HTTP fails.
        Jan 11, 2026: Phase 3 - Increased default timeout from 10s to 15s for better
        reliability with relay/NAT nodes that can exceed 10s latency.
        """
        target = f"{peer_host}:{peer_port}"
        breaker = self._circuit_registry.get_breaker("p2p")

        # Check circuit breaker before attempting request
        if not breaker.can_execute(target):
            state = breaker.get_state(target)
            if state == CircuitState.OPEN:
                # Circuit is open - skip this peer temporarily
                return None

        http_failed = False
        # Prepare payload outside try block so it's available for SSH fallback
        self._update_self_info()
        payload = self.self_info.to_dict()
        voter_node_ids = list(getattr(self, "voter_node_ids", []) or [])
        if voter_node_ids:
            payload["voter_node_ids"] = voter_node_ids
            payload["voter_quorum_size"] = int(getattr(self, "voter_quorum_size", 0) or 0)
            payload["voter_config_source"] = str(getattr(self, "voter_config_source", "") or "")

        try:
            # Adjust timeout based on circuit state (shorter for half-open probing)
            effective_timeout = self._circuit_registry.get_timeout("p2p", target, float(timeout))
            client_timeout = ClientTimeout(total=effective_timeout)

            async with get_client_session(client_timeout) as session:
                scheme = (scheme or "http").lower()
                url = f"{scheme}://{peer_host}:{peer_port}/heartbeat"
                async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                    data, json_error = await safe_json_response(resp, default=None, log_errors=False)
                    if json_error or data is None:
                        # Record failure with circuit breaker for empty/invalid responses
                        breaker.record_failure(target)
                        http_failed = True
                    else:
                        incoming_voters = data.get("voter_node_ids") or data.get("voters") or None
                        if incoming_voters:
                            voters_list: list[str] = []
                            if isinstance(incoming_voters, list):
                                voters_list = [str(v).strip() for v in incoming_voters if str(v).strip()]
                            elif isinstance(incoming_voters, str):
                                voters_list = [t.strip() for t in incoming_voters.split(",") if t.strip()]
                            if voters_list:
                                self._maybe_adopt_voter_node_ids(voters_list, source="learned")
                        info = NodeInfo.from_dict(data)
                        if not info.reported_host:
                            info.reported_host = info.host
                        if not info.reported_port:
                            info.reported_port = info.port
                        # Use the address we successfully reached instead of any
                        # self-reported interface address.
                        info.scheme = scheme
                        info.host = peer_host
                        info.port = peer_port
                        # Record success with circuit breaker
                        breaker.record_success(target)

                        # Phase 27: Cache peer for persistence across restarts
                        self._save_peer_to_cache(
                            info.node_id,
                            peer_host,
                            peer_port,
                            str(getattr(info, "tailscale_ip", "") or "")
                        )
                        self._update_peer_reputation(info.node_id, success=True)

                        return info
        except (aiohttp.ClientError, asyncio.TimeoutError, OSError, ValueError, KeyError):
            # Record failure with circuit breaker
            breaker.record_failure(target)
            http_failed = True

        # Dec 2025: SSH fallback via HybridTransport when HTTP fails
        if http_failed and self.hybrid_transport:
            info = await self._send_heartbeat_via_ssh_fallback(peer_host, peer_port, payload)
            if info:
                breaker.record_success(target)
                return info

        return None

    async def _send_heartbeat_via_ssh_fallback(
        self, peer_host: str, peer_port: int, payload: dict[str, Any]
    ) -> NodeInfo | None:
        """Send heartbeat via SSH when HTTP fails.

        Dec 2025: Uses HybridTransport SSH fallback for nodes behind NAT or
        with unreachable HTTP endpoints.

        Args:
            peer_host: Target peer hostname or IP
            peer_port: Target peer port
            payload: Heartbeat payload dict

        Returns:
            NodeInfo if successful, None otherwise
        """
        if not self.hybrid_transport:
            return None

        # Try to find node_id for this host
        node_id = self._find_node_id_for_host(peer_host)
        if not node_id:
            return None

        try:
            success, response = await self.hybrid_transport.send_heartbeat(
                node_id=node_id,
                host=peer_host,
                port=peer_port,
                self_info=payload,
            )

            if success and response:
                info = NodeInfo.from_dict(response)
                if not info.reported_host:
                    info.reported_host = info.host
                if not info.reported_port:
                    info.reported_port = info.port
                info.scheme = "http"  # SSH proxied to local HTTP
                info.host = peer_host
                info.port = peer_port

                # Cache peer for persistence
                self._save_peer_to_cache(
                    info.node_id,
                    peer_host,
                    peer_port,
                    str(getattr(info, "tailscale_ip", "") or "")
                )
                self._update_peer_reputation(info.node_id, success=True)

                logger.debug(f"[P2P] SSH fallback heartbeat successful to {node_id}")
                return info

        except Exception as e:  # noqa: BLE001
            logger.debug(f"[P2P] SSH fallback heartbeat failed for {node_id}: {e}")

        return None

    def _find_node_id_for_host(self, host: str) -> str | None:
        """Find node_id for a given host address.

        Dec 2025: Looks up node_id from cluster_hosts.yaml or peer cache.

        Args:
            host: Hostname or IP address

        Returns:
            node_id if found, None otherwise
        """
        # Check peers first
        with self.peers_lock:
            for peer in self.peers.values():
                if peer.host == host or getattr(peer, "tailscale_ip", None) == host:
                    return peer.node_id

        # Check cluster_hosts.yaml
        try:
            from app.sync.cluster_hosts import get_cluster_nodes
            configured_hosts = get_cluster_nodes()
            for name, cfg in configured_hosts.items():
                if cfg.tailscale_ip == host or cfg.ssh_host == host or cfg.best_ip == host:
                    return name
        except (AttributeError, ImportError):
            pass

        return None

    async def _bootstrap_from_known_peers(self) -> bool:
        """Import cluster membership from seed peers via `/relay/peers`.

        Heartbeats intentionally return only a single peer's NodeInfo, which
        makes initial convergence slow when only one seed peer is configured
        (common for cloud nodes). `/relay/peers` returns a snapshot of the
        sender's full peer list, allowing new nodes to quickly learn about the
        leader and other cluster members.
        """
        # Seed peers are configured via `--peers`, but relying on a single
        # coordinator makes clusters brittle. Also bootstrap from any
        # previously-seen directly-reachable peers so nodes can re-join after
        # restarts even if the original seed goes offline.
        known_seed_peers: list[str] = [p for p in (self.known_peers or []) if p]
        discovered_seed_peers: list[str] = []

        with self.peers_lock:
            peers_snapshot = [p for p in self.peers.values() if p.node_id != self.node_id]
        peers_snapshot.sort(key=lambda p: str(getattr(p, "node_id", "") or ""))

        for peer in peers_snapshot:
            if getattr(peer, "nat_blocked", False):
                # NAT-blocked nodes cannot serve as inbound seeds.
                continue
            if not peer.should_retry():
                continue

            scheme = (getattr(peer, "scheme", "http") or "http").lower()
            host = str(getattr(peer, "host", "") or "").strip()
            try:
                port = int(getattr(peer, "port", DEFAULT_PORT) or DEFAULT_PORT)
            except (ValueError):
                port = DEFAULT_PORT
            if host:
                discovered_seed_peers.append(f"{scheme}://{host}:{port}")

            rh = str(getattr(peer, "reported_host", "") or "").strip()
            try:
                rp = int(getattr(peer, "reported_port", 0) or 0)
            except (ValueError):
                rp = 0
            if rh and rp:
                discovered_seed_peers.append(f"{scheme}://{rh}:{rp}")

        seen: set[str] = set()
        seed_peers: list[str] = []
        ki = 0
        di = 0
        while ki < len(known_seed_peers) or di < len(discovered_seed_peers):
            if ki < len(known_seed_peers):
                candidate = known_seed_peers[ki]
                ki += 1
                if candidate and candidate not in seen:
                    seen.add(candidate)
                    seed_peers.append(candidate)
            if di < len(discovered_seed_peers):
                candidate = discovered_seed_peers[di]
                di += 1
                if candidate and candidate not in seen:
                    seen.add(candidate)
                    seed_peers.append(candidate)
        if not seed_peers:
            return False

        now = time.time()
        if now - self.last_peer_bootstrap < PEER_BOOTSTRAP_INTERVAL:
            return False

        max_seeds = int(os.environ.get("RINGRIFT_P2P_BOOTSTRAP_MAX_SEEDS_PER_RUN", "8") or 8)
        max_seeds = max(1, min(max_seeds, 32))

        timeout = ClientTimeout(total=8)
        bootstrapped = False
        imported_any = False

        async with get_client_session(timeout) as session:
            for idx, peer_addr in enumerate(seed_peers):
                if idx >= max_seeds:
                    break
                try:
                    scheme, host, port = self._parse_peer_address(peer_addr)
                    scheme = (scheme or "http").lower()
                    url = f"{scheme}://{host}:{port}/relay/peers"
                    async with session.get(url, headers=self._auth_headers()) as resp:
                        if resp.status != 200:
                            continue
                        data = await resp.json()

                    if not isinstance(data, dict) or not data.get("success"):
                        continue

                    bootstrapped = True

                    incoming_voters = data.get("voter_node_ids") or data.get("voters") or None
                    if incoming_voters:
                        voters_list: list[str] = []
                        if isinstance(incoming_voters, list):
                            voters_list = [str(v).strip() for v in incoming_voters if str(v).strip()]
                        elif isinstance(incoming_voters, str):
                            voters_list = [t.strip() for t in incoming_voters.split(",") if t.strip()]
                        if voters_list:
                            self._maybe_adopt_voter_node_ids(voters_list, source="learned")

                    peers_data = data.get("peers") or {}
                    if not isinstance(peers_data, dict):
                        continue

                    with self.peers_lock:
                        before = len(self.peers)
                        for node_id, peer_dict in peers_data.items():
                            if not node_id or node_id == self.node_id:
                                continue
                            try:
                                info = NodeInfo.from_dict(peer_dict)
                            except (AttributeError):
                                continue
                            existing = self.peers.get(info.node_id)
                            if existing:
                                # Preserve relay/NAT routing and retirement state when merging peer snapshots.
                                if getattr(existing, "nat_blocked", False) and not getattr(info, "nat_blocked", False):
                                    info.nat_blocked = True
                                    info.nat_blocked_since = float(getattr(existing, "nat_blocked_since", 0.0) or 0.0) or time.time()
                                    info.last_nat_probe = float(getattr(existing, "last_nat_probe", 0.0) or 0.0)
                                if (getattr(existing, "relay_via", "") or "") and not (getattr(info, "relay_via", "") or ""):
                                    info.relay_via = str(getattr(existing, "relay_via", "") or "")
                                if getattr(existing, "retired", False):
                                    info.retired = True
                                    info.retired_at = float(getattr(existing, "retired_at", 0.0) or 0.0)
                                # Preserve local reachability diagnostics.
                                info.consecutive_failures = int(getattr(existing, "consecutive_failures", 0) or 0)
                                info.last_failure_time = float(getattr(existing, "last_failure_time", 0.0) or 0.0)

                            self.peers[info.node_id] = info
                        after = len(self.peers)

                    # Jan 12, 2026: Sync to lock-free snapshot after gossip merge
                    self._sync_peer_snapshot()

                    new = max(0, after - before)
                    if new:
                        imported_any = True
                        logger.info(f"Bootstrap: imported {new} new peers from {host}:{port}")

                    leader_id = str(data.get("leader_id") or "").strip()
                    if leader_id and leader_id != self.node_id:
                        # If we're currently leader but the seed reports a higher-priority
                        # leader, step down to converge quickly.
                        if self.role == NodeRole.LEADER and leader_id > self.node_id:
                            logger.info(f"Bootstrap: stepping down for leader {leader_id}")
                        # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                        self._set_leader(leader_id, reason="bootstrap_discover_leader", save_state=False)
                except (aiohttp.ClientError, asyncio.TimeoutError, json.JSONDecodeError, KeyError, ValueError):
                    continue

        self.last_peer_bootstrap = now
        if bootstrapped:
            self._maybe_adopt_leader_from_peers()
            self._save_state()
        return imported_any

    async def _continuous_bootstrap_loop(self) -> None:
        """Phase 26.3: Continuously attempt to join cluster when isolated.

        This loop runs on ALL nodes (not just leader) and ensures that
        isolated nodes can rejoin the cluster without manual intervention.

        Triggers when:
        - Less than MIN_CONNECTED_PEERS alive peers
        - No leader known

        Uses multi-seed bootstrap with:
        1. Cached peers (highest reputation first)
        2. CLI-provided peers
        3. Hardcoded BOOTSTRAP_SEEDS
        4. Tailscale network scan (fallback)
        """
        # Dec 30, 2025: Conditional startup grace period
        # - If --peers provided: Wait STARTUP_GRACE_PERIOD for other nodes to restart
        # - If no --peers: Start immediately (we're already isolated, no point waiting)
        # This enables fast mesh join when nodes start without explicit peer list.
        if self.known_peers:
            logger.debug(f"[ContinuousBootstrap] Waiting {STARTUP_GRACE_PERIOD}s grace period (have known peers)")
            await asyncio.sleep(STARTUP_GRACE_PERIOD)
        else:
            # No peers configured - skip grace period but wait briefly for HTTP server
            logger.info("[ContinuousBootstrap] No known peers, skipping startup grace period")
            await asyncio.sleep(5)

        while self.running:
            try:
                await asyncio.sleep(ISOLATED_BOOTSTRAP_INTERVAL)

                # Count alive peers
                with self.peers_lock:
                    peers_alive = sum(
                        1 for p in self.peers.values()
                        if p.node_id != self.node_id and p.is_alive()
                    )

                # Check if we're isolated (few peers or no leader)
                is_isolated = peers_alive < MIN_CONNECTED_PEERS
                no_leader = self.leader_id is None or (
                    self.leader_id != self.node_id and
                    self.leader_id not in self.peers
                )

                if is_isolated or no_leader:
                    if is_isolated:
                        logger.warning(
                            f"Isolated: only {peers_alive} alive peers "
                            f"(need {MIN_CONNECTED_PEERS}), attempting bootstrap..."
                        )
                    elif no_leader:
                        logger.warning(
                            f"No valid leader (current: {self.leader_id}), "
                            f"attempting bootstrap..."
                        )

                    # Try bootstrap from multiple sources
                    bootstrapped = await self._bootstrap_from_multiple_seeds()

                    if bootstrapped:
                        logger.info(
                            f"Bootstrap successful! "
                            f"Now have {len([p for p in self.peers.values() if p.is_alive()])} alive peers"
                        )
                        # Try to adopt leader from newly discovered peers
                        self._maybe_adopt_leader_from_peers()

                        # If still no leader, start election
                        if not self.leader_id:
                            # CRITICAL: Check quorum before starting election to prevent quorum bypass
                            if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                                logger.warning("Skipping election after bootstrap: no voter quorum available")
                            else:
                                await self._start_election()
                    else:
                        # Fallback: try Tailscale peer discovery
                        logger.info("Bootstrap from seeds failed, trying Tailscale discovery...")
                        await self._discover_tailscale_peers()

            except asyncio.CancelledError:
                break
            except Exception as e:  # noqa: BLE001
                logger.error(f"Error in continuous bootstrap loop: {e}")
                await asyncio.sleep(30)  # Back off on errors

    async def _bootstrap_from_multiple_seeds(self) -> bool:
        """Phase 26.3: Try multiple seeds until we join the cluster.

        Priority order:
        1. Cached peers with high reputation (from peer_cache table)
        2. CLI --peers (self.known_peers)
        3. Hardcoded BOOTSTRAP_SEEDS

        Returns True if we successfully connected to any peer.
        """
        # Build seed list with priority ordering
        all_seeds: list[str] = []
        seen: set[str] = set()

        # 1. First, try cached peers by reputation (if available)
        cached_peers = self._get_bootstrap_peers_by_reputation(limit=3)
        for seed in cached_peers:
            if seed and seed not in seen:
                seen.add(seed)
                all_seeds.append(seed)

        # 2. Then, CLI peers and hardcoded seeds (already merged in self.known_peers)
        for seed in self.known_peers:
            if seed and seed not in seen:
                seen.add(seed)
                all_seeds.append(seed)

        if not all_seeds:
            logger.warning("No bootstrap seeds available")
            return False

        # Limit attempts per cycle
        max_attempts = min(MIN_BOOTSTRAP_ATTEMPTS * 2, len(all_seeds))
        timeout = ClientTimeout(total=10)
        success = False

        async with get_client_session(timeout) as session:
            for idx, seed_addr in enumerate(all_seeds[:max_attempts]):
                try:
                    scheme, host, port = self._parse_peer_address(seed_addr)
                    scheme = (scheme or "http").lower()
                    url = f"{scheme}://{host}:{port}/relay/peers"

                    async with session.get(url, headers=self._auth_headers()) as resp:
                        if resp.status != 200:
                            self._update_peer_reputation(seed_addr, success=False)
                            continue

                        data = await resp.json()
                        if not isinstance(data, dict) or not data.get("success"):
                            self._update_peer_reputation(seed_addr, success=False)
                            continue

                    # Successfully got peer list
                    self._update_peer_reputation(seed_addr, success=True)
                    success = True

                    # Import peers
                    peers_data = data.get("peers") or {}
                    if isinstance(peers_data, dict):
                        with self.peers_lock:
                            for node_id, peer_dict in peers_data.items():
                                if node_id and node_id != self.node_id:
                                    try:
                                        info = NodeInfo.from_dict(peer_dict)
                                        self.peers[info.node_id] = info
                                        # Cache the peer for future restarts
                                        self._save_peer_to_cache(
                                            info.node_id,
                                            str(getattr(info, "host", "") or ""),
                                            int(getattr(info, "port", DEFAULT_PORT) or DEFAULT_PORT),
                                            str(getattr(info, "tailscale_ip", "") or "")
                                        )
                                    except (ValueError, KeyError, IndexError, AttributeError):
                                        continue

                        # Jan 12, 2026: Sync to lock-free snapshot after relay peer import
                        self._sync_peer_snapshot()

                    # Adopt leader if provided
                    leader_id = str(data.get("leader_id") or "").strip()
                    if leader_id and leader_id != self.node_id:
                        if self.role == NodeRole.LEADER:
                            logger.info(f"Stepping down for discovered leader: {leader_id}")
                        # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                        self._set_leader(leader_id, reason="continuous_bootstrap_discover_leader", save_state=False)

                    # Handle cluster epoch (Phase 29)
                    incoming_epoch = data.get("cluster_epoch")
                    if incoming_epoch is not None:
                        try:
                            epoch = int(incoming_epoch)
                            if epoch > self._cluster_epoch:
                                logger.info(f"Adopting higher cluster epoch: {epoch} (was {self._cluster_epoch})")
                                self._cluster_epoch = epoch
                                self._save_cluster_epoch()
                        except (ValueError, TypeError):
                            pass

                    # Import voter config if provided
                    incoming_voters = data.get("voter_node_ids") or data.get("voters")
                    if incoming_voters:
                        voters_list = []
                        if isinstance(incoming_voters, list):
                            voters_list = [str(v).strip() for v in incoming_voters if str(v).strip()]
                        elif isinstance(incoming_voters, str):
                            voters_list = [t.strip() for t in incoming_voters.split(",") if t.strip()]
                        if voters_list:
                            self._maybe_adopt_voter_node_ids(voters_list, source="learned")

                    self._save_state()
                    logger.info(f"Bootstrap from {host}:{port}: imported {len(peers_data)} peers")
                    break  # Success, no need to try more seeds

                except asyncio.TimeoutError:
                    self._update_peer_reputation(seed_addr, success=False)
                    continue
                except Exception as e:  # noqa: BLE001
                    self._update_peer_reputation(seed_addr, success=False)
                    if self.verbose:
                        logger.debug(f"Bootstrap seed {seed_addr} failed: {e}")
                    continue

        return success

    def _load_bootstrap_seeds_from_config(self) -> list[str]:
        """Load bootstrap seed peers from distributed_hosts.yaml.

        Selects stable coordinator and voter nodes as default seeds when no --peers provided.
        This enables automatic peer discovery via Tailscale even when CLI args are missing.

        Returns:
            List of seed peer URLs (e.g., ["http://100.x.x.x:8770", ...])

        December 30, 2025: Added for automatic P2P peer discovery.
        """
        try:
            from app.config.cluster_config import get_cluster_nodes, get_coordinator_node

            seeds: list[str] = []
            seen_ips: set[str] = set()

            # Primary: coordinator node (most stable)
            coord = get_coordinator_node()
            if coord and getattr(coord, "tailscale_ip", None):
                ip = str(coord.tailscale_ip)
                if ip and ip not in seen_ips:
                    seeds.append(f"http://{ip}:{DEFAULT_PORT}")
                    seen_ips.add(ip)

            # Secondary: voter nodes (stable, always online)
            try:
                nodes = get_cluster_nodes()
                for node in nodes.values():
                    if getattr(node, "role", "") == "voter" and getattr(node, "tailscale_ip", None):
                        ip = str(node.tailscale_ip)
                        if ip and ip not in seen_ips:
                            seeds.append(f"http://{ip}:{DEFAULT_PORT}")
                            seen_ips.add(ip)
                            if len(seeds) >= 5:
                                break
            except Exception:  # noqa: BLE001
                pass

            # Fallback: any active nodes with Tailscale IPs
            if len(seeds) < 3:
                try:
                    nodes = get_cluster_nodes()
                    for node in nodes.values():
                        if getattr(node, "tailscale_ip", None) and getattr(node, "is_active", True):
                            ip = str(node.tailscale_ip)
                            if ip and ip not in seen_ips:
                                seeds.append(f"http://{ip}:{DEFAULT_PORT}")
                                seen_ips.add(ip)
                                if len(seeds) >= 5:
                                    break
                except Exception:  # noqa: BLE001
                    pass

            if seeds:
                logger.debug(f"Loaded {len(seeds)} bootstrap seeds from config: {seeds[:3]}...")

            return seeds

        except ImportError:
            logger.debug("cluster_config not available for bootstrap seeds")
            return []
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Could not load bootstrap seeds from config: {e}")
            return []

    def _is_node_proxy_only(self, node_id: str) -> bool:
        """Check if a node is configured as proxy_only in distributed_hosts.yaml.

        Jan 13, 2026: Added to prevent proxy nodes from becoming cluster leaders.
        Proxy nodes are SSH jump hosts or API proxies with no AI/training capability.

        Args:
            node_id: Node identifier to check

        Returns:
            True if node has status="proxy_only" in config
        """
        # Jan 13, 2026: Known aliases for proxy nodes that may appear under different names
        # These are nodes that registered with a different name than their config entry
        PROXY_ALIASES = {
            "aws-staging": "aws-proxy",  # EC2 staging instance is the proxy
        }

        try:
            hosts = self._load_distributed_hosts().get("hosts", {})
            # Check direct name first
            node_config = hosts.get(node_id, {})
            if node_config.get("status", "") == "proxy_only":
                return True
            # Check if this is a known alias for a proxy node
            if node_id in PROXY_ALIASES:
                alias_config = hosts.get(PROXY_ALIASES[node_id], {})
                if alias_config.get("status", "") == "proxy_only":
                    logger.debug(
                        f"[ProxyCheck] {node_id} is alias for {PROXY_ALIASES[node_id]} (proxy_only)"
                    )
                    return True
            return False
        except Exception:  # noqa: BLE001
            return False

    def _load_distributed_hosts(self) -> dict[str, Any]:
        """Load distributed hosts configuration for NetworkHealthMixin.

        Required by NetworkHealthMixin for cross-verifying P2P mesh health
        against Tailscale connectivity.

        Returns:
            Dict with structure: {"hosts": {node_name: {config...}}}
            Each host config includes: tailscale_ip, p2p_enabled, p2p_port, etc.

        December 30, 2025: Added to fix /network/health endpoint.
        """
        try:
            from app.config.cluster_config import load_cluster_config

            config = load_cluster_config()
            hosts_raw = getattr(config, "hosts_raw", {})

            # Convert to the format expected by NetworkHealthMixin
            # hosts_raw already has the right structure: {node_name: {config_dict}}
            return {"hosts": hosts_raw}

        except ImportError:
            logger.debug("cluster_config not available for distributed hosts")
            return {"hosts": {}}
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Could not load distributed hosts: {e}")
            return {"hosts": {}}

    # NOTE: _follower_discovery_loop() removed Dec 2025 (75 LOC).
    # Now runs via LoopManager as FollowerDiscoveryLoop.
    # See scripts/p2p/loops/discovery_loop.py for implementation.
    # The loop uses callbacks: get_known_peers, query_peer_list, add_peer, is_leader.

    async def _send_relay_heartbeat(self, relay_url: str) -> dict[str, Any]:
        """Send heartbeat via relay endpoint for NAT-blocked nodes.

        This is used when the peer URL is HTTPS (indicating a relay/proxy endpoint)
        or when direct heartbeats fail consistently.

        Returns dict with:
        - success: bool
        - peers: dict of all cluster peers
        - leader_id: current leader
        """
        try:
            self._update_self_info()

            timeout = ClientTimeout(total=15)
            async with get_client_session(timeout) as session:
                # Use /relay/heartbeat endpoint
                url = f"{relay_url.rstrip('/')}/relay/heartbeat"
                payload = self.self_info.to_dict()
                if self.pending_relay_acks:
                    payload["relay_ack"] = sorted(self.pending_relay_acks)
                if self.pending_relay_results:
                    payload["relay_results"] = list(self.pending_relay_results)
                async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                    if resp.status != 200:
                        return {"success": False, "error": f"HTTP {resp.status}"}

                    data = await resp.json()
                    if not data.get("success"):
                        return {"success": False, "error": data.get("error", "Unknown error")}

                    incoming_voters = data.get("voter_node_ids") or data.get("voters") or None
                    if incoming_voters:
                        voters_list: list[str] = []
                        if isinstance(incoming_voters, list):
                            voters_list = [str(v).strip() for v in incoming_voters if str(v).strip()]
                        elif isinstance(incoming_voters, str):
                            voters_list = [t.strip() for t in incoming_voters.split(",") if t.strip()]
                        if voters_list:
                            self._maybe_adopt_voter_node_ids(voters_list, source="learned")

                    # Clear pending acks/results only after a successful round-trip.
                    self.pending_relay_acks.clear()
                    self.pending_relay_results.clear()

                    # Update our peer list with all peers from relay
                    peers_data = data.get("peers", {})
                    with self.peers_lock:
                        for node_id, peer_dict in peers_data.items():
                            if node_id != self.node_id:
                                peer_info = NodeInfo.from_dict(peer_dict)
                                existing = self.peers.get(node_id)
                                if existing:
                                    if getattr(existing, "nat_blocked", False) and not getattr(peer_info, "nat_blocked", False):
                                        peer_info.nat_blocked = True
                                        peer_info.nat_blocked_since = float(getattr(existing, "nat_blocked_since", 0.0) or 0.0) or time.time()
                                        peer_info.last_nat_probe = float(getattr(existing, "last_nat_probe", 0.0) or 0.0)
                                    if (getattr(existing, "relay_via", "") or "") and not (getattr(peer_info, "relay_via", "") or ""):
                                        peer_info.relay_via = str(getattr(existing, "relay_via", "") or "")
                                    if getattr(existing, "retired", False):
                                        peer_info.retired = True
                                        peer_info.retired_at = float(getattr(existing, "retired_at", 0.0) or 0.0)
                                    peer_info.consecutive_failures = int(getattr(existing, "consecutive_failures", 0) or 0)
                                    peer_info.last_failure_time = float(getattr(existing, "last_failure_time", 0.0) or 0.0)
                                self.peers[node_id] = peer_info

                    # Execute any queued commands addressed to us.
                    commands = data.get("commands") or []
                    if isinstance(commands, list) and commands:
                        await self._execute_relay_commands(commands)

                    # Update leader if provided
                    leader_id = data.get("leader_id")
                    if leader_id and leader_id != self.node_id:
                        if self.leader_id != leader_id:
                            logger.info(f"Adopted leader from relay: {leader_id}")
                        # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                        self._set_leader(leader_id, reason="relay_poll_adopt_leader", save_state=False)

                    return {
                        "success": True,
                        "peers_received": len(peers_data) if isinstance(peers_data, dict) else 0,
                        "leader_id": leader_id,
                        "commands_received": len(commands) if isinstance(commands, list) else 0,
                    }
        except Exception as e:  # noqa: BLE001
            return {"success": False, "error": str(e)}

    async def _send_initial_relay_heartbeats(self) -> None:
        """Send immediate relay heartbeats on startup for NAT-blocked nodes.

        January 5, 2026: NAT-blocked nodes can't receive inbound connections,
        so they need to proactively register with relay-capable nodes to be
        discoverable by the cluster. This method sends relay heartbeats to
        all configured relay-capable nodes immediately at startup.

        Called after HTTP server starts but before regular heartbeat loop.
        """
        # Load relay-capable nodes from distributed_hosts.yaml
        relay_nodes: list[tuple[str, str, int]] = []  # (node_id, ip, port)
        try:
            from app.config.cluster_config import load_cluster_config
            config = load_cluster_config()
            nodes = getattr(config, "hosts_raw", {}) or {}

            for node_id, node_cfg in nodes.items():
                if node_id == self.node_id:
                    continue  # Skip self
                if not node_cfg.get("relay_capable", False):
                    continue
                if not node_cfg.get("p2p_enabled", True):
                    continue

                # Get the best IP to reach this node (prefer Tailscale)
                ip = node_cfg.get("tailscale_ip") or node_cfg.get("ssh_host", "")
                port = node_cfg.get("p2p_port", DEFAULT_PORT)
                if ip:
                    relay_nodes.append((node_id, ip, port))

        except ImportError:
            logger.warning("[P2P] cluster_config not available for initial relay heartbeats")
            return
        except Exception as e:  # noqa: BLE001
            logger.warning(f"[P2P] Failed to load relay-capable nodes: {e}")
            return

        if not relay_nodes:
            logger.info("[P2P] No relay-capable nodes configured for initial heartbeat")
            return

        logger.info(f"[P2P] Sending initial relay heartbeats to {len(relay_nodes)} relay-capable nodes")

        # Send relay heartbeats to all relay-capable nodes
        success_count = 0
        for node_id, ip, port in relay_nodes:
            relay_url = f"http://{ip}:{port}"
            try:
                result = await self._send_relay_heartbeat(relay_url)
                if result.get("success"):
                    success_count += 1
                    logger.info(f"[P2P] Initial relay heartbeat to {node_id} ({ip}:{port}) succeeded")
                else:
                    error = result.get("error", "unknown")
                    logger.debug(f"[P2P] Initial relay heartbeat to {node_id} failed: {error}")
            except Exception as e:  # noqa: BLE001
                logger.debug(f"[P2P] Initial relay heartbeat to {node_id} error: {e}")

        if success_count > 0:
            logger.info(f"[P2P] NAT-blocked node registered with {success_count}/{len(relay_nodes)} relay nodes")
        else:
            logger.warning(f"[P2P] Failed to register with any relay nodes - cluster discovery may be delayed")

    async def _send_startup_peer_announcements(self) -> None:
        """Send immediate announcements to all known peers on startup.

        January 7, 2026: Instead of waiting for the first heartbeat interval,
        immediately announce to all known peers. This reduces discovery latency
        from 15-30s down to 2-5s after startup.

        This is called for ALL nodes (not just NAT-blocked) to ensure fast
        peer discovery after restarts or cluster updates.
        """
        # Send to known peers from config
        success_count = 0
        total_attempts = 0

        for peer_addr in self.known_peers:
            try:
                scheme, host, port = self._parse_peer_address(peer_addr)
            except (AttributeError, ValueError):
                continue

            total_attempts += 1
            try:
                info = await self._send_heartbeat_to_peer(host, port, scheme=scheme)
                if info and info.node_id != self.node_id:
                    async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                        is_first_contact = info.node_id not in self.peers
                        info.last_heartbeat = time.time()
                        self.peers[info.node_id] = info
                    if is_first_contact:
                        logger.info(f"[P2P] Startup announcement discovered peer: {info.node_id}")
                    success_count += 1
            except Exception as e:  # noqa: BLE001
                logger.debug(f"[P2P] Startup announcement to {host}:{port} failed: {e}")

        if success_count > 0:
            logger.info(f"[P2P] Startup announcements: {success_count}/{total_attempts} peers reachable")
        elif total_attempts > 0:
            logger.warning(f"[P2P] Startup announcements: no peers reachable (tried {total_attempts})")

    async def _execute_relay_commands(self, commands: list[dict[str, Any]]) -> None:
        """Execute relay commands (polling mode for NAT-blocked nodes)."""
        now = time.time()
        for cmd in commands:
            try:
                cmd_id = str(cmd.get("id") or "")
                cmd_type = str(cmd.get("type") or "")
                payload = cmd.get("payload") or {}
                if not cmd_id or not cmd_type:
                    continue

                # Check for stale commands (>5 min old indicates relay/polling issues)
                cmd_ts = cmd.get("ts") or cmd.get("timestamp") or now
                cmd_age_secs = now - float(cmd_ts)
                if cmd_age_secs > 300:
                    logger.info(f"WARNING: Relay command {cmd_id} ({cmd_type}) is {cmd_age_secs:.0f}s old - relay delivery may be delayed")

                attempts = int(self.relay_command_attempts.get(cmd_id, 0) or 0) + 1
                self.relay_command_attempts[cmd_id] = attempts

                ok = False
                err = ""
                if cmd_type == "start_job":
                    job_type = JobType(str(payload.get("job_type") or "selfplay"))
                    board_type = str(payload.get("board_type") or "square8")
                    num_players = int(payload.get("num_players") or 2)
                    engine_mode = str(payload.get("engine_mode") or "mixed")
                    job_id = str(payload.get("job_id") or "")

                    if job_id:
                        with self.jobs_lock:
                            existing = self.local_jobs.get(job_id)
                        if existing and existing.status == "running":
                            ok = True
                        else:
                            job = await self._start_local_job(
                                job_type,
                                board_type=board_type,
                                num_players=num_players,
                                engine_mode=engine_mode,
                                job_id=job_id,
                            )
                            ok = job is not None
                    else:
                        job = await self._start_local_job(
                            job_type,
                            board_type=board_type,
                            num_players=num_players,
                            engine_mode=engine_mode,
                        )
                        ok = job is not None
                elif cmd_type == "cleanup":
                    asyncio.create_task(self._cleanup_local_disk())
                    ok = True
                elif cmd_type == "restart_stuck_jobs":
                    asyncio.create_task(self._restart_local_stuck_jobs())
                    ok = True
                elif cmd_type == "reduce_selfplay":
                    target = payload.get("target_selfplay_jobs", payload.get("target", 0))
                    reason = str(payload.get("reason") or "relay")
                    try:
                        target_jobs = int(target)
                    except (ValueError):
                        target_jobs = 0
                    await self._reduce_local_selfplay_jobs(target_jobs, reason=reason)
                    ok = True
                elif cmd_type == "cleanup_files":
                    files = payload.get("files", []) or []
                    reason = str(payload.get("reason") or "relay")
                    if not isinstance(files, list) or not files:
                        ok = False
                        err = "no_files"
                    else:
                        data_dir = self.get_data_directory()
                        freed_bytes = 0
                        deleted_count = 0
                        data_root = data_dir.resolve()
                        for file_path in files:
                            full_path = data_dir / (str(file_path or "").lstrip("/"))
                            try:
                                resolved = full_path.resolve()
                                resolved.relative_to(data_root)
                            except (AttributeError):
                                continue
                            if not resolved.exists():
                                continue
                            try:
                                size = resolved.stat().st_size
                                resolved.unlink()
                                freed_bytes += size
                                deleted_count += 1
                            except (AttributeError):
                                continue
                        print(
                            f"[P2P] Relay cleanup_files: {deleted_count} files deleted, "
                            f"{freed_bytes / 1e6:.1f}MB freed (reason={reason})"
                        )
                        ok = True
                elif cmd_type == "canonical_selfplay":
                    job_id = str(payload.get("job_id") or "")
                    board_type = str(payload.get("board_type") or "square8")
                    num_players = int(payload.get("num_players") or 2)
                    num_games = int(payload.get("num_games") or payload.get("games_per_node") or 500)
                    seed = int(payload.get("seed") or 0)
                    if not job_id:
                        ok = False
                        err = "missing_job_id"
                    else:
                        asyncio.create_task(
                            self._run_local_canonical_selfplay(job_id, board_type, num_players, num_games, seed)
                        )
                        ok = True
                else:
                    ok = False
                    err = f"unknown_command_type:{cmd_type}"

                if ok:
                    self._add_pending_relay_ack(cmd_id)
                    self._add_pending_relay_result({"id": cmd_id, "ok": True})
                    self.relay_command_attempts.pop(cmd_id, None)
                else:
                    if not err:
                        err = "command_failed"
                    if attempts >= RELAY_COMMAND_MAX_ATTEMPTS:
                        self._add_pending_relay_ack(cmd_id)
                        self._add_pending_relay_result({"id": cmd_id, "ok": False, "error": err})
                        self.relay_command_attempts.pop(cmd_id, None)
            except Exception as exc:
                try:
                    cmd_id = str(cmd.get("id") or "")
                    if cmd_id:
                        attempts = int(self.relay_command_attempts.get(cmd_id, 0) or 0)
                        if attempts >= RELAY_COMMAND_MAX_ATTEMPTS:
                            self._add_pending_relay_ack(cmd_id)
                            self._add_pending_relay_result({"id": cmd_id, "ok": False, "error": str(exc)})
                            self.relay_command_attempts.pop(cmd_id, None)
                except (ValueError, AttributeError):
                    continue

    async def _heartbeat_loop(self):
        """Send heartbeats to all known peers."""
        # Jan 11, 2026: Phase 5 - Initial heartbeat burst to prevent startup race
        # Send immediate heartbeats to all known peers so they discover us quickly
        # This fixes the issue where peers think we're dead before our first heartbeat
        logger.info("Sending initial heartbeat burst to known peers")
        for peer_addr in self.known_peers:
            try:
                scheme, host, port = self._parse_peer_address(peer_addr)
                await self._send_heartbeat_to_peer(host, port, scheme=scheme, timeout=10)
            except Exception:
                pass  # Best effort, regular loop will retry

        while self.running:
            try:
                # Jan 20, 2026: Check for and fix leadership state desync every heartbeat
                # This recovers from gossip race conditions where leader_id/role diverge
                try:
                    if self._recover_leadership_desync():
                        logger.info("[HeartbeatLoop] Recovered from leadership desync")
                except Exception as e:
                    logger.debug(f"[HeartbeatLoop] Desync check failed: {e}")

                # Send to known peers from config
                for peer_addr in self.known_peers:
                    try:
                        scheme, host, port = self._parse_peer_address(peer_addr)
                    except (AttributeError):
                        continue

                    # Use relay heartbeat for HTTPS endpoints (they're proxies/relays),
                    # explicitly configured relay peers (--relay-peers flag),
                    # or if this node is NAT-blocked and needs to relay ALL outbound heartbeats
                    use_relay = scheme == "https" or peer_addr in self.relay_peers or self._force_relay_mode
                    if use_relay:
                        # Relay/proxy endpoint, use relay heartbeat
                        relay_url = f"{scheme}://{host}" if port in (80, 443) else f"{scheme}://{host}:{port}"
                        result = await self._send_relay_heartbeat(relay_url)
                        if result.get("success"):
                            # Relay heartbeat already updates peers and leader
                            continue

                    info = await self._send_heartbeat_to_peer(host, port, scheme=scheme)
                    if info:
                        if info.node_id == self.node_id:
                            continue
                        # Dec 2025: Track first-contact for HOST_ONLINE emission
                        async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                            is_first_contact = info.node_id not in self.peers
                            info.last_heartbeat = time.time()
                            self.peers[info.node_id] = info
                        # Dec 2025: Emit HOST_ONLINE for newly discovered peers
                        if is_first_contact:
                            capabilities = []
                            if getattr(info, "has_gpu", False):
                                gpu_type = getattr(info, "gpu_type", "") or "gpu"
                                capabilities.append(gpu_type)
                            else:
                                capabilities.append("cpu")
                            await self._emit_host_online(info.node_id, capabilities)
                            logger.info(f"First-contact peer via heartbeat loop: {info.node_id}")
                        if info.role == NodeRole.LEADER and info.node_id != self.node_id:
                            async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                                peers_snapshot = list(self.peers.values())
                            conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])
                            if not self._is_leader_eligible(info, conflict_keys, require_alive=False):
                                continue
                            if self.role == NodeRole.LEADER and info.node_id <= self.node_id:
                                continue
                            if (
                                self.leader_id
                                and self.leader_id != info.node_id
                                and self._is_leader_lease_valid()
                                and info.node_id <= self.leader_id
                            ):
                                continue
                            if self.leader_id != info.node_id or self.role != NodeRole.FOLLOWER:
                                logger.info(f"Following configured leader from heartbeat: {info.node_id}")
                            prev_leader = self.leader_id
                            # Provisional lease: allow time for the leader to send
                            # a /coordinator lease renewal after we discover it via
                            # heartbeat (prevents leaderless oscillation right after
                            # restarts/partitions).
                            if prev_leader != info.node_id or not self._is_leader_lease_valid():
                                self.leader_lease_id = ""
                                self.leader_lease_expires = time.time() + LEADER_LEASE_DURATION
                            # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                            self._set_leader(info.node_id, reason="heartbeat_configured_leader", save_state=False)

                # Send to discovered peers (skip NAT-blocked peers and ambiguous endpoints).
                # Jan 12, 2026: Use cached snapshot to reduce lock contention (1s staleness OK for heartbeat)
                peers_snapshot = self._get_cached_peer_snapshot(max_age_seconds=1.0)
                conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])
                peer_list = [
                    p for p in peers_snapshot
                    if (
                        not p.nat_blocked
                        and self._endpoint_key(p) not in conflict_keys
                    )
                ]

                for peer in peer_list:
                    if peer.node_id != self.node_id:
                        if not peer.should_retry():
                            continue

                        # Jan 11, 2026: Phase 2 - Adaptive heartbeat timing based on consecutive failures
                        # This gives flaky peers time to recover without spamming them
                        failures = int(getattr(peer, "consecutive_failures", 0) or 0)
                        if failures == 0:
                            heartbeat_interval = HEARTBEAT_INTERVAL  # 15s for healthy peers
                        elif failures == 1:
                            heartbeat_interval = 5  # Quick retry after first failure
                        elif failures == 2:
                            heartbeat_interval = 10  # Second retry
                        elif failures < 5:
                            heartbeat_interval = 20  # Slower retries
                        else:
                            heartbeat_interval = 30  # Very slow for consistently failing

                        # Check if heartbeat is due for this peer
                        last_sent = float(getattr(peer, "last_heartbeat_sent", 0.0) or 0.0)
                        now = time.time()
                        if now - last_sent < heartbeat_interval:
                            continue  # Not time yet for this peer

                        # Mark the send time
                        peer.last_heartbeat_sent = now

                        peer_scheme = getattr(peer, "scheme", "http") or "http"
                        info = await self._send_heartbeat_to_peer(peer.host, peer.port, scheme=peer_scheme)
                        if not info and getattr(peer, "reported_host", "") and getattr(peer, "reported_port", 0):
                            # Multi-path retry: fall back to self-reported endpoint when the
                            # observed reachable endpoint fails (e.g., mixed overlays).
                            try:
                                rh = str(getattr(peer, "reported_host", "") or "").strip()
                                rp = int(getattr(peer, "reported_port", 0) or 0)
                            except (ValueError, AttributeError):
                                rh, rp = "", 0
                            if rh and rp and (rh != peer.host or rp != peer.port):
                                info = await self._send_heartbeat_to_peer(rh, rp, scheme=peer_scheme)
                        # Self-healing: Tailscale IP fallback when both primary and reported fail
                        if not info:
                            ts_ip = self._get_tailscale_ip_for_peer(peer.node_id)
                            if ts_ip and ts_ip != peer.host:
                                # Try Tailscale mesh IP (100.x.x.x)
                                info = await self._send_heartbeat_to_peer(ts_ip, peer.port, scheme=peer_scheme)
                                if info:
                                    logger.info(f"Reached {peer.node_id} via Tailscale ({ts_ip})")
                        if info:
                            info.consecutive_failures = 0
                            info.last_failure_time = 0.0
                            async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                                info.last_heartbeat = time.time()
                                self.peers[info.node_id] = info
                            if info.role == NodeRole.LEADER and self.role != NodeRole.LEADER:
                                if not self._is_leader_eligible(info, conflict_keys, require_alive=False):
                                    continue
                                if (
                                    self.leader_id
                                    and self.leader_id != info.node_id
                                    and self._is_leader_lease_valid()
                                    and info.node_id <= self.leader_id
                                ):
                                    continue
                                if self.leader_id != info.node_id:
                                    logger.info(f"Adopted leader from heartbeat: {info.node_id}")
                                prev_leader = self.leader_id
                                if prev_leader != info.node_id or not self._is_leader_lease_valid():
                                    self.leader_lease_id = ""
                                    self.leader_lease_expires = time.time() + LEADER_LEASE_DURATION
                                # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                                self._set_leader(info.node_id, reason="heartbeat_adopt_leader", save_state=False)
                        else:
                            async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                                existing = self.peers.get(peer.node_id)
                                if existing:
                                    existing.consecutive_failures = int(getattr(existing, "consecutive_failures", 0) or 0) + 1
                                    existing.last_failure_time = time.time()

                # If we're only connected to a seed peer (or lost cluster membership),
                # pull a fresh peer snapshot so leader election converges quickly.
                await self._bootstrap_from_known_peers()

                # Get current time for all time-based checks in this cycle
                now = time.time()

                # NAT-blocked nodes: poll a relay endpoint for peer snapshots + commands.
                if getattr(self.self_info, "nat_blocked", False):
                    if now - self.last_relay_heartbeat >= RELAY_HEARTBEAT_INTERVAL:
                        relay_urls: list[str] = []
                        leader_peer = self._get_leader_peer()
                        if leader_peer and leader_peer.node_id != self.node_id:
                            relay_urls.append(f"{leader_peer.scheme}://{leader_peer.host}:{leader_peer.port}")
                        for peer_addr in self.known_peers:
                            try:
                                scheme, host, port = self._parse_peer_address(peer_addr)
                            except (AttributeError):
                                continue
                            relay_urls.append(f"{scheme}://{host}:{port}")
                        seen: set[str] = set()
                        relay_urls = [u for u in relay_urls if not (u in seen or seen.add(u))]

                        for relay_url in relay_urls:
                            result = await self._send_relay_heartbeat(relay_url)
                            if result.get("success"):
                                self.last_relay_heartbeat = now
                                break

                # Check for dead peers
                await self._check_dead_peers_async()

                # Dec 30, 2025: Probe retired peers periodically to detect recovery
                # This runs every PEER_RECOVERY_RETRY_INTERVAL (120s) to actively probe
                # retired nodes that may have come back online after cluster restart.
                last_probe = getattr(self, "_last_retired_probe", 0.0)
                if now - last_probe >= PEER_RECOVERY_RETRY_INTERVAL:
                    self._last_retired_probe = now
                    try:
                        await self._probe_retired_peers_async()
                    except Exception as e:
                        logger.warning(f"Error in retired peer probe: {e}")

                # Self-healing: detect network partition and trigger Tailscale-priority mode
                if self._detect_network_partition():
                    self._enable_tailscale_priority()
                    # Also enable partition-local election if no voters reachable
                    if not self._has_voter_quorum():
                        self._enable_partition_local_election()
                    # Force refresh all IP sources to discover alternative paths
                    last_refresh = getattr(self, "_last_partition_ip_refresh", 0)
                    if time.time() - last_refresh > 60:  # Refresh at most once per minute
                        self._last_partition_ip_refresh = time.time()
                        asyncio.create_task(self._force_ip_refresh_all_sources())

                    # Jan 13, 2026: Exponential backoff during isolation
                    # Check if we're completely isolated (no alive peers)
                    alive_peers = sum(1 for p in self.peers.values() if p.is_alive() and p.node_id != self.node_id)
                    if alive_peers == 0:
                        # Track isolation start time
                        if not hasattr(self, "_isolation_start"):
                            self._isolation_start = time.time()
                            self._isolation_backoff_seconds = HEARTBEAT_INTERVAL
                            logger.warning("Node is isolated - no alive peers, starting exponential backoff")

                        # Calculate exponential backoff based on isolation duration
                        isolation_duration = time.time() - self._isolation_start
                        if isolation_duration > 60:  # After 1 min
                            self._isolation_backoff_seconds = min(30, self._isolation_backoff_seconds * 1.5)
                        if isolation_duration > 180:  # After 3 min
                            self._isolation_backoff_seconds = min(60, self._isolation_backoff_seconds * 1.5)
                        if isolation_duration > 300:  # After 5 min
                            self._isolation_backoff_seconds = min(120, self._isolation_backoff_seconds)

                        logger.debug(f"Isolated for {isolation_duration:.0f}s, backoff={self._isolation_backoff_seconds:.0f}s")
                        # Apply additional backoff sleep (on top of normal HEARTBEAT_INTERVAL)
                        extra_backoff = self._isolation_backoff_seconds - HEARTBEAT_INTERVAL
                        if extra_backoff > 0:
                            await asyncio.sleep(extra_backoff)
                    else:
                        # Reset isolation tracking when peers are reachable
                        if hasattr(self, "_isolation_start"):
                            logger.info(f"Isolation ended after {time.time() - self._isolation_start:.0f}s, {alive_peers} peers alive")
                            delattr(self, "_isolation_start")
                            if hasattr(self, "_isolation_backoff_seconds"):
                                delattr(self, "_isolation_backoff_seconds")
                elif getattr(self, "_tailscale_priority", False):
                    # Check if priority mode should expire
                    if time.time() > getattr(self, "_tailscale_priority_until", 0):
                        # Check if connectivity recovered
                        alive_count = sum(1 for p in self.peers.values() if p.is_alive() and p.node_id != self.node_id)
                        if alive_count > 0:
                            self._disable_tailscale_priority()

                # Self-healing: check if partition healed and restore original voters
                if hasattr(self, "_original_voters"):
                    self._restore_original_voters()

                # Dynamic voter management: promote/demote voters based on health
                # Only the leader manages voters to ensure consistency
                if self.role == NodeRole.LEADER:
                    self._manage_dynamic_voters()

                # Health-based leadership: step down if we can't reach enough peers
                if self.role == NodeRole.LEADER and not self._check_leader_health():
                    logger.info("Stepping down due to degraded health")
                    # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                    self._set_leader(None, reason="degraded_health", save_state=True)
                    self.leader_lease_id = ""
                    self.leader_lease_expires = 0.0
                    self._release_voter_grant_if_self()
                    # Jan 13, 2026: Add sleep before continue to prevent busy loop
                    # when repeatedly stepping down due to degraded health
                    await asyncio.sleep(HEARTBEAT_INTERVAL)
                    continue  # Skip leader duties this cycle

                # P0 Dec 2025: Monitor leader heartbeat for early warning
                # Emit LEADER_HEARTBEAT_MISSING if leader lease is approaching expiry
                if self.role == NodeRole.FOLLOWER and self.leader_id:
                    now = time.time()
                    # Warning threshold: 45 seconds (3x lease renewal interval)
                    heartbeat_warning_threshold = LEADER_LEASE_RENEW_INTERVAL * 3
                    time_until_expiry = self.leader_lease_expires - now
                    # Emit warning if lease will expire within warning threshold
                    if 0 < time_until_expiry < heartbeat_warning_threshold:
                        last_warning = getattr(self, "_last_heartbeat_missing_warning", 0.0)
                        # Only warn once per 30 seconds to avoid spam
                        if now - last_warning > 30:
                            self._last_heartbeat_missing_warning = now
                            delay_seconds = (LEADER_LEASE_DURATION - time_until_expiry)
                            try:
                                from app.distributed.data_events import emit_leader_heartbeat_missing
                                asyncio.create_task(emit_leader_heartbeat_missing(
                                    leader_id=self.leader_id,
                                    last_heartbeat=self.leader_lease_expires - LEADER_LEASE_DURATION,
                                    expected_interval=LEADER_LEASE_RENEW_INTERVAL,
                                    delay_seconds=delay_seconds,
                                    source=self.node_id,
                                ))
                            except ImportError:
                                pass  # Graceful degradation if event system not available

                # LEARNED LESSONS - Lease renewal to maintain leadership
                if self.role == NodeRole.LEADER:
                    await self._renew_leader_lease()

                # P2P monitoring: start/stop services based on leadership
                await self._stop_monitoring_if_not_leader()
                if self.role == NodeRole.LEADER:
                    await self._start_monitoring_if_leader()

                # P2P auto-deployer: start/stop based on leadership
                if self.role != NodeRole.LEADER and self._auto_deployer_task:
                    await self._stop_p2p_auto_deployer()
                elif self.role == NodeRole.LEADER and not self._auto_deployer_task:
                    await self._start_p2p_auto_deployer()

                # Report node resources to resource_optimizer for cluster-wide utilization tracking
                # This enables cooperative 60-80% utilization targeting across orchestrators
                if HAS_NEW_COORDINATION and get_resource_optimizer is not None:
                    try:
                        optimizer = get_resource_optimizer()
                        self._update_self_info()
                        node_resources = NodeResources(
                            node_id=self.node_id,
                            cpu_percent=self.self_info.cpu_percent,
                            memory_percent=self.self_info.memory_percent,
                            active_jobs=self.self_info.selfplay_jobs + self.self_info.training_jobs,
                            has_gpu=self.self_info.has_gpu,
                            gpu_name=self.self_info.gpu_type or "",
                        )
                        optimizer.report_node_resources(node_resources)
                    except (AttributeError):
                        pass  # Non-critical, don't disrupt heartbeat

                # Save state periodically
                self._save_state()

            except Exception as e:  # noqa: BLE001
                logger.info(f"Heartbeat error: {e}")

            # Notify systemd watchdog that we're still alive
            systemd_notify_watchdog()

            await asyncio.sleep(HEARTBEAT_INTERVAL)

    async def _voter_heartbeat_loop(self):
        """
        Dedicated high-frequency heartbeat loop for voter nodes.

        IMPROVEMENTS:
        - Faster heartbeat interval (10s vs 30s) for voter nodes
        - Aggressively clears NAT-blocked status on successful heartbeats
        - Maintains full mesh connectivity between all voters
        - Propagates voter list to ensure consistent quorum
        """
        # Only run if this node is a voter
        if self.node_id not in self.voter_node_ids:
            return

        logger.info(f"Starting voter heartbeat loop (interval={VOTER_HEARTBEAT_INTERVAL}s)")
        last_voter_mesh_refresh = 0.0

        while self.running:
            try:
                now = time.time()

                # Get all other voters
                other_voters = [v for v in self.voter_node_ids if v != self.node_id]

                # Jan 12, 2026: Consolidate peer updates to reduce lock contention
                # Collect all peer updates, then apply in a single lock acquisition
                peer_updates: dict[str, dict] = {}

                for voter_id in other_voters:
                    # Find voter peer info by IP mapping (Jan 2, 2026 fix)
                    # Peers dict uses IP:port keys from SWIM, not friendly node_ids
                    async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                        peer_key, voter_peer = self._find_voter_peer_by_ip(voter_id)

                    if not voter_peer:
                        # Try to discover voter from known peers
                        await self._discover_voter_peer(voter_id)
                        # Jan 13, 2026: Add brief sleep to prevent busy loop when many voters unavailable
                        await asyncio.sleep(0.1)
                        continue

                    # Attempt heartbeat to voter
                    success = await self._send_voter_heartbeat(voter_peer)

                    if success:
                        # AGGRESSIVE NAT RECOVERY: Clear NAT-blocked immediately on success
                        if VOTER_NAT_RECOVERY_AGGRESSIVE and voter_peer.nat_blocked:
                            logger.info(f"Voter {voter_id} (key={peer_key}) NAT-blocked status cleared (heartbeat succeeded)")
                            # Collect update instead of acquiring lock here
                            if peer_key:
                                peer_updates[peer_key] = {
                                    "nat_blocked": False,
                                    "nat_blocked_since": 0.0,
                                    "consecutive_failures": 0,
                                }
                    else:
                        # Try alternative endpoints
                        success = await self._try_voter_alternative_endpoints(voter_peer)

                        if not success:
                            # Collect failure update instead of acquiring lock here
                            if peer_key:
                                peer_updates[peer_key] = {"inc_failures": True}

                # Apply all peer updates in a single lock acquisition
                if peer_updates:
                    async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                        for key, updates in peer_updates.items():
                            if key in self.peers:
                                if "nat_blocked" in updates:
                                    self.peers[key].nat_blocked = updates["nat_blocked"]
                                    self.peers[key].nat_blocked_since = updates["nat_blocked_since"]
                                    self.peers[key].consecutive_failures = updates["consecutive_failures"]
                                elif updates.get("inc_failures"):
                                    self.peers[key].consecutive_failures = \
                                        int(getattr(self.peers[key], "consecutive_failures", 0) or 0) + 1

                # Periodic voter mesh refresh - ensure all voters know about each other
                if now - last_voter_mesh_refresh > VOTER_MESH_REFRESH_INTERVAL:
                    last_voter_mesh_refresh = now
                    await self._refresh_voter_mesh()

                # Jan 2, 2026: Voter health monitoring - check and log voter status
                # This runs every heartbeat cycle for proactive alerting
                health = self._check_voter_health()
                if not health.get("quorum_ok"):
                    logger.error(
                        f"[VoterHealth] QUORUM LOST: {health['voters_alive']}/{health['voters_total']} "
                        f"voters alive, need {health['quorum_size']}"
                    )
                elif health.get("quorum_threatened"):
                    logger.warning(
                        f"[VoterHealth] QUORUM THREATENED: {health['voters_alive']}/{health['voters_total']} "
                        f"voters alive (at threshold of {health['quorum_size']})"
                    )

            except Exception as e:  # noqa: BLE001
                logger.info(f"Voter heartbeat error: {e}")

            await asyncio.sleep(VOTER_HEARTBEAT_INTERVAL)

    async def _reconnect_dead_peers_loop(self) -> None:
        """Attempt to reconnect dead (but not retired) peers with exponential backoff.

        Jan 11, 2026: P2P Stability Fix - Phase 1

        Problem: When a peer goes "dead" (after 60-90s with no heartbeat), there's
        NO reconnection logic until it's "retired" (1800s). Dead nodes just stay dead
        in the peer list, causing cluster fragmentation.

        Solution: This loop actively probes dead peers with exponential backoff:
        - First 60s after death: retry every 15s (fast recovery for transient issues)
        - 60-120s: retry every 30s
        - 120-300s: retry every 60s
        - 300s+: retry every 120s (slow probing until retirement)

        This ensures dead peers can recover quickly while not overloading the network
        with reconnection attempts for truly offline nodes.
        """
        # Wait for startup to complete before starting reconnection attempts
        await asyncio.sleep(30)

        logger.info("Starting dead peer reconnection loop (Phase 1 P2P stability fix)")

        while self.running:
            try:
                now = time.time()
                async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                    peers_snapshot = list(self.peers.items())

                reconnect_attempts = 0
                reconnect_successes = 0

                for node_id, peer in peers_snapshot:
                    if node_id == self.node_id:
                        continue

                    # Skip alive or retired peers
                    if peer.is_alive():
                        continue
                    if getattr(peer, "retired", False):
                        continue

                    # Calculate time since death
                    dead_since = now - peer.last_heartbeat

                    # Skip if not actually dead yet (within timeout window)
                    # Jan 19, 2026: Use jittered timeout to prevent synchronized retries
                    if dead_since < get_jittered_peer_timeout(PEER_TIMEOUT):
                        continue

                    # Exponential backoff: 15s, 30s, 60s, 120s based on time since death
                    if dead_since < 60:
                        retry_interval = 15
                    elif dead_since < 120:
                        retry_interval = 30
                    elif dead_since < 300:
                        retry_interval = 60
                    else:
                        retry_interval = 120

                    # Check if it's time to retry this peer
                    last_retry = getattr(peer, "last_reconnect_attempt", 0.0)
                    if now - last_retry < retry_interval:
                        continue

                    # Mark the attempt time before trying (to prevent rapid retries on error)
                    peer.last_reconnect_attempt = now
                    reconnect_attempts += 1

                    # Try multiple paths to reach the peer
                    success = False
                    peer_scheme = getattr(peer, "scheme", "http") or "http"

                    # Path 1: Primary endpoint
                    try:
                        result = await self._send_heartbeat_to_peer(
                            peer.host, peer.port, scheme=peer_scheme, timeout=15
                        )
                        if result:
                            success = True
                    except Exception:
                        pass

                    # Path 2: Tailscale IP fallback
                    if not success:
                        ts_ip = self._get_tailscale_ip_for_peer(node_id)
                        if ts_ip and ts_ip != peer.host:
                            try:
                                result = await self._send_heartbeat_to_peer(
                                    ts_ip, peer.port, scheme=peer_scheme, timeout=15
                                )
                                if result:
                                    success = True
                                    logger.info(f"Reconnected to {node_id} via Tailscale ({ts_ip})")
                            except Exception:
                                pass

                    # Path 3: Reported host fallback
                    if not success:
                        reported_host = getattr(peer, "reported_host", "")
                        reported_port = getattr(peer, "reported_port", 0)
                        if reported_host and reported_port and (reported_host != peer.host or reported_port != peer.port):
                            try:
                                result = await self._send_heartbeat_to_peer(
                                    reported_host, reported_port, scheme=peer_scheme, timeout=15
                                )
                                if result:
                                    success = True
                                    logger.info(f"Reconnected to {node_id} via reported endpoint ({reported_host}:{reported_port})")
                            except Exception:
                                pass

                    if success:
                        reconnect_successes += 1
                        async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                            if node_id in self.peers:
                                self.peers[node_id].consecutive_failures = 0
                                self.peers[node_id].last_heartbeat = time.time()
                        logger.info(f"Reconnected to dead peer {node_id} (was dead for {dead_since:.0f}s)")

                        # Emit HOST_ONLINE event for recovered peer
                        try:
                            self._emit_host_online_sync(node_id, ["recovered"])
                        except Exception:
                            pass
                    else:
                        logger.debug(f"Reconnect to {node_id} failed (dead for {dead_since:.0f}s, retry in {retry_interval}s)")

                if reconnect_attempts > 0:
                    logger.debug(f"Reconnect loop: {reconnect_successes}/{reconnect_attempts} attempts succeeded")

            except Exception as e:
                logger.error(f"Error in dead peer reconnection loop: {e}")

            # Check every 15s for peers that need reconnection
            await asyncio.sleep(15)

    async def _send_voter_heartbeat(self, voter_peer) -> bool:
        """Send a heartbeat to a voter peer with shorter timeout."""
        try:
            peer_scheme = getattr(voter_peer, "scheme", "http") or "http"

            # Use Tailscale IP if available (more reliable for cross-provider)
            target_host = voter_peer.host
            ts_ip = self._get_tailscale_ip_for_peer(voter_peer.node_id)
            if ts_ip:
                target_host = ts_ip

            info = await self._send_heartbeat_to_peer(
                target_host,
                voter_peer.port,
                scheme=peer_scheme,
                timeout=VOTER_HEARTBEAT_TIMEOUT
            )

            if info:
                with self.peers_lock:
                    info.last_heartbeat = time.time()
                    info.consecutive_failures = 0
                    self.peers[info.node_id] = info

                # Update leader if this voter claims leadership
                if info.role == NodeRole.LEADER and info.node_id != self.node_id and self.leader_id != info.node_id:
                    logger.info(f"Discovered leader from voter heartbeat: {info.node_id}")
                    # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                    self._set_leader(info.node_id, reason="voter_heartbeat_discover_leader", save_state=False)
                    self.leader_lease_expires = time.time() + LEADER_LEASE_DURATION

                return True
        except (aiohttp.ClientError, asyncio.TimeoutError, json.JSONDecodeError, KeyError, AttributeError) as e:
            # Log voter coordination failures for debugging cluster connectivity issues
            logger.debug(f"Voter heartbeat failed for {voter_peer.node_id if voter_peer else 'unknown'}: {type(e).__name__}: {e}")
        return False

    async def _try_voter_alternative_endpoints(self, voter_peer) -> bool:
        """Try alternative endpoints for a voter peer."""
        peer_scheme = getattr(voter_peer, "scheme", "http") or "http"

        # Try 1: Tailscale IP
        ts_ip = self._get_tailscale_ip_for_peer(voter_peer.node_id)
        if ts_ip and ts_ip != voter_peer.host:
            info = await self._send_heartbeat_to_peer(ts_ip, voter_peer.port, scheme=peer_scheme, timeout=VOTER_HEARTBEAT_TIMEOUT)
            if info:
                logger.info(f"Reached voter {voter_peer.node_id} via Tailscale ({ts_ip})")
                with self.peers_lock:
                    info.last_heartbeat = time.time()
                    info.consecutive_failures = 0
                    self.peers[info.node_id] = info
                return True

        # Try 2: Reported host/port
        rh = str(getattr(voter_peer, "reported_host", "") or "").strip()
        rp = int(getattr(voter_peer, "reported_port", 0) or 0)
        if rh and rp and (rh != voter_peer.host or rp != voter_peer.port):
            info = await self._send_heartbeat_to_peer(rh, rp, scheme=peer_scheme, timeout=VOTER_HEARTBEAT_TIMEOUT)
            if info:
                logger.info(f"Reached voter {voter_peer.node_id} via reported endpoint ({rh}:{rp})")
                with self.peers_lock:
                    info.last_heartbeat = time.time()
                    info.consecutive_failures = 0
                    self.peers[info.node_id] = info
                return True

        return False

    async def _discover_voter_peer(self, voter_id: str):
        """Discover a voter peer from known peers."""
        # Ask known peers for the voter's endpoint
        for peer_addr in self.known_peers:
            try:
                scheme, host, port = self._parse_peer_address(peer_addr)
                async with aiohttp.ClientSession() as session, session.get(
                    f"{scheme}://{host}:{port}/relay/peers",
                    timeout=aiohttp.ClientTimeout(total=5),
                    headers=self._auth_headers()
                ) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        peers_data = data.get("peers", {})
                        if voter_id in peers_data:
                            peer_info = NodeInfo.from_dict(peers_data[voter_id])
                            with self.peers_lock:
                                self.peers[voter_id] = peer_info
                            logger.info(f"Discovered voter {voter_id} from {host}")
                            return
            except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError, ImportError):
                continue

    async def _refresh_voter_mesh(self):
        """Ensure all voters have knowledge of each other."""
        if not self.voter_node_ids:
            return

        # Jan 12, 2026: Use lock-free PeerSnapshot for read-only access
        peers_snapshot = self._peer_snapshot.get_snapshot()

        # Check how many voters we know about (outside lock)
        known_voters = [v for v in self.voter_node_ids if v in peers_snapshot or v == self.node_id]

        if len(known_voters) < len(self.voter_node_ids):
            missing_voters = [v for v in self.voter_node_ids if v not in known_voters]
            logger.info(f"Voter mesh incomplete, missing: {missing_voters}")

            # Try to discover missing voters
            for voter_id in missing_voters:
                await self._discover_voter_peer(voter_id)

    # NOTE: _nat_management_loop() removed Dec 2025 (32 LOC).
    # Now runs via LoopManager as NATManagementLoop.
    # See scripts/p2p/loops/network_loops.py for implementation.

    async def _detect_nat_type(self):
        """
        Detect NAT type using STUN-like probing.

        NAT Types:
        - Full Cone: Any external host can send packets to internal host
        - Restricted Cone: Only hosts that internal has contacted can respond
        - Port Restricted: Only hosts+ports that internal has contacted can respond
        - Symmetric: Different external IP:port for each destination (breaks P2P)
        """
        external_ips = set()

        # Probe multiple peers to detect if we get different external IPs
        # Jan 12, 2026: Use lock-free PeerSnapshot for read-only access
        alive_peers = [p for p in self._peer_snapshot.get_snapshot().values() if p.is_alive() and p.node_id != self.node_id]

        for peer in alive_peers[:5]:  # Probe up to 5 peers
            try:
                peer_scheme = getattr(peer, "scheme", "http") or "http"
                async with aiohttp.ClientSession() as session, session.get(
                    f"{peer_scheme}://{peer.host}:{peer.port}/health",
                    timeout=aiohttp.ClientTimeout(total=5),
                    headers=self._auth_headers()
                ) as resp:
                    if resp.status == 200:
                        # The peer would report our external IP if we had an endpoint for it
                        # For now, just track connectivity
                        await resp.json()
                        external_ips.add(peer.host)  # Track which peers we can reach
            except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
                continue

        # If we see ourselves with different IPs from different vantage points,
        # we likely have symmetric NAT
        if len(external_ips) > 1:
            self._nat_type = "symmetric"
            logger.info("Detected symmetric NAT (multiple external IPs seen)")
        elif len(external_ips) == 1:
            self._nat_type = "cone"
        else:
            self._nat_type = "unknown"

    async def _probe_nat_blocked_peers(self):
        """Probe NAT-blocked peers to see if they've become reachable."""
        # Jan 12, 2026: Use lock-free PeerSnapshot for read-only access
        nat_blocked_peers = [
            p for p in self._peer_snapshot.get_snapshot().values()
            if p.nat_blocked and p.node_id != self.node_id
        ]

        for peer in nat_blocked_peers:
            # Check if enough time has passed since blocking
            blocked_duration = time.time() - (peer.nat_blocked_since or 0)
            if blocked_duration < NAT_BLOCKED_RECOVERY_TIMEOUT:
                continue

            # Try to reach the peer
            peer_scheme = getattr(peer, "scheme", "http") or "http"

            # Try multiple endpoints
            endpoints_to_try = [(peer.host, peer.port)]

            # Add Tailscale IP
            ts_ip = self._get_tailscale_ip_for_peer(peer.node_id)
            if ts_ip and ts_ip != peer.host:
                endpoints_to_try.insert(0, (ts_ip, peer.port))  # Prefer Tailscale

            # Add reported endpoint
            rh = str(getattr(peer, "reported_host", "") or "").strip()
            rp = int(getattr(peer, "reported_port", 0) or 0)
            if rh and rp:
                endpoints_to_try.append((rh, rp))

            for host, port in endpoints_to_try:
                try:
                    async with aiohttp.ClientSession() as session, session.get(
                        f"{peer_scheme}://{host}:{port}/health",
                        timeout=aiohttp.ClientTimeout(total=NAT_BLOCKED_PROBE_TIMEOUT),
                        headers=self._auth_headers()
                    ) as resp:
                        if resp.status == 200:
                            # Peer is reachable! Clear NAT-blocked status
                            logger.info(f"NAT-blocked peer {peer.node_id} is now reachable at {host}:{port}")
                            with self.peers_lock:
                                if peer.node_id in self.peers:
                                    self.peers[peer.node_id].nat_blocked = False
                                    self.peers[peer.node_id].nat_blocked_since = 0.0
                                    self.peers[peer.node_id].host = host  # Update to working endpoint
                                    self.peers[peer.node_id].consecutive_failures = 0
                            break
                except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
                    continue

    async def _update_relay_preferences(self):
        """Update relay preferences based on connectivity patterns."""
        # Identify peers that consistently fail direct connections
        # Jan 12, 2026: Use lock-free PeerSnapshot for read-only access
        peers_needing_relay = [
            p for p in self._peer_snapshot.get_snapshot().values()
            if (getattr(p, "consecutive_failures", 0) or 0) >= NAT_RELAY_PREFERENCE_THRESHOLD
            and not p.nat_blocked
            and p.node_id != self.node_id
        ]

        for peer in peers_needing_relay:
            # Mark as preferring relay
            if not peer.nat_blocked:
                logger.info(f"Peer {peer.node_id} has {peer.consecutive_failures} consecutive failures, marking as NAT-blocked")
                with self.peers_lock:
                    if peer.node_id in self.peers:
                        self.peers[peer.node_id].nat_blocked = True
                        self.peers[peer.node_id].nat_blocked_since = time.time()
                        # Set relay to best available relay node (with configured fallback)
                        relay_node = self._select_best_relay(for_peer=peer.node_id)
                        if relay_node:
                            self.peers[peer.node_id].relay_via = relay_node

    async def _validate_relay_assignments(self) -> None:
        """Validate and update relay assignments for NAT-blocked peers.

        December 30, 2025: Proactive relay health check. Detects when a relay
        node has become unhealthy and automatically switches NAT-blocked peers
        to a healthier relay.
        """
        # Jan 12, 2026: Use lock-free PeerSnapshot for read-only access
        peers_snapshot = self._peer_snapshot.get_snapshot()
        nat_blocked_peers = [
            p for p in peers_snapshot.values()
            if getattr(p, "nat_blocked", False)
            and getattr(p, "relay_via", "")
            and p.node_id != self.node_id
        ]

        for peer in nat_blocked_peers:
            relay_id = str(getattr(peer, "relay_via", "") or "")
            if not relay_id:
                continue

            # Jan 12, 2026: Use lock-free PeerSnapshot for read-only access
            relay_peer = peers_snapshot.get(relay_id)

            # Check if relay is healthy
            relay_healthy = (
                relay_peer is not None
                and relay_peer.is_alive()
                and not getattr(relay_peer, "nat_blocked", False)
                and (getattr(relay_peer, "consecutive_failures", 0) or 0) < 2
            )

            if not relay_healthy:
                # Find a new relay (with configured fallback)
                new_relay = self._select_best_relay(for_peer=peer.node_id)
                if new_relay and new_relay != relay_id:
                    logger.info(
                        f"[RelayHealthCheck] Relay {relay_id} unhealthy for {peer.node_id}, "
                        f"switching to {new_relay}"
                    )
                    with self.peers_lock:
                        if peer.node_id in self.peers:
                            self.peers[peer.node_id].relay_via = new_relay

    def _select_best_relay(self, for_peer: str = "") -> str:
        """Select the best relay node based on connectivity and load.

        January 4, 2026: Multi-relay failover support. When for_peer is provided,
        checks configured relay_primary/secondary/tertiary from distributed_hosts.yaml
        before falling back to automatic selection. This ensures NAT-blocked nodes
        have deterministic relay paths that survive leader changes.

        Args:
            for_peer: Optional peer node_id to look up configured relay preferences.

        Returns:
            Node ID of the best relay, or empty string if none available.
        """
        # January 4, 2026: Check configured relay preferences for this peer
        if for_peer:
            configured_relays = self._get_configured_relays(for_peer)
            for relay_id in configured_relays:
                with self.peers_lock:
                    relay_peer = self.peers.get(relay_id)
                if relay_peer and self._is_valid_relay(relay_peer):
                    return relay_id

        # Fall back to automatic selection
        with self.peers_lock:
            candidates = [
                p for p in self.peers.values()
                if self._is_valid_relay(p)
            ]

        if not candidates:
            return ""

        # Prefer leader, then voters, then lowest load
        leader_peer = next((p for p in candidates if p.node_id == self.leader_id), None)
        if leader_peer:
            return leader_peer.node_id

        voter_peer = next((p for p in candidates if p.node_id in self.voter_node_ids), None)
        if voter_peer:
            return voter_peer.node_id

        # Lowest load
        candidates.sort(key=lambda p: getattr(p, "load_score", 100))
        return candidates[0].node_id if candidates else ""

    def _is_valid_relay(self, peer: "NodeInfo") -> bool:
        """Check if a peer is a valid relay candidate."""
        return (
            peer.is_alive()
            and not getattr(peer, "nat_blocked", False)
            and peer.node_id != self.node_id
            and (getattr(peer, "consecutive_failures", 0) or 0) < 2
        )

    def _get_configured_relays(self, peer_id: str) -> list[str]:
        """Get configured relay nodes for a peer from distributed_hosts.yaml.

        January 4, 2026: Multi-relay failover configuration.

        Returns:
            List of relay node IDs in priority order [primary, secondary, tertiary].
        """
        try:
            from app.config.cluster_config import load_cluster_config

            config = load_cluster_config()
            hosts_raw = getattr(config, "hosts_raw", {}) or {}
            host_config = hosts_raw.get(peer_id, {})

            relays = []
            for key in ["relay_primary", "relay_secondary", "relay_tertiary"]:
                relay_id = host_config.get(key, "")
                if relay_id:
                    relays.append(relay_id)
            return relays
        except (ImportError, AttributeError, KeyError, TypeError):
            return []

    # NOTE: _manifest_collection_loop removed Dec 27, 2025
    # Now handled by ManifestCollectionLoop via LoopManager
    # See scripts/p2p/loops/manifest_collection_loop.py

    def _record_selfplay_stats_sample(self, manifest: ClusterDataManifest) -> None:
        """Record a lightweight selfplay totals sample for dashboard charts."""
        try:
            sample = {
                "timestamp": time.time(),
                "manifest_collected_at": float(getattr(manifest, "collected_at", 0.0) or 0.0),
                "total_selfplay_games": int(getattr(manifest, "total_selfplay_games", 0) or 0),
                "by_board_type": manifest.by_board_type,
                "total_nodes": int(getattr(manifest, "total_nodes", 0) or 0),
            }
            self.selfplay_stats_history.append(sample)
            max_samples = int(getattr(self, "selfplay_stats_history_max_samples", 288) or 288)
            if max_samples > 0 and len(self.selfplay_stats_history) > max_samples:
                self.selfplay_stats_history = self.selfplay_stats_history[-max_samples:]
        except (ValueError, KeyError, IndexError, AttributeError):
            # Never let dashboard bookkeeping break manifest collection.
            return

    def _endpoint_key(self, info: NodeInfo) -> tuple[str, str, int] | None:
        """Return the normalized reachable endpoint key for a peer (scheme, host, port)."""
        host = str(getattr(info, "host", "") or "").strip()
        if not host:
            return None
        scheme = str(getattr(info, "scheme", "http") or "http").lower()
        try:
            port = int(getattr(info, "port", DEFAULT_PORT) or DEFAULT_PORT)
        except (ValueError):
            port = DEFAULT_PORT
        reported_host = str(getattr(info, "reported_host", "") or "").strip()
        try:
            reported_port = int(getattr(info, "reported_port", 0) or 0)
        except (ValueError):
            reported_port = 0

        if reported_host and reported_port > 0:
            # Reverse proxies / relays can cause inbound peer requests to appear as loopback.
            # Prefer the peer's self-reported advertised endpoint in that case so:
            # - endpoint conflict detection remains meaningful, and
            # - eligible leaders don't get filtered out as "conflicted".
            if host in {"127.0.0.1", "localhost", "0.0.0.0", "::1"} or self._is_tailscale_host(reported_host):
                host, port = reported_host, reported_port
        return (scheme, host, port)

    def _endpoint_conflict_keys(self, peers: list[NodeInfo]) -> set[tuple[str, str, int]]:
        """Compute endpoint keys that are shared by >1 node (NAT/port collisions)."""
        counts: dict[tuple[str, str, int], int] = {}
        for p in peers:
            # Ignore dead peers: stale node_ids can linger after restarts and would
            # otherwise permanently mark the live node as "conflicted".
            if not p.is_alive():
                continue
            key = self._endpoint_key(p)
            if not key:
                continue
            counts[key] = counts.get(key, 0) + 1
        return {k for k, v in counts.items() if v > 1}

    async def _probe_nat_blocked_peer(self, peer: NodeInfo) -> bool:
        """Probe a NAT-blocked peer to check if it's now directly reachable.

        Returns True if peer is reachable and NAT-blocked status was cleared.
        """
        if not peer.nat_blocked:
            return False

        now = time.time()
        nat_blocked_since = float(getattr(peer, "nat_blocked_since", 0.0) or 0.0)
        last_probe = float(getattr(peer, "last_nat_probe", 0.0) or 0.0)

        # Don't probe too frequently
        if now - last_probe < NAT_BLOCKED_PROBE_INTERVAL:
            return False

        # Don't probe if not blocked long enough
        if nat_blocked_since > 0 and (now - nat_blocked_since) < NAT_BLOCKED_RECOVERY_TIMEOUT:
            return False

        # Update last probe time
        with self.peers_lock:
            existing = self.peers.get(peer.node_id)
            if existing:
                existing.last_nat_probe = now

        try:
            url = self._url_for_peer(peer, "/status")
            timeout = ClientTimeout(total=NAT_BLOCKED_PROBE_TIMEOUT)
            async with get_client_session(timeout) as session:
                async with session.get(url, headers=self._auth_headers()) as resp:
                    if resp.status == 200:
                        # Peer is reachable! Clear NAT-blocked status
                        with self.peers_lock:
                            existing = self.peers.get(peer.node_id)
                            if existing and existing.nat_blocked:
                                existing.nat_blocked = False
                                existing.nat_blocked_since = 0.0
                                existing.relay_via = ""
                                logger.info(f"NAT recovery: {peer.node_id} is now directly reachable")
                                return True
        except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError) as e:
            # Probe failed - peer still not reachable (expected for NAT-blocked nodes)
            logger.debug(f"NAT recovery probe failed for {peer.node_id}: {type(e).__name__}")

        return False

    async def _sweep_nat_recovery(self) -> int:
        """Periodically probe NAT-blocked peers to check if they've become reachable.

        Returns the number of peers that recovered from NAT-blocked state.
        """
        recovered = 0
        # Jan 12, 2026: Use lock-free PeerSnapshot for read-only access
        nat_blocked_peers = [
            p for p in self._peer_snapshot.get_snapshot().values()
            if p.nat_blocked and p.is_alive()
        ]

        if not nat_blocked_peers:
            return 0

        # Probe in parallel but limit concurrency
        tasks = [self._probe_nat_blocked_peer(p) for p in nat_blocked_peers[:10]]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for i, result in enumerate(results):
            if isinstance(result, Exception):
                peer_id = nat_blocked_peers[i].node_id if i < len(nat_blocked_peers) else "unknown"
                logger.debug(f"NAT probe failed for {peer_id}: {result}")
            elif result is True:
                recovered += 1

        if recovered > 0:
            logger.info(f"NAT recovery sweep: {recovered} peer(s) recovered")

        return recovered

    def _compute_connectivity_score(self, peer: NodeInfo) -> float:
        """Compute connectivity score for leader eligibility ranking.

        Jan 2, 2026: Added to support leader eligibility refinement.

        Score components (0.0 to 1.0):
        - 0.4: Direct reachability (not NAT-blocked, not force_relay)
        - 0.3: Transport success rate (based on consecutive_failures)
        - 0.2: Connected peers (based on visible_peers if available)
        - 0.1: Role weight (leaders get slight boost)

        Returns:
            float: Connectivity score between 0.0 and 1.0
        """
        score = 0.0

        # Direct reachability: 0.4 points
        nat_blocked = getattr(peer, "nat_blocked", False)
        force_relay = getattr(peer, "force_relay_mode", False)
        if not nat_blocked and not force_relay:
            score += 0.4
        elif not nat_blocked:
            # Force relay but not NAT blocked - partial credit
            score += 0.2

        # Transport success rate: 0.3 points
        failures = int(getattr(peer, "consecutive_failures", 0) or 0)
        if failures == 0:
            score += 0.3
        elif failures < 3:
            score += 0.2
        elif failures < MAX_CONSECUTIVE_FAILURES:
            score += 0.1

        # Connected peers: 0.2 points
        visible_peers = int(getattr(peer, "visible_peers", 0) or 0)
        if visible_peers >= 10:
            score += 0.2
        elif visible_peers >= 5:
            score += 0.15
        elif visible_peers >= 2:
            score += 0.1

        # Role weight: 0.1 points
        if peer.role == NodeRole.LEADER:
            score += 0.1
        elif peer.role == NodeRole.FOLLOWER:
            score += 0.05

        return min(1.0, score)

    def _is_leader_eligible(
        self,
        peer: NodeInfo,
        conflict_keys: set[tuple[str, str, int]],
        *,
        require_alive: bool = True,
    ) -> bool:
        """Heuristic: leaders must be directly reachable and uniquely addressable.

        Jan 2, 2026: Enhanced to reject NAT-blocked and force_relay_mode nodes,
        and require minimum connectivity score of 0.3.
        """
        if require_alive and not peer.is_alive():
            return False
        voters = list(getattr(self, "voter_node_ids", []) or [])
        if voters and peer.node_id not in voters:
            return False
        if int(getattr(peer, "consecutive_failures", 0) or 0) >= MAX_CONSECUTIVE_FAILURES:
            return False
        # Jan 2, 2026: Reject NAT-blocked nodes
        if getattr(peer, "nat_blocked", False):
            return False
        # Jan 2, 2026: Reject force_relay_mode nodes (can't serve peers directly)
        if getattr(peer, "force_relay_mode", False):
            return False
        # Jan 13, 2026: Reject proxy_only nodes (cannot coordinate cluster operations)
        # These nodes are SSH jump hosts or API proxies with no AI/training capability
        # Check both NodeInfo.status and config (config is authoritative)
        node_status = getattr(peer, "status", "")
        if node_status == "proxy_only" or self._is_node_proxy_only(peer.node_id):
            return False
        # Jan 2, 2026: Require minimum connectivity score
        if self._compute_connectivity_score(peer) < 0.3:
            return False
        key = self._endpoint_key(peer)
        return not (key and key in conflict_keys)

    def _register_peer_with_dedup(self, info: NodeInfo) -> bool:
        """Register or update a peer with deduplication support.

        Dec 29, 2025: Implements peer deduplication by node_id. When the same
        node is discovered via multiple IPs (Tailscale, public, etc.), this
        method merges them into a single canonical entry instead of creating
        duplicate entries.

        Args:
            info: NodeInfo to register

        Returns:
            True if this was a new peer, False if updating existing
        """
        if not info or not info.node_id:
            return False

        with self.peers_lock:
            existing = self.peers.get(info.node_id)
            if existing is not None:
                # Merge new info into existing entry
                existing.merge_from(info)
                return False
            else:
                # New peer - initialize alternate_ips from host if available
                if info.host and not info.alternate_ips:
                    info.alternate_ips = set()
                self.peers[info.node_id] = info
                return True

    def _deduplicate_peers(self) -> int:
        """Periodic deduplication of peers dict.

        Dec 29, 2025: Scans for any duplicate entries that may have been
        created before deduplication was implemented, and merges them.

        Jan 13, 2026: Now actually removes duplicates instead of just logging.
        Keeps the most recently active node per IP (by last_heartbeat).

        Returns:
            Number of duplicates removed
        """
        removed = 0
        # This method is called periodically to clean up any legacy duplicates
        # Since we now key by node_id, duplicates shouldn't occur, but this
        # handles edge cases from state file loading and hostname changes
        with self.peers_lock:
            # Build IP -> node_id mapping to detect duplicates
            # Use reported_host (Tailscale) as primary dedup key since it's most stable
            ip_to_nodes: dict[str, list[str]] = {}
            for node_id, peer in self.peers.items():
                # Skip self
                if node_id == self.node_id:
                    continue
                # Prefer reported_host (Tailscale IPv6) for dedup - most stable identifier
                dedup_key = peer.reported_host or peer.effective_host or peer.host
                if dedup_key and dedup_key not in ("127.0.0.1", ""):
                    ip_to_nodes.setdefault(dedup_key, []).append(node_id)

            # Find IPs shared by multiple node_ids (duplicates)
            # Keep the most recently active one, remove the rest
            nodes_to_remove: set[str] = set()
            for ip, node_ids in ip_to_nodes.items():
                if len(node_ids) <= 1:
                    continue

                # Multiple nodes claim the same IP - keep the freshest one
                # Sort by last_heartbeat descending (most recent first)
                node_freshness = []
                for nid in node_ids:
                    peer = self.peers.get(nid)
                    if peer:
                        # Use last_heartbeat, fall back to last_seen
                        freshness = getattr(peer, "last_heartbeat", 0) or getattr(peer, "last_seen", 0) or 0
                        node_freshness.append((nid, freshness))

                if not node_freshness:
                    continue

                # Sort by freshness (most recent first)
                node_freshness.sort(key=lambda x: x[1], reverse=True)
                keeper = node_freshness[0][0]
                stale = [nid for nid, _ in node_freshness[1:]]

                if stale:
                    logger.info(
                        f"[Dedup] IP {ip}: keeping {keeper} (freshest), "
                        f"removing {len(stale)} stale entries: {stale}"
                    )
                    nodes_to_remove.update(stale)

            # Actually remove the duplicate nodes
            for node_id in nodes_to_remove:
                if node_id in self.peers:
                    del self.peers[node_id]
                    removed += 1
                    logger.info(f"[Dedup] Removed duplicate peer: {node_id}")

        if removed > 0:
            logger.info(f"[Dedup] Removed {removed} duplicate peer entries")

        return removed

    def _maybe_adopt_leader_from_peers(self) -> bool:
        """If we can already see a healthy leader, adopt it and avoid elections."""
        if self.role == NodeRole.LEADER:
            return False

        # Jan 12, 2026: Use lock-free PeerSnapshot for read-only access
        peers = [p for p in self._peer_snapshot.get_snapshot().values() if p.node_id != self.node_id]

        conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers])
        leaders = [
            p for p in peers
            if p.role == NodeRole.LEADER and self._is_leader_eligible(p, conflict_keys)
        ]

        voter_ids = list(getattr(self, "voter_node_ids", []) or [])
        if voter_ids:
            leaders = [p for p in leaders if p.node_id in voter_ids]

        if not leaders:
            return False

        # If multiple leaders exist (split brain), pick the lexicographically highest
        # ID (matches bully ordering) to converge.
        leader = sorted(leaders, key=lambda p: p.node_id)[-1]

        if self.leader_id != leader.node_id:
            logger.info(f"Adopted existing leader from peers: {leader.node_id}")
            # Jan 3, 2026 Sprint 13.3: Record election latency for "adopted" outcome
            # Only record if we were in an election (started_at > 0)
            if getattr(self, "_election_started_at", 0) > 0:
                self._record_election_latency("adopted")
        # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
        self._set_leader(leader.node_id, reason="join_existing_leader", save_state=True)
        self.last_leader_seen = time.time()  # Track when we last had a functioning leader
        return True

    async def _check_dead_peers_async(self):
        """Check for peers that have stopped responding (async version).

        This version uses AsyncLockWrapper to avoid blocking the event loop
        when acquiring the peers_lock.

        January 12, 2026: Refactored to move event emissions outside the lock
        to prevent deadlock risk when event handlers need the same lock.

        January 19, 2026: Added rate limiting (PEER_DEATH_RATE_LIMIT) to prevent
        cascade failures. When 5+ nodes are busy, ALL nodes would mark ALL of them
        dead simultaneously, causing gossip storms and further instability.
        Now max PEER_DEATH_RATE_LIMIT peers can be retired per check cycle.
        """
        now = time.time()
        dead_peers = []
        peers_to_purge = []

        # Jan 12, 2026: Collect event data inside lock, emit outside
        # This prevents deadlocks when event handlers need peers_lock
        retired_peers: list[tuple[str, float | None, float]] = []  # (node_id, last_hb, dead_for)
        recovered_peers: list[tuple[str, list[str]]] = []  # (node_id, capabilities)

        # Jan 19, 2026: Track candidates for retirement with their dead_for time
        # so we can sort by longest-dead and rate-limit retirements
        retirement_candidates: list[tuple[str, float, float | None]] = []  # (node_id, dead_for, last_hb)

        async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
            for node_id, info in self.peers.items():
                if not info.is_alive() and node_id != self.node_id:
                    dead_peers.append(node_id)
                    # Check if peer should be retired (long-dead)
                    try:
                        dead_for = now - float(getattr(info, "last_heartbeat", 0.0) or 0.0)
                    except (ValueError, AttributeError):
                        dead_for = float("inf")
                    if not getattr(info, "retired", False) and dead_for >= PEER_RETIRE_AFTER_SECONDS:
                        # Jan 19, 2026: Collect candidates instead of immediately retiring
                        last_hb = getattr(info, "last_heartbeat", None)
                        retirement_candidates.append((node_id, dead_for, last_hb))
                elif info.is_alive() and getattr(info, "retired", False):
                    # Peer came back: clear retirement and remove from dead tracking.
                    info.retired = False
                    info.retired_at = 0.0
                    # Jan 20, 2026: Clear cooldown on recovery
                    if self._cooldown_manager:
                        self._cooldown_manager.clear_cooldown(node_id)
                    else:
                        self._dead_peer_timestamps.pop(node_id, None)
                    # Collect data for event emission outside lock
                    caps = []
                    if hasattr(info, "gpu_type") and info.gpu_type:
                        caps.append(f"gpu:{info.gpu_type}")
                    recovered_peers.append((node_id, caps))
                    logger.info(f"Peer {node_id} recovered from retirement")

            for node_id in dead_peers:
                info = self.peers.get(node_id)
                if info and getattr(info, "retired", False):
                    continue
                logger.info(f"Peer {node_id} is dead (no heartbeat for {PEER_TIMEOUT}s)")

            # Auto-purge very old retired peers
            for node_id, info in self.peers.items():
                if getattr(info, "retired", False):
                    retired_at = getattr(info, "retired_at", 0.0) or 0.0
                    if retired_at > 0 and (now - retired_at) >= PEER_PURGE_AFTER_SECONDS:
                        peers_to_purge.append(node_id)

            for node_id in peers_to_purge:
                del self.peers[node_id]
                logger.info(f"Auto-purged stale peer: {node_id} (retired for >{PEER_PURGE_AFTER_SECONDS}s)")

            # December 2025: Emit cluster health event if state changed
            # This enables pipeline coordination to pause/resume based on cluster health
            final_alive_count = sum(
                1 for p in self.peers.values()
                if p.is_alive() and not getattr(p, "retired", False)
            )
            final_node_count = len([
                p for p in self.peers.values()
                if not getattr(p, "retired", False)
            ])
            # Add self if not in peers
            if self.node_id not in self.peers:
                final_alive_count += 1
                final_node_count += 1
            # Dec 28, 2025: Fixed signature mismatch - pass correct parameters
            is_healthy = final_alive_count > 0
            quorum_met = self._has_voter_quorum() if hasattr(self, '_has_voter_quorum') else True
            self._emit_cluster_health_event_sync(is_healthy, final_alive_count, quorum_met)

            # Jan 12, 2026: Sync to lock-free snapshot after peer retirement/purge
            self._sync_peer_snapshot()

            # Capture final counts for capacity events (inside lock for consistency)
            capacity_total = len(self.peers)
            capacity_alive = sum(1 for p in self.peers.values() if p.is_alive() and not getattr(p, "retired", False))
            capacity_gpu = sum(1 for p in self.peers.values() if p.is_alive() and not getattr(p, "retired", False) and getattr(p, "gpu_type", None))
            capacity_training = sum(1 for p in self.peers.values() if p.is_alive() and not getattr(p, "retired", False) and getattr(p, "training_enabled", False))

        # Jan 19, 2026: Apply rate limiting to peer retirements
        # Sort by longest-dead first, then only retire up to PEER_DEATH_RATE_LIMIT peers
        if retirement_candidates:
            # Sort by dead_for descending (longest-dead first)
            retirement_candidates.sort(key=lambda x: x[1], reverse=True)

            # Rate-limit: only retire the top N peers this cycle
            to_retire = retirement_candidates[:PEER_DEATH_RATE_LIMIT]
            skipped = len(retirement_candidates) - len(to_retire)

            if skipped > 0:
                logger.warning(
                    f"Rate-limiting peer retirement: retiring {len(to_retire)}/{len(retirement_candidates)} "
                    f"candidates this cycle (skipped {skipped} to prevent cascade)"
                )

            # Now actually retire these peers (need brief lock acquisition)
            async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                for node_id, dead_for, last_hb in to_retire:
                    info = self.peers.get(node_id)
                    if info and not getattr(info, "retired", False):
                        info.retired = True
                        info.retired_at = now
                        # Jan 20, 2026: Use adaptive cooldown manager
                        if self._cooldown_manager:
                            self._cooldown_manager.record_death(node_id)
                        else:
                            self._dead_peer_timestamps[node_id] = now
                        retired_peers.append((node_id, last_hb, dead_for))
                        logger.info(f"Retired peer {node_id} (dead for {dead_for:.0f}s)")

        # Jan 12, 2026: Emit events OUTSIDE the lock to prevent deadlocks
        # Event handlers may need to acquire peers_lock themselves
        for node_id, last_hb, dead_for in retired_peers:
            asyncio.create_task(self._emit_host_offline(node_id, "retired", last_hb))
            asyncio.create_task(self._emit_node_dead(node_id, "retired", last_hb, dead_for))
            asyncio.create_task(self._emit_cluster_capacity_changed(
                total_nodes=capacity_total,
                alive_nodes=capacity_alive,
                gpu_nodes=capacity_gpu,
                training_nodes=capacity_training,
                change_type="node_removed",
                change_details={"node_id": node_id, "reason": "peer_timeout"},
            ))

        for node_id, caps in recovered_peers:
            asyncio.create_task(self._emit_host_online(node_id, caps))
            asyncio.create_task(self._emit_cluster_capacity_changed(
                total_nodes=capacity_total,
                alive_nodes=capacity_alive,
                gpu_nodes=capacity_gpu,
                training_nodes=capacity_training,
                change_type="node_added",
                change_details={"node_id": node_id, "reason": "peer_recovered"},
            ))

        # Clear stale leader IDs after restarts/partitions
        if self.leader_id and not self._is_leader_lease_valid():
            old_leader_id = self.leader_id
            is_self_leader = (old_leader_id == self.node_id)
            logger.info(f"Clearing stale/expired leader lease: leader_id={old_leader_id}, is_self={is_self_leader}")

            if is_self_leader:
                # Jan 2026: Our own lease expired - use ULSM for broadcast-before-mutation
                await self._complete_step_down_async(TransitionReason.LEASE_EXPIRED)
            else:
                # Another node's stale lease - just clear locally, no broadcast needed
                # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                self._set_leader(None, reason="stale_remote_lease", save_state=False)
                self.leader_lease_id = ""
                self.leader_lease_expires = 0.0
                self.last_lease_renewal = 0.0
                # Dec 31, 2025: Set invalidation window to prevent gossip from re-setting stale leader
                # Window = 60 seconds - enough time for election to complete
                self._leader_invalidation_until = time.time() + 60.0
                # Emit LEADER_LOST before starting election (Dec 2025 fix)
                await self._emit_leader_lost(old_leader_id, "lease_expired")
                # CRITICAL: Check quorum before starting election to prevent quorum bypass
                if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                    logger.warning("Skipping election after stale lease clear: no voter quorum available")
                else:
                    asyncio.create_task(self._start_election())

        # If leader is dead, start election
        if self.leader_id and self.leader_id != self.node_id:
            async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                leader = self.peers.get(self.leader_id)
                peers_snapshot = [p for p in self.peers.values() if p.node_id != self.node_id]
            if leader:
                conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])
                if not self._is_leader_eligible(leader, conflict_keys):
                    reason = "dead" if not leader.is_alive() else "ineligible"
                    logger.info(f"Leader {self.leader_id} is {reason}, starting election")
                    # Dec 2025: Emit LEADER_LOST before clearing leader_id
                    old_leader_id = self.leader_id
                    # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                    self._set_leader(None, reason=f"leader_{reason}", save_state=False)
                    self.leader_lease_id = ""
                    self.leader_lease_expires = 0.0
                    self.last_lease_renewal = 0.0
                    # Dec 31, 2025: Set invalidation window to prevent gossip from re-setting dead leader
                    self._leader_invalidation_until = time.time() + 60.0
                    asyncio.create_task(self._emit_leader_lost(old_leader_id, reason))
                    # CRITICAL: Check quorum before starting election to prevent quorum bypass
                    if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                        logger.warning(f"Skipping election after leader {reason}: no voter quorum available")
                    else:
                        asyncio.create_task(self._start_election())
            else:
                # Dec 31, 2025: Leader not in peers dict - unknown/unreachable leader
                # This can happen if leader gossip propagated but peer discovery hasn't caught up
                # Clear the leader and allow election
                logger.warning(f"Leader {self.leader_id} not found in peers dict, clearing stale leader")
                old_leader_id = self.leader_id
                # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                self._set_leader(None, reason="leader_unknown_peer", save_state=False)
                self.leader_lease_id = ""
                self.leader_lease_expires = 0.0
                self.last_lease_renewal = 0.0
                self._leader_invalidation_until = time.time() + 60.0
                asyncio.create_task(self._emit_leader_lost(old_leader_id, "unknown_peer"))

        # If we're leaderless, periodically retry elections with adaptive backoff
        # December 29, 2025: Improved backoff to start faster then slow down
        if not self.leader_id and not self.election_in_progress:
            now = time.time()
            retry_count = int(getattr(self, "_election_retry_count", 0) or 0)
            # Adaptive backoff: 15s, 30s, 60s, 90s (capped)
            backoff_intervals = [15, 30, 60, 90]
            backoff_seconds = backoff_intervals[min(retry_count, len(backoff_intervals) - 1)]
            last_attempt = float(getattr(self, "last_election_attempt", 0.0) or 0.0)
            if now - last_attempt >= backoff_seconds:
                self.last_election_attempt = now
                self._election_retry_count = retry_count + 1
                # CRITICAL: Check quorum before starting election to prevent quorum bypass
                if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                    logger.warning(f"Skipping periodic election retry {retry_count + 1}: no voter quorum available")
                    # Jan 1, 2026: Check probabilistic fallback leadership when quorum unavailable
                    # This prevents indefinite cluster stalls when voters are unreachable
                    asyncio.create_task(self._check_probabilistic_leadership(now))
                else:
                    logger.info(f"Triggering election retry {retry_count + 1} after {backoff_seconds}s leaderless")
                    asyncio.create_task(self._start_election())
        elif self.leader_id:
            # Reset retry count when we have a leader
            self._election_retry_count = 0
            # Jan 1, 2026: Reset probabilistic claim probability when we have a leader
            self._provisional_claim_probability = PROVISIONAL_LEADER_INITIAL_PROBABILITY

    async def _probe_retired_peers_async(self) -> None:
        """Actively probe retired peers to detect recovery.

        Dec 30, 2025: Added to fix cluster connectivity after restart.
        Retired nodes don't send heartbeats, so we must probe them.
        This runs periodically (every PEER_RECOVERY_RETRY_INTERVAL) to
        detect nodes that have come back online after being retired.
        """
        # Collect retired peers (excluding self)
        async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
            retired = [
                p for p in self.peers.values()
                if getattr(p, "retired", False) and p.node_id != self.node_id
            ]

        if not retired:
            return

        logger.debug(f"Probing {len(retired)} retired peers for recovery")

        # Use a short timeout for health checks
        timeout = aiohttp.ClientTimeout(total=5)

        for peer in retired:
            try:
                # Try to reach the peer's health endpoint
                url = f"http://{peer.host}:{peer.port}/health"
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    async with session.get(url) as resp:
                        if resp.status == 200:
                            # Node is alive - un-retire it
                            async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                                if peer.node_id in self.peers:
                                    self.peers[peer.node_id].retired = False
                                    self.peers[peer.node_id].retired_at = 0.0
                                    self.peers[peer.node_id].last_heartbeat = time.time()

                            logger.info(f"Recovered retired peer via probe: {peer.node_id}")

                            # Emit HOST_ONLINE event
                            caps = []
                            if hasattr(peer, "gpu_type") and peer.gpu_type:
                                caps.append(f"gpu:{peer.gpu_type}")
                            await self._emit_host_online(peer.node_id, caps)

                            # Emit CLUSTER_CAPACITY_CHANGED
                            async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
                                alive_count = sum(
                                    1 for p in self.peers.values()
                                    if p.is_alive() and not getattr(p, "retired", False)
                                )
                                gpu_count = sum(
                                    1 for p in self.peers.values()
                                    if p.is_alive() and not getattr(p, "retired", False)
                                    and getattr(p, "gpu_type", None)
                                )
                                training_count = sum(
                                    1 for p in self.peers.values()
                                    if p.is_alive() and not getattr(p, "retired", False)
                                    and getattr(p, "training_enabled", False)
                                )
                            asyncio.create_task(self._emit_cluster_capacity_changed(
                                total_nodes=len(self.peers),
                                alive_nodes=alive_count,
                                gpu_nodes=gpu_count,
                                training_nodes=training_count,
                                change_type="node_added",
                                change_details={"node_id": peer.node_id, "reason": "peer_recovered_via_probe"},
                            ))

            except (aiohttp.ClientError, asyncio.TimeoutError, OSError) as e:
                # Still unreachable - remain retired
                logger.debug(f"Retired peer {peer.node_id} still unreachable: {e}")
                continue
            except Exception as e:
                # Unexpected error - log but don't crash
                logger.warning(f"Error probing retired peer {peer.node_id}: {e}")
                continue

    def _check_dead_peers(self):
        """Check for peers that have stopped responding.

        January 12, 2026: Refactored to move event emissions outside the lock
        to prevent deadlock risk when event handlers need the same lock.
        """
        now = time.time()

        # Jan 12, 2026: Collect event data inside lock, emit outside
        retired_peers: list[tuple[str, float | None, float]] = []  # (node_id, last_hb, dead_for)
        recovered_peers: list[tuple[str, list[str]]] = []  # (node_id, capabilities)

        with self.peers_lock:
            dead_peers = []
            for node_id, info in self.peers.items():
                if not info.is_alive() and node_id != self.node_id:
                    dead_peers.append(node_id)
                    # Retire long-dead peers so they don't pollute active scheduling.
                    try:
                        dead_for = now - float(getattr(info, "last_heartbeat", 0.0) or 0.0)
                    except (ValueError, AttributeError):
                        dead_for = float("inf")
                    if not getattr(info, "retired", False) and dead_for >= PEER_RETIRE_AFTER_SECONDS:
                        info.retired = True
                        info.retired_at = now
                        # Jan 20, 2026: Use adaptive cooldown manager
                        if self._cooldown_manager:
                            self._cooldown_manager.record_death(node_id)
                        else:
                            self._dead_peer_timestamps[node_id] = now
                        logger.info(f"Retiring peer {node_id} (offline for {int(dead_for)}s)")
                        # Collect data for event emission outside lock
                        last_hb = getattr(info, "last_heartbeat", None)
                        retired_peers.append((node_id, last_hb, dead_for))
                elif info.is_alive() and getattr(info, "retired", False):
                    # Peer came back: clear retirement and remove from dead tracking.
                    info.retired = False
                    info.retired_at = 0.0
                    # Jan 20, 2026: Clear cooldown on recovery
                    if self._cooldown_manager:
                        self._cooldown_manager.clear_cooldown(node_id)
                    else:
                        self._dead_peer_timestamps.pop(node_id, None)
                    # Collect data for event emission outside lock
                    caps = []
                    if hasattr(info, "gpu_type") and info.gpu_type:
                        caps.append(f"gpu:{info.gpu_type}")
                    recovered_peers.append((node_id, caps))
                    logger.info(f"Peer {node_id} recovered from retirement")

            for node_id in dead_peers:
                info = self.peers.get(node_id)
                if info and getattr(info, "retired", False):
                    continue
                logger.info(f"Peer {node_id} is dead (no heartbeat for {PEER_TIMEOUT}s)")
                # Don't remove immediately, just mark as dead for historical tracking

            # STABILITY FIX: Auto-purge very old retired peers to prevent unbounded list growth
            # This removes stale entries that would otherwise accumulate and cause confusion
            # (e.g., old leader role claims that don't match current elected leader)
            peers_to_purge = []
            for node_id, info in self.peers.items():
                if getattr(info, "retired", False):
                    retired_at = getattr(info, "retired_at", 0.0) or 0.0
                    if retired_at > 0 and (now - retired_at) >= PEER_PURGE_AFTER_SECONDS:
                        peers_to_purge.append(node_id)

            for node_id in peers_to_purge:
                del self.peers[node_id]
                logger.info(f"Auto-purged stale peer: {node_id} (retired for >{PEER_PURGE_AFTER_SECONDS}s)")

            # Jan 7, 2026 Session 17.43: Purge SWIM protocol entries (IP:7947)
            # These leak from the SWIM membership layer and pollute VoterHealth/EloSync
            swim_entries_to_purge = [
                peer_id for peer_id in self.peers.keys()
                if self._is_swim_peer_id(peer_id)
            ]
            for peer_id in swim_entries_to_purge:
                del self.peers[peer_id]
                logger.info(f"Purged SWIM protocol entry from peer list: {peer_id}")

            # December 2025: Emit cluster health event if state changed
            # This enables pipeline coordination to pause/resume based on cluster health
            final_alive_count = sum(
                1 for p in self.peers.values()
                if p.is_alive() and not getattr(p, "retired", False)
            )
            final_node_count = len([
                p for p in self.peers.values()
                if not getattr(p, "retired", False)
            ])
            # Add self if not in peers
            if self.node_id not in self.peers:
                final_alive_count += 1
                final_node_count += 1
            # Dec 28, 2025: Fixed signature mismatch - pass correct parameters
            is_healthy = final_alive_count > 0
            quorum_met = self._has_voter_quorum() if hasattr(self, '_has_voter_quorum') else True
            self._emit_cluster_health_event_sync(is_healthy, final_alive_count, quorum_met)

            # Jan 12, 2026: Sync to lock-free snapshot after peer retirement/purge (sync version)
            self._sync_peer_snapshot()

            # Capture final counts for capacity events (inside lock for consistency)
            capacity_alive = sum(1 for p in self.peers.values() if p.is_alive() and not getattr(p, "retired", False))
            capacity_gpu = sum(1 for p in self.peers.values() if p.is_alive() and not getattr(p, "retired", False) and getattr(p, "gpu_type", None))

        # Jan 12, 2026: Emit events OUTSIDE the lock to prevent deadlocks (sync version)
        for node_id, last_hb, dead_for in retired_peers:
            self._emit_host_offline_sync(node_id, "retired", last_hb)
            self._emit_node_dead_sync(node_id, "retired", last_hb, dead_for)
            self._emit_cluster_capacity_changed_sync(
                change_type="node_removed",
                node_id=node_id,
                total_nodes=capacity_alive,
                gpu_nodes=capacity_gpu,
                reason="peer_timeout",
            )

        for node_id, caps in recovered_peers:
            self._emit_host_online_sync(node_id, caps)
            self._emit_cluster_capacity_changed_sync(
                change_type="node_added",
                node_id=node_id,
                total_nodes=capacity_alive,
                gpu_nodes=capacity_gpu,
                reason="peer_recovered",
            )

        # LEARNED LESSONS - Clear stale leader IDs after restarts/partitions.
        #
        # Nodes persist `leader_id` but not lease metadata. After a restart, it's
        # possible to have `leader_id` point at an alive peer that is no longer a
        # leader (or to a leader whose lease is expired). Without an explicit lease
        # validity check, the cluster can get stuck leaderless and stop dispatching
        # jobs (while still "thinking" it has a leader).
        if self.leader_id and not self._is_leader_lease_valid():
            logger.info(f"Clearing stale/expired leader lease: leader_id={self.leader_id}")
            # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
            self._set_leader(None, reason="stale_lease_sync", save_state=False)
            self.leader_lease_id = ""
            self.leader_lease_expires = 0.0
            self.last_lease_renewal = 0.0
            # CRITICAL: Check quorum before starting election to prevent quorum bypass
            if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                logger.warning("Skipping election after stale lease clear (sync): no voter quorum available")
            else:
                asyncio.create_task(self._start_election())

        # If leader is dead, start election
        if self.leader_id and self.leader_id != self.node_id:
            with self.peers_lock:
                leader = self.peers.get(self.leader_id)
                peers_snapshot = [p for p in self.peers.values() if p.node_id != self.node_id]
            if leader:
                conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])
                if not self._is_leader_eligible(leader, conflict_keys):
                    reason = "dead" if not leader.is_alive() else "ineligible"
                    logger.info(f"Leader {self.leader_id} is {reason}, starting election")
                    # Dec 2025: Emit LEADER_LOST before clearing stale/ineligible leader
                    old_leader_id = self.leader_id
                    # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                    self._set_leader(None, reason=f"leader_{reason}_sync", save_state=False)
                    self.leader_lease_id = ""
                    self.leader_lease_expires = 0.0
                    self.last_lease_renewal = 0.0
                    asyncio.create_task(self._emit_leader_lost(old_leader_id, reason))
                    # CRITICAL: Check quorum before starting election to prevent quorum bypass
                    if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                        logger.warning(f"Skipping election after leader {reason} (sync): no voter quorum available")
                    else:
                        asyncio.create_task(self._start_election())
            else:
                # Dec 31, 2025: Leader not in peers dict - unknown/unreachable leader
                logger.warning(f"Leader {self.leader_id} not found in peers dict (sync), clearing stale leader")
                old_leader_id = self.leader_id
                # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                self._set_leader(None, reason="leader_unknown_peer_sync", save_state=False)
                self.leader_lease_id = ""
                self.leader_lease_expires = 0.0
                self.last_lease_renewal = 0.0
                self._leader_invalidation_until = time.time() + 60.0
                asyncio.create_task(self._emit_leader_lost(old_leader_id, "unknown_peer"))

        # If we're leaderless, periodically retry elections with adaptive backoff
        # December 29, 2025: Same adaptive backoff as async version
        if not self.leader_id and not self.election_in_progress:
            now = time.time()
            retry_count = int(getattr(self, "_election_retry_count", 0) or 0)
            # Adaptive backoff: 15s, 30s, 60s, 90s (capped)
            backoff_intervals = [15, 30, 60, 90]
            backoff_seconds = backoff_intervals[min(retry_count, len(backoff_intervals) - 1)]
            last_attempt = float(getattr(self, "last_election_attempt", 0.0) or 0.0)
            if now - last_attempt >= backoff_seconds:
                self.last_election_attempt = now
                self._election_retry_count = retry_count + 1
                # CRITICAL: Check quorum before starting election to prevent quorum bypass
                if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                    logger.warning(f"Skipping periodic election retry {retry_count + 1} (sync): no voter quorum available")
                else:
                    logger.info(f"Triggering election retry {retry_count + 1} after {backoff_seconds}s leaderless (sync)")
                    asyncio.create_task(self._start_election())
        elif self.leader_id:
            # Reset retry count when we have a leader
            self._election_retry_count = 0

    async def _start_election(self):
        """Start leader election using Bully algorithm."""
        # Jan 3, 2026 Sprint 13.3: Track election start time for latency metrics
        self._start_election_timing()

        # Jan 19, 2026: Don't participate in elections until state loaded
        # CRITICAL FIX: Nodes were voting at 5s but state loads in 30-50s,
        # causing elections with incomplete cluster view (split-brain, thrashing)
        elapsed = time.time() - getattr(self, "_startup_time", 0)
        if elapsed < ELECTION_PARTICIPATION_DELAY:
            logger.info(
                f"[Election] Skipping: still in startup grace "
                f"({elapsed:.0f}s < {ELECTION_PARTICIPATION_DELAY}s)"
            )
            return

        # Jan 5, 2026: Global election cooldown to prevent rapid election storms
        # This complements the per-loop backoff in _maybe_trigger_election()
        # Jan 19, 2026: Reduced from 30s to 15s to match ELECTION_TIMEOUT
        # CRITICAL FIX: 30s cooldown > 15s timeout caused 15s dead-leader gaps
        ELECTION_GLOBAL_COOLDOWN = 15.0  # seconds
        now = time.time()
        last_election = getattr(self, "_last_election_completed", 0.0)
        if now - last_election < ELECTION_GLOBAL_COOLDOWN:
            logger.debug(
                f"[Election] Skipping: global cooldown ({now - last_election:.1f}s < {ELECTION_GLOBAL_COOLDOWN}s)"
            )
            return

        self._update_self_info()

        # NAT-blocked nodes cannot act as a leader because peers can't reach them.
        if getattr(self.self_info, "nat_blocked", False):
            return

        # Jan 13, 2026: Strict quorum enforcement (P2P Cluster Stability Plan Phase 2)
        # Check quorum BEFORE proceeding with election
        voter_node_ids = list(getattr(self, "voter_node_ids", []) or [])
        if voter_node_ids:
            if self.node_id not in voter_node_ids:
                # December 29, 2025: Non-voters can request elections from voters
                # instead of just returning silently
                await self._request_election_from_voters("non_voter_detected_leaderless")
                return

            # Jan 13, 2026: Use strict quorum check when available
            try:
                from scripts.p2p.leader_election import should_block_election
                # Jan 12, 2026: Use lock-free PeerSnapshot for read-only access
                snapshot = self._peer_snapshot.get_snapshot()
                should_block, reason = should_block_election(
                    voter_node_ids,
                    snapshot,
                    self.node_id,
                )
                if should_block:
                    logger.warning(f"[Election] Blocked: {reason}")
                    self._safe_emit_event("ELECTION_BLOCKED", {
                        "node_id": self.node_id,
                        "reason": reason,
                        "voter_count": len(voter_node_ids),
                        "timestamp": time.time(),
                    })
                    return
            except ImportError:
                # Fall back to legacy quorum check
                if not self._has_voter_quorum():
                    return

        # Jan 12, 2026: Use lock-free PeerSnapshot for read-only access
        snapshot = self._peer_snapshot.get_snapshot()
        peers_snapshot = [p for p in snapshot.values() if p.node_id != self.node_id]

        conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])

        if self.leader_id and self.leader_id != self.node_id:
            # Jan 12, 2026: Use lock-free PeerSnapshot for read-only access
            leader = snapshot.get(self.leader_id)
            leader_ok = (
                leader is not None
                and leader.is_alive()
                and leader.role == NodeRole.LEADER
                and self._is_leader_eligible(leader, conflict_keys)
                and self._is_leader_lease_valid()
            )
            if leader_ok:
                return
            # Drop stale/ineligible leader so we don't keep advertising it.
            # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
            self._set_leader(None, reason="stale_ineligible_leader", save_state=False)
            self.leader_lease_id = ""
            self.leader_lease_expires = 0.0
        if self._maybe_adopt_leader_from_peers():
            return

        if self.election_in_progress:
            return

        # =========================================================================
        # Jan 2, 2026: Leader Stickiness
        # If we recently were the leader, try to reclaim immediately.
        # If we're not the recent leader, check if any peer was and give them priority.
        # =========================================================================
        if self._was_recently_leader() and self._in_incumbent_grace_period():
            # We recently stepped down - try to reclaim leadership immediately
            logger.info(
                f"Incumbent advantage: attempting immediate leadership reclaim "
                f"(stepped down {time.time() - self._last_step_down_time:.1f}s ago)"
            )
            self.election_in_progress = True
            # C1 fix: Use leader_state_lock for role changes
            with self.leader_state_lock:
                self.role = NodeRole.CANDIDATE
            try:
                # Skip bully algorithm - try to become leader directly
                conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])
                if self._is_leader_eligible(self.self_info, conflict_keys):
                    await self._become_leader()
                    if self.role == NodeRole.LEADER:
                        logger.info("Incumbent reclaimed leadership successfully")
                        return
            finally:
                # C1 fix: Use leader_state_lock for role changes
                with self.leader_state_lock:
                    if self.role == NodeRole.CANDIDATE:
                        self.role = NodeRole.FOLLOWER
                self.election_in_progress = False
            # If we failed to reclaim, fall through to normal election
            logger.info("Incumbent reclaim failed, falling back to normal election")

        self.election_in_progress = True
        # C1 fix: Use leader_state_lock for role changes
        with self.leader_state_lock:
            self.role = NodeRole.CANDIDATE
        logger.info(f"Starting election, my ID: {self.node_id}")

        try:
            # Send election message to all nodes with higher IDs
            # Jan 12, 2026: Use lock-free PeerSnapshot for read-only access
            election_snapshot = self._peer_snapshot.get_snapshot()
            higher_nodes = [
                p for p in election_snapshot.values()
                if (
                    p.node_id > self.node_id
                    and self._is_leader_eligible(p, conflict_keys)
                )
            ]
            voter_node_ids = list(getattr(self, "voter_node_ids", []) or [])
            if voter_node_ids:
                higher_nodes = [p for p in higher_nodes if p.node_id in voter_node_ids]

            got_response = False

            timeout = ClientTimeout(total=ELECTION_TIMEOUT)
            async with get_client_session(timeout) as session:
                for peer in higher_nodes:
                    try:
                        url = self._url_for_peer(peer, "/election")
                        async with session.post(url, json={"candidate_id": self.node_id}, headers=self._auth_headers()) as resp:
                            if resp.status == 200:
                                data = await resp.json()
                                if data.get("response") == "ALIVE":
                                    got_response = True
                                    logger.info(f"Higher node {peer.node_id} responded")
                    except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
                        pass  # Network errors expected during elections

            # If no higher node responded, we become leader
            if not got_response:
                # Only become leader if we're eligible (unique + directly reachable).
                if self._is_leader_eligible(self.self_info, conflict_keys):
                    await self._become_leader()
            else:
                # Wait for coordinator message
                await asyncio.sleep(ELECTION_TIMEOUT * 2)
                # If no coordinator arrives, fall back to adopting any eligible leader we can see.
                self._maybe_adopt_leader_from_peers()

        finally:
            self.election_in_progress = False
            # C1 fix: Use leader_state_lock for role changes
            with self.leader_state_lock:
                if self.role == NodeRole.CANDIDATE:
                    # Jan 3, 2026 Sprint 13.3: Record election latency for "timeout" outcome
                    # Election ended without becoming leader or adopting one
                    if getattr(self, "_election_started_at", 0) > 0:
                        self._record_election_latency("timeout")
                    self.role = NodeRole.FOLLOWER

    async def _become_leader(self):
        """Become the cluster leader with lease-based leadership."""
        self._update_self_info()
        if getattr(self.self_info, "nat_blocked", False):
            logger.info(f"Refusing leadership while NAT-blocked: {self.node_id}")
            return
        if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
            logger.info(f"Refusing leadership without voter quorum: {self.node_id}")
            return
        import uuid
        lease_id = f"{self.node_id}_{int(time.time())}_{uuid.uuid4().hex[:8]}"
        lease_expires = await self._acquire_voter_lease_quorum(lease_id, int(LEADER_LEASE_DURATION))
        if getattr(self, "voter_node_ids", []) and not lease_expires:
            logger.error(f"Failed to obtain voter lease quorum; refusing leadership: {self.node_id}")
            # Jan 3, 2026 Sprint 13.3: Record election latency for "lost" outcome (no quorum)
            self._record_election_latency("lost")
            # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
            self._set_leader(None, reason="election_failed_no_quorum", save_state=False)
            self.leader_lease_id = ""
            self.leader_lease_expires = 0.0
            self.last_lease_renewal = 0.0
            self._release_voter_grant_if_self()
            # Jan 5, 2026: Mark election completed (even on failure) for cooldown tracking
            self._last_election_completed = time.time()
            self._save_state()
            return

        logger.info(f"I am now the leader: {self.node_id}")
        # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
        self._set_leader(self.node_id, reason="become_leader", save_state=False)
        self.last_leader_seen = time.time()  # Track when we last had a functioning leader

        # Jan 5, 2026: Register self in peers dict when becoming leader
        # This ensures the leader is visible in peers iteration and quorum checks
        self._register_self_in_peers()

        # Jan 3, 2026 Sprint 13.3: Record election latency for "won" outcome
        self._record_election_latency("won")
        # Dec 31, 2025: Track leadership acquisition time and reset quorum fail counters
        self._last_become_leader_time = time.time()
        self._quorum_fail_count = 0
        self._is_leader_quorum_fail_count = 0

        # Phase 29: Increment cluster epoch on leadership change
        # This helps resolve split-brain when partitions merge
        self._increment_cluster_epoch()

        # Phase 15.1.1: Increment lease epoch and create fence token
        # This provides split-brain protection by ensuring each leadership
        # term has a unique, monotonically increasing epoch
        self._lease_epoch += 1
        self._fence_token = f"{self.node_id}:{self._lease_epoch}:{time.time()}"
        logger.info(f"Leader lease fencing: epoch={self._lease_epoch}, token={self._fence_token}")

        # CRITICAL: Emit LEADER_ELECTED event (Dec 2025 fix)
        # This enables LeadershipCoordinator and other components to track leadership changes
        asyncio.create_task(self._emit_leader_elected(self.node_id, getattr(self, "cluster_epoch", 0)))

        # Lease-based leadership (voter-backed when enabled).
        self.leader_lease_id = lease_id
        self.leader_lease_expires = float(lease_expires or (time.time() + LEADER_LEASE_DURATION))
        self.last_lease_renewal = time.time()

        # Announce to all peers with lease information
        with self.peers_lock:
            peers = list(self.peers.values())

        timeout = ClientTimeout(total=5)
        async with get_client_session(timeout) as session:
            for peer in peers:
                if peer.node_id != self.node_id:
                    try:
                        url = self._url_for_peer(peer, "/coordinator")
                        await session.post(url, json={
                            "leader_id": self.node_id,
                            "lease_id": self.leader_lease_id,
                            "lease_expires": self.leader_lease_expires,
                            "voter_node_ids": list(getattr(self, "voter_node_ids", []) or []),
                            # Phase 15.1.1: Include epoch and fence token for split-brain protection
                            "lease_epoch": self._lease_epoch,
                            "fence_token": self._fence_token,
                        }, headers=self._auth_headers())
                    except (aiohttp.ClientError, asyncio.TimeoutError, KeyError, IndexError, AttributeError):
                        pass  # Network errors expected during leader announcements

        # Jan 5, 2026: Mark election completed for cooldown tracking
        self._last_election_completed = time.time()

        self._save_state()

        # Start monitoring services when becoming leader
        await self._start_monitoring_if_leader()

        # Start P2P auto-deployer when becoming leader
        await self._start_p2p_auto_deployer()

    # ============================================
    # Probabilistic Fallback Leadership (Jan 1, 2026)
    # ============================================

    async def _check_probabilistic_leadership(self, now: float) -> None:
        """Check if we should claim provisional leadership using probabilistic fallback.

        Jan 1, 2026: When normal elections repeatedly fail (e.g., voter quorum unavailable),
        nodes can claim provisional leadership with increasing probability. This prevents
        indefinite cluster stalls while still preferring proper elections.

        Design:
        1. Only activate after PROVISIONAL_LEADER_MIN_LEADERLESS_TIME (5 min)
        2. Probability starts low, grows exponentially over time
        3. If random check passes, claim PROVISIONAL_LEADER state
        4. Announce to peers and collect acknowledgments
        5. Promote to full LEADER after quorum ack or timeout with no challengers
        """
        import random

        # Skip if we already have a leader or are claiming
        if self.leader_id or self.role in (NodeRole.LEADER, NodeRole.PROVISIONAL_LEADER, NodeRole.CANDIDATE):
            return

        # Rate limit checks
        if now - self._last_provisional_check < PROVISIONAL_LEADER_CHECK_INTERVAL:
            return
        self._last_provisional_check = now

        # Check if we've been leaderless long enough
        leaderless_duration = now - self.last_leader_seen
        if leaderless_duration < PROVISIONAL_LEADER_MIN_LEADERLESS_TIME:
            return

        # Check if we're eligible (not NAT-blocked, preferably GPU node)
        self._update_self_info()
        if getattr(self.self_info, "nat_blocked", False):
            logger.debug("Skipping probabilistic leadership: NAT-blocked")
            return

        # Calculate current probability based on leaderless duration
        # Probability grows exponentially beyond minimum threshold
        minutes_beyond_minimum = (leaderless_duration - PROVISIONAL_LEADER_MIN_LEADERLESS_TIME) / 60.0
        current_prob = min(
            PROVISIONAL_LEADER_MAX_PROBABILITY,
            PROVISIONAL_LEADER_INITIAL_PROBABILITY * (PROVISIONAL_LEADER_PROBABILITY_GROWTH_RATE ** minutes_beyond_minimum)
        )
        self._provisional_claim_probability = current_prob

        # Roll the dice
        roll = random.random()
        logger.debug(f"Probabilistic leadership check: roll={roll:.3f}, threshold={current_prob:.3f}, "
                    f"leaderless={int(leaderless_duration)}s")

        if roll >= current_prob:
            return  # Not claiming this time

        # We're claiming provisional leadership
        logger.info(f"Claiming provisional leadership after {int(leaderless_duration)}s leaderless "
                   f"(prob={current_prob:.2%}, roll={roll:.3f})")

        await self._claim_provisional_leadership()

    async def _claim_provisional_leadership(self) -> None:
        """Claim provisional leadership and announce to peers.

        Provisional leaders can dispatch work but must be confirmed by:
        - Quorum acknowledgment from peers, OR
        - No challengers after timeout period (with node_id tiebreaker if contested)
        """
        import uuid

        now = time.time()

        # Set provisional state
        # C1 fix: Use leader_state_lock for role/leader_id changes
        with self.leader_state_lock:
            self.role = NodeRole.PROVISIONAL_LEADER
            self._provisional_leader_claimed_at = now
            self._provisional_leader_acks = {self.node_id}  # Self-ack
            self._provisional_leader_challengers = {}

            # Create a provisional lease (shorter than normal)
            provisional_lease_id = f"PROVISIONAL_{self.node_id}_{uuid.uuid4().hex[:8]}"
            self.leader_lease_id = provisional_lease_id
            self.leader_lease_expires = now + PROVISIONAL_LEADER_QUORUM_TIMEOUT
            self.last_lease_renewal = now

            # Set ourselves as leader (provisional) so peers know who's claiming
            self.leader_id = self.node_id

        logger.info(f"Provisional leadership claimed: lease={provisional_lease_id}")

        # Announce provisional claim to all peers
        async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
            peers = [p for p in self.peers.values() if p.node_id != self.node_id and p.is_alive()]

        if not peers:
            # No peers to acknowledge, promote immediately
            logger.info("No alive peers to acknowledge, promoting immediately to full leader")
            await self._promote_provisional_to_leader("no_peers")
            return

        # Send provisional leadership claim to all peers
        timeout = aiohttp.ClientTimeout(total=5)
        acks_received = 0
        challengers = []

        async with get_client_session(timeout) as session:
            for peer in peers:
                try:
                    url = self._url_for_peer(peer, "/provisional-leader/claim")
                    async with session.post(
                        url,
                        json={
                            "claimant_id": self.node_id,
                            "lease_id": provisional_lease_id,
                            "claimed_at": now,
                        },
                        headers=self._auth_headers(),
                    ) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            if data.get("ack"):
                                self._provisional_leader_acks.add(peer.node_id)
                                acks_received += 1
                                logger.debug(f"Provisional ack from {peer.node_id}")
                            elif data.get("challenge"):
                                challenger_id = data.get("challenger_id", peer.node_id)
                                self._provisional_leader_challengers[challenger_id] = now
                                challengers.append(challenger_id)
                                logger.info(f"Provisional challenge from {challenger_id}")
                except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
                    pass  # Network errors expected

        logger.info(f"Provisional claim results: {acks_received} acks, {len(challengers)} challengers")

        # Handle challenges using node_id tiebreaker
        if challengers:
            # Find highest challenger
            all_claimants = [self.node_id] + challengers
            all_claimants.sort(reverse=True)
            winner = all_claimants[0]

            if winner != self.node_id:
                logger.info(f"Stepping down from provisional: {winner} > {self.node_id}")
                self._step_down_from_provisional()
                return

            logger.info(f"Won provisional tiebreaker against {challengers}")

        # Check if we have quorum
        total_peers = len(peers) + 1  # Include self
        quorum_size = (total_peers // 2) + 1
        current_acks = len(self._provisional_leader_acks)

        if current_acks >= quorum_size:
            logger.info(f"Quorum achieved ({current_acks}/{quorum_size}), promoting to full leader")
            await self._promote_provisional_to_leader("quorum_achieved")
        else:
            # Schedule a follow-up check after timeout period
            logger.info(f"Quorum not yet achieved ({current_acks}/{quorum_size}), waiting for timeout")
            asyncio.get_event_loop().call_later(
                PROVISIONAL_LEADER_QUORUM_TIMEOUT,
                lambda: asyncio.create_task(self._check_provisional_promotion())
            )

    async def _check_provisional_promotion(self) -> None:
        """Check if provisional leader should be promoted after timeout period."""
        if self.role != NodeRole.PROVISIONAL_LEADER:
            return  # Already promoted or stepped down

        now = time.time()
        claim_duration = now - self._provisional_leader_claimed_at

        if claim_duration < PROVISIONAL_LEADER_QUORUM_TIMEOUT:
            return  # Not time yet

        # Check for any challengers that won during the timeout
        if self._provisional_leader_challengers:
            all_claimants = [self.node_id] + list(self._provisional_leader_challengers.keys())
            all_claimants.sort(reverse=True)
            winner = all_claimants[0]

            if winner != self.node_id:
                logger.info(f"Challenger {winner} won during timeout period")
                self._step_down_from_provisional()
                return

        # No successful challengers, promote to full leader
        logger.info(f"Provisional timeout elapsed with no successful challengers, promoting to full leader")
        await self._promote_provisional_to_leader("timeout_no_challengers")

    async def _promote_provisional_to_leader(self, reason: str) -> None:
        """Promote from provisional to full leader.

        Args:
            reason: Why we're promoting (quorum_achieved, timeout_no_challengers, no_peers)
        """
        if self.role == NodeRole.LEADER:
            return  # Already promoted

        logger.info(f"Promoting from provisional to full leader: {reason}")

        # Clear provisional state
        self._provisional_leader_claimed_at = 0.0
        self._provisional_leader_acks.clear()
        self._provisional_leader_challengers.clear()

        # Full leader transition (subset of _become_leader, but without voter lease)
        import uuid
        now = time.time()

        # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
        self._set_leader(self.node_id, reason=f"promote_provisional_{reason}", save_state=False)
        self.last_leader_seen = now

        # Jan 5, 2026: Register self in peers dict when promoted to leader
        self._register_self_in_peers()

        # Create a new lease ID to mark full leadership
        lease_id = f"FALLBACK_{self.node_id}_{int(now)}_{uuid.uuid4().hex[:8]}"
        self.leader_lease_id = lease_id
        self.leader_lease_expires = now + LEADER_LEASE_DURATION
        self.last_lease_renewal = now

        # Track that this is fallback leadership (for monitoring)
        self._fallback_leader_since = now
        self._fallback_leader_reason = reason

        # Increment epochs
        self._increment_cluster_epoch()
        self._lease_epoch += 1
        self._fence_token = f"{self.node_id}:{self._lease_epoch}:{now}"

        # Emit LEADER_ELECTED event
        asyncio.create_task(self._emit_leader_elected(self.node_id, getattr(self, "cluster_epoch", 0)))

        # Announce to all peers
        async with NonBlockingAsyncLockWrapper(self.peers_lock, "peers_lock", timeout=5.0):
            peers = list(self.peers.values())

        timeout = aiohttp.ClientTimeout(total=5)
        async with get_client_session(timeout) as session:
            for peer in peers:
                if peer.node_id != self.node_id:
                    try:
                        url = self._url_for_peer(peer, "/coordinator")
                        await session.post(
                            url,
                            json={
                                "leader_id": self.node_id,
                                "lease_id": self.leader_lease_id,
                                "lease_expires": self.leader_lease_expires,
                                "fallback_leadership": True,
                                "lease_epoch": self._lease_epoch,
                                "fence_token": self._fence_token,
                            },
                            headers=self._auth_headers(),
                        )
                    except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
                        pass

        self._save_state()

        # Start monitoring services
        await self._start_monitoring_if_leader()
        await self._start_p2p_auto_deployer()

        logger.info(f"Full fallback leadership established: lease={lease_id}")

    def _step_down_from_provisional(self) -> None:
        """Step down from provisional leadership (lost to challenger).

        Jan 2026: Uses ULSM for broadcast-before-mutation pattern.
        """
        logger.info("Stepping down from provisional leadership via ULSM")

        # Clear provisional-specific state first (ULSM doesn't know about these)
        self._provisional_leader_claimed_at = 0.0
        self._provisional_leader_acks.clear()
        self._provisional_leader_challengers.clear()

        # Use ULSM step-down (broadcasts to peers, then clears leader state)
        self._schedule_step_down_sync(TransitionReason.ARBITER_OVERRIDE)

        # Notify voters of lease revocation
        try:
            asyncio.create_task(self._notify_voters_lease_revoked())
        except RuntimeError:
            # Not in async context, schedule on event loop if available
            loop = asyncio.get_event_loop()
            if loop.is_running():
                asyncio.run_coroutine_threadsafe(self._notify_voters_lease_revoked(), loop)

    def _is_provisional_leader(self) -> bool:
        """Check if we are currently a provisional leader."""
        return self.role == NodeRole.PROVISIONAL_LEADER and self.leader_id == self.node_id

    # =========================================================================
    # ULSM TIERED FALLBACK - Jan 2026
    # Three-tier leadership fallback for cluster resilience
    # =========================================================================

    async def _check_leadership_fallback(self, now: float) -> None:
        """Tiered leadership fallback for cluster resilience.

        Jan 2026: ULSM tiered fallback mechanism with three levels:

        Tier 1 (0-60s): Fast election retries
            - Only if we have voter quorum
            - Try normal elections with increasing urgency

        Tier 2 (60-180s): Probabilistic provisional
            - Existing mechanism with exponential probability growth
            - Fallback when elections fail repeatedly

        Tier 3 (180s+): Deterministic fallback
            - Skip probability, highest eligible node_id wins
            - Last resort to ensure cluster always has leadership

        Args:
            now: Current timestamp
        """
        # Skip if we already have a leader or are claiming
        if self.leader_id:
            return

        # Skip if we're already in a leadership role
        if self.role in (NodeRole.LEADER, NodeRole.PROVISIONAL_LEADER, NodeRole.CANDIDATE):
            return

        leaderless_duration = now - self.last_leader_seen

        # Tier 1: Fast election retries (0-60s)
        if leaderless_duration < PROVISIONAL_LEADER_MIN_LEADERLESS_TIME:
            if self._has_voter_quorum():
                # Increment election retry counter
                self._election_retry_count = getattr(self, "_election_retry_count", 0) + 1

                if self._election_retry_count >= ELECTION_RETRY_COUNT_BEFORE_PROVISIONAL:
                    logger.info(f"Tier 1 fallback: {self._election_retry_count} election retries, "
                               f"proceeding to provisional fallback")
                else:
                    logger.debug(f"Tier 1 fallback: Retrying election (attempt {self._election_retry_count})")
                    await self._start_election()
            return

        # Tier 2: Probabilistic provisional (60-180s)
        if leaderless_duration < DETERMINISTIC_FALLBACK_TIME:
            await self._check_probabilistic_leadership(now)
            return

        # Tier 3: Deterministic fallback (180s+)
        logger.info(f"Tier 3 fallback: Deterministic election after {int(leaderless_duration)}s leaderless")
        await self._deterministic_leader_election()

    async def _deterministic_leader_election(self) -> None:
        """Deterministic fallback: highest eligible node_id becomes leader.

        Jan 2026: When probabilistic fallback has failed for too long,
        use deterministic approach: highest alphabetically-sorted node_id
        among eligible nodes wins. This guarantees convergence.
        """
        # Collect eligible nodes (alive, not NAT-blocked, preferably GPU)
        self._update_self_info()

        if getattr(self.self_info, "nat_blocked", False):
            logger.debug("Deterministic fallback: skipping, we are NAT-blocked")
            return

        # Build list of eligible node IDs
        eligible_nodes = []

        # Check self eligibility
        if not getattr(self.self_info, "nat_blocked", False):
            eligible_nodes.append(self.node_id)

        # Check peers - Jan 12, 2026: Use lock-free PeerSnapshot for read-only access
        for peer_id, peer in self._peer_snapshot.get_snapshot().items():
            if peer.is_alive() and not getattr(peer, "nat_blocked", False):
                eligible_nodes.append(peer_id)

        if not eligible_nodes:
            logger.warning("Deterministic fallback: no eligible nodes found")
            return

        # Highest node_id wins (deterministic tiebreaker)
        eligible_nodes.sort(reverse=True)
        winner = eligible_nodes[0]

        logger.info(f"Deterministic fallback: winner={winner} (from {len(eligible_nodes)} eligible)")

        if winner == self.node_id:
            # We won - claim provisional leadership
            logger.info("Deterministic fallback: We won, claiming provisional leadership")
            await self._claim_provisional_leadership()
        else:
            # Someone else should win - wait for them
            logger.debug(f"Deterministic fallback: {winner} should claim leadership, waiting")
            # Reset our retry counter since we're deferring
            self._election_retry_count = 0

    async def _request_election_from_voters(self, reason: str = "non_voter_request") -> bool:
        """December 29, 2025: Non-voters can request that voters start an election.

        Instead of silently returning when a non-voter tries to start an election,
        this method sends requests to known voters to have them start one.

        Args:
            reason: Why the election is being requested

        Returns:
            True if at least one voter accepted the request
        """
        voter_node_ids = list(getattr(self, "voter_node_ids", []) or [])
        if not voter_node_ids:
            return False

        logger.info(f"Non-voter {self.node_id} requesting election from voters: {reason}")

        # Rate limit election requests to avoid spamming
        now = time.time()
        last_request = getattr(self, "_last_election_request", 0.0)
        if now - last_request < 30:  # At most once per 30 seconds
            logger.debug("Skipping election request: rate limited")
            return False
        self._last_election_request = now

        accepted = False
        # Jan 12, 2026: Copy-on-write - single snapshot instead of lock-per-voter
        with self.peers_lock:
            peers_snapshot = dict(self.peers)

        async with aiohttp.ClientSession() as session:
            for voter_id in voter_node_ids[:3]:  # Limit to 3 voters
                voter = peers_snapshot.get(voter_id)
                if not voter or not voter.is_alive():
                    continue

                try:
                    url = self._url_for_peer(voter, "/election/request")
                    async with session.post(
                        url,
                        json={"requester_id": self.node_id, "reason": reason},
                        headers=self._auth_headers(),
                        timeout=aiohttp.ClientTimeout(total=5.0),
                    ) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            if data.get("accepted"):
                                logger.info(
                                    f"Voter {voter_id} accepted election request: {data.get('action')}"
                                )
                                accepted = True
                                break  # One voter accepting is enough
                except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                    logger.debug(f"Failed to request election from {voter_id}: {e}")
                    continue

        if not accepted:
            logger.warning(f"No voters accepted election request from {self.node_id}")
        return accepted

    async def _check_emergency_coordinator_fallback(self):
        """DECENTRALIZED: When voter quorum is unreachable for >5 min, any GPU node can coordinate.

        EMERGENCY COORDINATOR: This is a last-resort fallback when the normal voter-based
        leadership cannot be established due to:
        - Too many voters being offline
        - Network partition isolating voters
        - Cluster-wide issues

        In this mode, the node acts as a temporary coordinator WITHOUT voter consensus.
        It will relinquish control once voter quorum is restored.
        """
        now = time.time()

        # Only check every 60 seconds
        last_check = getattr(self, "_last_emergency_coord_check", 0)
        if now - last_check < 60:
            return
        self._last_emergency_coord_check = now

        # Skip if we already are a leader
        if self.role == NodeRole.LEADER:
            return

        # Skip if we have a known leader
        if self.leader_id:
            self._emergency_coordinator_since = 0
            return

        # Check if we have voter quorum
        if self._has_voter_quorum():
            self._emergency_coordinator_since = 0
            return  # Normal election should work

        # Track how long we've been without voter quorum
        quorum_missing_since = getattr(self, "_quorum_missing_since", 0)
        if quorum_missing_since == 0:
            self._quorum_missing_since = now
            return

        EMERGENCY_THRESHOLD = 300  # 5 minutes without quorum triggers emergency
        quorum_missing_duration = now - quorum_missing_since

        if quorum_missing_duration < EMERGENCY_THRESHOLD:
            return

        # Check if we're eligible (must be GPU node, not NAT-blocked)
        self._update_self_info()
        if not getattr(self.self_info, "has_gpu", False):
            return
        if getattr(self.self_info, "nat_blocked", False):
            return

        # Use consistent hashing to determine which node should be emergency coordinator
        # This prevents multiple nodes from declaring themselves coordinator
        with self.peers_lock:
            candidates = [self.node_id]
            for peer in self.peers.values():
                if not peer.is_alive():
                    continue
                if not getattr(peer, "has_gpu", False):
                    continue
                if getattr(peer, "nat_blocked", False):
                    continue
                candidates.append(peer.node_id)

        if not candidates:
            return

        # Deterministic selection: highest node_id wins (simple, consistent)
        candidates.sort(reverse=True)
        designated_coordinator = candidates[0]

        if designated_coordinator != self.node_id:
            return  # Another node should be coordinator

        # Become emergency coordinator (without voter lease)
        logger.info(f"EMERGENCY COORDINATOR: Taking leadership without voter quorum "
              f"(quorum missing for {int(quorum_missing_duration)}s, {len(candidates)} candidates)")

        # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
        self._set_leader(self.node_id, reason="emergency_coordinator", save_state=False)
        self.last_leader_seen = now
        self._emergency_coordinator_since = now

        # Use a special lease ID to mark emergency mode
        import uuid
        self.leader_lease_id = f"EMERGENCY_{self.node_id}_{uuid.uuid4().hex[:8]}"
        self.leader_lease_expires = now + 120  # Short lease - needs frequent renewal
        self.last_lease_renewal = now

        # Announce emergency leadership
        with self.peers_lock:
            peers = list(self.peers.values())

        timeout = ClientTimeout(total=5)
        async with get_client_session(timeout) as session:
            for peer in peers:
                if peer.node_id != self.node_id:
                    try:
                        url = self._url_for_peer(peer, "/coordinator")
                        await session.post(url, json={
                            "leader_id": self.node_id,
                            "lease_id": self.leader_lease_id,
                            "lease_expires": self.leader_lease_expires,
                            "emergency": True,
                        }, headers=self._auth_headers())
                    except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError):
                        pass  # Network errors expected during emergency coordination

        self._save_state()
        logger.info(f"EMERGENCY COORDINATOR: {self.node_id} is now emergency leader")

    def _get_peer_health_score(self, peer_id: str) -> float:
        """Calculate health score for a peer (0-100, higher is healthier).

        HEALTH-BASED PEER SELECTION: Considers multiple factors to pick
        the best peer for data sync, avoiding overloaded or unreliable nodes.

        Jan 3, 2026: Updated to use PeerHealthScore for composite tracking and
        PeerCircuitBreaker for fine-grained failure isolation.
        """
        with self.peers_lock:
            peer = self.peers.get(peer_id)
        if not peer or not peer.is_alive():
            return 0.0

        # Check per-peer circuit breaker first (Jan 3, 2026)
        breaker = self._peer_circuit_breakers.get(peer_id)
        if breaker and not breaker.should_allow_request():
            return 0.0  # Circuit is open, don't use this peer

        # Use PeerHealthScore if available (Jan 3, 2026)
        health_score = self._peer_health_scores.get(peer_id)
        if health_score:
            # Convert composite score (0-1) to (0-100) and apply resource penalties
            base_score = health_score.composite_score * 100.0
        else:
            base_score = 100.0

        score = base_score

        # Penalize high resource usage
        cpu = float(getattr(peer, "cpu_percent", 0) or 0)
        memory = float(getattr(peer, "memory_percent", 0) or 0)
        disk = float(getattr(peer, "disk_percent", 0) or 0)

        score -= cpu * 0.3  # CPU impact
        score -= memory * 0.2  # Memory impact
        score -= max(0, disk - 50) * 0.5  # Disk penalty above 50%

        # Penalize consecutive failures (circuit breaker input)
        failures = int(getattr(peer, "consecutive_failures", 0) or 0)
        score -= failures * 10

        # Penalize NAT-blocked peers (slower sync)
        if getattr(peer, "nat_blocked", False):
            score -= 20

        # Bonus for GPU nodes (typically more powerful)
        if getattr(peer, "has_gpu", False):
            score += 10

        # Legacy circuit breaker check (for backward compatibility)
        circuit_breaker = getattr(self, "_p2p_circuit_breaker", {})
        breaker_info = circuit_breaker.get(peer_id, {})
        if breaker_info.get("open_until", 0) > time.time():
            score = 0  # Circuit is open, don't use this peer

        return max(0.0, min(100.0, score))

    def _record_p2p_sync_result(self, peer_id: str, success: bool, latency_ms: float = 0.0):
        """Record P2P sync result for circuit breaker, metrics, and reputation.

        CIRCUIT BREAKER: After 3 consecutive failures, open circuit for 5 minutes.
        This prevents wasting time on unreliable peers.

        PEER REPUTATION: Also records sync result for reputation tracking.

        Jan 3, 2026: Updated to use PeerCircuitBreaker and PeerHealthScore classes
        for fine-grained tracking. Legacy dict-based breaker kept for compatibility.
        """
        if not hasattr(self, "_p2p_circuit_breaker"):
            self._p2p_circuit_breaker = {}
        if not hasattr(self, "_p2p_sync_metrics"):
            self._p2p_sync_metrics = {"success": 0, "failure": 0, "bytes": 0}

        # Jan 3, 2026: Use new PeerCircuitBreaker class
        if peer_id not in self._peer_circuit_breakers:
            self._peer_circuit_breakers[peer_id] = PeerCircuitBreaker(peer_id=peer_id)
        peer_breaker = self._peer_circuit_breakers[peer_id]

        # Jan 3, 2026: Use new PeerHealthScore class
        if peer_id not in self._peer_health_scores:
            self._peer_health_scores[peer_id] = PeerHealthScore(peer_id=peer_id)
        health_score = self._peer_health_scores[peer_id]

        # Record for new classes
        health_score.record_request(success=success, latency_ms=latency_ms)
        if success:
            peer_breaker.record_success()
        else:
            peer_breaker.record_failure()

        # Legacy dict-based breaker (for backward compatibility)
        breaker = self._p2p_circuit_breaker.get(peer_id, {"failures": 0, "open_until": 0})

        # Record for reputation tracking
        self._record_peer_interaction(peer_id, success, "sync")

        if success:
            breaker["failures"] = 0
            breaker["open_until"] = 0
            self._p2p_sync_metrics["success"] += 1
        else:
            breaker["failures"] = breaker.get("failures", 0) + 1
            self._p2p_sync_metrics["failure"] += 1

            # Open circuit after 3 failures
            if breaker["failures"] >= 3:
                breaker["open_until"] = time.time() + 300  # 5 minute cooldown
                logger.info(f"CIRCUIT BREAKER: Opening circuit for {peer_id} (3 failures)")

        self._p2p_circuit_breaker[peer_id] = breaker

        # Jan 3, 2026: Log if peer health is degraded
        if health_score.is_degraded():
            logger.warning(
                f"PEER_HEALTH_DEGRADED: {peer_id} score={health_score.composite_score:.2f}, "
                f"success_rate={health_score.success_rate:.2f}"
            )

    async def _p2p_data_sync(self):
        """DECENTRALIZED: Nodes sync data directly with peers without leader coordination.

        P2P DATA SYNC with enhancements:
        - Health-based peer selection (avoids overloaded nodes)
        - Circuit breaker (skips unreliable peers)
        - Delta sync (only syncs files newer than last sync)
        - Model file prioritization (syncs models first)
        - ADAPTIVE INTERVALS: adjusts based on cluster activity and success rate
        """
        now = time.time()

        # ADAPTIVE INTERVAL: Uses activity-aware interval instead of fixed 5 min
        interval = self._get_adaptive_sync_interval("data")
        last_check = getattr(self, "_last_p2p_sync_check", 0)
        if now - last_check < interval:
            return
        self._last_p2p_sync_check = now

        # Skip if leader is actively managing sync (avoid conflicts)
        if self.role == NodeRole.LEADER:
            return  # Leader uses centralized sync

        # Skip if a sync is already in progress
        if getattr(self, "sync_in_progress", False):
            return

        # Skip if under disk pressure
        if getattr(self.self_info, "disk_percent", 0) > 85:
            return

        # Get our local manifest (use cache for speed)
        local_manifest = getattr(self, "local_data_manifest", None)
        if not local_manifest:
            try:
                local_manifest = self.sync_planner.collect_local_manifest_cached(max_cache_age=600)
                with self.manifest_lock:
                    self.local_data_manifest = local_manifest
            except (AttributeError):
                return

        # Get local file set with timestamps for delta sync
        local_files = {}
        for file_info in getattr(local_manifest, "files", []) or []:
            rel_path = getattr(file_info, "relative_path", "")
            if rel_path:
                local_files[rel_path] = getattr(file_info, "modified_at", 0)

        # Check peer manifests from gossip cache
        peer_manifests = getattr(self, "_gossip_peer_manifests", {})
        if not peer_manifests:
            return

        # Find files we're missing that peers have (with prioritization)
        files_to_sync: dict[str, list[tuple]] = {}  # peer_id -> [(file, priority)]
        file_hashes: dict[str, str] = {}  # file_path -> hash (for dedup tracking)
        last_sync_time = getattr(self, "_last_successful_p2p_sync", 0)

        for peer_id, peer_manifest in peer_manifests.items():
            if peer_id == self.node_id:
                continue

            # Check circuit breaker
            health = self._get_peer_health_score(peer_id)
            if health <= 0:
                continue

            peer_files = getattr(peer_manifest, "files", []) or []
            for file_info in peer_files:
                rel_path = getattr(file_info, "relative_path", "")
                modified_at = getattr(file_info, "modified_at", 0)
                file_hash = getattr(file_info, "file_hash", "")
                file_size = getattr(file_info, "size_bytes", 0)

                if not rel_path:
                    continue

                # Skip if we have this file with same or newer timestamp
                if rel_path in local_files and local_files[rel_path] >= modified_at:
                    continue

                # Skip if file is older than last sync (delta optimization)
                if modified_at < last_sync_time and rel_path in local_files:
                    continue

                # DATA DEDUPLICATION: Skip if we already synced this file (by hash)
                if file_hash and self._is_file_already_synced(file_hash):
                    self._record_dedup_skip(file_count=1, bytes_saved=file_size)
                    continue

                # Calculate priority (models > ELO/training DBs > training data > selfplay)
                priority = 0
                if "models/" in rel_path or rel_path.endswith(".pt") or rel_path.endswith(".onnx"):
                    priority = 100  # Highest priority for models
                elif rel_path.endswith(".db") and ("unified_elo" in rel_path or "elo_ratings" in rel_path):
                    priority = 90  # Very high priority for ELO database
                elif rel_path.endswith(".db") and ("canonical_" in rel_path or "consolidated_training" in rel_path or "training_pool" in rel_path):
                    priority = 80  # High priority for training databases
                elif "training/" in rel_path:
                    priority = 50
                elif rel_path.endswith(".db"):
                    priority = 30  # Medium priority for other databases
                else:
                    priority = 10

                if peer_id not in files_to_sync:
                    files_to_sync[peer_id] = []
                files_to_sync[peer_id].append((rel_path, priority, health))

                # Track hash for dedup recording after sync
                if file_hash:
                    file_hashes[rel_path] = file_hash

        if not files_to_sync:
            return

        # Select best peer using health score AND file count
        def peer_score(peer_id):
            files = files_to_sync[peer_id]
            health = self._get_peer_health_score(peer_id)
            file_score = sum(f[1] for f in files)  # Sum of priorities
            return health * 0.4 + file_score * 0.6

        best_peer = max(files_to_sync.keys(), key=peer_score)
        files_with_priority = files_to_sync[best_peer]

        # Sort by priority (highest first) and take top 10
        files_with_priority.sort(key=lambda x: x[1], reverse=True)
        files_to_request = [f[0] for f in files_with_priority[:10]]

        # Check if peer is alive
        with self.peers_lock:
            peer = self.peers.get(best_peer)

        if not peer or not peer.is_alive():
            return

        # Log and initiate sync
        total_missing = sum(len(f) for f in files_to_sync.values())
        model_files = sum(1 for f in files_to_request if "models/" in f or f.endswith(".pt"))
        logger.info(f"P2P SYNC: Missing {total_missing} files, requesting {len(files_to_request)} "
              f"({model_files} models) from {best_peer} (health={self._get_peer_health_score(best_peer):.0f})")

        try:
            import uuid
            job = DataSyncJob(
                job_id=f"p2p_{uuid.uuid4().hex[:8]}",
                source_node=best_peer,
                target_node=self.node_id,
                files=files_to_request,
            )

            self.sync_in_progress = True
            try:
                success = await self._request_node_sync(job)
                self._record_p2p_sync_result(best_peer, success)
                self._record_sync_result_for_adaptive("data", success)  # ADAPTIVE INTERVAL

                if success:
                    logger.info(f"P2P SYNC: Completed {len(files_to_request)} files from {best_peer}")
                    self._last_successful_p2p_sync = now
                    # Invalidate manifest cache
                    cache_path = self.sync_planner.get_manifest_cache_path()
                    if cache_path.exists():
                        cache_path.unlink()
                    # Update metrics
                    if hasattr(self, "_p2p_sync_metrics"):
                        self._p2p_sync_metrics["bytes"] += job.bytes_transferred
                    # DATA DEDUPLICATION: Record synced file hashes
                    for fpath in files_to_request:
                        if fpath in file_hashes:
                            self._record_synced_file(file_hashes[fpath], 0)
                else:
                    logger.info(f"P2P SYNC: Failed from {best_peer}: {job.error_message}")
            finally:
                self.sync_in_progress = False

        except Exception as e:  # noqa: BLE001
            logger.info(f"P2P SYNC: Error: {e}")
            self._record_sync_result_for_adaptive("data", False)  # ADAPTIVE: record failure
            self._record_p2p_sync_result(best_peer, False)
            self.sync_in_progress = False

    async def _p2p_model_sync(self):
        """DECENTRALIZED: Sync model files via P2P for faster model distribution.

        MODEL P2P SYNC: Ensures all nodes have access to latest trained models
        without relying on leader-coordinated sync. Prioritizes:
        - Newer models (by timestamp)
        - Models for active board configurations
        - NNUE models (smaller, faster to sync)
        - ADAPTIVE INTERVALS: faster during training, slower when idle
        """
        now = time.time()

        # ADAPTIVE INTERVAL: Uses activity-aware interval (faster during training)
        interval = self._get_adaptive_sync_interval("model")
        last_check = getattr(self, "_last_p2p_model_sync", 0)
        if now - last_check < interval:
            return
        self._last_p2p_model_sync = now

        # Skip if sync in progress
        if getattr(self, "sync_in_progress", False):
            return

        # Get model files from local manifest
        local_manifest = getattr(self, "local_data_manifest", None)
        if not local_manifest:
            return

        local_models = set()
        for file_info in getattr(local_manifest, "files", []) or []:
            rel_path = getattr(file_info, "relative_path", "")
            if rel_path and ("models/" in rel_path or rel_path.endswith((".pt", ".onnx", ".bin"))):
                local_models.add(rel_path)

        # Check peer manifests for models we're missing
        peer_manifests = getattr(self, "_gossip_peer_manifests", {})
        missing_models: dict[str, list[str]] = {}

        for peer_id, peer_manifest in peer_manifests.items():
            if peer_id == self.node_id:
                continue

            health = self._get_peer_health_score(peer_id)
            if health <= 0:
                continue

            peer_files = getattr(peer_manifest, "files", []) or []
            for file_info in peer_files:
                rel_path = getattr(file_info, "relative_path", "")
                if not rel_path:
                    continue
                if not ("models/" in rel_path or rel_path.endswith((".pt", ".onnx", ".bin"))):
                    continue
                if rel_path in local_models:
                    continue

                if peer_id not in missing_models:
                    missing_models[peer_id] = []
                missing_models[peer_id].append(rel_path)

        if not missing_models:
            return

        # Pick healthiest peer with models
        best_peer = max(missing_models.keys(), key=lambda p: self._get_peer_health_score(p))
        models_to_sync = missing_models[best_peer][:5]  # Max 5 models per cycle

        with self.peers_lock:
            peer = self.peers.get(best_peer)
        if not peer or not peer.is_alive():
            return

        logger.info(f"MODEL SYNC: Requesting {len(models_to_sync)} models from {best_peer}")

        try:
            import uuid
            job = DataSyncJob(
                job_id=f"model_{uuid.uuid4().hex[:8]}",
                source_node=best_peer,
                target_node=self.node_id,
                files=models_to_sync,
            )

            self.sync_in_progress = True
            try:
                success = await self._request_node_sync(job)
                self._record_p2p_sync_result(best_peer, success)
                self._record_sync_result_for_adaptive("model", success)  # ADAPTIVE INTERVAL
                if success:
                    logger.info(f"MODEL SYNC: Got {len(models_to_sync)} models from {best_peer}")
            finally:
                self.sync_in_progress = False
        except Exception as e:  # noqa: BLE001
            logger.info(f"MODEL SYNC: Error: {e}")
            self._record_sync_result_for_adaptive("model", False)  # ADAPTIVE: record failure
            self.sync_in_progress = False

    async def _p2p_training_db_sync(self):
        """DECENTRALIZED: Sync training databases via P2P for improved training diversity.

        TRAINING DB P2P SYNC: Ensures all nodes have access to consolidated training
        data without relying on leader-coordinated sync. Prioritizes:
        - canonical_*.db (canonical training data)
        - consolidated_training*.db (merged training data)
        - training_pool*.db (training pool databases)

        This improves training diversity by ensuring all nodes can train on
        cluster-wide data, not just their local selfplay games.

        ADAPTIVE INTERVALS: faster during training, slower when idle.
        """
        now = time.time()

        # ADAPTIVE INTERVAL: Uses activity-aware interval (faster during training)
        interval = self._get_adaptive_sync_interval("training_db")
        last_check = getattr(self, "_last_p2p_training_db_sync", 0)
        if now - last_check < interval:
            return
        self._last_p2p_training_db_sync = now

        # Skip if sync is in progress
        if getattr(self, "sync_in_progress", False):
            return

        # Skip if under disk pressure
        if getattr(self.self_info, "disk_percent", 0) > 80:
            return

        # Get our local files
        local_manifest = getattr(self, "local_data_manifest", None)
        if not local_manifest:
            return

        local_dbs = set()
        local_db_sizes = {}
        for file_info in getattr(local_manifest, "files", []) or []:
            rel_path = getattr(file_info, "relative_path", "")
            if rel_path.endswith(".db"):
                local_dbs.add(rel_path)
                local_db_sizes[rel_path] = getattr(file_info, "size_bytes", 0)

        # Check peer manifests for training databases
        peer_manifests = getattr(self, "_gossip_peer_manifests", {})
        if not peer_manifests:
            return

        # Find training databases we're missing or have smaller versions of
        missing_dbs: dict[str, list[tuple]] = {}  # peer_id -> [(db_path, size)]

        for peer_id, peer_manifest in peer_manifests.items():
            if peer_id == self.node_id:
                continue

            # Check circuit breaker
            health = self._get_peer_health_score(peer_id)
            if health <= 0:
                continue

            peer_files = getattr(peer_manifest, "files", []) or []
            for file_info in peer_files:
                rel_path = getattr(file_info, "relative_path", "")
                size = getattr(file_info, "size_bytes", 0)

                # Only sync training-related databases and ELO database
                if not rel_path.endswith(".db"):
                    continue
                if not ("canonical_" in rel_path or "consolidated_training" in rel_path or
                        "training_pool" in rel_path or "unified_elo" in rel_path or
                        "elo_ratings" in rel_path):
                    continue

                # Skip empty databases
                if size < 1024:
                    continue

                # Check if we don't have it or have a smaller version
                local_size = local_db_sizes.get(rel_path, 0)
                if local_size >= size:
                    continue

                if peer_id not in missing_dbs:
                    missing_dbs[peer_id] = []
                missing_dbs[peer_id].append((rel_path, size, health))

        if not missing_dbs:
            return

        # Pick healthiest peer with training DBs
        best_peer = max(missing_dbs.keys(), key=lambda p: self._get_peer_health_score(p))
        dbs_to_sync = [db[0] for db in missing_dbs[best_peer][:3]]  # Max 3 DBs per cycle

        # Check if peer is alive
        with self.peers_lock:
            peer = self.peers.get(best_peer)
        if not peer or not peer.is_alive():
            return

        logger.info(f"TRAINING DB SYNC: Requesting {len(dbs_to_sync)} training DBs from {best_peer}")

        try:
            import uuid
            job = DataSyncJob(
                job_id=f"traindb_{uuid.uuid4().hex[:8]}",
                source_node=best_peer,
                target_node=self.node_id,
                files=dbs_to_sync,
            )

            self.sync_in_progress = True
            try:
                success = await self._request_node_sync(job)
                self._record_p2p_sync_result(best_peer, success)
                self._record_sync_result_for_adaptive("training_db", success)  # ADAPTIVE INTERVAL
                if success:
                    logger.info(f"TRAINING DB SYNC: Got {len(dbs_to_sync)} training DBs from {best_peer}")
            finally:
                self.sync_in_progress = False
        except Exception as e:  # noqa: BLE001
            logger.info(f"TRAINING DB SYNC: Error: {e}")
            self._record_sync_result_for_adaptive("training_db", False)  # ADAPTIVE: record failure
            self.sync_in_progress = False

    async def _gossip_state_to_peers(self):
        """DECENTRALIZED: Share node state with random peers using gossip protocol.

        GOSSIP PROTOCOL: Instead of relying solely on leader to collect state,
        nodes share information with neighbors, and it propagates through the cluster.

        Benefits:
        - Faster state propagation (O(log N) instead of O(N))
        - Works without a leader
        - Resilient to network partitions (state eventually converges)
        - Reduces load on leader

        Implementation:
        1. Each node maintains local state (jobs, resources, health)
        2. Periodically send state to K random peers (fanout)
        3. Receive state from peers and update local view
        4. Include version/timestamp to handle conflicts (last-write-wins)
        """
        now = time.time()

        # Rate limit: gossip every 30 seconds
        last_gossip = getattr(self, "_last_gossip_time", 0)
        if now - last_gossip < 30:
            return
        self._last_gossip_time = now

        # Prepare our state to share
        self._update_self_info()
        local_state = {
            "node_id": self.node_id,
            "timestamp": now,
            "version": int(now * 1000),  # Millisecond version for conflict resolution
            "role": self.role.value if hasattr(self.role, "value") else str(self.role),
            "leader_id": self.leader_id,
            "leader_lease_expires": getattr(self, "leader_lease_expires", 0),
            "selfplay_jobs": getattr(self.self_info, "selfplay_jobs", 0),
            "training_jobs": getattr(self.self_info, "training_jobs", 0),
            "gpu_percent": getattr(self.self_info, "gpu_percent", 0),
            "cpu_percent": getattr(self.self_info, "cpu_percent", 0),
            "memory_percent": getattr(self.self_info, "memory_percent", 0),
            "disk_percent": getattr(self.self_info, "disk_percent", 0),
            "has_gpu": getattr(self.self_info, "has_gpu", False),
            "gpu_name": getattr(self.self_info, "gpu_name", ""),
            "voter_quorum_ok": self._has_voter_quorum(),
        }

        # DISTRIBUTED TRAINING COORDINATION: Include active training configs
        # This allows nodes to coordinate training without a leader
        local_state["active_training_configs"] = self._get_local_active_training_configs()

        # DISTRIBUTED ELO: Include ELO summary for cluster-wide visibility
        local_state["elo_summary"] = self._get_local_elo_summary()

        # GOSSIP-BASED LEADER HINTS: Share leader preference for faster elections
        local_state["leader_hint"] = self._get_leader_hint()

        # PEER REPUTATION: Share peer reliability scores
        local_state["peer_reputation"] = self._get_peer_reputation_summary()

        # DISTRIBUTED TOURNAMENT: Share tournament proposals and active tournaments
        local_state["tournament"] = self._get_tournament_gossip_state()

        # Include manifest summary if available
        local_manifest = getattr(self, "local_data_manifest", None)
        if local_manifest:
            local_state["manifest_summary"] = {
                "total_files": getattr(local_manifest, "total_files", 0),
                "selfplay_games": getattr(local_manifest, "selfplay_games", 0),
                "collected_at": getattr(local_manifest, "collected_at", 0),
            }

        # Jan 11, 2026: Phase 6 - Dynamic gossip fanout based on peer failures
        # When detecting many peer failures, increase fanout for faster convergence
        with self.peers_lock:
            all_peers = list(self.peers.values())
            alive_peers = [
                p for p in all_peers
                if p.is_alive() and not getattr(p, "retired", False)
            ]

        if not alive_peers:
            return

        # Count recent failures (peers that became dead within last 2 minutes)
        now = time.time()
        recent_failures = sum(
            1 for p in all_peers
            if not p.is_alive() and not getattr(p, "retired", False)
            and now - p.last_heartbeat < 120
        )

        # Dynamic fanout: base 5, increase when failures detected
        base_fanout = 5
        if recent_failures > 5:
            # Emergency mode: 5 + 5 = 10 fanout (limited to 15)
            gossip_fanout = min(15, base_fanout + 5)
        elif recent_failures > 2:
            # Elevated mode: 5 + 3 = 8 fanout (limited to 12)
            gossip_fanout = min(12, base_fanout + 3)
        else:
            gossip_fanout = base_fanout

        import random
        peers_to_gossip = random.sample(alive_peers, min(gossip_fanout, len(alive_peers)))

        # Send gossip to selected peers
        timeout = ClientTimeout(total=5)
        async with get_client_session(timeout) as session:
            for peer in peers_to_gossip:
                try:
                    # Include known state about other nodes (propagation)
                    gossip_payload = {
                        "sender": self.node_id,
                        "sender_state": local_state,
                        "known_states": self._get_gossip_known_states(),
                        # Phase 28: Peer-of-peer discovery - share peer endpoints
                        "peer_endpoints": self._get_peer_endpoints_for_gossip(),
                        # Phase 29: Cluster epoch for split-brain resolution
                        "cluster_epoch": self._cluster_epoch,
                    }

                    # GOSSIP COMPRESSION: Compress payload with gzip to reduce network transfer
                    json_bytes = json.dumps(gossip_payload).encode("utf-8")
                    original_size = len(json_bytes)
                    compressed_bytes = gzip.compress(json_bytes, compresslevel=6)
                    compressed_size = len(compressed_bytes)

                    # Track compression metrics
                    self._record_gossip_compression(original_size, compressed_size)

                    start_time = time.time()
                    for url in self._urls_for_peer(peer, "/gossip"):
                        try:
                            headers = self._auth_headers()
                            headers["Content-Encoding"] = "gzip"
                            headers["Content-Type"] = "application/json"
                            async with session.post(url, data=compressed_bytes, headers=headers) as resp:
                                if resp.status == 200:
                                    # Process response (peer shares their state back)
                                    # Check if response is compressed
                                    content_encoding = resp.headers.get("Content-Encoding", "")
                                    if content_encoding == "gzip":
                                        response_bytes = await resp.read()
                                        try:
                                            decompressed = gzip.decompress(response_bytes)
                                            payload_bytes = decompressed
                                        except OSError as e:
                                            logger.debug(
                                                f"[Gossip] Gzip decode failed from {peer.node_id}: {e}. "
                                                "Falling back to plain JSON."
                                            )
                                            payload_bytes = response_bytes
                                        response_data = json.loads(payload_bytes.decode("utf-8"))
                                    else:
                                        response_data = await resp.json()
                                    self._process_gossip_response(response_data)
                                    # Record metrics
                                    latency_ms = (time.time() - start_time) * 1000
                                    self._record_gossip_metrics("sent", peer.node_id)
                                    self._record_gossip_metrics("latency", peer.node_id, latency_ms)
                                    break
                        except (aiohttp.ClientError, asyncio.TimeoutError, json.JSONDecodeError, AttributeError):
                            continue
                except (aiohttp.ClientError, asyncio.TimeoutError, json.JSONDecodeError, AttributeError):
                    pass

    def _get_gossip_known_states(self) -> dict[str, dict]:
        """Get known states about other nodes to propagate via gossip."""
        known = {}
        gossip_states = getattr(self, "_gossip_peer_states", {})
        # Only share recent states (last 5 minutes)
        cutoff = time.time() - 300
        for node_id, state in gossip_states.items():
            if state.get("timestamp", 0) > cutoff:
                known[node_id] = state
        return known

    def _get_peer_endpoints_for_gossip(self) -> list[dict[str, Any]]:
        """Phase 28: Get peer endpoints to share via gossip for peer-of-peer discovery.

        Returns a list of alive peer endpoints with connection info.
        This enables nodes to discover peers they can't reach directly.
        """
        endpoints = []
        with self.peers_lock:
            # Get alive, non-retired peers
            alive_peers = [
                p for p in self.peers.values()
                if p.node_id != self.node_id and p.is_alive() and not getattr(p, "retired", False)
            ]

        # Limit to top N peers to avoid payload bloat
        for peer in alive_peers[:GOSSIP_MAX_PEER_ENDPOINTS]:
            endpoint = {
                "node_id": peer.node_id,
                "host": str(getattr(peer, "host", "") or ""),
                "port": int(getattr(peer, "port", DEFAULT_PORT) or DEFAULT_PORT),
                "tailscale_ip": str(getattr(peer, "tailscale_ip", "") or ""),
                "is_alive": True,
                "last_heartbeat": float(getattr(peer, "last_heartbeat", 0) or 0),
            }
            endpoints.append(endpoint)

        return endpoints

    def _process_gossip_response(self, response: dict):
        """Process gossip response from a peer, updating our view of the cluster."""
        if not response:
            return

        # Initialize gossip state storage if needed
        if not hasattr(self, "_gossip_peer_states"):
            self._gossip_peer_states = {}
        if not hasattr(self, "_gossip_peer_manifests"):
            self._gossip_peer_manifests = {}

        # Process sender's state
        sender_state = response.get("sender_state", {})
        if sender_state:
            sender_id = sender_state.get("node_id")
            if sender_id and sender_id != self.node_id:
                existing = self._gossip_peer_states.get(sender_id, {})
                # Last-write-wins conflict resolution
                if sender_state.get("version", 0) > existing.get("version", 0):
                    self._gossip_peer_states[sender_id] = sender_state

                    # Update leader info if sender claims to know a leader
                    # Dec 31, 2025: Fixed gossip race condition that kept resetting stale leaders
                    # Three guards prevent accepting stale/unreachable leaders:
                    # 1. Check leader invalidation window (set when we clear a dead leader)
                    # 2. Check if we're mid-election (election_in_progress)
                    # 3. Verify the claimed leader is in our peers list and is alive
                    if sender_state.get("leader_id") and not self.leader_id:
                        claimed_leader = sender_state.get("leader_id")
                        lease_expires = sender_state.get("leader_lease_expires", 0)
                        now = time.time()
                        # Guard 1: Don't accept leader during invalidation window
                        if now < getattr(self, "_leader_invalidation_until", 0):
                            logger.debug(
                                f"Gossip: Ignoring leader claim {claimed_leader} during invalidation window "
                                f"({self._leader_invalidation_until - now:.1f}s remaining)"
                            )
                        # Guard 2: Don't accept leader during election
                        elif getattr(self, "election_in_progress", False):
                            logger.debug(f"Gossip: Ignoring leader claim {claimed_leader} during election")
                        # Guard 3: Verify lease and leader aliveness
                        elif lease_expires > now:
                            # Verify claimed leader is a known, alive peer
                            with contextlib.suppress(Exception):
                                leader_peer = self.peers.get(claimed_leader)
                                if leader_peer and leader_peer.is_alive():
                                    # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                                    self._set_leader(claimed_leader, reason="gossip_accept_leader", save_state=False)
                                    self.last_leader_seen = now
                                    logger.debug(f"Gossip: Accepted leader {claimed_leader} from {sender_id}")
                                elif not leader_peer:
                                    logger.debug(f"Gossip: Ignoring leader claim {claimed_leader} - not in peers")
                                else:
                                    logger.debug(f"Gossip: Ignoring leader claim {claimed_leader} - peer not alive")

        # Process known states (propagation)
        known_states = response.get("known_states", {})
        for node_id, state in known_states.items():
            if node_id == self.node_id:
                continue
            existing = self._gossip_peer_states.get(node_id, {})
            if state.get("version", 0) > existing.get("version", 0):
                self._gossip_peer_states[node_id] = state

        # Process manifest info for P2P sync
        peer_manifests = response.get("peer_manifests", {})
        for node_id, manifest_data in peer_manifests.items():
            if node_id != self.node_id:
                with contextlib.suppress(Exception):
                    self._gossip_peer_manifests[node_id] = NodeDataManifest.from_dict(manifest_data)

        # Process tournament gossip for distributed scheduling
        for node_id, state in known_states.items():
            if node_id == self.node_id:
                continue
            tournament_state = state.get("tournament")
            if tournament_state:
                with contextlib.suppress(Exception):
                    self._process_tournament_gossip(node_id, tournament_state)

        # Check for tournament consensus after processing gossip
        with contextlib.suppress(Exception):
            self._check_tournament_consensus()

        # Phase 28: Process peer endpoints for peer-of-peer discovery
        peer_endpoints = response.get("peer_endpoints") or []
        if peer_endpoints:
            self._process_gossip_peer_endpoints(peer_endpoints)

        # Phase 29: Process cluster epoch for split-brain resolution
        incoming_epoch = response.get("cluster_epoch")
        if incoming_epoch is not None:
            self._handle_incoming_cluster_epoch(incoming_epoch, response)

    def _process_gossip_peer_endpoints(self, peer_endpoints: list[dict]) -> None:
        """Phase 28: Process peer endpoints learned via gossip.

        Enables discovery of peers we can't reach directly through intermediaries.
        """
        for endpoint in peer_endpoints:
            node_id = endpoint.get("node_id")
            if not node_id or node_id == self.node_id:
                continue

            # Store in gossip-learned endpoints for later connection attempts
            host = endpoint.get("tailscale_ip") or endpoint.get("host")
            port = endpoint.get("port", DEFAULT_PORT)

            if host and port:
                self._gossip_learned_endpoints[node_id] = {
                    "host": host,
                    "port": port,
                    "tailscale_ip": endpoint.get("tailscale_ip", ""),
                    "last_heartbeat": endpoint.get("last_heartbeat", 0),
                    "learned_at": time.time(),
                }

                # If this is an unknown peer, try to connect
                if node_id not in self.peers:
                    # Queue for async connection attempt
                    asyncio.create_task(self._try_connect_gossip_peer(node_id, host, port))

    async def _try_connect_gossip_peer(self, node_id: str, host: str, port: int) -> None:
        """Phase 28: Attempt to connect to a peer learned via gossip."""
        try:
            # Check if already connected
            if node_id in self.peers and self.peers[node_id].is_alive():
                return

            # Jan 20, 2026: Use adaptive cooldown with probe-based recovery.
            # Replaces static 1-hour cooldown that was causing 25-40% node loss.
            if self._cooldown_manager:
                if self._cooldown_manager.is_in_cooldown(node_id):
                    # Attempt probe-based early recovery
                    recovered = await self._cooldown_manager.probe_and_recover(
                        node_id, host, port
                    )
                    if recovered:
                        logger.info(f"Peer {node_id} recovered via probe during cooldown")
                    else:
                        tier = self._cooldown_manager.get_cooldown_tier(node_id)
                        remaining = self._cooldown_manager.get_remaining_cooldown(node_id)
                        logger.debug(
                            f"Skipping gossip-learned peer {node_id} - "
                            f"tier {tier} cooldown, {remaining:.0f}s remaining"
                        )
                        return
            else:
                # Fallback to static cooldown if manager not available
                DEAD_PEER_COOLDOWN_SECONDS = 3600  # 1 hour
                if node_id in self._dead_peer_timestamps:
                    dead_time = self._dead_peer_timestamps[node_id]
                    if time.time() - dead_time < DEAD_PEER_COOLDOWN_SECONDS:
                        logger.debug(
                            f"Skipping gossip-learned peer {node_id} - died {int(time.time() - dead_time)}s ago"
                        )
                        return
                    else:
                        # Cooldown expired, allow reconnection attempt
                        del self._dead_peer_timestamps[node_id]

            logger.info(f"Attempting connection to gossip-learned peer: {node_id} at {host}:{port}")

            # Try to send heartbeat
            info = await self._send_heartbeat_to_peer(host, port)
            if info:
                with self.peers_lock:
                    self.peers[info.node_id] = info
                logger.info(f"Successfully connected to gossip-learned peer: {info.node_id}")

                # Save to cache for future restarts
                self._save_peer_to_cache(
                    info.node_id, host, port,
                    str(getattr(info, "tailscale_ip", "") or "")
                )
        except Exception as e:  # noqa: BLE001
            if self.verbose:
                logger.debug(f"Failed to connect to gossip-learned peer {node_id}: {e}")

    async def _tcp_probe_peer(self, node_id: str, host: str, port: int) -> bool:
        """Probe a peer via HTTP /status to check if it's reachable.

        This is used by DeadPeerCooldownManager for probe-based early recovery.
        A successful probe indicates the node is back online and can be reconnected.

        Args:
            node_id: The node identifier (for logging)
            host: The host to probe
            port: The port to probe

        Returns:
            True if the peer responded to /status, False otherwise
        """
        try:
            url = f"http://{host}:{port}/status"
            timeout = ClientTimeout(total=5.0)
            async with get_client_session(timeout) as session:
                async with session.get(url, headers=self._auth_headers()) as resp:
                    if resp.status == 200:
                        logger.debug(f"TCP probe success for {node_id} at {host}:{port}")
                        return True
        except (aiohttp.ClientError, asyncio.TimeoutError, AttributeError) as e:
            logger.debug(f"TCP probe failed for {node_id}: {type(e).__name__}")
        return False

    def _wire_cooldown_manager_probe(self) -> None:
        """Wire the TCP probe function to the cooldown manager.

        Called during startup to enable probe-based early recovery.
        """
        if self._cooldown_manager:
            self._cooldown_manager.set_probe_func(self._tcp_probe_peer)
            logger.info("Cooldown manager probe function wired")

    def _wire_connection_pool_dynamic_sizing(self) -> None:
        """Wire cluster size callback to the connection pool.

        January 20, 2026: Enables dynamic pool scaling based on cluster size.
        Fixes connection exhaustion with 40+ node clusters.
        """
        try:
            from scripts.p2p.connection_pool import get_connection_pool
            pool = get_connection_pool()
            pool.set_cluster_size_callback(self._get_cluster_size_for_pool)
            logger.info("Connection pool dynamic sizing wired")
        except ImportError as e:
            logger.debug(f"Connection pool not available for dynamic sizing: {e}")
        except Exception as e:  # noqa: BLE001
            logger.warning(f"Failed to wire connection pool sizing: {e}")

    def _get_cluster_size_for_pool(self) -> int:
        """Get current cluster size for connection pool dynamic sizing.

        Returns the number of alive peers plus self.
        """
        try:
            # Use peer snapshot for lock-free access
            alive_count = sum(
                1 for p in self._peer_snapshot.get_snapshot().values()
                if p.is_alive() and not getattr(p, "retired", False)
            )
            return alive_count + 1  # +1 for self
        except Exception:  # noqa: BLE001
            return 40  # Default to reasonable cluster size

    def _is_peer_alive_for_circuit_breaker(self, host: str) -> bool:
        """Check if a peer is alive based on gossip/P2P membership.

        January 20, 2026: Used by CircuitBreakerDecayLoop for external alive
        verification. Enables immediate circuit recovery when gossip reports
        a node is alive, instead of waiting for TTL expiry.

        Args:
            host: Host identifier (node_id or Tailscale IP)

        Returns:
            True if the host is known to be alive from P2P membership
        """
        try:
            # Check by node_id first
            for peer in self._peer_snapshot.get_snapshot().values():
                if peer.node_id == host:
                    return peer.is_alive() and not getattr(peer, "retired", False)

                # Also check by Tailscale IP
                tailscale_ip = getattr(peer, "tailscale_ip", "")
                if tailscale_ip and host == tailscale_ip:
                    return peer.is_alive() and not getattr(peer, "retired", False)

                # Check by regular host IP
                peer_host = getattr(peer, "host", "")
                if peer_host and host == peer_host:
                    return peer.is_alive() and not getattr(peer, "retired", False)

            # Host not found in peers
            return False
        except Exception:  # noqa: BLE001
            return False

    def _handle_incoming_cluster_epoch(self, incoming_epoch: Any, response: dict) -> None:
        """Phase 29: Handle incoming cluster epoch for split-brain resolution."""
        try:
            epoch = int(incoming_epoch)
        except (ValueError, TypeError):
            return

        if epoch > self._cluster_epoch:
            # Accept higher epoch - this cluster partition is more authoritative
            logger.info(f"Adopting higher cluster epoch: {epoch} (was {self._cluster_epoch})")
            self._cluster_epoch = epoch
            self._save_cluster_epoch()

            # If response includes a leader, adopt it
            sender_state = response.get("sender_state", {})
            incoming_leader = sender_state.get("leader_id")
            if incoming_leader and incoming_leader != self.node_id:
                if self.role == NodeRole.LEADER:
                    logger.info(f"Stepping down: higher epoch cluster has leader {incoming_leader}")
                # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                self._set_leader(incoming_leader, reason="higher_epoch_leader", save_state=False)

    # Gossip metrics methods (provided by GossipProtocolMixin after Dec 28, 2025 merge):
    # _record_gossip_metrics, _record_gossip_compression, _reset_gossip_metrics_hourly
    # _get_gossip_metrics_summary, _get_gossip_health_status

    async def _gossip_anti_entropy_repair(self):
        """DECENTRALIZED: Periodic full state reconciliation with random peer.

        ANTI-ENTROPY REPAIR: Gossip protocols can miss updates due to:
        - Network partitions
        - Message loss
        - Node restarts

        Solution: Periodically do full state exchange with a random peer to
        ensure eventual consistency. This catches any missed updates.

        Frequency: Every 5 minutes with a random healthy peer
        """
        now = time.time()

        # Rate limit: anti-entropy every 5 minutes
        last_repair = getattr(self, "_last_anti_entropy_repair", 0)
        if now - last_repair < 300:
            return
        self._last_anti_entropy_repair = now

        # Select a random healthy peer for full state exchange
        with self.peers_lock:
            alive_peers = [
                p for p in self.peers.values()
                if p.is_alive() and not getattr(p, "retired", False)
            ]

        if not alive_peers:
            return

        import random
        peer = random.choice(alive_peers)

        # Prepare full state dump (not just recent states)
        full_state = {
            "anti_entropy": True,  # Flag for full state exchange
            "sender": self.node_id,
            "timestamp": now,
            "all_known_states": {},
        }

        # Include all known peer states (not just recent)
        gossip_states = getattr(self, "_gossip_peer_states", {})
        for node_id, state in gossip_states.items():
            full_state["all_known_states"][node_id] = state

        # Include our own state
        self._update_self_info()
        full_state["all_known_states"][self.node_id] = {
            "node_id": self.node_id,
            "timestamp": now,
            "version": int(now * 1000),
            "role": self.role.value if hasattr(self.role, "value") else str(self.role),
            "leader_id": self.leader_id,
            "selfplay_jobs": getattr(self.self_info, "selfplay_jobs", 0),
            "training_jobs": getattr(self.self_info, "training_jobs", 0),
        }

        # Send anti-entropy request
        start_time = time.time()
        timeout = ClientTimeout(total=10)  # Longer timeout for full exchange
        try:
            async with get_client_session(timeout) as session:
                for url in self._urls_for_peer(peer, "/gossip/anti-entropy"):
                    try:
                        async with session.post(url, json=full_state, headers=self._auth_headers()) as resp:
                            if resp.status == 200:
                                response_data = await resp.json()
                                latency = (time.time() - start_time) * 1000
                                self._record_gossip_metrics("latency", peer.node_id, latency)

                                # Process peer's full state
                                peer_states = response_data.get("all_known_states", {})
                                updates = 0
                                for node_id, state in peer_states.items():
                                    if node_id == self.node_id:
                                        continue
                                    existing = self._gossip_peer_states.get(node_id, {})
                                    if state.get("version", 0) > existing.get("version", 0):
                                        self._gossip_peer_states[node_id] = state
                                        updates += 1
                                        self._record_gossip_metrics("update", node_id)

                                # Check for stale states we have that peer doesn't know
                                our_nodes = set(self._gossip_peer_states.keys())
                                peer_nodes = set(peer_states.keys())
                                stale_candidates = our_nodes - peer_nodes - {self.node_id}

                                for stale_node in stale_candidates:
                                    stale_state = self._gossip_peer_states.get(stale_node, {})
                                    # If state is older than 10 minutes and peer doesn't know it,
                                    # the node might be offline - mark as stale
                                    if stale_state.get("timestamp", 0) < now - 600:
                                        self._record_gossip_metrics("stale", stale_node)

                                if updates > 0:
                                    self._record_gossip_metrics("anti_entropy")
                                    logger.debug(f"[GOSSIP] Anti-entropy repair: {updates} state updates from {peer.node_id}")

                                return
                    except (aiohttp.ClientError, asyncio.TimeoutError, json.JSONDecodeError, KeyError, ValueError):
                        continue
        except (AttributeError, KeyError, ValueError, TypeError):
            pass  # Silent failure, will retry next cycle

    # =========================================================================
    # DISTRIBUTED TRAINING COORDINATION
    # =========================================================================
    # These functions enable nodes to coordinate training decisions without
    # relying on a leader, using gossip to share training state cluster-wide.
    # =========================================================================

    def _get_local_active_training_configs(self) -> list[dict]:
        """Get list of training configs currently running on this node.

        DISTRIBUTED TRAINING: Share what training this node is doing so other
        nodes can avoid duplicate training for the same configuration.

        Returns list of dicts with:
        - config_key: e.g. "square8_2p"
        - job_type: "nnue", "cmaes", etc.
        - started_at: timestamp when training started
        """
        active_configs = []
        with self.jobs_lock:
            for _job_id, job in self.local_jobs.items():
                job_type = getattr(job, "job_type", "")
                # Only include training-type jobs
                if job_type in ("nnue", "nnue_training", "training", "cmaes"):
                    board_type = getattr(job, "board_type", "")
                    num_players = getattr(job, "num_players", 2)
                    if board_type:
                        config_key = f"{board_type}_{num_players}p"
                        started_at = getattr(job, "started_at", time.time())
                        active_configs.append({
                            "config_key": config_key,
                            "job_type": job_type,
                            "started_at": started_at,
                        })
        return active_configs

    def _get_cluster_active_training_configs(self) -> dict[str, list[str]]:
        """Get all active training configs across the cluster via gossip.

        DISTRIBUTED TRAINING COORDINATION: Query gossip state to see what
        training is running cluster-wide. This enables nodes to avoid
        duplicate training without leader coordination.

        Returns: { config_key -> [list of node_ids training that config] }
        """
        cluster_configs: dict[str, list[str]] = {}

        # Include our own training
        for config in self._get_local_active_training_configs():
            config_key = config["config_key"]
            if config_key not in cluster_configs:
                cluster_configs[config_key] = []
            cluster_configs[config_key].append(self.node_id)

        # Include training from gossip state
        gossip_states = getattr(self, "_gossip_peer_states", {})
        now = time.time()
        for node_id, state in gossip_states.items():
            # Skip stale states (older than 2 minutes)
            if state.get("timestamp", 0) < now - 120:
                continue
            # Skip our own state
            if node_id == self.node_id:
                continue

            active_training = state.get("active_training_configs", [])
            for config in active_training:
                config_key = config.get("config_key", "")
                if config_key:
                    if config_key not in cluster_configs:
                        cluster_configs[config_key] = []
                    if node_id not in cluster_configs[config_key]:
                        cluster_configs[config_key].append(node_id)

        return cluster_configs

    def _is_config_being_trained_cluster_wide(self, config_key: str) -> tuple[bool, list[str]]:
        """Check if a config is already being trained somewhere in the cluster.

        DISTRIBUTED TRAINING: Before starting training for a config, check if
        another node is already training it. This avoids wasted resources.

        Returns: (is_being_trained, list_of_nodes_training_it)
        """
        cluster_configs = self._get_cluster_active_training_configs()
        training_nodes = cluster_configs.get(config_key, [])
        return (len(training_nodes) > 0, training_nodes)

    def _should_claim_training_slot(self, config_key: str) -> bool:
        """Decide if this node should claim a training slot for a config.

        DISTRIBUTED TRAINING COORDINATION: Use a deterministic algorithm to
        decide which node gets to train a config when multiple nodes want to.

        Algorithm:
        - If no one is training this config, the node with lowest ID claims it
        - If already training, don't start a duplicate
        - Include jitter to handle race conditions
        """
        is_training, _training_nodes = self._is_config_being_trained_cluster_wide(config_key)

        if is_training:
            # Config is already being trained somewhere
            return False

        # Get all nodes that might want to train (GPU nodes with data)
        candidate_nodes = [self.node_id]
        gossip_states = getattr(self, "_gossip_peer_states", {})
        now = time.time()
        for node_id, state in gossip_states.items():
            if state.get("timestamp", 0) < now - 120:
                continue
            if state.get("has_gpu", False):
                training_jobs = state.get("training_jobs", 0)
                # Only consider nodes with capacity (< 3 training jobs)
                if training_jobs < 3:
                    candidate_nodes.append(node_id)

        # Sort deterministically
        candidate_nodes = sorted(set(candidate_nodes))

        # The node with lowest ID that has capacity claims the slot
        # Add position-based jitter: higher position = less likely to claim
        import random
        my_position = candidate_nodes.index(self.node_id) if self.node_id in candidate_nodes else len(candidate_nodes)

        # First candidate always claims, others have decreasing probability
        claim_probability = max(0.1, 1.0 - (my_position * 0.3))

        return random.random() < claim_probability

    # =========================================================================
    # TRAINING TRIGGER IDEMPOTENCY (Phase 4 - Dec 2025)
    # =========================================================================
    # Hash-based deduplication to prevent duplicate training during leader
    # transitions. Each training trigger is hashed and stored; subsequent
    # triggers with the same hash within the TTL are rejected.
    # =========================================================================

    def _compute_training_trigger_hash(self, config_key: str, game_count: int) -> str:
        """Compute a hash for training trigger deduplication.

        IDEMPOTENCY: Hash is based on:
        - config_key (board_type + num_players)
        - game_count bucket (rounded to 1000 to allow minor variations)
        - time bucket (15-minute windows)

        This allows the same trigger to be rejected if attempted multiple times
        within a 15-minute window for the same approximate data state.
        """
        import hashlib

        # Round game count to nearest 1000 to tolerate minor variations
        game_bucket = (game_count // 1000) * 1000

        # Use 15-minute time buckets
        time_bucket = int(time.time() // 900) * 900

        hash_input = f"{config_key}:{game_bucket}:{time_bucket}"
        return hashlib.sha256(hash_input.encode()).hexdigest()[:16]

    def _is_training_trigger_duplicate(self, trigger_hash: str) -> bool:
        """Check if a training trigger is a duplicate.

        IDEMPOTENCY: Returns True if this trigger hash was seen recently.
        """
        if not hasattr(self, "_training_trigger_cache"):
            self._training_trigger_cache: dict[str, float] = {}

        now = time.time()
        ttl = 900  # 15-minute TTL for trigger cache

        # Cleanup old entries
        expired = [h for h, ts in self._training_trigger_cache.items() if now - ts > ttl]
        for h in expired:
            del self._training_trigger_cache[h]

        # Check if duplicate
        return trigger_hash in self._training_trigger_cache

    def _record_training_trigger(self, trigger_hash: str) -> None:
        """Record a training trigger for deduplication."""
        if not hasattr(self, "_training_trigger_cache"):
            self._training_trigger_cache = {}

        self._training_trigger_cache[trigger_hash] = time.time()

    def _check_training_idempotency(self, config_key: str, game_count: int) -> tuple[bool, str]:
        """Check if training can proceed (idempotency check).

        Returns:
            (can_proceed, trigger_hash) - can_proceed is False if duplicate
        """
        trigger_hash = self._compute_training_trigger_hash(config_key, game_count)

        if self._is_training_trigger_duplicate(trigger_hash):
            logger.info(f"IDEMPOTENT: Training trigger {trigger_hash[:8]} for {config_key} is duplicate, skipping")
            return False, trigger_hash

        return True, trigger_hash

    def _get_distributed_training_summary(self) -> dict:
        """Get summary of distributed training state for /status endpoint."""
        cluster_configs = self._get_cluster_active_training_configs()
        return {
            "active_configs": list(cluster_configs.keys()),
            "total_training_jobs": sum(len(nodes) for nodes in cluster_configs.values()),
            "configs_by_node_count": {k: len(v) for k, v in cluster_configs.items()},
        }

    # =========================================================================
    # DISTRIBUTED ELO
    # =========================================================================
    # Share ELO ratings via gossip for cluster-wide visibility without
    # requiring every node to query the ELO database directly.
    # =========================================================================

    def _get_local_elo_summary(self) -> dict:
        """Get summary of local ELO ratings for gossip propagation.

        DISTRIBUTED ELO: Share top models and their ratings via gossip so all
        nodes have visibility into model performance without querying the DB.

        LAZY LOADING: Defers ELO query until after startup (60s) to avoid
        slowing node initialization. Uses 10-minute cache to reduce DB load.

        Returns dict with:
        - top_models: List of top 5 models with ratings
        - total_models: Total number of rated models
        - last_update: Timestamp of last ELO update
        """
        now = time.time()
        cache_key = "_elo_summary_cache"
        cache_time_key = "_elo_summary_cache_time"
        startup_key = "_elo_startup_time"
        cached = getattr(self, cache_key, None)
        cached_time = getattr(self, cache_time_key, 0)

        # Track startup time for lazy loading
        if not hasattr(self, startup_key):
            setattr(self, startup_key, now)

        startup_time = getattr(self, startup_key, now)

        # LAZY LOADING: Don't query ELO during first 60s of startup
        if now - startup_time < 60:
            return {"top_models": [], "total_models": 0, "last_update": 0, "deferred": True}

        # Use 10-minute cache (was 5 min) to reduce DB load
        if cached and now - cached_time < 600:
            return cached

        summary = {
            "top_models": [],
            "total_models": 0,
            "last_update": 0,
        }

        try:
            from app.tournament import get_elo_database
            db = get_elo_database()

            # Get top 5 models by ELO (single optimized query)
            with db._get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT participant_id, rating, games_played, last_update,
                           (SELECT COUNT(*) FROM elo_ratings) as total,
                           (SELECT MAX(last_update) FROM elo_ratings) as max_updated
                    FROM elo_ratings
                    ORDER BY rating DESC
                    LIMIT 5
                """)
                rows = cursor.fetchall()

                if rows:
                    summary["total_models"] = rows[0][4] if rows[0][4] else 0
                    summary["last_update"] = rows[0][5] if rows[0][5] else 0

                for row in rows:
                    summary["top_models"].append({
                        "model": row[0],
                        "elo": round(row[1]),
                        "games": row[2],
                    })

        except (KeyError, IndexError, AttributeError, ImportError, sqlite3.OperationalError):
            # Silently fail - ELO summary is optional
            # Dec 2025: Added sqlite3.OperationalError to handle corrupted databases gracefully
            pass

        # Cache the result
        setattr(self, cache_key, summary)
        setattr(self, cache_time_key, now)

        return summary

    def _get_cluster_elo_summary(self) -> dict:
        """Get cluster-wide ELO summary from gossip state.

        DISTRIBUTED ELO: Aggregate ELO info from all nodes via gossip to get
        a cluster-wide view of model performance.
        """
        all_models = {}
        gossip_states = getattr(self, "_gossip_peer_states", {})
        now = time.time()

        # Include our own ELO summary
        local_summary = self._get_local_elo_summary()
        for model_info in local_summary.get("top_models", []):
            model_name = model_info.get("model", "")
            if model_name:
                all_models[model_name] = model_info

        # Include ELO summaries from gossip
        for _node_id, state in gossip_states.items():
            if state.get("timestamp", 0) < now - 300:  # Skip stale states
                continue

            elo_summary = state.get("elo_summary", {})
            for model_info in elo_summary.get("top_models", []):
                model_name = model_info.get("model", "")
                if model_name:
                    # Keep highest ELO seen for each model
                    existing = all_models.get(model_name, {})
                    if model_info.get("elo", 0) > existing.get("elo", 0):
                        all_models[model_name] = model_info

        # Sort by ELO and return top 10
        sorted_models = sorted(all_models.values(), key=lambda x: x.get("elo", 0), reverse=True)
        return {
            "top_models": sorted_models[:10],
            "total_unique_models": len(all_models),
        }

    def _load_curriculum_weights(self) -> dict[str, float]:
        """Load curriculum weights for selfplay prioritization."""
        if not HAS_CURRICULUM_WEIGHTS or load_curriculum_weights is None:
            return {}
        try:
            return load_curriculum_weights()
        except Exception as e:  # noqa: BLE001
            logger.debug(f"[P2P] Failed to load curriculum weights: {e}")
            return {}

    # =========================================================================
    # AUTOMATIC NODE RECOVERY
    # =========================================================================
    # Detect stuck/unhealthy nodes via gossip and trigger automatic recovery
    # (service restart) to maintain cluster health without manual intervention.
    # =========================================================================

    async def _check_node_recovery(self):
        """DECENTRALIZED: Detect and recover stuck nodes via gossip.

        AUTOMATIC NODE RECOVERY: Uses gossip to detect nodes that are:
        - Unresponsive (stale gossip timestamp)
        - Stuck (high failure count, no job progress)
        - Resource-exhausted (high disk/memory)

        Recovery actions:
        - SSH to node and restart the ringrift-p2p service
        - Only leader attempts recovery to avoid duplicate restarts
        - Rate limit recovery attempts (one per node per 10 minutes)
        """
        # Only leader performs recovery to avoid duplicate restarts
        if self.role != NodeRole.LEADER:
            return

        now = time.time()

        # Rate limit: check every 2 minutes
        last_check = getattr(self, "_last_node_recovery_check", 0)
        if now - last_check < 120:
            return
        self._last_node_recovery_check = now

        # Initialize recovery tracking
        if not hasattr(self, "_node_recovery_attempts"):
            self._node_recovery_attempts = {}  # node_id -> last_attempt_time
        if not hasattr(self, "_node_recovery_metrics"):
            self._node_recovery_metrics = {"attempts": 0, "successes": 0, "failures": 0}

        # Check each peer for health issues
        gossip_states = getattr(self, "_gossip_peer_states", {})
        nodes_to_recover = []

        with self.peers_lock:
            for node_id, peer in self.peers.items():
                if node_id == self.node_id:
                    continue

                # Skip recently recovered nodes (10 minute cooldown)
                last_attempt = self._node_recovery_attempts.get(node_id, 0)
                if now - last_attempt < 600:
                    continue

                # Check for unhealthy indicators
                needs_recovery = False
                reason = ""

                # 1. Peer not alive (no recent heartbeat)
                if not peer.is_alive():
                    needs_recovery = True
                    reason = "not responding to heartbeat"

                # 2. Stale gossip state (no updates in 5 minutes)
                elif node_id in gossip_states:
                    state = gossip_states[node_id]
                    state_age = now - state.get("timestamp", 0)
                    if state_age > 300:
                        needs_recovery = True
                        reason = f"stale gossip ({int(state_age)}s old)"

                # 3. High consecutive failures
                elif getattr(peer, "consecutive_failures", 0) >= 5:
                    needs_recovery = True
                    reason = f"high failure count ({peer.consecutive_failures})"

                # 4. Disk nearly full (>95%)
                elif getattr(peer, "disk_percent", 0) > 95:
                    needs_recovery = True
                    reason = f"disk full ({peer.disk_percent}%)"

                if needs_recovery:
                    nodes_to_recover.append((node_id, peer, reason))

        # Attempt recovery for identified nodes (max 2 per cycle)
        for node_id, peer, reason in nodes_to_recover[:2]:
            logger.info(f"NODE RECOVERY: Attempting to recover {node_id} ({reason})")
            self._node_recovery_attempts[node_id] = now
            self._node_recovery_metrics["attempts"] += 1

            # ALERTING: Notify on node recovery attempt
            asyncio.create_task(self.notifier.send(
                title="Node Recovery Initiated",
                message=f"Attempting to recover node {node_id}: {reason}",
                level="warning",
                fields={
                    "Node": node_id,
                    "Reason": reason,
                    "Host": getattr(peer, "host", "unknown"),
                },
                node_id=self.node_id,
            ))

            success = await self._attempt_node_recovery(node_id, peer)
            if success:
                self._node_recovery_metrics["successes"] += 1
                logger.info(f"NODE RECOVERY: Successfully restarted {node_id}")
                # ALERTING: Notify on successful recovery
                asyncio.create_task(self.notifier.send(
                    title="Node Recovery Success",
                    message=f"Successfully recovered node {node_id}",
                    level="info",
                    fields={"Node": node_id, "Reason": reason},
                    node_id=self.node_id,
                ))
            else:
                self._node_recovery_metrics["failures"] += 1
                logger.info(f"NODE RECOVERY: Failed to restart {node_id}")
                # ALERTING: Notify on failed recovery
                asyncio.create_task(self.notifier.send(
                    title="Node Recovery Failed",
                    message=f"Failed to recover node {node_id} ({reason})",
                    level="error",
                    fields={
                        "Node": node_id,
                        "Reason": reason,
                        "Action": "Manual intervention may be required",
                    },
                    node_id=self.node_id,
                ))

    async def _attempt_node_recovery(self, node_id: str, peer) -> bool:
        """Attempt to recover a node by restarting its service via SSH.

        Returns True if recovery command succeeded, False otherwise.
        """
        host = getattr(peer, "host", None)
        if not host:
            return False

        try:
            pass

            # Try to restart the service via SSH
            cmd = f"timeout 30 ssh -o ConnectTimeout=10 -o StrictHostKeyChecking=no {host} 'sudo systemctl restart ringrift-p2p'"
            proc = await asyncio.create_subprocess_shell(
                cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            _stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=45)

            if proc.returncode == 0:
                return True
            else:
                logger.info(f"NODE RECOVERY: SSH failed for {node_id}: {stderr.decode()[:100]}")
                return False

        except asyncio.TimeoutError:
            logger.info(f"NODE RECOVERY: SSH timeout for {node_id}")
            return False
        except Exception as e:  # noqa: BLE001
            logger.info(f"NODE RECOVERY: Error recovering {node_id}: {e}")
            return False

    def _get_node_recovery_metrics(self) -> dict:
        """Get node recovery metrics for /status endpoint."""
        metrics = getattr(self, "_node_recovery_metrics", {"attempts": 0, "successes": 0, "failures": 0})
        attempts = getattr(self, "_node_recovery_attempts", {})
        now = time.time()

        # Count nodes in recovery cooldown
        in_cooldown = sum(1 for t in attempts.values() if now - t < 600)

        return {
            "total_attempts": metrics.get("attempts", 0),
            "successes": metrics.get("successes", 0),
            "failures": metrics.get("failures", 0),
            "nodes_in_cooldown": in_cooldown,
        }

    # =========================================================================
    # GOSSIP-BASED LEADER HINTS
    # =========================================================================
    # Share leader preferences via gossip to enable faster leader elections.
    # When current leader fails, nodes can quickly converge on a new leader
    # based on hints from peers rather than running full election.
    # =========================================================================

    def _get_leader_hint(self) -> dict:
        """Get this node's leader hint for gossip propagation.

        LEADER HINTS: Share information about preferred leader candidates to
        enable faster convergence during elections. Hints include:
        - Current known leader and lease expiry
        - Preferred successor (highest-priority eligible node)
        - This node's priority rank
        """
        hint = {
            "current_leader": self.leader_id,
            "lease_expires": getattr(self, "leader_lease_expires", 0),
            "preferred_successor": None,
            "my_priority": 0,
        }

        # Calculate this node's priority (lower is better for Bully algorithm)
        # But we want to express it as a score (higher is better)
        with self.peers_lock:
            all_nodes = [self.node_id] + [p.node_id for p in self.peers.values() if p.is_alive()]

        all_nodes_sorted = sorted(all_nodes, reverse=True)  # Bully: higher ID wins
        if self.node_id in all_nodes_sorted:
            hint["my_priority"] = len(all_nodes_sorted) - all_nodes_sorted.index(self.node_id)

        # Find preferred successor (highest priority eligible node that's not current leader)
        voter_ids = list(getattr(self, "voter_node_ids", []) or [])
        for node_id in all_nodes_sorted:
            if node_id == self.leader_id:
                continue
            if voter_ids and node_id not in voter_ids:
                continue
            hint["preferred_successor"] = node_id
            break

        return hint

    def _get_cluster_leader_consensus(self) -> dict:
        """Get cluster consensus on leader from gossip hints.

        LEADER CONSENSUS: Aggregate leader hints from all nodes to determine
        if there's agreement on who the leader is/should be.
        """
        leader_votes = {}
        successor_votes = {}
        gossip_states = getattr(self, "_gossip_peer_states", {})
        now = time.time()

        # Count our vote
        our_hint = self._get_leader_hint()
        if our_hint["current_leader"]:
            leader_votes[our_hint["current_leader"]] = leader_votes.get(our_hint["current_leader"], 0) + 1
        if our_hint["preferred_successor"]:
            successor_votes[our_hint["preferred_successor"]] = successor_votes.get(our_hint["preferred_successor"], 0) + 1

        # Count votes from gossip
        for _node_id, state in gossip_states.items():
            if state.get("timestamp", 0) < now - 120:  # Skip stale states
                continue

            hint = state.get("leader_hint", {})
            leader = hint.get("current_leader")
            successor = hint.get("preferred_successor")

            if leader:
                leader_votes[leader] = leader_votes.get(leader, 0) + 1
            if successor:
                successor_votes[successor] = successor_votes.get(successor, 0) + 1

        # Find consensus leader and successor
        consensus_leader = max(leader_votes.items(), key=lambda x: x[1])[0] if leader_votes else None
        consensus_successor = max(successor_votes.items(), key=lambda x: x[1])[0] if successor_votes else None

        result = {
            "consensus_leader": consensus_leader,
            "leader_agreement": leader_votes.get(consensus_leader, 0) if consensus_leader else 0,
            "consensus_successor": consensus_successor,
            "successor_agreement": successor_votes.get(consensus_successor, 0) if consensus_successor else 0,
            "total_voters": len(gossip_states) + 1,
        }

        # ALERTING: Check for low leader consensus (only leader alerts, rate limited)
        if self.role == NodeRole.LEADER and result["total_voters"] >= 3:
            agreement_ratio = result["leader_agreement"] / result["total_voters"]
            last_low_consensus_alert = getattr(self, "_last_low_consensus_alert", 0)
            if agreement_ratio < 0.5 and now - last_low_consensus_alert > 3600:  # Alert once per hour max
                self._last_low_consensus_alert = now
                asyncio.create_task(self.notifier.send(
                    title="Low Leader Consensus",
                    message=f"Only {result['leader_agreement']}/{result['total_voters']} nodes agree on leader",
                    level="warning",
                    fields={
                        "Agreement": f"{agreement_ratio*100:.0f}%",
                        "Consensus Leader": str(consensus_leader),
                        "Total Voters": str(result["total_voters"]),
                        "Action": "Check for network partitions or stale nodes",
                    },
                    node_id=self.node_id,
                ))

        return result

    # =========================================================================
    # PEER REPUTATION TRACKING
    # =========================================================================
    # Track peer reliability over time for better peer selection in P2P sync,
    # gossip, and other distributed operations.
    # =========================================================================

    def _record_peer_interaction(self, peer_id: str, success: bool, interaction_type: str = "general"):
        """Record a peer interaction for reputation tracking.

        PEER REPUTATION: Track success/failure rates for different interaction types:
        - sync: File sync operations
        - gossip: Gossip message exchanges
        - heartbeat: Heartbeat responses
        - command: Remote command executions
        """
        if not hasattr(self, "_peer_reputation"):
            self._peer_reputation = {}

        if peer_id not in self._peer_reputation:
            self._peer_reputation[peer_id] = {
                "total_success": 0,
                "total_failure": 0,
                "recent_success": 0,
                "recent_failure": 0,
                "last_success": 0,
                "last_failure": 0,
                "last_reset": time.time(),
                "by_type": {},
            }

        rep = self._peer_reputation[peer_id]
        now = time.time()

        # Reset recent counters every hour
        if now - rep["last_reset"] > 3600:
            rep["recent_success"] = 0
            rep["recent_failure"] = 0
            rep["last_reset"] = now

        if success:
            rep["total_success"] += 1
            rep["recent_success"] += 1
            rep["last_success"] = now
        else:
            rep["total_failure"] += 1
            rep["recent_failure"] += 1
            rep["last_failure"] = now

        # Track by type
        if interaction_type not in rep["by_type"]:
            rep["by_type"][interaction_type] = {"success": 0, "failure": 0}
        if success:
            rep["by_type"][interaction_type]["success"] += 1
        else:
            rep["by_type"][interaction_type]["failure"] += 1

    def _get_peer_reputation_score(self, peer_id: str) -> float:
        """Get reputation score for a peer (0-100, higher is better).

        PEER REPUTATION SCORE: Combines multiple factors:
        - Recent success rate (70% weight) - last hour
        - Historical success rate (20% weight) - all time
        - Recency bonus (10% weight) - recent activity
        """
        if not hasattr(self, "_peer_reputation"):
            return 50.0  # Default neutral score

        rep = self._peer_reputation.get(peer_id)
        if not rep:
            return 50.0

        now = time.time()

        # Recent success rate (last hour)
        recent_total = rep["recent_success"] + rep["recent_failure"]
        recent_rate = rep["recent_success"] / max(1, recent_total)

        # Historical success rate
        total = rep["total_success"] + rep["total_failure"]
        historical_rate = rep["total_success"] / max(1, total)

        # Recency bonus (active peers get a boost)
        last_interaction = max(rep["last_success"], rep["last_failure"])
        recency_hours = (now - last_interaction) / 3600 if last_interaction > 0 else 24
        recency_score = max(0, 1.0 - (recency_hours / 24))  # Decays over 24 hours

        # Weighted score
        score = (recent_rate * 70) + (historical_rate * 20) + (recency_score * 10)

        return min(100.0, max(0.0, score))

    def _get_peer_reputation_summary(self) -> dict:
        """Get summary of peer reputation for gossip propagation.

        Share top/bottom peers by reputation to help cluster converge on
        reliable peer selection.
        """
        if not hasattr(self, "_peer_reputation"):
            return {"reliable_peers": [], "unreliable_peers": []}

        scores = []
        for peer_id in self._peer_reputation:
            score = self._get_peer_reputation_score(peer_id)
            scores.append((peer_id, score))

        scores.sort(key=lambda x: x[1], reverse=True)

        return {
            "reliable_peers": [{"peer": p, "score": round(s)} for p, s in scores[:5] if s >= 70],
            "unreliable_peers": [{"peer": p, "score": round(s)} for p, s in scores[-3:] if s < 30],
        }

    def _get_cluster_peer_reputation(self) -> dict:
        """Aggregate peer reputation from gossip for cluster-wide view."""
        all_scores = {}
        gossip_states = getattr(self, "_gossip_peer_states", {})
        now = time.time()

        # Include our own reputation data
        local_summary = self._get_peer_reputation_summary()
        for peer_info in local_summary.get("reliable_peers", []):
            peer_id = peer_info["peer"]
            if peer_id not in all_scores:
                all_scores[peer_id] = []
            all_scores[peer_id].append(peer_info["score"])

        # Include reputation from gossip
        for _node_id, state in gossip_states.items():
            if state.get("timestamp", 0) < now - 300:
                continue

            rep_summary = state.get("peer_reputation", {})
            for peer_info in rep_summary.get("reliable_peers", []):
                peer_id = peer_info["peer"]
                if peer_id not in all_scores:
                    all_scores[peer_id] = []
                all_scores[peer_id].append(peer_info["score"])

        # Calculate average scores
        avg_scores = {peer: sum(scores) / len(scores) for peer, scores in all_scores.items() if scores}
        sorted_peers = sorted(avg_scores.items(), key=lambda x: x[1], reverse=True)

        return {
            "most_reliable": [{"peer": p, "avg_score": round(s)} for p, s in sorted_peers[:10]],
            "peers_tracked": len(all_scores),
        }

    # ============================================================================
    # ADAPTIVE SYNC INTERVAL MANAGEMENT
    # ============================================================================
    # Dynamically adjusts P2P sync intervals based on:
    # - Cluster activity (training happening = more frequent sync)
    # - Success rate (failures = back off, successes = speed up)
    # - Data freshness (new data in cluster = more frequent sync)
    # ============================================================================

    def _init_adaptive_sync_intervals(self):
        """Initialize adaptive sync interval tracking."""
        self._adaptive_intervals = {
            "data": P2P_DATA_SYNC_BASE,
            "model": P2P_MODEL_SYNC_BASE,
            "training_db": P2P_TRAINING_DB_SYNC_BASE,
        }
        self._sync_success_streak = {
            "data": 0,
            "model": 0,
            "training_db": 0,
        }
        self._sync_failure_streak = {
            "data": 0,
            "model": 0,
            "training_db": 0,
        }
        self._last_interval_adjustment = 0

    def _get_adaptive_sync_interval(self, sync_type: str) -> float:
        """Get the current adaptive interval for a sync type.

        ADAPTIVE SYNC INTERVALS: Intervals adjust based on:
        1. Cluster activity (training = faster sync for models)
        2. Success rate (failures = back off)
        3. Base/min/max bounds per sync type

        Args:
            sync_type: One of "data", "model", "training_db"

        Returns:
            Current interval in seconds
        """
        if not hasattr(self, "_adaptive_intervals"):
            self._init_adaptive_sync_intervals()

        # Get current interval
        current = self._adaptive_intervals.get(sync_type, P2P_DATA_SYNC_BASE)

        # Apply activity-based adjustment
        activity_factor = self._calculate_cluster_activity_factor()

        # Get bounds for this sync type
        if sync_type == "data":
            min_interval = P2P_DATA_SYNC_MIN
            max_interval = P2P_DATA_SYNC_MAX
        elif sync_type == "model":
            min_interval = P2P_MODEL_SYNC_MIN
            max_interval = P2P_MODEL_SYNC_MAX
        elif sync_type == "training_db":
            min_interval = P2P_TRAINING_DB_SYNC_MIN
            max_interval = P2P_TRAINING_DB_SYNC_MAX
        else:
            min_interval = 120
            max_interval = 600

        # Apply activity factor (0.5-1.0 = active cluster, 1.0-2.0 = idle cluster)
        adjusted = current * activity_factor

        # Clamp to bounds
        return max(min_interval, min(max_interval, adjusted))

    def _calculate_cluster_activity_factor(self) -> float:
        """Calculate cluster activity factor for sync interval adjustment.

        CLUSTER ACTIVITY FACTOR:
        - < 1.0: Active cluster (training, selfplay) = faster sync
        - 1.0: Normal activity
        - > 1.0: Idle cluster = slower sync

        Returns:
            Activity factor (0.5 to 2.0)
        """
        now = time.time()

        # Check training activity (with defensive checks)
        training_active = False
        training_lock = getattr(self, "training_lock", None)
        training_jobs = getattr(self, "training_jobs", {})
        if training_lock and training_jobs:
            try:
                with training_lock:
                    for job in training_jobs.values():
                        if getattr(job, "status", None) == "running":
                            training_active = True
                            break
            except (AttributeError):
                pass

        # Check selfplay activity (count active jobs)
        selfplay_count = 0
        jobs_lock = getattr(self, "jobs_lock", None)
        selfplay_jobs = getattr(self, "selfplay_jobs", {})
        if jobs_lock and selfplay_jobs:
            try:
                with jobs_lock:
                    for job in selfplay_jobs.values():
                        if getattr(job, "status", None) == "running":
                            selfplay_count += 1
            except (AttributeError):
                pass

        # Check recent data generation from gossip
        recent_data = False
        gossip_states = getattr(self, "_gossip_node_states", {}) or {}
        for _node_id, state in gossip_states.items():
            if not isinstance(state, dict):
                continue
            last_game = state.get("last_game_time", 0)
            if now - last_game < 300:  # Game in last 5 min
                recent_data = True
                break

        # Calculate factor
        factor = 1.0

        if training_active:
            factor *= 0.5  # Much faster sync during training
        elif selfplay_count >= 5:
            factor *= 0.7  # Faster sync with active selfplay
        elif selfplay_count > 0:
            factor *= 0.85  # Slightly faster with some activity

        if recent_data:
            factor *= 0.9  # Faster sync when new data available

        # If completely idle (no jobs, stale data), slow down
        if selfplay_count == 0 and not training_active and not recent_data:
            factor *= 1.5

        return max(0.5, min(2.0, factor))

    def _record_sync_result_for_adaptive(self, sync_type: str, success: bool):
        """Record sync result to adjust adaptive intervals.

        ADAPTIVE INTERVAL ADJUSTMENT:
        - On success: reduce interval (speed up) up to min
        - On failure: increase interval (back off) up to max

        Args:
            sync_type: One of "data", "model", "training_db"
            success: Whether sync succeeded
        """
        if not hasattr(self, "_adaptive_intervals"):
            self._init_adaptive_sync_intervals()

        if success:
            self._sync_success_streak[sync_type] = self._sync_success_streak.get(sync_type, 0) + 1
            self._sync_failure_streak[sync_type] = 0

            # After 3 consecutive successes, speed up
            if self._sync_success_streak[sync_type] >= 3:
                current = self._adaptive_intervals[sync_type]
                new_interval = current * P2P_SYNC_SPEEDUP_FACTOR

                # Get min bound
                if sync_type == "data":
                    min_interval = P2P_DATA_SYNC_MIN
                elif sync_type == "model":
                    min_interval = P2P_MODEL_SYNC_MIN
                else:
                    min_interval = P2P_TRAINING_DB_SYNC_MIN

                self._adaptive_intervals[sync_type] = max(min_interval, new_interval)
                self._sync_success_streak[sync_type] = 0  # Reset streak
        else:
            self._sync_failure_streak[sync_type] = self._sync_failure_streak.get(sync_type, 0) + 1
            self._sync_success_streak[sync_type] = 0

            # On any failure, back off
            current = self._adaptive_intervals[sync_type]
            new_interval = current * P2P_SYNC_BACKOFF_FACTOR

            # Get max bound
            if sync_type == "data":
                max_interval = P2P_DATA_SYNC_MAX
            elif sync_type == "model":
                max_interval = P2P_MODEL_SYNC_MAX
            else:
                max_interval = P2P_TRAINING_DB_SYNC_MAX

            self._adaptive_intervals[sync_type] = min(max_interval, new_interval)

    def _get_sync_interval_summary(self) -> dict:
        """Get summary of current adaptive sync intervals for monitoring."""
        if not hasattr(self, "_adaptive_intervals"):
            self._init_adaptive_sync_intervals()

        return {
            "data_interval": round(self._get_adaptive_sync_interval("data")),
            "model_interval": round(self._get_adaptive_sync_interval("model")),
            "training_db_interval": round(self._get_adaptive_sync_interval("training_db")),
            "activity_factor": round(self._calculate_cluster_activity_factor(), 2),
            "data_streak": {
                "success": self._sync_success_streak.get("data", 0),
                "failure": self._sync_failure_streak.get("data", 0),
            },
            "model_streak": {
                "success": self._sync_success_streak.get("model", 0),
                "failure": self._sync_failure_streak.get("model", 0),
            },
        }

    # ============================================================================
    # SELFPLAY DATA DEDUPLICATION
    # ============================================================================
    # Tracks synced files and game IDs to avoid redundant transfers during P2P sync.
    # Uses bloom filter for efficient game ID tracking and file hash caching.
    # ============================================================================

    def _init_data_deduplication(self):
        """Initialize data deduplication tracking."""
        self._synced_file_hashes: set[str] = set()  # Hash -> synced
        self._known_game_ids: set[str] = set()  # Game IDs we have
        self._dedup_stats = {
            "files_skipped": 0,
            "games_skipped": 0,
            "bytes_saved": 0,
            "last_cleanup": time.time(),
        }
        self._dedup_lock = threading.Lock()

    def _record_synced_file(self, file_hash: str, file_size: int):
        """Record a file as synced for deduplication.

        DATA DEDUPLICATION: Track file hashes we've synced to avoid
        re-syncing the same file from different peers.

        Args:
            file_hash: Hash of the synced file
            file_size: Size in bytes (for metrics)
        """
        if not hasattr(self, "_synced_file_hashes"):
            self._init_data_deduplication()

        with self._dedup_lock:
            self._synced_file_hashes.add(file_hash)

    def _is_file_already_synced(self, file_hash: str) -> bool:
        """Check if file was already synced based on hash.

        Args:
            file_hash: Hash to check

        Returns:
            True if file was already synced
        """
        if not hasattr(self, "_synced_file_hashes"):
            self._init_data_deduplication()

        if not file_hash:
            return False

        with self._dedup_lock:
            return file_hash in self._synced_file_hashes

    def _record_game_ids(self, game_ids: list[str]):
        """Record game IDs as known for deduplication.

        GAME ID TRACKING: Track game IDs we have to avoid syncing
        duplicate games from different DB files.

        Args:
            game_ids: List of game IDs to record
        """
        if not hasattr(self, "_known_game_ids"):
            self._init_data_deduplication()

        with self._dedup_lock:
            self._known_game_ids.update(game_ids)

    # NOTE: _filter_unknown_games removed Dec 27, 2025 (dead code, never called)

    def _record_dedup_skip(self, file_count: int = 0, game_count: int = 0, bytes_saved: int = 0):
        """Record deduplication skip for metrics.

        Args:
            file_count: Number of files skipped
            game_count: Number of games skipped
            bytes_saved: Bytes saved by skipping
        """
        if not hasattr(self, "_dedup_stats"):
            self._init_data_deduplication()

        with self._dedup_lock:
            self._dedup_stats["files_skipped"] += file_count
            self._dedup_stats["games_skipped"] += game_count
            self._dedup_stats["bytes_saved"] += bytes_saved

    # NOTE: _cleanup_dedup_cache removed Dec 27, 2025 (dead code, never called)

    def _get_dedup_summary(self) -> dict:
        """Get deduplication metrics summary."""
        if not hasattr(self, "_dedup_stats"):
            self._init_data_deduplication()

        with self._dedup_lock:
            return {
                "files_skipped": self._dedup_stats.get("files_skipped", 0),
                "games_skipped": self._dedup_stats.get("games_skipped", 0),
                "bytes_saved_mb": round(self._dedup_stats.get("bytes_saved", 0) / (1024 * 1024), 2),
                "known_file_hashes": len(self._synced_file_hashes),
                "known_game_ids": len(self._known_game_ids),
            }

    def _get_swim_raft_status(self) -> dict[str, Any]:
        """Get SWIM/Raft protocol status summary.

        Returns status of the new P2P protocols (Phase 5, Dec 26, 2025):
        - SWIM: Gossip-based membership with 5s failure detection
        - Raft: Replicated work queue with sub-second failover

        These protocols are enabled via feature flags and provide
        gradual migration from HTTP/Bully to SWIM/Raft.
        """
        try:
            from scripts.p2p.constants import (  # noqa: I001
                SWIM_ENABLED, RAFT_ENABLED, MEMBERSHIP_MODE, CONSENSUS_MODE
            )
        except ImportError:
            SWIM_ENABLED = False
            RAFT_ENABLED = False
            MEMBERSHIP_MODE = "http"
            CONSENSUS_MODE = "bully"

        # Check SWIM status
        swim_status = {
            "enabled": SWIM_ENABLED,
            "available": False,
            "started": getattr(self, "_swim_started", False),
            "alive_count": 0,
        }
        try:
            from app.p2p.swim_adapter import SWIM_AVAILABLE
            swim_status["available"] = SWIM_AVAILABLE

            # Get membership summary if SWIM is active
            if hasattr(self, "get_swim_membership_summary"):
                summary = self.get_swim_membership_summary()
                swim_status.update({
                    "started": summary.get("swim_started", False),
                    "alive_count": summary.get("swim", {}).get("alive", 0),
                    "suspected_count": summary.get("swim", {}).get("suspected", 0),
                    "failed_count": summary.get("swim", {}).get("failed", 0),
                })
        except ImportError:
            pass
        except Exception as e:  # noqa: BLE001
            swim_status["error"] = str(e)

        # Check Raft status
        raft_status = {
            "enabled": RAFT_ENABLED,
            "available": False,
            "initialized": getattr(self, "_raft_initialized", False),
            "is_leader": False,
        }
        try:
            from scripts.p2p.consensus_mixin import PYSYNCOBJ_AVAILABLE
            raft_status["available"] = PYSYNCOBJ_AVAILABLE

            # Get Raft consensus status if available
            if hasattr(self, "get_raft_status"):
                raft_info = self.get_raft_status()
                raft_status.update({
                    "initialized": raft_info.get("raft_initialized", False),
                    "is_leader": raft_info.get("is_raft_leader", False),
                    "leader_address": raft_info.get("raft_leader", ""),
                    "should_use_raft": raft_info.get("should_use_raft", False),
                })
                if "work_queue_status" in raft_info:
                    wq = raft_info["work_queue_status"]
                    raft_status["work_queue"] = {
                        "pending": wq.get("by_status", {}).get("pending", 0),
                        "claimed": wq.get("by_status", {}).get("claimed", 0),
                        "completed": wq.get("by_status", {}).get("completed", 0),
                    }
        except ImportError:
            pass
        except Exception as e:  # noqa: BLE001
            raft_status["error"] = str(e)

        return {
            "membership_mode": MEMBERSHIP_MODE,
            "consensus_mode": CONSENSUS_MODE,
            "swim": swim_status,
            "raft": raft_status,
            "hybrid_status": {
                "swim_fallback_active": not swim_status.get("started", False) and MEMBERSHIP_MODE != "http",
                "raft_fallback_active": not raft_status.get("initialized", False) and CONSENSUS_MODE != "bully",
            },
        }

    def _get_cluster_observability(self) -> dict[str, Any]:
        """Get cluster observability metrics for debugging.

        December 30, 2025: Added to help diagnose idle GPU nodes and
        peer visibility discrepancies across the cluster.

        Returns:
            Dict with:
            - unhealthy_nodes: Nodes in unhealthy set (excluded from work)
            - gossip_discovered_peers: Count of peers found via gossip
            - cluster_job_distribution: Jobs per node for balance analysis
        """
        result: dict[str, Any] = {}

        # 1. Unhealthy nodes from node_selector
        try:
            if hasattr(self, "node_selector") and self.node_selector:
                unhealthy_set = getattr(self.node_selector, "_unhealthy_nodes", set())
                unhealthy_reasons = getattr(self.node_selector, "_unhealthy_reasons", {})
                result["unhealthy_nodes"] = {
                    "count": len(unhealthy_set),
                    "node_ids": list(unhealthy_set),
                    "reasons": dict(unhealthy_reasons),
                }
            else:
                result["unhealthy_nodes"] = {"error": "node_selector not available"}
        except Exception as e:  # noqa: BLE001
            result["unhealthy_nodes"] = {"error": str(e)}

        # 2. Gossip-discovered peers
        try:
            gossip_endpoints = getattr(self, "_gossip_learned_endpoints", {})
            result["gossip_discovered_peers"] = {
                "count": len(gossip_endpoints),
                "node_ids": list(gossip_endpoints.keys()),
            }
        except Exception as e:  # noqa: BLE001
            result["gossip_discovered_peers"] = {"error": str(e)}

        # 3. Cluster job distribution (for balance analysis)
        try:
            with self.peers_lock:
                job_distribution = {}
                for node_id, peer in self.peers.items():
                    if peer.is_alive():
                        job_distribution[node_id] = {
                            "selfplay_jobs": int(getattr(peer, "selfplay_jobs", 0) or 0),
                            "training_jobs": int(getattr(peer, "training_jobs", 0) or 0),
                            "gpu_percent": float(getattr(peer, "gpu_percent", 0) or 0),
                        }
                # Add self
                job_distribution[self.node_id] = {
                    "selfplay_jobs": int(getattr(self.self_info, "selfplay_jobs", 0) or 0),
                    "training_jobs": int(getattr(self.self_info, "training_jobs", 0) or 0),
                    "gpu_percent": float(getattr(self.self_info, "gpu_percent", 0) or 0),
                }

            # Compute summary stats
            if job_distribution:
                all_jobs = [d["selfplay_jobs"] for d in job_distribution.values()]
                avg_jobs = sum(all_jobs) / len(all_jobs) if all_jobs else 0
                max_jobs = max(all_jobs) if all_jobs else 0
                min_jobs = min(all_jobs) if all_jobs else 0
                idle_count = sum(1 for j in all_jobs if j == 0)
                result["cluster_job_distribution"] = {
                    "node_count": len(job_distribution),
                    "avg_selfplay_jobs": round(avg_jobs, 1),
                    "max_selfplay_jobs": max_jobs,
                    "min_selfplay_jobs": min_jobs,
                    "idle_nodes": idle_count,
                    "per_node": job_distribution,
                }
            else:
                result["cluster_job_distribution"] = {"error": "no peers available"}
        except Exception as e:  # noqa: BLE001
            result["cluster_job_distribution"] = {"error": str(e)}

        return result

    def _get_data_summary_cached(self) -> dict[str, Any]:
        """Get cached data summary for /status endpoint.

        January 13, 2026: Added as part of unified data discovery infrastructure.
        Returns game counts from local canonical databases for quick access.
        For full multi-source data, use /data/summary endpoint.

        Returns:
            Dict with total game counts per config from local canonical DBs
        """
        try:
            # Use cached game counts from selfplay scheduler if available
            if hasattr(self, "selfplay_scheduler") and self.selfplay_scheduler:
                counts = getattr(self.selfplay_scheduler, "_p2p_game_counts", None)
                if counts:
                    total = sum(counts.values())
                    return {
                        "total_games": total,
                        "by_config": dict(counts),
                        "source": "selfplay_scheduler_cache",
                        "config_count": len(counts),
                    }

            # Fall back to local canonical DB scan
            game_counts = self._seed_selfplay_scheduler_game_counts_sync()
            if game_counts:
                return {
                    "total_games": sum(game_counts.values()),
                    "by_config": game_counts,
                    "source": "canonical_databases",
                    "config_count": len(game_counts),
                }

            return {
                "total_games": 0,
                "by_config": {},
                "source": "none",
                "error": "No game data available",
            }

        except Exception as e:  # noqa: BLE001
            return {
                "total_games": 0,
                "by_config": {},
                "source": "error",
                "error": str(e),
            }

    def _get_cooldown_stats(self) -> dict[str, Any]:
        """Get adaptive dead peer cooldown statistics for monitoring.

        January 20, 2026: Added to expose cooldown manager metrics in /status.
        Helps diagnose peer recovery issues and verify adaptive cooldown is working.

        Returns:
            Dict with:
            - enabled: Whether the adaptive cooldown manager is active
            - nodes_in_cooldown: Number of nodes currently in cooldown
            - stats: Cooldown manager statistics (probes, recoveries, etc.)
            - in_cooldown: List of nodes currently in cooldown with their tier/remaining time
        """
        if not self._cooldown_manager:
            # Fallback to legacy dict tracking
            return {
                "enabled": False,
                "nodes_in_cooldown": len(self._dead_peer_timestamps),
                "fallback_mode": True,
                "dead_peer_timestamps": {
                    node_id: {"dead_since": ts, "age_seconds": time.time() - ts}
                    for node_id, ts in self._dead_peer_timestamps.items()
                },
            }

        try:
            stats = self._cooldown_manager.get_stats()
            in_cooldown = self._cooldown_manager.get_all_in_cooldown()
            return {
                "enabled": True,
                "nodes_in_cooldown": stats.get("nodes_in_cooldown", 0),
                "stats": stats,
                "in_cooldown": in_cooldown,
                "fallback_mode": False,
            }
        except Exception as e:  # noqa: BLE001
            return {
                "enabled": True,
                "error": str(e),
            }

    def _get_fallback_status(self) -> dict[str, Any]:
        """Get fallback mechanism status for debugging partition issues.

        Session 17.41 (Jan 6, 2026): Exposes visibility into why fallback mechanisms
        aren't activating during network partitions. This helps diagnose issues where
        the work queue has items but workers can't claim jobs because the leader is
        unreachable and fallbacks haven't kicked in.

        Returns:
            Dict with:
            - autonomous_queue: Whether local queue fallback is active
            - work_discovery: Multi-channel work discovery status
            - leader_status: Leader contact timing
            - partition_healer: Partition healing escalation state
        """
        result: dict[str, Any] = {}
        now = time.time()

        # 1. Autonomous queue status
        try:
            loop = getattr(self, "_autonomous_queue_loop", None)
            if loop is not None:
                loop_status = loop.get_status() if hasattr(loop, "get_status") else {}
                result["autonomous_queue"] = {
                    "active": loop_status.get("activated", False),
                    "enabled": loop_status.get("enabled", False),
                    "running": loop_status.get("running", False),
                    "activation_reason": loop_status.get("activation_reason", ""),
                    "no_leader_duration": loop_status.get("no_leader_duration", 0.0),
                    "queue_depth": loop_status.get("queue_depth", 0),
                }
            else:
                result["autonomous_queue"] = {"error": "loop_not_initialized"}
        except Exception as e:  # noqa: BLE001
            result["autonomous_queue"] = {"error": str(e)}

        # 2. Work discovery manager status
        try:
            from scripts.p2p.loops.job_loops import get_work_discovery_manager
            manager = get_work_discovery_manager()
            if manager is not None:
                mgr_status = manager.get_status() if hasattr(manager, "get_status") else {}
                result["work_discovery"] = {
                    "enabled": mgr_status.get("enabled", False),
                    "active_channels": mgr_status.get("active_channels", []),
                    "last_work_time": mgr_status.get("last_work_time", 0.0),
                    "claims_via_leader": mgr_status.get("claims_via_leader", 0),
                    "claims_via_peer": mgr_status.get("claims_via_peer", 0),
                    "claims_via_local": mgr_status.get("claims_via_local", 0),
                }
            else:
                result["work_discovery"] = {"error": "manager_not_initialized"}
        except ImportError:
            result["work_discovery"] = {"error": "import_failed"}
        except Exception as e:  # noqa: BLE001
            result["work_discovery"] = {"error": str(e)}

        # 3. Leader contact status
        try:
            last_leader_seen = getattr(self, "last_leader_seen", now)
            leader_unreachable_duration = now - last_leader_seen
            result["leader_status"] = {
                "last_leader_seen": last_leader_seen,
                "leader_unreachable_duration": round(leader_unreachable_duration, 1),
                "is_leaderless": self.leader_id is None or self.leader_id == "",
                "current_leader_id": self.leader_id,
                "is_self_leader": self._is_leader(),
            }
        except Exception as e:  # noqa: BLE001
            result["leader_status"] = {"error": str(e)}

        # 4. Partition healer status (if available)
        try:
            from scripts.p2p.partition_healer import get_partition_healer
            healer = get_partition_healer()
            healer_status = healer.get_status()
            result["partition_healer"] = {
                "escalation_level": healer_status.get("escalation_level", 0),
                "last_healing_attempt": healer_status.get("last_healing_attempt", 0.0),
                "healing_in_progress": healer_status.get("healing_in_progress", False),
                "has_orchestrator": healer_status.get("has_orchestrator", False),
                "election_ready": healer_status.get("election_ready", True),
            }
        except ImportError:
            result["partition_healer"] = {"error": "import_failed"}
        except Exception as e:  # noqa: BLE001
            result["partition_healer"] = {"error": str(e)}

        return result

    # ============================================================================
    # DISTRIBUTED TOURNAMENT SCHEDULING
    # ============================================================================
    # Allows tournaments to be scheduled and coordinated via gossip protocol
    # without requiring a leader. Uses consensus to elect tournament coordinator.
    # ============================================================================

    def _init_distributed_tournament_scheduling(self):
        """Initialize distributed tournament scheduling state."""
        self._tournament_proposals: dict[str, dict] = {}  # proposal_id -> proposal
        self._tournament_votes: dict[str, dict[str, str]] = {}  # proposal_id -> {node_id: vote}
        self._active_tournaments_gossip: dict[str, dict] = {}  # tournament_id -> state
        self._last_tournament_check = 0
        self._tournament_coordination_lock = threading.Lock()

    # NOTE: _propose_tournament, _vote_on_tournament_proposal removed Dec 27, 2025 (dead code, never called)

    def _get_tournament_gossip_state(self) -> dict:
        """Get tournament state for gossip propagation.

        TOURNAMENT GOSSIP: Share active tournament info via gossip so nodes
        can coordinate without leader.

        Returns:
            Dict with proposals and active tournaments
        """
        if not hasattr(self, "_tournament_proposals"):
            self._init_distributed_tournament_scheduling()

        now = time.time()

        with self._tournament_coordination_lock:
            # Only share recent proposals (last 10 min)
            active_proposals = {
                pid: p for pid, p in self._tournament_proposals.items()
                if now - p.get("proposed_at", 0) < 600 and p.get("status") == "proposed"
            }

        # Get active distributed tournaments
        active_tournaments = {}
        for tid, state in getattr(self, "distributed_tournament_state", {}).items():
            if hasattr(state, "status") and state.status == "running":
                active_tournaments[tid] = {
                    "job_id": tid,
                    "coordinator": self.node_id,  # We're coordinating if we have it
                    "progress": state.completed_matches / max(1, state.total_matches),
                    "status": state.status,
                }

        return {
            "proposals": list(active_proposals.values()),
            "active": active_tournaments,
            "last_update": now,
        }

    def _process_tournament_gossip(self, node_id: str, tournament_state: dict):
        """Process tournament info received via gossip.

        GOSSIP PROCESSING: When receiving tournament state from peers,
        - Record their proposals and votes
        - Check if any proposals reached consensus
        - Start tournaments that we're elected to coordinate

        Args:
            node_id: ID of node that sent this state
            tournament_state: Tournament state from gossip
        """
        if not hasattr(self, "_tournament_proposals"):
            self._init_distributed_tournament_scheduling()

        if not tournament_state or not isinstance(tournament_state, dict):
            return

        time.time()

        # Process proposals from gossip
        for proposal in tournament_state.get("proposals", []):
            if not isinstance(proposal, dict):
                continue

            proposal_id = proposal.get("proposal_id")
            if not proposal_id:
                continue

            with self._tournament_coordination_lock:
                if proposal_id not in self._tournament_proposals:
                    # New proposal from peer - add it and auto-approve
                    self._tournament_proposals[proposal_id] = proposal.copy()
                    self._tournament_proposals[proposal_id]["votes"][self.node_id] = "approve"
                else:
                    # Merge votes
                    existing = self._tournament_proposals[proposal_id]
                    for voter, vote in proposal.get("votes", {}).items():
                        if voter not in existing["votes"]:
                            existing["votes"][voter] = vote

    def _check_tournament_consensus(self):
        """Check if any tournament proposals have reached consensus.

        CONSENSUS CHECK: A proposal is approved when majority of alive peers approve.
        The coordinator is elected as the highest-ID approving voter node.
        """
        if not hasattr(self, "_tournament_proposals"):
            return

        now = time.time()

        # Get alive peer count for quorum
        with self.peers_lock:
            alive_peers = [p for p in self.peers.values() if p.is_alive()]
        alive_count = len(alive_peers) + 1  # +1 for self

        quorum = (alive_count // 2) + 1

        with self._tournament_coordination_lock:
            for proposal_id, proposal in list(self._tournament_proposals.items()):
                if proposal.get("status") != "proposed":
                    continue

                # Count votes
                approve_votes = [
                    voter for voter, vote in proposal.get("votes", {}).items()
                    if vote == "approve"
                ]

                if len(approve_votes) >= quorum:
                    # Consensus reached! Elect coordinator (highest ID)
                    coordinator = max(approve_votes)
                    proposal["status"] = "approved"
                    proposal["coordinator"] = coordinator

                    logger.info(f"TOURNAMENT: Proposal {proposal_id} approved! "
                          f"Coordinator: {coordinator} ({len(approve_votes)}/{alive_count} votes)")

                    # If we're the coordinator, start the tournament
                    if coordinator == self.node_id:
                        asyncio.create_task(self._start_tournament_from_proposal(proposal))

                # Expire old proposals
                elif now - proposal.get("proposed_at", 0) > 600:
                    proposal["status"] = "expired"

    async def _start_tournament_from_proposal(self, proposal: dict):
        """Start a tournament from an approved proposal.

        COORDINATOR DUTIES: When elected as coordinator, start the tournament
        and manage match distribution to workers.

        Args:
            proposal: Approved proposal dict
        """
        import uuid

        job_id = f"tournament_{uuid.uuid4().hex[:8]}"
        agent_ids = proposal.get("agent_ids", [])

        if len(agent_ids) < 2:
            logger.info("TOURNAMENT: Cannot start - need at least 2 agents")
            return

        # Create round-robin pairings
        pairings = []
        for i, a1 in enumerate(agent_ids):
            for a2 in agent_ids[i+1:]:
                for game_num in range(proposal.get("games_per_pairing", 2)):
                    pairings.append({
                        "agent1": a1,
                        "agent2": a2,
                        "game_num": game_num,
                        "status": "pending",
                    })

        state = DistributedTournamentState(
            job_id=job_id,
            board_type=proposal.get("board_type", "square8"),
            num_players=proposal.get("num_players", 2),
            agent_ids=agent_ids,
            games_per_pairing=proposal.get("games_per_pairing", 2),
            total_matches=len(pairings),
            pending_matches=pairings,
            status="running",
            started_at=time.time(),
            last_update=time.time(),
        )

        # Find workers
        with self.peers_lock:
            workers = [p.node_id for p in self.peers.values() if p.is_healthy()]
        state.worker_nodes = workers

        if not state.worker_nodes:
            logger.info(f"TOURNAMENT: No workers available for {job_id}")
            return

        self.distributed_tournament_state[job_id] = state

        logger.info(f"TOURNAMENT: Started {job_id} from proposal {proposal.get('proposal_id')}: "
              f"{len(agent_ids)} agents, {len(pairings)} matches, {len(workers)} workers")

        # Launch coordinator task
        asyncio.create_task(self.job_manager.run_distributed_tournament(job_id))

    def _get_distributed_tournament_summary(self) -> dict:
        """Get summary of distributed tournament scheduling for status endpoint."""
        if not hasattr(self, "_tournament_proposals"):
            self._init_distributed_tournament_scheduling()

        with self._tournament_coordination_lock:
            pending_proposals = sum(
                1 for p in self._tournament_proposals.values()
                if p.get("status") == "proposed"
            )
            approved_proposals = sum(
                1 for p in self._tournament_proposals.values()
                if p.get("status") == "approved"
            )

        active_tournaments = sum(
            1 for s in getattr(self, "distributed_tournament_state", {}).values()
            if hasattr(s, "status") and s.status == "running"
        )

        return {
            "pending_proposals": pending_proposals,
            "approved_proposals": approved_proposals,
            "active_tournaments": active_tournaments,
            "enabled": True,
        }

    async def _start_monitoring_if_leader(self):
        """Start Prometheus/Grafana when we become leader (P2P monitoring resilience)."""
        if not self.monitoring_manager:
            return
        if self.role != NodeRole.LEADER:
            return
        if self._monitoring_was_leader:
            return  # Already started

        try:
            # Update peer list for Prometheus config
            with self.peers_lock:
                peer_list = [
                    {"node_id": p.node_id, "host": p.host, "port": getattr(p, "metrics_port", 9091)}
                    for p in self.peers.values()
                    if p.node_id != self.node_id and p.is_healthy()
                ]
            self.monitoring_manager.update_peers(peer_list)

            # Start monitoring services
            success = await self.monitoring_manager.start_as_leader()
            if success:
                logger.info("Monitoring services started on leader node")
                self._monitoring_was_leader = True
            else:
                logger.error("Failed to start monitoring services")
        except Exception as e:  # noqa: BLE001
            logger.error(f"starting monitoring services: {e}")

    async def _stop_monitoring_if_not_leader(self):
        """Stop Prometheus/Grafana when we step down from leadership."""
        if not self.monitoring_manager:
            return
        if not self._monitoring_was_leader:
            return  # Never started

        if self.role != NodeRole.LEADER:
            try:
                await self.monitoring_manager.stop()
                logger.info("Monitoring services stopped (no longer leader)")
                self._monitoring_was_leader = False
            except Exception as e:  # noqa: BLE001
                logger.error(f"stopping monitoring services: {e}")

    async def _start_p2p_auto_deployer(self):
        """Start P2P auto-deployer when we become leader.

        The auto-deployer ensures P2P orchestrator is running on all cluster nodes.
        This solves the fundamental gap where P2P deployment was manual-only.
        """
        if self.role != NodeRole.LEADER:
            return
        if self._auto_deployer_task is not None:
            return  # Already running

        try:
            from app.coordination.p2p_auto_deployer import P2PAutoDeployer, P2PDeploymentConfig

            config = P2PDeploymentConfig(
                check_interval_seconds=300.0,  # Check every 5 minutes
                min_coverage_percent=90.0,
            )
            self.p2p_auto_deployer = P2PAutoDeployer(config=config)

            # Run as background task
            self._auto_deployer_task = asyncio.create_task(
                self.p2p_auto_deployer.run_daemon(),
                name="p2p_auto_deployer"
            )
            logger.info("P2P Auto-Deployer started (leader responsibility)")
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to start P2P auto-deployer: {e}")

    async def _stop_p2p_auto_deployer(self):
        """Stop P2P auto-deployer when we step down from leadership."""
        if self.p2p_auto_deployer:
            self.p2p_auto_deployer.stop()
        if self._auto_deployer_task:
            self._auto_deployer_task.cancel()
            with contextlib.suppress(asyncio.CancelledError):
                await self._auto_deployer_task
            self._auto_deployer_task = None
        self.p2p_auto_deployer = None
        logger.info("P2P Auto-Deployer stopped")

    async def _update_monitoring_peers(self):
        """Update Prometheus config with current peer list."""
        if not self.monitoring_manager or not self._monitoring_was_leader:
            return
        if self.role != NodeRole.LEADER:
            return

        try:
            with self.peers_lock:
                peer_list = [
                    {"node_id": p.node_id, "host": p.host, "port": getattr(p, "metrics_port", 9091)}
                    for p in self.peers.values()
                    if p.node_id != self.node_id and p.is_healthy()
                ]
            self.monitoring_manager.update_peers(peer_list)
            await self.monitoring_manager.reload_config()
        except Exception as e:  # noqa: BLE001
            logger.error(f"updating monitoring peers: {e}")

    async def _renew_leader_lease(self):
        """Renew our leadership lease and broadcast to peers."""
        if self.role != NodeRole.LEADER:
            return
        # Dec 31, 2025: Add grace period for quorum failures to prevent flapping
        # Require 3 consecutive failures (~45 seconds) before stepping down
        if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
            self._quorum_fail_count = getattr(self, "_quorum_fail_count", 0) + 1
            # Jan 2, 2026: Use _count_alive_voters() to check IP:port matches
            voters_alive = self._count_alive_voters()
            quorum_size = getattr(self, "voter_quorum_size", 0)
            logger.warning(
                f"[LeaseRenewal] Voter quorum check failed ({self._quorum_fail_count}/3): "
                f"voters_alive={voters_alive}, quorum_size={quorum_size}"
            )
            if self._quorum_fail_count >= 3:
                logger.info(f"Lost voter quorum (3 consecutive failures); stepping down: {self.node_id}")
                # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                self._set_leader(None, reason="lost_voter_quorum", save_state=False)
                self.leader_lease_id = ""
                self.leader_lease_expires = 0.0
                self.last_lease_renewal = 0.0
                self._quorum_fail_count = 0
                self._release_voter_grant_if_self()
                self._save_state()
            return
        else:
            # Reset counter on success
            self._quorum_fail_count = 0

        now = time.time()
        if now - self.last_lease_renewal < LEADER_LEASE_RENEW_INTERVAL:
            return  # Too soon to renew

        lease_id = str(self.leader_lease_id or "")
        if not lease_id:
            lease_id = f"{self.node_id}_{int(time.time())}_{uuid.uuid4().hex[:8]}"
        lease_expires = await self._acquire_voter_lease_quorum(lease_id, int(LEADER_LEASE_DURATION))
        if getattr(self, "voter_node_ids", []) and not lease_expires:
            # Voter quorum failed - try arbiter fallback before stepping down
            logger.info("Voter lease quorum failed; checking arbiter...")
            arbiter_leader = await self._query_arbiter_for_leader()
            if arbiter_leader == self.node_id:
                # Arbiter still recognizes us as leader - extend lease provisionally
                logger.info("Arbiter confirms us as leader despite quorum failure; continuing with provisional lease")
                lease_expires = now + LEADER_LEASE_DURATION / 2  # Shorter lease until quorum recovers
            elif arbiter_leader:
                # Arbiter says someone else is leader - defer to arbiter
                logger.info(f"Arbiter reports different leader ({arbiter_leader}); stepping down")
                # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                self._set_leader(arbiter_leader, reason="arbiter_override", save_state=False)
                self.leader_lease_id = ""
                self.leader_lease_expires = 0.0
                self.last_lease_renewal = 0.0
                self._release_voter_grant_if_self()
                self._save_state()
                return
            else:
                # Arbiter also unreachable - step down to be safe
                logger.error(f"Failed to renew voter lease quorum and arbiter unreachable; stepping down: {self.node_id}")
                # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                self._set_leader(None, reason="arbiter_unreachable", save_state=False)
                self.leader_lease_id = ""
                self.leader_lease_expires = 0.0
                self.last_lease_renewal = 0.0
                self._release_voter_grant_if_self()
                self._save_state()
                return

        self.leader_lease_id = lease_id
        self.leader_lease_expires = float(lease_expires or (now + LEADER_LEASE_DURATION))
        self.last_lease_renewal = now

        # Jan 5, 2026: Renew self-leadership in state machine during lease renewal
        # Prevents the leader self-acknowledgment bug where leader_id is only set
        # during state transitions, causing the cluster to appear leaderless
        if hasattr(self, "_leadership_sm") and self._leadership_sm:
            try:
                self._leadership_sm.renew_self_leadership()
            except Exception as e:
                logger.debug(f"[LeaseRenewal] Failed to renew self-leadership: {e}")

        # Broadcast lease renewal to all peers
        with self.peers_lock:
            peers = list(self.peers.values())

        timeout = ClientTimeout(total=3)
        try:
            async with get_client_session(timeout) as session:
                for peer in peers:
                    if peer.node_id != self.node_id and peer.is_alive():
                        try:
                            url = self._url_for_peer(peer, "/coordinator")
                            await session.post(url, json={
                                "leader_id": self.node_id,
                                "lease_id": self.leader_lease_id,
                                "lease_expires": self.leader_lease_expires,
                                "lease_renewal": True,
                                "voter_node_ids": list(getattr(self, "voter_node_ids", []) or []),
                            }, headers=self._auth_headers())
                        except (aiohttp.ClientError, asyncio.TimeoutError, KeyError, IndexError, AttributeError):
                            pass  # Network errors expected during lease renewal
        except Exception as e:  # noqa: BLE001
            logger.info(f"Lease renewal error: {e}")

    def _is_leader_lease_valid(self) -> bool:
        """Check if the current leader's lease is still valid."""
        if not self.leader_id:
            return False

        # Jan 13, 2026: Reject proxy_only leaders - they should never have been elected
        # This forces a re-election if a proxy_only node somehow became leader
        if self._is_node_proxy_only(self.leader_id):
            logger.warning(
                f"[LeaderValidation] Current leader {self.leader_id} is proxy_only - invalidating lease"
            )
            return False

        # Jan 19, 2026: Use symmetric grace period for both leader and followers
        # CRITICAL FIX: Previous code used strict check for leader (no grace) but
        # lenient check for followers (+90s grace). This caused 90s split-brain window:
        # - Leader steps down at t=180
        # - Followers still see leader valid until t=270
        # - During t=180-270: leader="none" on leader, leader=old on followers
        # Now using consistent 30s grace for everyone to prevent split-brain.
        grace = 30  # Same grace for leader and followers
        return time.time() < self.leader_lease_expires + grace

    async def _check_and_resolve_split_brain(self) -> bool:
        """Check for split-brain (multiple leaders) and resolve by stepping down if needed.

        LEARNED LESSONS - This addresses the cluster status showing multiple leaders.
        Uses Bully algorithm: highest node_id wins.

        Returns True if we stepped down (caller should skip leadership duties).
        """
        if self.role != NodeRole.LEADER:
            return False

        with self.peers_lock:
            peers_snapshot = [p for p in self.peers.values() if p.node_id != self.node_id]

        conflict_keys = self._endpoint_conflict_keys([self.self_info, *peers_snapshot])

        # Gather all peers claiming to be leader.
        other_leaders = [peer for peer in peers_snapshot if peer.role == NodeRole.LEADER and peer.is_alive()]

        voter_ids = list(getattr(self, "voter_node_ids", []) or [])

        # PROACTIVE VOTER ACK VERIFICATION: Even when no other leaders are visible,
        # periodically verify that voters still acknowledge us as leader.
        # This catches split-brain where we can't see the other partition's leader.
        if not other_leaders and voter_ids:
            now = time.time()
            last_voter_check = float(getattr(self, "_last_voter_ack_check", 0) or 0)
            # Check every 30 seconds (more frequent than lease renewal)
            if now - last_voter_check >= 30:
                self._last_voter_ack_check = now
                leased_leader = await self._determine_leased_leader_from_voters()
                if leased_leader and leased_leader != self.node_id:
                    logger.info(f"VOTER ACK CHECK: Voters grant to {leased_leader}, not us; stepping down")
                    # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                    self._set_leader(leased_leader, reason="voter_ack_check_other_leader", save_state=True)
                    self.leader_lease_id = ""
                    self.leader_lease_expires = 0.0
                    self._release_voter_grant_if_self()
                    return True
            return False  # No split-brain detected

        if not other_leaders:
            return False  # No split-brain
        if voter_ids:
            # In quorum-gated clusters, only voters may safely lead.
            if self.node_id not in voter_ids:
                print(
                    f"[P2P] SPLIT-BRAIN detected, but {self.node_id} is not a voter; stepping down."
                )
                # December 2025: Emit SPLIT_BRAIN_DETECTED event
                leaders_detected = [p.node_id for p in other_leaders] + [self.node_id]
                await self._emit_split_brain_detected(
                    detected_leaders=leaders_detected,
                    resolution_action="step_down_non_voter",
                )
                # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                self._set_leader(None, reason="split_brain_non_voter", save_state=True)
                self.leader_lease_id = ""
                self.leader_lease_expires = 0.0
                self._release_voter_grant_if_self()
                return True

            leased_leader = await self._determine_leased_leader_from_voters()
            if leased_leader and leased_leader != self.node_id:
                print(
                    f"[P2P] SPLIT-BRAIN resolved by voter quorum: stepping down for lease-holder {leased_leader}"
                )
                # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
                self._set_leader(leased_leader, reason="split_brain_voter_quorum", save_state=True)
                self.leader_lease_id = ""
                self.leader_lease_expires = 0.0
                self._release_voter_grant_if_self()
                return True

        # Find the highest-priority *eligible* leader (including ourselves).
        considered_leaders = other_leaders
        if voter_ids:
            # Prefer voter leaders when resolving conflicts; non-voter leaders
            # are treated as noise from older configs/versions.
            voter_leaders = [p for p in other_leaders if p.node_id in voter_ids]
            if voter_leaders:
                considered_leaders = voter_leaders

        eligible_leaders = [p for p in considered_leaders if self._is_leader_eligible(p, conflict_keys)]
        if self._is_leader_eligible(self.self_info, conflict_keys):
            eligible_leaders.append(self.self_info)

        # If none are eligible, fall back to bully ordering (best-effort).
        candidates = eligible_leaders or ([*considered_leaders, self.self_info])
        highest_leader = max(candidates, key=lambda p: p.node_id)

        if highest_leader.node_id != self.node_id:
            # We're not the highest-priority leader - step down
            logger.info(f"SPLIT-BRAIN detected! Found leaders: {[p.node_id for p in other_leaders]}")
            logger.info(f"Stepping down in favor of higher-priority leader: {highest_leader.node_id}")
            # December 2025: Emit SPLIT_BRAIN_DETECTED event
            leaders_detected = [p.node_id for p in other_leaders] + [self.node_id]
            await self._emit_split_brain_detected(
                detected_leaders=leaders_detected,
                resolution_action="step_down_bully",
            )
            # Jan 3, 2026: Use _set_leader() for atomic leadership assignment (Phase 4)
            self._set_leader(highest_leader.node_id, reason="split_brain_resolution", save_state=False)
            self.leader_lease_id = ""
            self.leader_lease_expires = 0.0
            # Jan 1, 2026: Phase 3B-C - notify voters before releasing local grant
            await self._notify_voters_lease_revoked()
            self._release_voter_grant_if_self()
            self._save_state()
            return True

        # We are the highest - other leaders should step down
        # Send coordinator message to assert our leadership
        logger.info(f"SPLIT-BRAIN detected! Asserting leadership over: {[p.node_id for p in other_leaders]}")

        # Emit SPLIT_BRAIN_DETECTED event for this case (asserting leadership)
        leaders_detected = [p.node_id for p in other_leaders] + [self.node_id]
        await self._emit_split_brain_detected(
            detected_leaders=leaders_detected,
            resolution_action="assert_leadership",
        )

        timeout = ClientTimeout(total=5)
        async with get_client_session(timeout) as session:
            for peer in other_leaders:
                try:
                    url = self._url_for_peer(peer, "/coordinator")
                    await session.post(
                        url,
                        json={
                            "leader_id": self.node_id,
                            "lease_id": self.leader_lease_id,
                            "lease_expires": self.leader_lease_expires,
                            "voter_node_ids": list(getattr(self, "voter_node_ids", []) or []),
                        },
                        headers=self._auth_headers(),
                    )
                except (aiohttp.ClientError, asyncio.TimeoutError, KeyError, IndexError, AttributeError):
                    pass  # Network errors expected during step-down notifications

        return False  # We remain leader

    async def _job_management_loop(self):
        """Manage jobs - leader coordinates cluster, all nodes handle local operations."""
        while self.running:
            try:
                # ==== DECENTRALIZED OPERATIONS (all nodes) ====
                # These run on every node to ensure cluster health even without a leader
                # Makes the cluster resilient to leader instability

                # Local data consolidation: merge siloed job DBs to main selfplay.db
                await self._consolidate_selfplay_data()

                # Local stuck job detection: each node monitors its own processes
                await self._check_local_stuck_jobs()

                # Local resource cleanup: handle disk/memory pressure independently
                await self._local_resource_cleanup()

                # Local job management: start/stop jobs based on node capacity
                await self._manage_local_jobs_decentralized()

                # Local GPU auto-scaling: optimize GPU utilization independently
                await self._local_gpu_auto_scale()

                # Leaderless training fallback: trigger local training if no leader for too long
                await self._check_local_training_fallback()

                # Emergency coordinator: if voter quorum unavailable for >5min, take leadership
                await self._check_emergency_coordinator_fallback()

                # P2P data sync: nodes can sync data directly without leader
                await self._p2p_data_sync()

                # P2P model sync: dedicated model distribution (more frequent)
                await self._p2p_model_sync()

                # P2P training DB sync: sync training databases for diversity
                await self._p2p_training_db_sync()

                # Gossip protocol: share state with random peers
                await self._gossip_state_to_peers()

                # Anti-entropy repair: periodic full state reconciliation
                await self._gossip_anti_entropy_repair()

                # ==== LEADER-ONLY OPERATIONS ====
                if self.role == NodeRole.LEADER:
                    # LEARNED LESSONS - Check for split-brain before acting as leader
                    if await self._check_and_resolve_split_brain():
                        # We stepped down, skip this cycle
                        await asyncio.sleep(JOB_CHECK_INTERVAL)
                        continue

                    await self._manage_cluster_jobs()
                    # Cluster rebalancing: migrate jobs from weak to powerful nodes
                    await self._check_cluster_balance()
                    # Phase 3: Check if training should be triggered automatically
                    await self._check_and_trigger_training()
                    # Phase 5: Check improvement cycles for automated training
                    await self._check_improvement_cycles()
                    # Cluster-wide stuck job detection (remote nodes)
                    await self._check_and_kill_stuck_jobs()
                    # Work queue rebalancing: assign queued work to idle nodes
                    await self._auto_rebalance_from_work_queue()
                    # Self-healing: auto-scale GPU utilization toward 60-80% target
                    await self._auto_scale_gpu_utilization()
                    # Self-healing: probe NAT-blocked peers to check if they've become reachable
                    await self._sweep_nat_recovery()
                    # Self-healing: detect and recover stuck nodes via SSH restart
                    await self._check_node_recovery()
            except Exception as e:  # noqa: BLE001
                logger.info(f"Job management error: {e}")

            await asyncio.sleep(JOB_CHECK_INTERVAL)

    async def _check_and_kill_stuck_jobs(self) -> int:
        """Detect and terminate stuck training/selfplay jobs.

        A job is considered stuck if:
        - Training: No log output for 10+ minutes while process still running
        - Selfplay: No new games generated for 15+ minutes

        Returns:
            Number of stuck jobs terminated
        """
        killed = 0
        now = time.time()
        TRAINING_STUCK_THRESHOLD = 600  # 10 minutes
        SELFPLAY_STUCK_THRESHOLD = 900  # 15 minutes

        # Check training jobs
        with self.training_lock:
            training_snapshot = list(self.training_jobs.values())

        for job in training_snapshot:
            if job.status != "running":
                continue
            started = getattr(job, "started_at", 0) or 0
            last_progress = getattr(job, "last_progress_time", started) or started
            if now - last_progress > TRAINING_STUCK_THRESHOLD and now - started > TRAINING_STUCK_THRESHOLD:
                logger.info(f"STUCK DETECTED: Training job {job.job_id} on {job.target_node} - no progress for {int((now - last_progress)/60)}min")
                # Try to kill the process on the target node
                target_node = job.target_node
                if target_node and target_node != self.node_id:
                    await self._remote_kill_stuck_job(target_node, job.job_id, "training")
                else:
                    # Local kill
                    try:
                        import subprocess
                        subprocess.run(["pkill", "-9", "-f", f"train.*{job.job_id}"], timeout=5, capture_output=True)
                    except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError, KeyError, IndexError, AttributeError, ImportError):
                        pass
                job.status = "failed"
                job.error_message = "Killed: no progress detected"
                job.completed_at = now
                killed += 1
                logger.info(f"Killed stuck training job {job.job_id}")
                # ALERTING: Notify when stuck job is killed
                asyncio.create_task(self.notifier.send(
                    title="Stuck Job Killed",
                    message=f"Training job {job.job_id} killed after no progress for {int((now - last_progress)/60)}min",
                    level="warning",
                    fields={
                        "Job ID": job.job_id,
                        "Type": job.job_type,
                        "Node": job.target_node or "local",
                        "Config": f"{job.board_type}_{job.num_players}p",
                        "Stuck For": f"{int((now - last_progress)/60)} minutes",
                    },
                    node_id=self.node_id,
                ))

        # Check for GPU nodes with 0% GPU but running GPU jobs
        with self.peers_lock:
            peers_snapshot = list(self.peers.values())

        for peer in peers_snapshot:
            if not peer.is_alive():
                continue
            gpu_percent = float(getattr(peer, "gpu_percent", 0) or 0)
            selfplay_jobs = int(getattr(peer, "selfplay_jobs", 0) or 0)
            has_gpu = bool(getattr(peer, "has_gpu", False))

            # Check for stuck GPU selfplay (has GPU, jobs running, but 0% GPU util)
            if has_gpu and selfplay_jobs > 0 and gpu_percent == 0:
                last_gpu_active = getattr(peer, "_last_gpu_active_time", 0)
                if last_gpu_active == 0:
                    peer._last_gpu_active_time = now
                elif now - last_gpu_active > SELFPLAY_STUCK_THRESHOLD:
                    logger.info(f"STUCK DETECTED: {peer.node_id} has {selfplay_jobs} jobs but 0% GPU for {int((now - last_gpu_active)/60)}min")
                    # Don't auto-kill selfplay, just log - might be CPU selfplay
            elif has_gpu and gpu_percent > 5:
                peer._last_gpu_active_time = now

        if killed > 0:
            logger.info(f"Self-healing: killed {killed} stuck job(s)")
        return killed

    async def _check_local_stuck_jobs(self) -> int:
        """DECENTRALIZED: Detect and kill stuck processes on THIS node only.

        Runs on ALL nodes (not just leader) to ensure each node can self-heal
        even when there's no functioning leader in the cluster.

        Detects:
        - GPU selfplay processes with 0% GPU utilization for too long
        - Training processes that haven't made progress
        - Orphaned processes that aren't tracked in local_jobs

        Returns:
            Number of stuck processes terminated
        """
        killed = 0
        now = time.time()
        STUCK_THRESHOLD = 900  # 15 minutes

        # Only check periodically to avoid excessive process scanning
        last_check = getattr(self, "_last_local_stuck_check", 0)
        if now - last_check < 300:  # Check every 5 minutes
            return 0
        self._last_local_stuck_check = now

        # Check if local GPU is at 0% but we have running GPU selfplay jobs
        gpu_percent = float(getattr(self.self_info, "gpu_percent", 0) or 0)
        selfplay_jobs = int(getattr(self.self_info, "selfplay_jobs", 0) or 0)
        has_gpu = bool(getattr(self.self_info, "has_gpu", False))

        if has_gpu and selfplay_jobs > 0 and gpu_percent < 5:
            last_gpu_active = getattr(self, "_local_last_gpu_active", 0)
            if last_gpu_active == 0:
                self._local_last_gpu_active = now
            elif now - last_gpu_active > STUCK_THRESHOLD:
                logger.info(f"LOCAL STUCK: {selfplay_jobs} selfplay jobs but {gpu_percent:.0f}% GPU for {int((now - last_gpu_active)/60)}min")
                # Kill all local GPU selfplay processes and let them restart
                try:
                    # Use asyncio.to_thread to avoid blocking event loop (fix Dec 2025)
                    returncode, _, _ = await asyncio.to_thread(
                        self._run_subprocess_sync, ["pkill", "-9", "-f", "gpu_selfplay"], 10
                    )
                    if returncode == 0:
                        killed += 1
                        logger.info("LOCAL: Killed stuck GPU selfplay processes")
                        # Clear job tracking so they restart
                        with self.jobs_lock:
                            gpu_jobs = [jid for jid, job in self.local_jobs.items()
                                       if "gpu" in str(getattr(job, "job_type", "")).lower()]
                            for jid in gpu_jobs:
                                del self.local_jobs[jid]
                        self._local_last_gpu_active = now
                except Exception as e:  # noqa: BLE001
                    logger.info(f"LOCAL: Failed to kill stuck processes: {e}")
        elif has_gpu and gpu_percent >= 5:
            self._local_last_gpu_active = now

        # Check for orphaned selfplay processes (processes running but not in job tracking)
        try:
            # Count actual selfplay processes
            # Use asyncio.to_thread to avoid blocking event loop (fix Dec 2025)
            # Jan 12, 2026: Use Python subprocess to exclude non-Python processes from count
            # The old pgrep -fc "selfplay|gpu_selfplay" was counting SSH dispatcher processes
            # and shell wrappers (zsh, bash) from Claude commands
            def _count_local_selfplay_processes() -> int:
                """Count local selfplay processes, excluding SSH dispatchers and shells."""
                try:
                    # Get all selfplay-related PIDs
                    result = subprocess.run(
                        ["pgrep", "-f", "selfplay|gpu_selfplay"],
                        capture_output=True,
                        text=True,
                        timeout=5,
                    )
                    if result.returncode != 0 or not result.stdout.strip():
                        return 0
                    all_pids = set(result.stdout.strip().split())

                    # Get PIDs to exclude (SSH and shell processes)
                    excluded_pids: set[str] = set()
                    for pattern in ("^ssh", "ssh ", "/bin/zsh", "/bin/bash", "/bin/sh"):
                        try:
                            exclude_result = subprocess.run(
                                ["pgrep", "-f", pattern],
                                capture_output=True,
                                text=True,
                                timeout=5,
                            )
                            if exclude_result.returncode == 0 and exclude_result.stdout.strip():
                                excluded_pids.update(exclude_result.stdout.strip().split())
                        except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError):
                            pass

                    # Return count excluding non-Python processes
                    return len(all_pids - excluded_pids)
                except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError):
                    return 0

            actual_processes = await asyncio.to_thread(_count_local_selfplay_processes)

            with self.jobs_lock:
                tracked_jobs = len(self.local_jobs)

            # If we have way more processes than tracked jobs, we have orphans
            if actual_processes > tracked_jobs + 10:
                last_orphan_check = getattr(self, "_last_orphan_kill", 0)
                if now - last_orphan_check > 3600:  # Max once per hour
                    logger.info(f"LOCAL: Orphan detection: {actual_processes} processes vs {tracked_jobs} tracked")
                    # Don't auto-kill orphans yet, just warn
                    # Could add aggressive cleanup here if needed
                    self._last_orphan_kill = now
        except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError, ValueError, KeyError, IndexError, AttributeError, ImportError):
            pass

        if killed > 0:
            logger.info(f"LOCAL self-healing: terminated {killed} stuck process(es)")
        return killed

    async def _remote_kill_stuck_job(self, target_node: str, job_id: str, job_type: str) -> bool:
        """Send kill command to remote node for stuck job."""
        with self.peers_lock:
            peer = self.peers.get(target_node)
        if not peer or not peer.is_alive():
            return False

        try:
            timeout = ClientTimeout(total=10)
            async with get_client_session(timeout) as session:
                url = self._url_for_peer(peer, "/job/kill")
                payload = {"job_id": job_id, "job_type": job_type, "reason": "stuck"}
                async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                    return resp.status == 200
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to kill stuck job on {target_node}: {e}")
            return False

    async def _manage_local_jobs_decentralized(self) -> int:
        """DECENTRALIZED: Each node manages its own job count based on gossip state.

        Runs on ALL nodes to ensure selfplay continues even during leader elections.
        Each node autonomously:
        1. Checks its own resource pressure (disk, memory, CPU)
        2. Uses gossip state to calculate proportional job count
        3. Starts or stops local jobs as needed

        PHASE 3 DECENTRALIZATION (Dec 2025):
        - With Serf providing reliable failure detection, we can act quickly
        - Proportional allocation based on gossip cluster capacity
        - 30-second timeout for faster leader-failure recovery

        Returns:
            Number of jobs started/stopped
        """
        changes = 0
        now = time.time()

        # Rate limit: check every 30 seconds (reduced from 60s for faster response)
        last_check = getattr(self, "_last_local_job_manage", 0)
        if now - last_check < 30:
            return 0
        self._last_local_job_manage = now

        # Skip if leader is managing (avoid conflicts)
        # But continue if leaderless for > 30 seconds (reduced from 60s for Serf reliability)
        # Dec 30, 2025: Also allow self-assignment if leader exists but isn't dispatching work
        if self.role == NodeRole.LEADER:
            return 0  # Leader uses centralized management
        if self.leader_id:
            leaderless_duration = now - getattr(self, "last_leader_seen", now)
            work_dispatch_gap = now - getattr(self, "last_work_from_leader", now)

            # Defer to leader only if BOTH conditions are met:
            # 1. Leader was seen recently (alive)
            # 2. Leader has been dispatching work recently (active)
            if leaderless_duration < LEADERLESS_TRAINING_TIMEOUT:
                if work_dispatch_gap < LEADER_WORK_DISPATCH_TIMEOUT:
                    return 0  # Have a functioning leader that's actively dispatching
                else:
                    # Leader is present but not dispatching work - allow self-assignment
                    logger.info(
                        f"LOCAL: Leader present but no work dispatched in {work_dispatch_gap:.0f}s "
                        f"(timeout={LEADER_WORK_DISPATCH_TIMEOUT}s) - self-assigning"
                    )

        # Update self info
        self._update_self_info()
        node = self.self_info

        # Check resource pressure - don't start jobs if under pressure
        if node.disk_percent >= DISK_WARNING_THRESHOLD:
            logger.info(f"LOCAL: Disk at {node.disk_percent:.0f}% - skipping job starts")
            await self._cleanup_local_disk()
            return 0

        if node.memory_percent >= MEMORY_WARNING_THRESHOLD:
            logger.info(f"LOCAL: Memory at {node.memory_percent:.0f}% - skipping job starts")
            return 0

        # Calculate target jobs for this node (delegated to SelfplayScheduler Dec 2025)
        target_selfplay = self.selfplay_scheduler.get_target_jobs_for_node(node)
        current_jobs = int(getattr(node, "selfplay_jobs", 0) or 0)

        # Dec 30, 2025: Cluster-aware job balancing
        # Prevent job concentration on a few nodes by checking cluster average
        # Skip spawning if we're already above average + 2 to give idle nodes a chance
        try:
            # Jan 10, 2026: Copy-on-read pattern - minimize lock hold time
            with self.peers_lock:
                peers_snapshot = list(self.peers.values())
            # Compute job counts outside lock (is_alive() can be slow)
            peer_job_counts = [
                int(getattr(p, "selfplay_jobs", 0) or 0)
                for p in peers_snapshot
                if p.is_alive() and hasattr(p, "selfplay_jobs")
            ]
            if peer_job_counts:
                avg_jobs = sum(peer_job_counts) / len(peer_job_counts)
                if current_jobs > avg_jobs + 2:
                    logger.info(
                        f"LOCAL: Skipping job spawn - {current_jobs} jobs > cluster avg {avg_jobs:.1f}+2 "
                        f"(letting underutilized nodes catch up)"
                    )
                    return 0
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Cluster balance check error: {e}")

        # Start jobs if below target
        if current_jobs < target_selfplay:
            needed = min(target_selfplay - current_jobs, 3)  # Max 3 per cycle
            logger.info(f"LOCAL: Starting {needed} selfplay job(s) ({current_jobs}/{target_selfplay})")

            # Dec 27, 2025: Generate batch ID and emit BATCH_SCHEDULED
            batch_id = f"selfplay_{self.node_id}_{int(time.time())}"
            first_config = self.selfplay_scheduler.pick_weighted_config(node)
            config_key = f"{first_config['board_type']}_{first_config['num_players']}p" if first_config else "mixed"
            await self._emit_batch_scheduled(
                batch_id=batch_id,
                batch_type="selfplay",
                config_key=config_key,
                job_count=needed,
                target_nodes=[self.node_id],
                reason="local_job_management",
            )

            jobs_dispatched = 0
            jobs_failed = 0

            # Jan 2026 fix: Detect GPU tier for proper job type selection
            # High-end GPUs (GH200, H100, H200, A100, 5090, 4090) get GPU_SELFPLAY/GUMBEL_SELFPLAY
            # Mid-tier GPUs get HYBRID_SELFPLAY (CPU rules + GPU eval)
            gpu_name_raw = getattr(node, "gpu_name", "") or ""
            gpu_name = gpu_name_raw.upper()
            has_gpu = bool(getattr(node, "has_gpu", False))

            # Session 17.50: YAML fallback when runtime GPU detection fails
            # Runtime detection can fail on some nodes (vGPU, containers, driver issues)
            if not has_gpu or not gpu_name:
                yaml_has_gpu, yaml_gpu_name, yaml_vram = self._check_yaml_gpu_config()
                if yaml_has_gpu:
                    logger.info(
                        f"LOCAL: Runtime GPU detection failed but YAML shows GPU "
                        f"({yaml_gpu_name}, {yaml_vram}GB). Using YAML config."
                    )
                    has_gpu = True
                    gpu_name_raw = yaml_gpu_name
                    gpu_name = yaml_gpu_name.upper()

            is_high_end_gpu = any(tag in gpu_name for tag in ("H100", "H200", "GH200", "A100", "5090", "4090"))
            is_apple_gpu = "MPS" in gpu_name or "APPLE" in gpu_name

            # Log GPU tier detection for visibility
            if has_gpu and is_high_end_gpu:
                logger.info(f"LOCAL: High-end GPU detected ({gpu_name_raw}) - using GPU/Gumbel selfplay")
            elif has_gpu and not is_apple_gpu:
                logger.info(f"LOCAL: Mid-tier GPU detected ({gpu_name_raw}) - using hybrid selfplay")

            import random  # Import once outside loop

            for _ in range(needed):
                try:
                    # Pick a config weighted by priority (using SelfplayScheduler manager)
                    config = self.selfplay_scheduler.pick_weighted_config(node)
                    if config:
                        # Jan 2026: Select job type based on GPU tier (consistent with cluster dispatch)
                        if has_gpu and is_high_end_gpu and not is_apple_gpu:
                            # High-end GPUs: 50% GUMBEL (quality) / 50% GPU_SELFPLAY (volume)
                            if random.random() < 0.5:
                                job_type = JobType.GUMBEL_SELFPLAY
                                engine_mode = "gumbel-mcts"
                            else:
                                job_type = JobType.GPU_SELFPLAY
                                engine_mode = "gpu"
                        elif has_gpu and not is_apple_gpu:
                            # Mid-tier GPUs: HYBRID mode for rule fidelity
                            job_type = JobType.HYBRID_SELFPLAY
                            engine_mode = "mixed"
                        else:
                            # CPU-only or Apple MPS: CPU selfplay
                            job_type = JobType.SELFPLAY
                            engine_mode = config.get("engine_mode", "gumbel-mcts")

                        job = await self._start_local_job(
                            job_type,
                            board_type=config["board_type"],
                            num_players=config["num_players"],
                            engine_mode=engine_mode,
                        )
                        if job:
                            changes += 1
                            jobs_dispatched += 1
                        else:
                            jobs_failed += 1
                except Exception as e:  # noqa: BLE001
                    logger.info(f"LOCAL: Failed to start selfplay: {e}")
                    jobs_failed += 1
                    break

            # Dec 27, 2025: Emit BATCH_DISPATCHED after loop completes
            await self._emit_batch_dispatched(
                batch_id=batch_id,
                batch_type="selfplay",
                config_key=config_key,
                jobs_dispatched=jobs_dispatched,
                jobs_failed=jobs_failed,
                target_nodes=[self.node_id],
            )

        # Stop jobs if way over target (2x or more)
        elif current_jobs > target_selfplay * 2:
            excess = current_jobs - target_selfplay
            logger.info(f"LOCAL: Reducing selfplay jobs by {excess} ({current_jobs}/{target_selfplay})")
            await self._reduce_local_selfplay_jobs(target_selfplay, reason="over_target")
            changes += excess

        if changes > 0:
            logger.info(f"LOCAL job management: {changes} change(s)")
        return changes

    async def _local_gpu_auto_scale(self) -> int:
        """DECENTRALIZED: Each GPU node manages its own GPU utilization.

        Runs on ALL GPU nodes to ensure optimal GPU usage without leader.
        Targets 60-80% GPU utilization by starting/stopping GPU selfplay jobs.

        Returns:
            Number of GPU jobs started
        """
        started = 0
        now = time.time()

        # Rate limit: check every 2 minutes
        last_check = getattr(self, "_last_local_gpu_scale", 0)
        if now - last_check < 120:
            return 0
        self._last_local_gpu_scale = now

        # Skip if not a GPU node
        if not getattr(self.self_info, "has_gpu", False):
            return 0

        # Skip if training is running (training uses GPU)
        training_jobs = int(getattr(self.self_info, "training_jobs", 0) or 0)
        if training_jobs > 0:
            return 0

        # Skip if memory is critical (consistent with _local_selfplay_management)
        memory_percent = float(getattr(self.self_info, "memory_percent", 0) or 0)
        if memory_percent >= MEMORY_WARNING_THRESHOLD:
            logger.info(f"LOCAL: Memory at {memory_percent:.0f}% - skipping GPU auto-scale")
            return 0

        # DECENTRALIZED: Always allow local GPU scaling
        # Leader manages cluster-wide coordination, but each node optimizes its own GPU
        # Use smaller batches when leader is present to avoid conflicts
        has_leader = bool(self.leader_id or self.role == NodeRole.LEADER)
        max_jobs_per_cycle = 1 if has_leader else 3

        TARGET_GPU_MIN = 60.0
        TARGET_GPU_MAX = 80.0
        MIN_IDLE_TIME = 120 if has_leader else 60  # Faster response when leaderless

        gpu_percent = float(getattr(self.self_info, "gpu_percent", 0) or 0)
        int(getattr(self.self_info, "selfplay_jobs", 0) or 0)
        gpu_name = (getattr(self.self_info, "gpu_name", "") or "").lower()

        # Session 17.50: YAML fallback when runtime GPU detection fails
        if not gpu_name:
            yaml_has_gpu, yaml_gpu_name, _ = self._check_yaml_gpu_config()
            if yaml_has_gpu and yaml_gpu_name:
                gpu_name = yaml_gpu_name.lower()
                logger.debug(f"LOCAL: Using YAML GPU name: {yaml_gpu_name}")

        # Track GPU idle time
        if gpu_percent < TARGET_GPU_MIN:
            idle_since = getattr(self, "_local_gpu_idle_since", 0)
            if idle_since == 0:
                self._local_gpu_idle_since = now
            elif now - idle_since > MIN_IDLE_TIME:
                # Calculate new jobs to add
                gpu_headroom = TARGET_GPU_MAX - gpu_percent
                is_high_end_gpu = any(tag in gpu_name for tag in ("h100", "h200", "gh200", "5090", "a100", "4090"))
                if any(tag in gpu_name for tag in ("h100", "h200", "gh200", "5090")):
                    jobs_per_10_percent = 2
                elif any(tag in gpu_name for tag in ("a100", "4090", "3090")):
                    jobs_per_10_percent = 1.5
                else:
                    jobs_per_10_percent = 1

                new_jobs = max(1, int(gpu_headroom / 10 * jobs_per_10_percent))
                new_jobs = min(new_jobs, max_jobs_per_cycle)  # Cap based on leader presence

                # Dec 2025 fix: Use GUMBEL/GPU_SELFPLAY for high-end GPUs
                job_type_str = "GUMBEL/GPU" if is_high_end_gpu else "diverse/hybrid"
                logger.info(f"LOCAL: {gpu_percent:.0f}% GPU util, starting {new_jobs} {job_type_str} selfplay job(s)")

                # Dec 27, 2025: Generate batch ID and emit BATCH_SCHEDULED
                batch_id = f"gpu_selfplay_{self.node_id}_{int(time.time())}"
                first_config = self.selfplay_scheduler.pick_weighted_config(self.self_info)
                config_key = f"{first_config['board_type']}_{first_config['num_players']}p" if first_config else "mixed"
                await self._emit_batch_scheduled(
                    batch_id=batch_id,
                    batch_type="selfplay",
                    config_key=config_key,
                    job_count=new_jobs,
                    target_nodes=[self.node_id],
                    reason="gpu_auto_scale",
                )

                gpu_jobs_dispatched = 0
                gpu_jobs_failed = 0
                for _ in range(new_jobs):
                    try:
                        config = self.selfplay_scheduler.pick_weighted_config(self.self_info)
                        if config:
                            # Dec 2025 fix: Select job type based on GPU capabilities
                            if is_high_end_gpu:
                                # High-end GPUs: 50% GUMBEL (quality) / 50% GPU_SELFPLAY (volume)
                                import random
                                if random.random() < 0.5:
                                    job_type = JobType.GUMBEL_SELFPLAY
                                    engine_mode = "gumbel-mcts"
                                else:
                                    job_type = JobType.GPU_SELFPLAY
                                    engine_mode = "gpu"
                            else:
                                # Mid-tier GPUs: HYBRID mode for rule fidelity
                                job_type = JobType.HYBRID_SELFPLAY
                                engine_mode = "mixed"

                            job = await self._start_local_job(
                                job_type,
                                board_type=config["board_type"],
                                num_players=config["num_players"],
                                engine_mode=engine_mode,
                            )
                            if job:
                                started += 1
                                gpu_jobs_dispatched += 1
                            else:
                                gpu_jobs_failed += 1
                    except Exception as e:  # noqa: BLE001
                        logger.info(f"LOCAL: Failed to start selfplay: {e}")
                        gpu_jobs_failed += 1
                        break

                # Dec 27, 2025: Emit BATCH_DISPATCHED after loop completes
                await self._emit_batch_dispatched(
                    batch_id=batch_id,
                    batch_type="selfplay",
                    config_key=config_key,
                    jobs_dispatched=gpu_jobs_dispatched,
                    jobs_failed=gpu_jobs_failed,
                    target_nodes=[self.node_id],
                )

                self._local_gpu_idle_since = now  # Reset after action
        else:
            self._local_gpu_idle_since = 0  # GPU is busy, reset

        if started > 0:
            logger.info(f"LOCAL GPU auto-scale: started {started} job(s)")
        return started

    async def _local_resource_cleanup(self):
        """DECENTRALIZED: Each node handles its own resource pressure.

        Runs on ALL nodes to ensure resource cleanup without leader coordination.
        Handles disk cleanup, memory pressure, and log rotation.
        """
        now = time.time()

        # Rate limit: check every 5 minutes
        last_check = getattr(self, "_last_local_resource_check", 0)
        if now - last_check < 300:
            return
        self._last_local_resource_check = now

        self._update_self_info()
        node = self.self_info

        # Disk cleanup
        if node.disk_percent >= DISK_CLEANUP_THRESHOLD:
            logger.info(f"LOCAL: Disk at {node.disk_percent:.0f}% - triggering cleanup")
            await self._cleanup_local_disk()

        # Memory pressure - reduce jobs and clear caches
        if node.memory_percent >= MEMORY_CRITICAL_THRESHOLD:
            logger.warning(f"LOCAL: Memory CRITICAL at {node.memory_percent:.0f}% - emergency cleanup")
            await self._reduce_local_selfplay_jobs(0, reason="memory_critical")
            # Jan 10, 2026: Clear gossip caches to free memory
            self._emergency_memory_cleanup()
        elif node.memory_percent >= MEMORY_WARNING_THRESHOLD:
            current = int(getattr(node, "selfplay_jobs", 0) or 0)
            target = max(1, current // 2)
            logger.info(f"LOCAL: Memory warning at {node.memory_percent:.0f}% - reducing jobs to {target}")
            await self._reduce_local_selfplay_jobs(target, reason="memory_warning")

        # Clean up old completed/failed jobs to prevent memory leak
        self.job_manager.cleanup_completed_jobs()

    # NOTE: _cleanup_old_completed_jobs() removed Dec 2025 (31 LOC).
    # Use self.job_manager.cleanup_completed_jobs() directly.

    # NOTE: _get_elo_based_priority_boost() removed Dec 2025 (~45 LOC).
    # Use self.selfplay_scheduler.get_elo_based_priority_boost() instead.

    # NOTE: _pick_weighted_selfplay_config() removed Dec 2025 (95 LOC).
    # Use self.selfplay_scheduler.pick_weighted_config() instead.
    # See scripts/p2p/managers/selfplay_scheduler.py for implementation.

    async def _auto_scale_gpu_utilization(self) -> int:
        """Auto-scale selfplay jobs to reach 60-80% GPU utilization.

        Detects underutilized GPU nodes and starts selfplay jobs to improve
        cluster throughput while maintaining game quality and rule fidelity.

        Dec 2025 fix: Job type is selected based on GPU capabilities:
        - High-end GPUs (GH200, H100, A100, 5090, 4090): 50% GUMBEL / 50% GPU_SELFPLAY
        - Mid-tier GPUs: HYBRID mode (CPU rules + GPU eval) for rule fidelity

        Returns:
            Number of new selfplay jobs started
        """
        TARGET_GPU_MIN = 60.0  # Target minimum GPU utilization
        TARGET_GPU_MAX = 80.0  # Target maximum GPU utilization
        MIN_IDLE_TIME = 120    # Seconds of low GPU before scaling up

        started = 0
        now = time.time()

        # Rate limit auto-scaling (once per 2 minutes)
        last_scale = getattr(self, "_last_gpu_auto_scale", 0)
        if now - last_scale < 120:
            return 0

        with self.peers_lock:
            peers_snapshot = list(self.peers.values())

        underutilized_gpu_nodes = []

        # Load policy manager for filtering
        policy_manager = None
        try:
            from app.coordination.node_policies import get_policy_manager
            policy_manager = get_policy_manager()
        except ImportError:
            pass

        for peer in peers_snapshot:
            if not peer.is_alive():
                continue
            has_gpu = bool(getattr(peer, "has_gpu", False))
            if not has_gpu:
                continue

            # Policy check: skip nodes that don't allow selfplay
            if policy_manager and not policy_manager.is_work_allowed(peer.node_id, "selfplay"):
                continue

            gpu_percent = float(getattr(peer, "gpu_percent", 0) or 0)
            gpu_name = (getattr(peer, "gpu_name", "") or "").lower()
            selfplay_jobs = int(getattr(peer, "selfplay_jobs", 0) or 0)
            training_jobs = int(getattr(peer, "training_jobs", 0) or 0)

            # Skip if already training
            if training_jobs > 0:
                continue

            # Check if underutilized
            if gpu_percent < TARGET_GPU_MIN:
                # Track how long it's been underutilized
                idle_key = f"_gpu_idle_since_{peer.node_id}"
                idle_since = getattr(self, idle_key, 0)
                if idle_since == 0:
                    setattr(self, idle_key, now)
                elif now - idle_since > MIN_IDLE_TIME:
                    # Calculate how many more jobs to add
                    gpu_headroom = TARGET_GPU_MAX - gpu_percent
                    # Estimate jobs based on GPU tier
                    if any(tag in gpu_name for tag in ("h100", "h200", "gh200", "5090")):
                        jobs_per_10_percent = 2
                    elif any(tag in gpu_name for tag in ("a100", "4090", "3090")):
                        jobs_per_10_percent = 1.5
                    else:
                        jobs_per_10_percent = 1

                    new_jobs = max(1, int(gpu_headroom / 10 * jobs_per_10_percent))
                    new_jobs = min(new_jobs, 4)  # Cap at 4 new jobs per cycle

                    underutilized_gpu_nodes.append({
                        "node_id": peer.node_id,
                        "gpu_percent": gpu_percent,
                        "gpu_name": gpu_name,
                        "current_jobs": selfplay_jobs,
                        "new_jobs": new_jobs,
                    })
            else:
                # GPU is utilized, reset idle timer
                idle_key = f"_gpu_idle_since_{peer.node_id}"
                setattr(self, idle_key, 0)

        # Start GPU selfplay on underutilized nodes
        for node_info in underutilized_gpu_nodes[:3]:  # Max 3 nodes per cycle
            node_id = node_info["node_id"]
            new_jobs = node_info["new_jobs"]

            gpu_name = (node_info.get("gpu_name", "") or "").upper()
            is_high_end = any(tag in gpu_name for tag in ("H100", "H200", "GH200", "A100", "5090", "4090"))
            job_type_str = "GUMBEL/GPU" if is_high_end else "diverse/hybrid"
            print(
                f"[P2P] Auto-scale: {node_id} at {node_info['gpu_percent']:.0f}% GPU, "
                f"starting {new_jobs} {job_type_str} selfplay job(s)"
            )

            for _ in range(new_jobs):
                try:
                    # Schedule selfplay job (type selected by _schedule_diverse_selfplay_on_node)
                    job = await self._schedule_diverse_selfplay_on_node(node_id)
                    if job:
                        started += 1
                except Exception as e:  # noqa: BLE001
                    logger.error(f"Failed to start diverse selfplay on {node_id}: {e}")
                    break

        if started > 0:
            self._last_gpu_auto_scale = now
            logger.info(f"Auto-scale: started {started} new diverse/hybrid selfplay job(s)")

        return started

    async def _auto_rebalance_from_work_queue(self) -> int:
        """Auto-rebalance: assign queued work to idle GPU nodes.

        When idle GPU-heavy nodes are detected, check the work queue for pending
        high-priority work and dispatch it. This ensures queued work gets done
        before falling back to selfplay auto-scaling.

        Returns:
            Number of work items dispatched
        """
        GPU_IDLE_THRESHOLD = 10.0  # Node is idle if GPU < 10%
        MIN_IDLE_TIME = 60  # Seconds of idle before assigning work
        GPU_HEAVY_TAGS = ['gh200', 'h100', 'h200', 'a100', '4090', '5090']

        dispatched = 0
        now = time.time()

        # Rate limit rebalancing (once per minute)
        last_rebalance = getattr(self, "_last_work_queue_rebalance", 0)
        if now - last_rebalance < 60:
            return 0

        # Check if work queue is available
        wq = get_work_queue()
        if wq is None:
            return 0

        # Get queue status
        queue_status = wq.get_queue_status()
        pending_count = queue_status.get("by_status", {}).get("pending", 0)
        if pending_count == 0:
            return 0  # No work to dispatch

        # Find idle GPU-heavy nodes
        idle_nodes = []

        with self.peers_lock:
            peers_snapshot = list(self.peers.values())

        for peer in peers_snapshot:
            if not peer.is_alive() or peer.retired:
                continue

            has_gpu = bool(getattr(peer, "has_gpu", False))
            if not has_gpu:
                continue

            gpu_name = (getattr(peer, "gpu_name", "") or "").upper()
            is_gpu_heavy = any(tag.upper() in gpu_name for tag in GPU_HEAVY_TAGS)
            if not is_gpu_heavy:
                continue

            gpu_percent = float(getattr(peer, "gpu_percent", 0) or 0)
            training_jobs = int(getattr(peer, "training_jobs", 0) or 0)
            selfplay_jobs = int(getattr(peer, "selfplay_jobs", 0) or 0)

            # Skip if already busy
            if training_jobs > 0:
                continue

            # Check if truly idle
            if gpu_percent < GPU_IDLE_THRESHOLD:
                # Track how long it's been idle
                idle_key = f"_wq_idle_since_{peer.node_id}"
                idle_since = getattr(self, idle_key, 0)
                if idle_since == 0:
                    setattr(self, idle_key, now)
                elif now - idle_since > MIN_IDLE_TIME:
                    # Get allowed work types for this node
                    try:
                        from app.coordination.node_policies import get_policy_manager
                        pm = get_policy_manager()
                        allowed = list(pm.get_allowed_work_types(peer.node_id))
                    except ImportError:
                        allowed = ["training", "gpu_cmaes", "tournament", "selfplay"]

                    idle_nodes.append({
                        "node_id": peer.node_id,
                        "peer": peer,
                        "gpu_percent": gpu_percent,
                        "gpu_name": gpu_name,
                        "allowed": allowed,
                        "selfplay_jobs": selfplay_jobs,
                    })
            else:
                # Not idle, reset timer
                idle_key = f"_wq_idle_since_{peer.node_id}"
                setattr(self, idle_key, 0)

        # Dispatch work to idle nodes
        for node_info in idle_nodes[:5]:  # Max 5 nodes per cycle
            node_id = node_info["node_id"]
            allowed = node_info["allowed"]

            # Try to claim work for this node using Raft or SQLite based on consensus mode
            # claim_work_distributed() returns a dict with work_id, work_type, config, etc.
            work_item = self.claim_work_distributed(node_id, allowed)
            if work_item is None:
                continue

            # Get work_type - may be string or WorkType enum
            work_type_str = work_item.get("work_type", "unknown")
            if hasattr(work_type_str, "value"):
                work_type_str = work_type_str.value
            work_id = work_item.get("work_id", "unknown")

            print(
                f"[P2P] Work queue rebalance: {node_id} idle at {node_info['gpu_percent']:.0f}% GPU, "
                f"assigning {work_type_str} work ({work_id})"
            )

            # Dispatch work to the node
            success = await self._dispatch_queued_work(node_info["peer"], work_item)
            if success:
                # Mark work as started using distributed method for Raft consistency
                self.start_work_distributed(work_id)
                dispatched += 1
                # Reset idle timer since we assigned work
                idle_key = f"_wq_idle_since_{node_id}"
                setattr(self, idle_key, 0)
            else:
                # Failed to dispatch, reset work status for retry
                self.fail_work_distributed(work_id, "dispatch_failed")

        if dispatched > 0:
            self._last_work_queue_rebalance = now
            logger.info(f"Work queue rebalance: dispatched {dispatched} work item(s) to idle nodes")

        return dispatched

    async def _dispatch_queued_work(self, peer: NodeInfo, work_item: dict) -> bool:
        """Dispatch a work queue item to a specific node.

        Routes different work types to appropriate endpoints.

        Args:
            peer: Target node info
            work_item: Work item dict with work_id, work_type, config, etc.
                       (from claim_work_distributed)
        """
        from app.coordination.work_queue import WorkType

        try:
            timeout = ClientTimeout(total=30)
            async with get_client_session(timeout) as session:
                # Handle both dict and object formats for backward compatibility
                if isinstance(work_item, dict):
                    work_type = work_item.get("work_type")
                    config = work_item.get("config", {})
                    work_id = work_item.get("work_id")
                else:
                    work_type = work_item.work_type
                    config = work_item.config
                    work_id = work_item.work_id

                # Convert string work_type to enum if needed
                if isinstance(work_type, str):
                    try:
                        work_type = WorkType(work_type)
                    except ValueError:
                        logger.warning(f"Unknown work type string: {work_type}")
                        return False

                if work_type == WorkType.TRAINING:
                    # Route to training endpoint
                    url = self._url_for_peer(peer, "/start_job")
                    payload = {
                        "job_type": "training",
                        "board_type": config.get("board_type", "square8"),
                        "num_players": config.get("num_players", 2),
                        "work_id": work_id,
                    }
                elif work_type == WorkType.GPU_CMAES:
                    # Route to CMA-ES endpoint
                    url = self._url_for_peer(peer, "/cmaes/start")
                    payload = {
                        "board_type": config.get("board_type", "square8"),
                        "num_players": config.get("num_players", 2),
                        "work_id": work_id,
                    }
                elif work_type == WorkType.TOURNAMENT:
                    # Route to tournament endpoint
                    url = self._url_for_peer(peer, "/tournament/start")
                    payload = {
                        "board_type": config.get("board_type", "square8"),
                        "num_players": config.get("num_players", 2),
                        "work_id": work_id,
                    }
                elif work_type == WorkType.SELFPLAY:
                    # Route to selfplay endpoint
                    url = self._url_for_peer(peer, "/selfplay/start")
                    payload = {
                        "board_type": config.get("board_type", "square8"),
                        "num_players": config.get("num_players", 2),
                        "num_games": config.get("num_games", 500),
                        "work_id": work_id,
                    }
                else:
                    logger.warning(f"Unknown work type: {work_type}")
                    return False

                async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                    if resp.status == 200:
                        logger.info(f"Dispatched {work_type.value} work to {peer.node_id}")
                        return True
                    else:
                        error = await resp.text()
                        logger.warning(f"Failed to dispatch work to {peer.node_id}: {error}")
                        return False

        except Exception as e:  # noqa: BLE001
            logger.error(f"Error dispatching work to {peer.node_id}: {e}")
            return False

    async def _schedule_diverse_selfplay_on_node(self, node_id: str) -> dict | None:
        """Schedule a diverse selfplay job on a specific node.

        Job type is selected based on GPU capabilities (Dec 2025 fix):
        - High-end GPUs (GH200, H100, A100, 5090, 4090): 50% GUMBEL / 50% GPU_SELFPLAY
        - Mid-tier GPUs: HYBRID mode (CPU rules + GPU eval) for rule fidelity
        Rotates through all board/player configurations for diversity.
        """
        with self.peers_lock:
            peer = self.peers.get(node_id)
        if not peer or not peer.is_alive():
            return None

        # Policy check: ensure selfplay is allowed on this node
        try:
            from app.coordination.node_policies import is_work_allowed
            if not is_work_allowed(node_id, "selfplay"):
                logger.debug(f"Selfplay not allowed on {node_id} by policy")
                return None
        except ImportError:
            pass

        # Rotate through diverse configurations with priority-based weighting
        # Uses board_priority_overrides from config (0=CRITICAL, 1=HIGH, 2=MEDIUM, 3=LOW)
        board_priority_overrides = get_board_priority_overrides()

        diverse_configs = [
            ("hexagonal", 2), ("hexagonal", 3), ("hexagonal", 4),
            ("square19", 3), ("square19", 4), ("square19", 2),
            ("hex8", 2), ("hex8", 3), ("hex8", 4),
            ("square8", 3), ("square8", 4), ("square8", 2),
        ]

        # Build weighted list based on priority overrides
        # Lower priority value = more weight (0=CRITICAL gets 4x, 3=LOW gets 1x)
        weighted_configs = []
        for board_type, num_players in diverse_configs:
            config_key = f"{board_type}_{num_players}p"
            priority = board_priority_overrides.get(config_key, 3)  # Default LOW
            weight = 4 - priority  # 0->4, 1->3, 2->2, 3->1
            weighted_configs.extend([(board_type, num_players)] * weight)

        # Round-robin through weighted list based on node-specific counter
        counter_key = f"_diverse_config_counter_{node_id}"
        counter = getattr(self, counter_key, 0)
        setattr(self, counter_key, counter + 1)
        board_type, num_players = weighted_configs[counter % len(weighted_configs)]

        # Determine job type based on node GPU capabilities (Dec 2025 fix)
        # High-end GPUs should use GUMBEL_SELFPLAY (50%) or GPU_SELFPLAY (50%)
        # Mid-tier GPUs use HYBRID mode for 100% rule fidelity
        gpu_name = (peer.gpu_name or "").upper()
        has_gpu = bool(peer.has_gpu)

        # Session 17.50: YAML fallback when runtime GPU detection fails
        if not has_gpu or not gpu_name:
            yaml_has_gpu, yaml_gpu_name, _ = self._check_yaml_gpu_config(node_id)
            if yaml_has_gpu:
                has_gpu = True
                if yaml_gpu_name:
                    gpu_name = yaml_gpu_name.upper()

        is_high_end_gpu = any(tag in gpu_name for tag in ("H100", "H200", "GH200", "A100", "5090", "4090"))
        is_apple_gpu = "MPS" in gpu_name or "APPLE" in gpu_name

        if has_gpu and is_high_end_gpu and not is_apple_gpu:
            # High-end GPUs: 50% GUMBEL (quality) / 50% GPU_SELFPLAY (volume)
            import random
            if random.random() < 0.5:
                job_type = "gumbel_selfplay"
                engine_mode = "gumbel-mcts"
            else:
                job_type = "gpu_selfplay"
                engine_mode = "gpu"
        else:
            # Mid-tier or no GPU: HYBRID mode for rule fidelity
            job_type = "hybrid_selfplay"
            engine_mode = "mixed"

        try:
            timeout = ClientTimeout(total=30)
            async with get_client_session(timeout) as session:
                url = self._url_for_peer(peer, "/selfplay/start")
                payload = {
                    "board_type": board_type,
                    "num_players": num_players,
                    "num_games": 200,  # Smaller batches for diversity
                    "engine_mode": engine_mode,
                    "auto_scaled": True,
                    "job_type": job_type,
                }
                async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        logger.info(f"Started diverse selfplay on {node_id}: {board_type} {num_players}p")
                        return data
                    else:
                        error = await resp.text()
                        logger.info(f"Diverse selfplay start failed on {node_id}: {error}")
                        return None
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to schedule diverse selfplay on {node_id}: {e}")
            return None

    # Backward compatibility alias (GPU selfplay now redirects to diverse/hybrid)
    _schedule_gpu_selfplay_on_node = _schedule_diverse_selfplay_on_node

    # NOTE: _target_selfplay_jobs_for_node() removed Dec 2025 (160 LOC).
    # Use self.selfplay_scheduler.get_target_jobs_for_node() instead.

    # NOTE: _get_hybrid_job_targets() removed Dec 2025 (38 LOC).
    # Use self.selfplay_scheduler.get_hybrid_job_targets() instead.

    # NOTE: _should_spawn_cpu_only_jobs() removed Dec 2025 (33 LOC).
    # Use self.selfplay_scheduler.should_spawn_cpu_only_jobs() instead.


    async def _check_cluster_balance(self) -> dict[str, Any]:
        """Check and rebalance jobs across the cluster.

        This method identifies:
        1. Powerful nodes that are underutilized (high capacity, low jobs)
        2. Weak nodes that are overloaded (low capacity, high jobs)

        When imbalance is detected, it reduces jobs on weak nodes so the
        scheduler can assign them to more powerful nodes.

        Returns dict with rebalancing actions taken.
        """
        try:
            with self.peers_lock:
                alive_peers = [p for p in self.peers.values() if p.is_alive()]

            all_nodes = [*alive_peers, self.self_info]
            healthy_nodes = [n for n in all_nodes if n.is_healthy()]

            if len(healthy_nodes) < 2:
                return {"action": "none", "reason": "insufficient_nodes"}

            # Calculate capacity and utilization for each node
            node_stats = []
            for node in healthy_nodes:
                target = self.selfplay_scheduler.get_target_jobs_for_node(node)
                current = int(getattr(node, "selfplay_jobs", 0) or 0)
                utilization = current / max(1, target)  # How full is this node
                capacity_score = target  # Higher = more powerful

                node_stats.append({
                    "node": node,
                    "target": target,
                    "current": current,
                    "utilization": utilization,
                    "capacity": capacity_score,
                    "load_score": node.get_load_score(),
                })

            # Find underutilized powerful nodes (capacity > median, utilization < 50%)
            sorted_by_capacity = sorted(node_stats, key=lambda x: x["capacity"], reverse=True)
            median_capacity = sorted_by_capacity[len(sorted_by_capacity) // 2]["capacity"]

            underutilized_powerful = [
                n for n in node_stats
                if n["capacity"] > median_capacity and n["utilization"] < 0.5
            ]

            # Find overloaded weak nodes (capacity < median, utilization > 100%)
            overloaded_weak = [
                n for n in node_stats
                if n["capacity"] < median_capacity and n["utilization"] > 1.0
            ]

            if not underutilized_powerful or not overloaded_weak:
                return {"action": "none", "reason": "balanced"}

            # Calculate rebalancing opportunity
            spare_capacity = sum(
                max(0, n["target"] - n["current"]) for n in underutilized_powerful
            )
            excess_load = sum(
                max(0, n["current"] - n["target"]) for n in overloaded_weak
            )

            if spare_capacity < 2 or excess_load < 2:
                return {"action": "none", "reason": "minimal_imbalance"}

            # Migrate: reduce jobs on weak nodes
            rebalance_actions = []
            jobs_to_migrate = min(spare_capacity, excess_load)

            for weak_node in sorted(overloaded_weak, key=lambda x: x["utilization"], reverse=True):
                if jobs_to_migrate <= 0:
                    break

                node = weak_node["node"]
                reduce_by = min(
                    weak_node["current"] - weak_node["target"],
                    jobs_to_migrate
                )
                new_target = weak_node["current"] - reduce_by

                if reduce_by > 0:
                    print(
                        f"[P2P] Cluster rebalance: {node.node_id} overloaded "
                        f"({weak_node['current']}/{weak_node['target']} jobs, "
                        f"{weak_node['utilization']*100:.0f}% util) - reducing by {reduce_by}"
                    )

                    if node.node_id == self.node_id:
                        await self._reduce_local_selfplay_jobs(new_target, reason="cluster_rebalance")
                    else:
                        await self._request_reduce_selfplay(node, new_target, reason="cluster_rebalance")

                    rebalance_actions.append({
                        "node": node.node_id,
                        "reduced_by": reduce_by,
                        "new_target": new_target,
                    })
                    jobs_to_migrate -= reduce_by

            # Record rebalancing metric
            if rebalance_actions:
                self.record_metric(
                    "cluster_rebalance",
                    len(rebalance_actions),
                    metadata={
                        "spare_capacity": spare_capacity,
                        "excess_load": excess_load,
                        "actions": rebalance_actions,
                    },
                )

            return {
                "action": "rebalanced",
                "spare_capacity": spare_capacity,
                "excess_load": excess_load,
                "actions": rebalance_actions,
            }

        except Exception as e:  # noqa: BLE001
            logger.info(f"Cluster balance check error: {e}")
            return {"action": "error", "error": str(e)}

    async def _manage_cluster_jobs(self):
        """Manage jobs across the cluster (leader only).

        LEARNED LESSONS incorporated:
        - Check disk space BEFORE starting jobs (Vast.ai 91-93% disk issue)
        - Check memory to prevent OOM (AWS instance crashed at 31GB+)
        - Trigger cleanup when approaching limits
        - Use is_healthy() not just is_alive()
        """
        logger.info("Leader: Managing cluster jobs...")

        # Track cluster management run via JobOrchestrationManager (Jan 2026)
        if hasattr(self, "job_orchestration") and self.job_orchestration:
            self.job_orchestration.record_cluster_management_run()

        # Gather cluster state
        with self.peers_lock:
            alive_peers = [p for p in self.peers.values() if p.is_alive()]

        # Add self
        self._update_self_info()
        all_nodes = [*alive_peers, self.self_info]

        # Phase 1: Handle resource warnings and cleanup
        for node in all_nodes:
            # LEARNED LESSONS - Proactive disk cleanup before hitting critical
            if node.disk_percent >= DISK_CLEANUP_THRESHOLD:
                logger.info(f"{node.node_id}: Disk at {node.disk_percent:.0f}% - triggering cleanup")
                if node.node_id == self.node_id:
                    await self._cleanup_local_disk()
                else:
                    await self._request_remote_cleanup(node)
                continue  # Skip job creation this cycle

            # Load shedding: when a node is under memory/disk pressure, ask it to
            # stop excess selfplay jobs so it can recover (prevents OOM + disk-full).
            pressure_reasons: list[str] = []
            if node.memory_percent >= MEMORY_WARNING_THRESHOLD:
                pressure_reasons.append("memory")
            if node.disk_percent >= DISK_WARNING_THRESHOLD:
                pressure_reasons.append("disk")

            if pressure_reasons:
                desired = self.selfplay_scheduler.get_target_jobs_for_node(node)
                if node.memory_percent >= MEMORY_CRITICAL_THRESHOLD or node.disk_percent >= DISK_CRITICAL_THRESHOLD:
                    desired = 0

                if node.selfplay_jobs > desired:
                    reason = "+".join(pressure_reasons)
                    print(
                        f"[P2P] {node.node_id}: Load shedding (reason={reason}) "
                        f"{node.selfplay_jobs}->{desired} selfplay jobs"
                    )
                    if node.node_id == self.node_id:
                        await self._reduce_local_selfplay_jobs(desired, reason=reason)
                    else:
                        await self._request_reduce_selfplay(node, desired, reason=reason)

        # Phase 1.5: LEARNED LESSONS - Detect stuck jobs (GPU idle with running processes)
        # This addresses the vast-5090-quad issue where 582 processes ran at 0% GPU
        for node in all_nodes:
            if not node.has_gpu or node.selfplay_jobs <= 0:
                # No GPU or no jobs running - not stuck
                if node.node_id in self.gpu_idle_since:
                    del self.gpu_idle_since[node.node_id]
                continue

            # Check if GPU is idle (< threshold) with jobs running
            gpu_name = (node.gpu_name or "").upper()
            is_cuda_gpu = "MPS" not in gpu_name and "APPLE" not in gpu_name
            if not is_cuda_gpu:
                continue  # Skip Apple Silicon, doesn't have nvidia-smi

            if node.gpu_percent < GPU_IDLE_THRESHOLD:
                # GPU idle with jobs running - track or take action
                if node.node_id not in self.gpu_idle_since:
                    self.gpu_idle_since[node.node_id] = time.time()
                    logger.info(f"{node.node_id}: GPU idle ({node.gpu_percent:.0f}%) with {node.selfplay_jobs} jobs - monitoring")
                else:
                    idle_duration = time.time() - self.gpu_idle_since[node.node_id]
                    if idle_duration >= GPU_IDLE_RESTART_TIMEOUT:
                        logger.info(f"{node.node_id}: STUCK! GPU idle for {idle_duration:.0f}s with {node.selfplay_jobs} jobs")
                        logger.info(f"{node.node_id}: Requesting job restart...")
                        if node.node_id == self.node_id:
                            await self._restart_local_stuck_jobs()
                        else:
                            await self._request_job_restart(node)
                        del self.gpu_idle_since[node.node_id]
            else:
                # GPU is working - clear idle tracking
                if node.node_id in self.gpu_idle_since:
                    del self.gpu_idle_since[node.node_id]

        # Phase 1.6: Detect runaway selfplay processes (lost tracking / manual runs).
        # If a node reports an absurd number of selfplay processes, request a
        # restart sweep to kill untracked jobs and recover capacity.
        for node in all_nodes:
            try:
                target_selfplay = self.selfplay_scheduler.get_target_jobs_for_node(node)
                dynamic_threshold = max(16, target_selfplay * 3)
                runaway_threshold = (
                    int(RUNAWAY_SELFPLAY_PROCESS_THRESHOLD)
                    if int(RUNAWAY_SELFPLAY_PROCESS_THRESHOLD) > 0
                    else int(dynamic_threshold)
                )
                if int(getattr(node, "selfplay_jobs", 0) or 0) < runaway_threshold:
                    continue
            except (ValueError, AttributeError):
                continue

            print(
                f"[P2P] {node.node_id}: RUNAWAY selfplay count ({node.selfplay_jobs}) "
                f">= {runaway_threshold}  requesting restart sweep"
            )
            if node.node_id == self.node_id:
                await self._restart_local_stuck_jobs()
            else:
                await self._request_job_restart(node)

        # Phase 2: Calculate desired job distribution for healthy nodes
        # LEARNED LESSONS - Sort nodes by load score for load balancing
        # Least-loaded nodes get jobs first to ensure even distribution
        healthy_nodes = [n for n in all_nodes if n.is_healthy()]
        healthy_nodes.sort(key=lambda n: n.get_load_score())

        if healthy_nodes:
            load_summary = ", ".join(
                f"{n.node_id[:12]}={n.get_load_score():.0f}%"
                for n in healthy_nodes[:5]
            )
            logger.info(f"Load balancing: {load_summary}")

        for node in healthy_nodes:
            load_score = node.get_load_score()
            if load_score >= LOAD_MAX_FOR_NEW_JOBS:
                logger.info(f"{node.node_id}: Load {load_score:.0f}% - skipping new job starts")
                continue

            # LEARNED LESSONS - Reduce target when approaching limits
            # Base targets:
            # - GPU nodes: fixed concurrency tuned for GPU throughput.
            # - CPU-only nodes: scale with CPU cores (and cap by memory).
            # HYBRID MODE: Get separate GPU and CPU-only job targets (delegated)
            hybrid_targets = self.selfplay_scheduler.get_hybrid_job_targets(node)
            gpu_job_target = hybrid_targets.get("gpu_jobs", 0)
            cpu_only_target = hybrid_targets.get("cpu_only_jobs", 0)
            total_target = hybrid_targets.get("total_jobs", gpu_job_target)

            # Backward compat: use total_target like the old target_selfplay
            target_selfplay = total_target

            # Check if node needs more jobs
            if node.selfplay_jobs < target_selfplay:
                needed = target_selfplay - node.selfplay_jobs
                logger.info(f"{node.node_id} needs {needed} more selfplay jobs")

                # Job configuration diversity - cycle through different AI methods
                # LEARNED LESSONS - Prioritize varied AI methods for better training:
                # - nn-only: Neural network evaluation (NNUE + Descent)
                # - best-vs-pool: Tournament-style asymmetric play (best model vs varied pool)
                # - nn-vs-mcts: NN player vs MCTS player (asymmetric tournament)
                # - nn-vs-minimax: NN player vs Minimax player (asymmetric tournament)
                # - nn-vs-descent: NN player vs heuristic Descent (asymmetric tournament)
                # - tournament-varied: Each player gets different AI type (max variety)
                # - mcts-only: Pure Monte Carlo Tree Search
                # - descent-only: Gradient descent based evaluation (no NN)
                # - minimax-only: Classic minimax with alpha-beta pruning
                # NOTE: Heuristic-only modes removed to ensure NN/strong AI in every game
                selfplay_configs = [
                    # ================================================================
                    # GUMBEL MCTS - HIGHEST PRIORITY (70% of jobs should use Gumbel)
                    # GPU-accelerated Gumbel Top-K MCTS for high-quality training data
                    # 3p/4p get priority 12 (underrepresented), 2p get priority 10
                    # ================================================================
                    {"board_type": "hex8", "num_players": 2, "engine_mode": "gumbel-mcts", "priority": 10},
                    {"board_type": "hex8", "num_players": 3, "engine_mode": "gumbel-mcts", "priority": 12},  # Dec 27: +20% for underrepresented
                    {"board_type": "hex8", "num_players": 4, "engine_mode": "gumbel-mcts", "priority": 12},  # Dec 27: +20% for underrepresented
                    {"board_type": "square8", "num_players": 2, "engine_mode": "gumbel-mcts", "priority": 10},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "gumbel-mcts", "priority": 12},  # Dec 27: +20% for underrepresented
                    {"board_type": "square8", "num_players": 4, "engine_mode": "gumbel-mcts", "priority": 10},  # Already has most data
                    {"board_type": "square19", "num_players": 2, "engine_mode": "gumbel-mcts", "priority": 12},  # Dec 27: underrepresented
                    {"board_type": "square19", "num_players": 3, "engine_mode": "gumbel-mcts", "priority": 12},  # Dec 27: underrepresented
                    {"board_type": "square19", "num_players": 4, "engine_mode": "gumbel-mcts", "priority": 12},  # Dec 27: underrepresented
                    {"board_type": "hexagonal", "num_players": 2, "engine_mode": "gumbel-mcts", "priority": 12},  # Dec 27: underrepresented
                    {"board_type": "hexagonal", "num_players": 3, "engine_mode": "gumbel-mcts", "priority": 12},  # Dec 27: underrepresented
                    {"board_type": "hexagonal", "num_players": 4, "engine_mode": "gumbel-mcts", "priority": 12},  # Dec 27: underrepresented
                    # ================================================================
                    # UNDERREPRESENTED COMBINATIONS - MIXED MODE (PRIORITY 8)
                    # "mixed" mode provides varied AI matchups (NNUE, MCTS, heuristic combos)
                    # for maximum training data diversity (~30% of jobs)
                    # ================================================================
                    {"board_type": "hexagonal", "num_players": 3, "engine_mode": "mixed", "priority": 8},
                    {"board_type": "hexagonal", "num_players": 2, "engine_mode": "mixed", "priority": 8},
                    {"board_type": "hexagonal", "num_players": 4, "engine_mode": "mixed", "priority": 8},
                    {"board_type": "hex8", "num_players": 2, "engine_mode": "mixed", "priority": 8},
                    {"board_type": "hex8", "num_players": 3, "engine_mode": "mixed", "priority": 8},
                    {"board_type": "hex8", "num_players": 4, "engine_mode": "mixed", "priority": 8},
                    {"board_type": "square19", "num_players": 3, "engine_mode": "mixed", "priority": 8},
                    {"board_type": "square19", "num_players": 2, "engine_mode": "mixed", "priority": 8},
                    {"board_type": "square19", "num_players": 4, "engine_mode": "mixed", "priority": 8},
                    # ================================================================
                    # UNDERREPRESENTED COMBINATIONS (PRIORITY 7)
                    # Specific AI matchup modes for variety
                    # ================================================================
                    {"board_type": "square19", "num_players": 2, "engine_mode": "nn-only", "priority": 7},
                    {"board_type": "square19", "num_players": 2, "engine_mode": "nn-vs-mcts", "priority": 7},
                    {"board_type": "hexagonal", "num_players": 2, "engine_mode": "nn-only", "priority": 7},
                    {"board_type": "hexagonal", "num_players": 2, "engine_mode": "nn-vs-mcts", "priority": 7},
                    {"board_type": "hexagonal", "num_players": 3, "engine_mode": "nn-only", "priority": 7},
                    {"board_type": "hexagonal", "num_players": 3, "engine_mode": "nn-vs-mcts", "priority": 7},
                    # ================================================================
                    # SQUARE8 MULTI-PLAYER WITH MIXED MODE (PRIORITY 6.5)
                    # ================================================================
                    {"board_type": "square8", "num_players": 3, "engine_mode": "mixed", "priority": 7},
                    {"board_type": "square8", "num_players": 4, "engine_mode": "mixed", "priority": 7},
                    {"board_type": "square8", "num_players": 2, "engine_mode": "mixed", "priority": 6},
                    # ================================================================
                    # CROSS-AI MATCHES (PRIORITY 6) - Variety via asymmetric opponents
                    # heuristic/random vs strong AI (MCTS, Minimax, Descent, NN)
                    # ================================================================
                    {"board_type": "square8", "num_players": 2, "engine_mode": "heuristic-vs-nn", "priority": 6},
                    {"board_type": "square8", "num_players": 2, "engine_mode": "heuristic-vs-mcts", "priority": 6},
                    {"board_type": "square8", "num_players": 2, "engine_mode": "random-vs-mcts", "priority": 6},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "heuristic-vs-nn", "priority": 6},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "heuristic-vs-mcts", "priority": 6},
                    {"board_type": "square19", "num_players": 2, "engine_mode": "heuristic-vs-mcts", "priority": 6},
                    {"board_type": "hexagonal", "num_players": 2, "engine_mode": "heuristic-vs-mcts", "priority": 6},
                    {"board_type": "hexagonal", "num_players": 3, "engine_mode": "heuristic-vs-mcts", "priority": 6},
                    # ================================================================
                    # NEURAL NETWORK MODES (PRIORITY 5)
                    # ================================================================
                    {"board_type": "square8", "num_players": 2, "engine_mode": "nn-only", "priority": 5},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "nn-only", "priority": 5},
                    {"board_type": "square8", "num_players": 4, "engine_mode": "nn-only", "priority": 5},
                    {"board_type": "square8", "num_players": 2, "engine_mode": "best-vs-pool", "priority": 5},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "best-vs-pool", "priority": 5},
                    {"board_type": "square19", "num_players": 3, "engine_mode": "nn-only", "priority": 5},
                    {"board_type": "square19", "num_players": 4, "engine_mode": "nn-only", "priority": 5},
                    {"board_type": "hexagonal", "num_players": 4, "engine_mode": "nn-only", "priority": 5},
                    # ================================================================
                    # ASYMMETRIC TOURNAMENT MODES (PRIORITY 5) - NN vs other AI types
                    # ================================================================
                    {"board_type": "square8", "num_players": 2, "engine_mode": "nn-vs-mcts", "priority": 5},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "nn-vs-mcts", "priority": 5},
                    {"board_type": "square8", "num_players": 4, "engine_mode": "nn-vs-mcts", "priority": 5},
                    {"board_type": "square8", "num_players": 2, "engine_mode": "nn-vs-minimax", "priority": 5},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "nn-vs-minimax", "priority": 5},
                    {"board_type": "square8", "num_players": 2, "engine_mode": "nn-vs-descent", "priority": 5},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "nn-vs-descent", "priority": 5},
                    {"board_type": "square19", "num_players": 3, "engine_mode": "nn-vs-mcts", "priority": 5},
                    {"board_type": "square19", "num_players": 4, "engine_mode": "nn-vs-mcts", "priority": 5},
                    {"board_type": "hexagonal", "num_players": 4, "engine_mode": "nn-vs-mcts", "priority": 5},
                    # ================================================================
                    # TOURNAMENT-VARIED (PRIORITY 4) - Max diversity, always includes NN
                    # ================================================================
                    {"board_type": "square8", "num_players": 2, "engine_mode": "tournament-varied", "priority": 4},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "tournament-varied", "priority": 4},
                    {"board_type": "square8", "num_players": 4, "engine_mode": "tournament-varied", "priority": 4},
                    {"board_type": "square19", "num_players": 2, "engine_mode": "tournament-varied", "priority": 4},
                    {"board_type": "square19", "num_players": 3, "engine_mode": "tournament-varied", "priority": 4},
                    {"board_type": "hexagonal", "num_players": 2, "engine_mode": "tournament-varied", "priority": 4},
                    {"board_type": "hexagonal", "num_players": 3, "engine_mode": "tournament-varied", "priority": 4},
                    # ================================================================
                    # CPU-BOUND AI METHODS (PRIORITY 3) - MCTS, Descent, Minimax
                    # For CPU-only instances and variety
                    # ================================================================
                    {"board_type": "hexagonal", "num_players": 4, "engine_mode": "mcts-only", "priority": 3},
                    {"board_type": "hexagonal", "num_players": 3, "engine_mode": "descent-only", "priority": 3},
                    {"board_type": "square19", "num_players": 4, "engine_mode": "mcts-only", "priority": 3},
                    {"board_type": "square19", "num_players": 3, "engine_mode": "mcts-only", "priority": 3},
                    {"board_type": "square19", "num_players": 3, "engine_mode": "descent-only", "priority": 3},
                    {"board_type": "square8", "num_players": 2, "engine_mode": "mcts-only", "priority": 3},
                    {"board_type": "square8", "num_players": 2, "engine_mode": "descent-only", "priority": 3},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "descent-only", "priority": 3},
                    {"board_type": "square8", "num_players": 4, "engine_mode": "mcts-only", "priority": 3},
                    # ================================================================
                    # MINIMAX (PRIORITY 2) - Classical approach, good for variety
                    # ================================================================
                    {"board_type": "square8", "num_players": 2, "engine_mode": "minimax-only", "priority": 2},
                    {"board_type": "square8", "num_players": 3, "engine_mode": "minimax-only", "priority": 2},
                    {"board_type": "square19", "num_players": 2, "engine_mode": "descent-only", "priority": 2},
                    {"board_type": "square19", "num_players": 2, "engine_mode": "minimax-only", "priority": 2},
                    {"board_type": "hexagonal", "num_players": 2, "engine_mode": "descent-only", "priority": 2},
                    {"board_type": "hexagonal", "num_players": 2, "engine_mode": "minimax-only", "priority": 2},
                    # NO PURE HEURISTIC-ONLY MODES - all modes include at least one
                    # strong AI (NN/MCTS/Descent/Minimax) for quality training data
                ]

                # LEARNED LESSONS - Weighted selection favoring high priority configs
                # Expand configs by priority for weighted random selection
                node_mem = int(getattr(node, "memory_gb", 0) or 0)
                filtered_configs = selfplay_configs
                if node_mem and node_mem < 48:
                    # Smaller CPU nodes should avoid square19/hexagonal to reduce
                    # OOM risk and thrash. Keep them productive with square8.
                    filtered_configs = [cfg for cfg in selfplay_configs if cfg.get("board_type") == "square8"]

                weighted_configs = []
                for cfg in filtered_configs:
                    weighted_configs.extend([cfg] * cfg.get("priority", 1))

                # FIXED: Start all needed jobs with fair config distribution
                # Instead of max 2, start up to 10 at a time to quickly fill all configs
                # Use round-robin across unique configs to ensure coverage
                unique_configs = list({(c["board_type"], c["num_players"]): c for c in filtered_configs}.values())
                jobs_to_start = min(needed, 10)  # Start up to 10 jobs per iteration

                # HYBRID MODE: Calculate how many GPU vs CPU-only jobs to spawn
                # If node already has gpu_job_target GPU jobs, spawn CPU-only jobs instead
                current_gpu_jobs = min(node.selfplay_jobs, gpu_job_target)
                remaining_gpu_slots = max(0, gpu_job_target - current_gpu_jobs)
                remaining_cpu_slots = max(0, cpu_only_target)  # Can always spawn CPU-only if capacity
                should_use_cpu_only = self.selfplay_scheduler.should_spawn_cpu_only_jobs(node) and cpu_only_target > 0

                for i in range(jobs_to_start):
                    # LEARNED LESSONS - Smart CPU/GPU task routing:
                    # - High-end GPUs (H100/H200/A100/5090/4090) get GPU_SELFPLAY for max throughput
                    #   with automatic CPU validation to ensure data quality
                    # - Mid-tier GPUs get HYBRID_SELFPLAY (CPU rules + GPU eval)
                    # - CPU-only nodes or GPU-saturated nodes get CPU_SELFPLAY
                    # This ensures expensive GPU resources are utilized properly
                    # while CPU instances handle CPU-bound tasks efficiently
                    gpu_name = (node.gpu_name or "").upper()
                    node_has_gpu = bool(node.has_gpu)

                    # Session 17.50: YAML fallback when runtime GPU detection fails
                    if not node_has_gpu or not gpu_name:
                        yaml_has_gpu, yaml_gpu_name, _ = self._check_yaml_gpu_config(node.node_id)
                        if yaml_has_gpu:
                            node_has_gpu = True
                            if yaml_gpu_name:
                                gpu_name = yaml_gpu_name.upper()

                    is_high_end_gpu = any(tag in gpu_name for tag in ("H100", "H200", "GH200", "A100", "5090", "4090"))
                    is_apple_gpu = "MPS" in gpu_name or "APPLE" in gpu_name

                    # GPU utilization check: if GPU is at 0% with jobs running, those jobs
                    # are probably CPU-only. This is an opportunity to START GPU work, not avoid it.
                    # Only consider GPU "unavailable" if there are existing GPU jobs AND utilization is 0%
                    # (which would indicate driver/container issues)
                    # Jan 7, 2026: Fixed to use new gpu_job_count field (gpu_selfplay_jobs never existed)
                    gpu_percent = getattr(node, "gpu_percent", 0) or 0
                    gpu_job_count = getattr(node, "gpu_job_count", 0) or 0
                    gpu_failure_count = getattr(node, "gpu_failure_count", 0) or 0
                    last_gpu_failure = getattr(node, "last_gpu_job_failure", 0) or 0
                    gpu_seems_unavailable = (
                        node_has_gpu
                        and not is_apple_gpu
                        and (
                            # Driver issue: GPU jobs running but 0% utilization
                            (gpu_job_count >= 2 and gpu_percent < 1)
                            # Recent consecutive failures: 3+ failures in last 5 minutes
                            or (gpu_failure_count >= 3 and time.time() - last_gpu_failure < 300)
                        )
                    )
                    if gpu_seems_unavailable:
                        reason = "driver issue" if (gpu_job_count >= 2 and gpu_percent < 1) else f"{gpu_failure_count} recent failures"
                        logger.info(f"WARNING: {node.node_id} has GPU but appears unavailable ({reason})")
                    elif node_has_gpu and gpu_percent < 10 and node.selfplay_jobs > 0:
                        # GPU idle but has CPU jobs - this is normal, will prioritize GPU work
                        logger.debug(f"Node {node.node_id} has {node.selfplay_jobs} CPU jobs but GPU idle ({gpu_percent:.0f}%) - will add GPU work")

                    # Jan 7, 2026: Role-based job preference to ensure nodes run appropriate job types
                    job_preference = self._get_node_job_preference(node.node_id)

                    # Role enforcement: respect node's configured role
                    if job_preference == "training_only":
                        logger.debug(f"Skipping {node.node_id} for selfplay (training_only role)")
                        continue
                    elif job_preference == "cpu_only":
                        # Coordinator/CPU nodes: force CPU-only selfplay
                        spawn_cpu_only = True
                        if remaining_cpu_slots > 0:
                            remaining_cpu_slots -= 1
                        else:
                            logger.debug(f"Skipping {node.node_id} - cpu_only role but no CPU slots")
                            continue
                    elif job_preference == "gpu_only" and node_has_gpu and not gpu_seems_unavailable:
                        # GPU-only nodes: must use GPU, skip if no slots
                        if remaining_gpu_slots > 0:
                            remaining_gpu_slots -= 1
                            spawn_cpu_only = False
                        else:
                            logger.debug(f"Skipping {node.node_id} - gpu_only role but no GPU slots")
                            continue
                    else:
                        # "both" role: use original hybrid logic
                        # HYBRID MODE: Decide between GPU and CPU-only based on capacity
                        spawn_cpu_only = False
                        if remaining_gpu_slots > 0 and not gpu_seems_unavailable:
                            remaining_gpu_slots -= 1
                        elif should_use_cpu_only and remaining_cpu_slots > 0:
                            spawn_cpu_only = True
                            remaining_cpu_slots -= 1
                        elif gpu_seems_unavailable:
                            spawn_cpu_only = True

                    if spawn_cpu_only:
                        # Pure CPU selfplay to utilize excess CPU capacity
                        job_type = JobType.CPU_SELFPLAY
                        task_type_str = "CPU-only (hybrid mode)"
                    elif node.has_gpu and is_high_end_gpu and not is_apple_gpu and not gpu_seems_unavailable:
                        # High-end CUDA GPUs: Mix of GPU_SELFPLAY (volume) and GUMBEL_SELFPLAY (quality)
                        # GPU selfplay now has high parity with CPU rules (2025-12 upgrade)
                        # Use Gumbel MCTS ~50% of time for high-quality training data (self-improvement loop)
                        # Increased from 20% to close the training loop - AlphaZero needs NN+MCTS data
                        import random
                        if random.random() < 0.5:  # 50% chance for Gumbel MCTS (quality) - was 20%
                            job_type = JobType.GUMBEL_SELFPLAY
                            task_type_str = "GUMBEL (high-quality)"
                        else:  # 50% for GPU selfplay (volume)
                            job_type = JobType.GPU_SELFPLAY
                            task_type_str = "GPU (high-parity)"
                    elif node.has_gpu and not is_apple_gpu and not gpu_seems_unavailable:
                        # Mid-tier GPUs: Use hybrid (CPU rules + GPU eval)
                        job_type = JobType.HYBRID_SELFPLAY
                        task_type_str = "HYBRID (accel)"
                    else:
                        job_type = JobType.SELFPLAY
                        task_type_str = "CPU-only"

                    gpu_info = f"gpu={node.gpu_name or 'none'}, gpu%={getattr(node, 'gpu_percent', 0):.0f}" if node.has_gpu else "no-gpu"
                    logger.info(f"Assigning {task_type_str} task to {node.node_id} ({gpu_info}, load={node.get_load_score():.0f}%)")

                    # FIXED: Round-robin config selection to ensure all configs get coverage
                    # Use unique_configs list for fair distribution across all 9 board/player combos
                    if self.improvement_cycle_manager and hasattr(self.improvement_cycle_manager, 'get_next_selfplay_config_for_node'):
                        # Node-aware dynamic selection: routes hex/sq19/3p/4p to powerful nodes
                        node_gpu_power = node.gpu_power_score() if hasattr(node, 'gpu_power_score') else 0
                        node_memory = int(getattr(node, 'memory_gb', 0) or 0)
                        config = self.improvement_cycle_manager.get_next_selfplay_config_for_node(
                            node_gpu_power=node_gpu_power,
                            node_memory_gb=node_memory,
                            cluster_data=self.cluster_data_manifest
                        )
                    else:
                        # Round-robin across unique configs for fair coverage
                        # Each iteration picks the next config in the list
                        config_idx = i % len(unique_configs)
                        config = unique_configs[config_idx]

                    # Track diversity metrics for monitoring (delegated to SelfplayScheduler)
                    self.selfplay_scheduler.track_diversity(config)

                    if node.node_id == self.node_id:
                        await self._start_local_job(
                            job_type,
                            board_type=config["board_type"],
                            num_players=config["num_players"],
                            engine_mode=config["engine_mode"],
                        )
                    else:
                        await self._request_remote_job(
                            node, job_type,
                            board_type=config["board_type"],
                            num_players=config["num_players"],
                            engine_mode=config["engine_mode"],
                        )

    def _emergency_memory_cleanup(self) -> None:
        """Emergency memory cleanup when memory is critical.

        Jan 10, 2026: Clears gossip caches and triggers garbage collection
        to free memory when above MEMORY_CRITICAL_THRESHOLD (90%).
        """
        import gc

        # Clear gossip state caches
        gossip_states = getattr(self, "_gossip_peer_states", None)
        gossip_manifests = getattr(self, "_gossip_peer_manifests", None)

        states_cleared = 0
        manifests_cleared = 0

        if gossip_states:
            states_cleared = len(gossip_states)
            gossip_states.clear()

        if gossip_manifests:
            manifests_cleared = len(gossip_manifests)
            gossip_manifests.clear()

        # Force garbage collection
        gc.collect()

        logger.info(
            f"Emergency memory cleanup: cleared {states_cleared} gossip states, "
            f"{manifests_cleared} manifests, ran gc.collect()"
        )

    async def _cleanup_local_disk(self):
        """Clean up disk space on local node.

        LEARNED LESSONS - Automatically archive old data:
        - Remove deprecated selfplay databases
        - Compress and archive old logs
        - Clear /tmp files older than 24h
        """
        logger.info("Running local disk cleanup...")
        try:
            # Prefer the shared disk monitor (used by cron/resilience) for consistent cleanup policy.
            disk_monitor = Path(self._get_ai_service_path()) / "scripts" / "disk_monitor.py"
            if disk_monitor.exists():
                usage = self._get_resource_usage()
                disk_percent = float(usage.get("disk_percent", 0.0) or 0.0)
                cmd = [
                    sys.executable,  # Use venv Python
                    str(disk_monitor),
                    "--threshold",
                    str(DISK_CLEANUP_THRESHOLD),
                    "--ringrift-path",
                    str(self.ringrift_path),
                    "--aggressive",
                ]
                if disk_percent >= DISK_CRITICAL_THRESHOLD:
                    cmd.append("--force")

                out = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    timeout=300,
                    cwd=str(Path(self._get_ai_service_path())),
                )
                if out.returncode == 0:
                    logger.info("Disk monitor cleanup completed")
                else:
                    logger.info(f"Disk monitor cleanup failed: {out.stderr[:200]}")
            else:
                # Minimal fallback: clear old logs if disk monitor isn't available.
                log_dir = Path(self._get_ai_service_path()) / "logs"
                if log_dir.exists():
                    for logfile in log_dir.rglob("*.log"):
                        if time.time() - logfile.stat().st_mtime > 7 * 86400:  # 7 days
                            logfile.unlink()
                            logger.info(f"Cleaned old log: {logfile}")

        except Exception as e:  # noqa: BLE001
            logger.info(f"Disk cleanup error: {e}")

    async def _request_remote_cleanup(self, node: NodeInfo):
        """Request a remote node to clean up disk space."""
        try:
            if getattr(node, "nat_blocked", False):
                cmd_id = await self._enqueue_relay_command_for_peer(node, "cleanup", {})
                if cmd_id:
                    logger.info(f"Enqueued relay cleanup for {node.node_id}")
                else:
                    logger.info(f"Relay queue full; skipping cleanup enqueue for {node.node_id}")
                return
            timeout = ClientTimeout(total=HTTP_TOTAL_TIMEOUT)
            async with get_client_session(timeout) as session:
                last_err: str | None = None
                for url in self._urls_for_peer(node, "/cleanup"):
                    try:
                        async with session.post(url, json={}, headers=self._auth_headers()) as resp:
                            if resp.status == 200:
                                logger.info(f"Cleanup requested on {node.node_id}")
                                return
                            last_err = f"http_{resp.status}"
                    except Exception as e:  # noqa: BLE001
                        last_err = str(e)
                        continue
                if last_err:
                    logger.info(f"Cleanup request failed on {node.node_id}: {last_err}")
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to request cleanup from {node.node_id}: {e}")

    async def _reduce_local_selfplay_jobs(self, target_selfplay_jobs: int, *, reason: str) -> dict[str, Any]:
        """Best-effort: stop excess selfplay jobs on this node (load shedding).

        Used when disk/memory pressure is high: we want the node to recover and
        avoid OOM/disk-full scenarios, even if it means reducing throughput.

        Jan 2026: Uses asyncio.to_thread() for blocking calls to avoid event loop stalls.
        """
        try:
            target = max(0, int(target_selfplay_jobs))
        except (ValueError):
            target = 0

        # First, get an overall count using the same mechanism used for cluster
        # reporting (includes untracked processes).
        # Jan 2026: Use asyncio.to_thread() to avoid blocking event loop
        try:
            selfplay_before, _training_before = await asyncio.to_thread(self._count_local_jobs)
        except (AttributeError):
            selfplay_before = 0

        # Hard shedding (target=0): reuse the existing restart sweep, which
        # kills both tracked and untracked selfplay processes.
        if target <= 0:
            await self._restart_local_stuck_jobs()
            try:
                selfplay_after, _training_after = await asyncio.to_thread(self._count_local_jobs)
            except (AttributeError):
                selfplay_after = 0
            return {
                "running_before": int(selfplay_before),
                "running_after": int(selfplay_after),
                "stopped": max(0, int(selfplay_before) - int(selfplay_after)),
                "target": 0,
                "reason": reason,
            }

        with self.jobs_lock:
            running: list[tuple[str, ClusterJob]] = [
                (job_id, job)
                for job_id, job in self.local_jobs.items()
                if job.status == "running"
                and job.job_type in (JobType.SELFPLAY, JobType.GPU_SELFPLAY, JobType.HYBRID_SELFPLAY, JobType.CPU_SELFPLAY, JobType.GUMBEL_SELFPLAY)
            ]

        if selfplay_before <= target and len(running) <= target:
            return {
                "running_before": int(selfplay_before),
                "running_after": int(selfplay_before),
                "stopped": 0,
                "target": target,
                "reason": reason,
            }

        # Stop newest-first to avoid killing long-running jobs near completion.
        running.sort(key=lambda pair: float(getattr(pair[1], "started_at", 0.0) or 0.0), reverse=True)
        to_stop = running[target:]

        stopped = 0
        with self.jobs_lock:
            for _job_id, job in to_stop:
                try:
                    if job.pid:
                        os.kill(int(job.pid), signal.SIGTERM)
                    job.status = "stopped"
                    stopped += 1
                except (ValueError, AttributeError):
                    continue

        # If job tracking was lost, we may still have a large number of
        # untracked selfplay processes. Best-effort kill enough to hit target.
        # Jan 2026: Use asyncio.to_thread() to avoid blocking event loop
        try:
            selfplay_mid, _training_mid = await asyncio.to_thread(self._count_local_jobs)
        except (AttributeError):
            selfplay_mid = max(0, int(selfplay_before) - stopped)

        if selfplay_mid > target:
            try:
                import shutil

                if shutil.which("pgrep"):
                    pids: list[int] = []
                    # December 2025: Added selfplay.py - unified entry point
                    # Jan 2026: Use async subprocess helper to avoid blocking event loop
                    for pattern in (
                        "selfplay.py",
                        "run_self_play_soak.py",
                        "run_gpu_selfplay.py",
                        "run_hybrid_selfplay.py",
                        "run_random_selfplay.py",
                    ):
                        returncode, stdout, _stderr = await self._run_subprocess_async(
                            ["pgrep", "-f", pattern], timeout=5
                        )
                        if returncode == 0 and stdout.strip():
                            for token in stdout.strip().split():
                                try:
                                    pids.append(int(token))
                                except (ValueError, AttributeError):
                                    continue

                    # Kill newest-ish (highest PID) first.
                    pids = sorted(set(pids), reverse=True)
                    excess = int(selfplay_mid) - int(target)
                    killed = 0
                    for pid in pids:
                        if killed >= excess:
                            break
                        try:
                            os.kill(pid, signal.SIGTERM)
                            killed += 1
                        except (AttributeError):
                            continue
                    stopped += killed
            except (AttributeError):
                pass

        if stopped:
            self._save_state()

        # Jan 2026: Use asyncio.to_thread() to avoid blocking event loop
        try:
            selfplay_after, _training_after = await asyncio.to_thread(self._count_local_jobs)
        except (AttributeError):
            selfplay_after = max(0, int(selfplay_before) - stopped)

        return {
            "running_before": int(selfplay_before),
            "running_after": int(selfplay_after),
            "stopped": int(max(0, int(selfplay_before) - int(selfplay_after))),
            "target": target,
            "reason": reason,
        }

    async def _request_reduce_selfplay(self, node: NodeInfo, target_selfplay_jobs: int, *, reason: str) -> None:
        """Ask a node to shed excess selfplay (used for memory/disk pressure)."""
        try:
            target = max(0, int(target_selfplay_jobs))
        except (ValueError):
            target = 0

        if getattr(node, "nat_blocked", False):
            payload = {"target_selfplay_jobs": target, "reason": reason}
            cmd_id = await self._enqueue_relay_command_for_peer(node, "reduce_selfplay", payload)
            if cmd_id:
                logger.info(f"Enqueued relay reduce_selfplay for {node.node_id} (target={target}, reason={reason})")
            else:
                logger.info(f"Relay queue full for {node.node_id}; skipping reduce_selfplay enqueue")
            return

        timeout = ClientTimeout(total=HTTP_TOTAL_TIMEOUT)
        async with get_client_session(timeout) as session:
            last_err: str | None = None
            payload = {"target_selfplay_jobs": target, "reason": reason}
            for url in self._urls_for_peer(node, "/reduce_selfplay"):
                try:
                    async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                        if resp.status == 200:
                            logger.info(f"Requested load shedding on {node.node_id} (target={target}, reason={reason})")
                            return
                        last_err = f"http_{resp.status}"
                except Exception as e:  # noqa: BLE001
                    last_err = str(e)
                    continue
            if last_err:
                logger.info(f"reduce_selfplay request failed on {node.node_id}: {last_err}")

    async def _restart_local_stuck_jobs(self):
        """Kill stuck selfplay processes and let job management restart them.

        LEARNED LESSONS - Addresses the issue where processes accumulate but GPU stays at 0%.
        """
        logger.info("Restarting stuck local selfplay jobs...")
        try:
            # Kill tracked selfplay jobs (avoid broad pkill patterns).
            jobs_to_clear: list[str] = []
            pids_to_kill: set[int] = set()
            with self.jobs_lock:
                for job_id, job in self.local_jobs.items():
                    if job.job_type not in (JobType.SELFPLAY, JobType.GPU_SELFPLAY, JobType.HYBRID_SELFPLAY, JobType.CPU_SELFPLAY, JobType.GUMBEL_SELFPLAY):
                        continue
                    jobs_to_clear.append(job_id)
                    if job.pid:
                        try:
                            pids_to_kill.add(int(job.pid))
                        except (ValueError, AttributeError):
                            continue

            # Sweep for untracked selfplay processes (e.g. lost local_jobs state) and kill them too.
            try:
                import shutil

                if shutil.which("pgrep"):
                    # December 2025: Added selfplay.py - unified entry point
                    for pattern in (
                        "selfplay.py",
                        "run_self_play_soak.py",
                        "run_gpu_selfplay.py",
                        "run_hybrid_selfplay.py",
                        "run_random_selfplay.py",
                    ):
                        out = subprocess.run(
                            ["pgrep", "-f", pattern],
                            capture_output=True,
                            text=True,
                            timeout=5,
                        )
                        if out.returncode == 0 and out.stdout.strip():
                            for token in out.stdout.strip().split():
                                try:
                                    pids_to_kill.add(int(token))
                                except (ValueError, AttributeError):
                                    continue
            except (ValueError, AttributeError):
                pass

            pids_to_kill.discard(int(os.getpid()))

            killed = 0
            for pid in sorted(pids_to_kill):
                try:
                    os.kill(pid, signal.SIGKILL)
                    killed += 1
                except (AttributeError):
                    continue

            # Clear our job tracking - they'll be restarted next cycle.
            with self.jobs_lock:
                for job_id in jobs_to_clear:
                    self.local_jobs.pop(job_id, None)

            logger.info(f"Killed {killed} processes, cleared {len(jobs_to_clear)} job records")
        except Exception as e:  # noqa: BLE001
            logger.error(f"killing stuck processes: {e}")

    async def _request_job_restart(self, node: NodeInfo):
        """Request a remote node to restart its stuck selfplay jobs."""
        try:
            if getattr(node, "nat_blocked", False):
                cmd_id = await self._enqueue_relay_command_for_peer(node, "restart_stuck_jobs", {})
                if cmd_id:
                    logger.info(f"Enqueued relay restart_stuck_jobs for {node.node_id}")
                else:
                    logger.info(f"Relay queue full for {node.node_id}; skipping restart enqueue")
                return
            timeout = ClientTimeout(total=HTTP_TOTAL_TIMEOUT)
            async with get_client_session(timeout) as session:
                last_err: str | None = None
                for url in self._urls_for_peer(node, "/restart_stuck_jobs"):
                    try:
                        async with session.post(url, json={}, headers=self._auth_headers()) as resp:
                            if resp.status != 200:
                                last_err = f"http_{resp.status}"
                                continue
                            data = await resp.json()
                            if data.get("success"):
                                logger.info(f"Job restart requested on {node.node_id}")
                                return
                            last_err = str(data.get("error") or "restart_failed")
                    except Exception as e:  # noqa: BLE001
                        last_err = str(e)
                        continue
                if last_err:
                    logger.info(f"Job restart request failed on {node.node_id}: {last_err}")
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to request job restart from {node.node_id}: {e}")

    async def _start_local_job(
        self,
        job_type: JobType,
        board_type: str = "square8",
        num_players: int = 2,
        engine_mode: str = "gumbel-mcts",  # GPU-accelerated Gumbel MCTS
        job_id: str | None = None,
        cuda_visible_devices: str | None = None,
        export_params: dict[str, Any] | None = None,
        simulation_budget: int | None = None,  # Gumbel MCTS budget (None = use tier default)
    ) -> ClusterJob | None:
        """Start a job on the local node.

        SAFEGUARD: Checks coordination safeguards before spawning.
        """
        try:
            # SAFEGUARD: Check safeguards before spawning
            if HAS_SAFEGUARDS and _safeguards:
                task_type_str = job_type.value if hasattr(job_type, 'value') else str(job_type)
                allowed, reason = check_before_spawn(task_type_str, self.node_id)
                if not allowed:
                    logger.info(f"SAFEGUARD blocked {task_type_str} on {self.node_id}: {reason}")
                    # Track blocked spawn via JobOrchestrationManager
                    if hasattr(self, "job_orchestration") and self.job_orchestration:
                        self.job_orchestration.record_spawn_blocked(f"safeguard:{reason}")
                    return None

                # Apply backpressure delay
                delay = _safeguards.get_delay()
                if delay > 0:
                    logger.info(f"SAFEGUARD applying {delay:.1f}s backpressure delay")
                    await asyncio.sleep(delay)

            if job_id:
                job_id = str(job_id)
                with self.jobs_lock:
                    existing = self.local_jobs.get(job_id)
                if existing and existing.status == "running":
                    return existing
            else:
                job_id = str(uuid.uuid4())[:8]

            if job_type == JobType.SELFPLAY:
                # Normalize engine_mode to what run_self_play_soak.py supports.
                # LEARNED LESSONS - Variety of AI methods for better training:
                # - nn-only: Uses NNUE/neural network evaluation
                # - best-vs-pool: Tournament-style with varied opponents
                # - mcts-only/descent-only/minimax-only: Single AI method
                # Jan 17, 2026: Added nnue-guided, brs, maxn, paranoid for harness diversity
                supported_engine_modes = {
                    "descent-only",
                    "mixed",
                    "random-only",
                    "heuristic-only",
                    "minimax-only",
                    "mcts-only",
                    "nn-only",
                    "best-vs-pool",
                    # GPU-accelerated Gumbel MCTS modes
                    "gumbel",
                    "gumbel-mcts",
                    "gumbel-mcts-only",
                    # NNUE-guided search (fast + strong)
                    "nnue-guided",
                    "nnue",
                    # Multiplayer-optimized engines
                    "maxn",             # MaxN search for 3-4 player
                    "brs",              # Best Response Search
                    "paranoid",         # Paranoid minimax (assumes opponents cooperate)
                    "policy-only",      # Neural policy only (no search)
                    # Cross-AI asymmetric matches for variety
                    "nn-vs-mcts",
                    "nn-vs-minimax",
                    "nn-vs-descent",
                    "tournament-varied",
                    "heuristic-vs-nn",
                    "heuristic-vs-mcts",
                    "random-vs-mcts",
                }
                # Normalize engine mode - map aliases to what run_self_play_soak.py expects
                gumbel_aliases = {"gumbel", "gumbel-mcts"}
                if engine_mode in gumbel_aliases:
                    engine_mode_norm = "gumbel-mcts-only"  # Actual mode name in run_self_play_soak.py
                elif engine_mode in supported_engine_modes:
                    engine_mode_norm = engine_mode
                else:
                    engine_mode_norm = "nn-only"

                # Memory-safety defaults for large boards.
                num_games = 1000
                extra_args: list[str] = []
                if board_type in ("square19", "hexagonal"):
                    num_games = 200 if board_type == "square19" else 100
                    extra_args.extend(["--memory-constrained"])

                output_dir = Path(
                    self.ringrift_path,
                    "ai-service",
                    "data",
                    "selfplay",
                    "p2p",
                    f"{board_type}_{num_players}p",
                    job_id,
                )
                output_dir.mkdir(parents=True, exist_ok=True)

                # Use venv python if available, otherwise fall back to system python3
                venv_python = Path(self._get_ai_service_path(), "venv", "bin", "python")
                python_exec = str(venv_python) if venv_python.exists() else "python3"

                cmd = [
                    python_exec,
                    self._get_script_path("run_self_play_soak.py"),
                    "--num-games", str(num_games),
                    "--board-type", board_type,
                    "--num-players", str(num_players),
                    "--engine-mode", engine_mode_norm,
                    "--max-moves", "10000",  # LEARNED LESSONS - Avoid draws due to move limit
                    "--log-jsonl", str(output_dir / "games.jsonl"),
                    "--summary-json", str(output_dir / "summary.json"),
                    "--record-db", str(output_dir / "games.db"),
                    "--lean-db",
                    "--verbose", "0",
                    *extra_args,
                ]

                # Start process
                env = os.environ.copy()
                env["PYTHONPATH"] = self._get_ai_service_path()
                env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"
                env["RINGRIFT_JOB_ORIGIN"] = "p2p_orchestrator"

                # SAFEGUARD: Final check before spawning (load + rate limit)
                can_spawn, spawn_reason = self._can_spawn_process(f"selfplay-{board_type}-{num_players}p")
                if not can_spawn:
                    logger.info(f"BLOCKED selfplay spawn: {spawn_reason}")
                    return None

                log_handle = open(output_dir / "run.log", "a")  # noqa: SIM115
                try:
                    proc = subprocess.Popen(
                        cmd,
                        stdout=log_handle,
                        stderr=subprocess.STDOUT,
                        env=env,
                        cwd=self.ringrift_path,
                    )
                    self._record_spawn()  # Track spawn for rate limiting
                finally:
                    log_handle.close()

                job = ClusterJob(
                    job_id=job_id,
                    job_type=job_type,
                    node_id=self.node_id,
                    board_type=board_type,
                    num_players=num_players,
                    engine_mode=engine_mode_norm,
                    pid=proc.pid,
                    started_at=time.time(),
                    status="running",
                )

                with self.jobs_lock:
                    self.local_jobs[job_id] = job

                logger.info(f"Started {job_type.value} job {job_id} (PID {proc.pid})")
                self._save_state()

                # Track job start via JobOrchestrationManager (Jan 2026)
                if hasattr(self, "job_orchestration") and self.job_orchestration:
                    self.job_orchestration.record_job_started(job_type.value)

                # Dec 31, 2025: Add process monitoring to track completion/failure
                # Previously jobs remained in "running" status indefinitely
                asyncio.create_task(self._monitor_selfplay_process(
                    job_id, proc, output_dir, board_type, num_players, "selfplay"
                ))

                return job

            elif job_type == JobType.CPU_SELFPLAY:
                # Pure CPU selfplay for high-CPU nodes with limited GPU VRAM
                # Uses CPU-efficient engine modes
                # This enables utilizing excess CPU capacity on Vast.ai hosts etc.

                # CPU-friendly engine modes - include descent, mcts, and nn-only (work on CPU)
                # run_self_play_soak.py supports all these modes
                cpu_engine_modes = {
                    "descent-only", "minimax-only", "mcts-only", "heuristic-only",
                    "random-only", "mixed", "nn-only", "best-vs-pool",
                    # Cross-AI asymmetric matches
                    "nn-vs-mcts", "nn-vs-minimax", "nn-vs-descent", "tournament-varied",
                    "heuristic-vs-nn", "heuristic-vs-mcts", "random-vs-mcts",
                }
                engine_mode_norm = engine_mode if engine_mode in cpu_engine_modes else "nn-only"

                # CPU-only jobs can handle more games per batch
                num_games = 2000
                extra_args: list[str] = []
                if board_type in ("square19", "hexagonal"):
                    num_games = 400 if board_type == "square19" else 200
                    extra_args.extend(["--memory-constrained"])

                output_dir = Path(
                    self.ringrift_path,
                    "ai-service",
                    "data",
                    "selfplay",
                    "p2p",
                    f"{board_type}_{num_players}p_cpu",  # Separate subdir for CPU-only
                    job_id,
                )
                output_dir.mkdir(parents=True, exist_ok=True)

                venv_python = Path(self._get_ai_service_path(), "venv", "bin", "python")
                python_exec = str(venv_python) if venv_python.exists() else "python3"

                cmd = [
                    python_exec,
                    self._get_script_path("run_self_play_soak.py"),
                    "--num-games", str(num_games),
                    "--board-type", board_type,
                    "--num-players", str(num_players),
                    "--engine-mode", engine_mode_norm,
                    "--max-moves", "10000",
                    "--log-jsonl", str(output_dir / "games.jsonl"),
                    "--summary-json", str(output_dir / "summary.json"),
                    "--record-db", str(output_dir / "games.db"),
                    "--lean-db",
                    "--verbose", "0",
                    *extra_args,
                ]

                env = os.environ.copy()
                env["PYTHONPATH"] = self._get_ai_service_path()
                env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"
                env["RINGRIFT_JOB_ORIGIN"] = "p2p_orchestrator"
                env["CUDA_VISIBLE_DEVICES"] = ""  # Disable GPU for CPU-only jobs

                can_spawn, spawn_reason = self._can_spawn_process(f"cpu-selfplay-{board_type}-{num_players}p")
                if not can_spawn:
                    logger.info(f"BLOCKED CPU selfplay spawn: {spawn_reason}")
                    return None

                log_handle = open(output_dir / "run.log", "a")  # noqa: SIM115
                try:
                    proc = subprocess.Popen(
                        cmd,
                        stdout=log_handle,
                        stderr=subprocess.STDOUT,
                        env=env,
                        cwd=self.ringrift_path,
                    )
                    self._record_spawn()
                finally:
                    log_handle.close()

                job = ClusterJob(
                    job_id=job_id,
                    job_type=job_type,
                    node_id=self.node_id,
                    board_type=board_type,
                    num_players=num_players,
                    engine_mode=engine_mode_norm,
                    pid=proc.pid,
                    started_at=time.time(),
                    status="running",
                )

                with self.jobs_lock:
                    self.local_jobs[job_id] = job

                logger.info(f"Started {job_type.value} job {job_id} (PID {proc.pid}) [CPU-only hybrid mode]")
                self._save_state()

                # Track job start via JobOrchestrationManager (Jan 2026)
                if hasattr(self, "job_orchestration") and self.job_orchestration:
                    self.job_orchestration.record_job_started(job_type.value)

                # Dec 31, 2025: Add process monitoring to track completion/failure
                asyncio.create_task(self._monitor_selfplay_process(
                    job_id, proc, output_dir, board_type, num_players, "cpu_selfplay"
                ))

                return job

            elif job_type == JobType.GPU_SELFPLAY:
                # GPU selfplay using run_gpu_selfplay.py for high-quality training data
                # Uses vectorized GPU game simulation with heuristic-guided move selection

                # Normalize board type for CLI
                board_arg = {
                    "hex8": "hex8",
                    "hex": "hex8",
                    "square8": "square8",
                    "square19": "square19",
                    "hexagonal": "hexagonal",
                }.get(board_type, "square8")

                # Number of games per batch
                num_games = 100
                if board_arg == "square19":
                    num_games = 50  # Square19 games are longer
                elif board_arg == "hexagonal":
                    num_games = 100

                output_dir = Path(
                    self.ringrift_path,
                    "ai-service",
                    "data",
                    "games",
                )
                output_dir.mkdir(parents=True, exist_ok=True)

                # Use venv python if available, otherwise fall back to system python3
                venv_python = Path(self._get_ai_service_path(), "venv", "bin", "python")
                python_exec = str(venv_python) if venv_python.exists() else "python3"

                cmd = [
                    python_exec,
                    self._get_script_path("run_gpu_selfplay.py"),
                    "--board", board_arg,
                    "--num-players", str(num_players),
                    "--num-games", str(num_games),
                    "--output-dir", str(output_dir),
                ]

                # Start process with GPU environment
                env = os.environ.copy()
                env["PYTHONPATH"] = self._get_ai_service_path()
                env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"
                env["RINGRIFT_JOB_ORIGIN"] = "p2p_orchestrator"

                if cuda_visible_devices is not None and str(cuda_visible_devices).strip():
                    env["CUDA_VISIBLE_DEVICES"] = str(cuda_visible_devices).strip()
                # Choose a GPU automatically if not explicitly pinned.
                elif "CUDA_VISIBLE_DEVICES" not in env:
                    gpu_count = 0
                    try:
                        out = subprocess.run(
                            ["nvidia-smi", "--query-gpu=name", "--format=csv,noheader"],
                            capture_output=True, text=True, timeout=5
                        )
                        if out.returncode == 0 and out.stdout.strip():
                            gpu_count = len([line for line in out.stdout.splitlines() if line.strip()])
                    except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError, KeyError, IndexError, AttributeError):
                        gpu_count = 0

                    if gpu_count > 0:
                        with self.jobs_lock:
                            running_gpu_jobs = sum(
                                1 for j in self.local_jobs.values()
                                if j.job_type == JobType.GPU_SELFPLAY and j.status == "running"
                            )
                        env["CUDA_VISIBLE_DEVICES"] = str(running_gpu_jobs % gpu_count)
                    else:
                        env["CUDA_VISIBLE_DEVICES"] = "0"

                # SAFEGUARD: Final check before spawning (load + rate limit)
                can_spawn, spawn_reason = self._can_spawn_process(f"gpu-selfplay-{board_type}-{num_players}p")
                if not can_spawn:
                    logger.info(f"BLOCKED GPU selfplay spawn: {spawn_reason}")
                    return None

                log_handle = open(output_dir / "gpu_run.log", "a")  # noqa: SIM115
                try:
                    proc = subprocess.Popen(
                        cmd,
                        stdout=log_handle,
                        stderr=subprocess.STDOUT,
                        env=env,
                        cwd=self.ringrift_path,
                    )
                    self._record_spawn()  # Track spawn for rate limiting
                finally:
                    log_handle.close()

                # Use gumbel-mcts for GPU selfplay (177x speedup with GPU tree)
                gpu_engine_mode = "gumbel-mcts"
                batch_size = num_games

                job = ClusterJob(
                    job_id=job_id,
                    job_type=job_type,
                    node_id=self.node_id,
                    board_type=board_type,
                    num_players=num_players,
                    engine_mode=gpu_engine_mode,
                    pid=proc.pid,
                    started_at=time.time(),
                    status="running",
                )

                with self.jobs_lock:
                    self.local_jobs[job_id] = job

                # Jan 7, 2026: Track GPU job count for adaptive dispatch decisions
                self._update_gpu_job_count(+1)

                logger.info(f"Started GPU selfplay job {job_id} (PID {proc.pid}, batch={batch_size})")

                # Track diversity metrics for monitoring (Phase 2B, Dec 2025)
                self.selfplay_scheduler.track_diversity({
                    "board_type": board_type,
                    "num_players": num_players,
                    "engine_mode": gpu_engine_mode,
                })

                self._save_state()

                # Track job start via JobOrchestrationManager (Jan 2026)
                if hasattr(self, "job_orchestration") and self.job_orchestration:
                    self.job_orchestration.record_job_started(job_type.value)

                # Monitor GPU selfplay and trigger CPU validation when complete
                asyncio.create_task(self._monitor_gpu_selfplay_and_validate(
                    job_id, proc, output_dir, board_type, num_players
                ))

                return job

            elif job_type == JobType.HYBRID_SELFPLAY:
                # Hybrid CPU/GPU selfplay using run_self_play_soak.py
                # Uses CPU for game rules (100% canonical) but GPU for heuristic evaluation
                # This is the recommended default for GPU nodes
                # NOTE: run_hybrid_selfplay.py doesn't exist, use run_self_play_soak.py instead

                # Normalize engine_mode
                # run_self_play_soak.py supports: random-only, heuristic-only, mixed, nnue-guided, mcts,
                # gumbel-mcts-only, maxn-only, brs-only, policy-only, diverse, diverse-cpu
                # Map profile engine modes to soak script equivalents
                hybrid_engine_modes = {"random-only", "heuristic-only", "mixed", "nnue-guided", "mcts",
                                       "gumbel-mcts-only", "maxn-only", "brs-only", "policy-only", "diverse"}
                nn_modes = {"nn-only", "best-vs-pool", "nn-vs-mcts", "nn-vs-minimax", "nn-vs-descent", "tournament-varied"}
                # Map DIVERSE_PROFILES engine_mode values to run_self_play_soak.py equivalents
                engine_mode_map = {
                    "gumbel-mcts": "gumbel-mcts-only",  # High-quality Gumbel MCTS
                    "maxn": "maxn-only",                # Max-N for multiplayer
                    "brs": "brs-only",                  # Best-Reply Search
                    "minimax": "minimax-only",          # Minimax search
                }
                if engine_mode in hybrid_engine_modes:
                    engine_mode_norm = engine_mode
                elif engine_mode in engine_mode_map:
                    engine_mode_norm = engine_mode_map[engine_mode]  # Map to soak script name
                elif engine_mode in nn_modes:
                    engine_mode_norm = "nnue-guided"  # Use neural network
                elif engine_mode in ("mcts-only", "descent-only"):
                    engine_mode_norm = "mcts"  # Use MCTS
                elif engine_mode == "minimax-only":
                    engine_mode_norm = "minimax-only"  # Minimax search
                else:
                    # Default to diverse mode for GPU nodes (uses all AI types)
                    engine_mode_norm = "diverse"

                # Game counts based on board type
                num_games = 1000
                if board_type == "square19":
                    num_games = 500
                elif board_type in ("hex", "hexagonal"):
                    num_games = 300

                # Jan 7, 2026: Use _get_ai_service_path() to avoid doubled ai-service/ path
                output_dir = Path(
                    self._get_ai_service_path(),
                    "data",
                    "selfplay",
                    "p2p_hybrid",
                    f"{board_type}_{num_players}p",
                    job_id,
                )
                output_dir.mkdir(parents=True, exist_ok=True)

                # Board type is passed directly - run_self_play_soak.py accepts:
                # 'hex8', 'hexagonal', 'square8', 'square19'
                board_arg = board_type

                # Use venv python if available, otherwise fall back to system python3
                venv_python = Path(self._get_ai_service_path(), "venv", "bin", "python")
                python_exec = str(venv_python) if venv_python.exists() else "python3"

                cmd = [
                    python_exec,
                    self._get_script_path("run_self_play_soak.py"),
                    "--board-type", board_arg,
                    "--num-players", str(num_players),
                    "--num-games", str(num_games),
                    "--log-jsonl", str(output_dir / "games.jsonl"),
                    "--summary-json", str(output_dir / "summary.json"),
                    "--record-db", str(output_dir / "games.db"),
                    "--lean-db",
                    "--engine-mode", engine_mode_norm,
                    "--max-moves", "10000",
                    "--verbose", "0",
                ]

                # Start process with GPU environment
                env = os.environ.copy()
                env["PYTHONPATH"] = self._get_ai_service_path()
                env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"
                env["RINGRIFT_JOB_ORIGIN"] = "p2p_orchestrator"

                if cuda_visible_devices is not None and str(cuda_visible_devices).strip():
                    env["CUDA_VISIBLE_DEVICES"] = str(cuda_visible_devices).strip()
                elif "CUDA_VISIBLE_DEVICES" not in env:
                    gpu_count = 0
                    try:
                        out = subprocess.run(
                            ["nvidia-smi", "--query-gpu=name", "--format=csv,noheader"],
                            capture_output=True,
                            text=True,
                            timeout=5,
                        )
                        if out.returncode == 0 and out.stdout.strip():
                            gpu_count = len([line for line in out.stdout.splitlines() if line.strip()])
                    except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError, KeyError, IndexError, AttributeError):
                        gpu_count = 0

                    if gpu_count > 0:
                        with self.jobs_lock:
                            running_hybrid_jobs = sum(
                                1
                                for j in self.local_jobs.values()
                                if j.job_type == JobType.HYBRID_SELFPLAY and j.status == "running"
                            )
                        env["CUDA_VISIBLE_DEVICES"] = str(running_hybrid_jobs % gpu_count)
                    else:
                        env["CUDA_VISIBLE_DEVICES"] = "0"

                # SAFEGUARD: Final check before spawning (load + rate limit)
                can_spawn, spawn_reason = self._can_spawn_process(f"hybrid-selfplay-{board_type}-{num_players}p")
                if not can_spawn:
                    logger.info(f"BLOCKED hybrid selfplay spawn: {spawn_reason}")
                    return None

                log_handle = open(output_dir / "hybrid_run.log", "a")  # noqa: SIM115
                try:
                    proc = subprocess.Popen(
                        cmd,
                        stdout=log_handle,
                        stderr=subprocess.STDOUT,
                        env=env,
                        cwd=self.ringrift_path,
                    )
                    self._record_spawn()  # Track spawn for rate limiting
                finally:
                    log_handle.close()

                job = ClusterJob(
                    job_id=job_id,
                    job_type=job_type,
                    node_id=self.node_id,
                    board_type=board_type,
                    num_players=num_players,
                    engine_mode=engine_mode_norm,
                    pid=proc.pid,
                    started_at=time.time(),
                    status="running",
                )

                with self.jobs_lock:
                    self.local_jobs[job_id] = job

                logger.info(f"Started HYBRID selfplay job {job_id} (PID {proc.pid})")

                # Track diversity metrics for monitoring (Phase 2B, Dec 2025)
                self.selfplay_scheduler.track_diversity({
                    "board_type": board_type,
                    "num_players": num_players,
                    "engine_mode": engine_mode_norm,
                })

                self._save_state()

                # Track job start via JobOrchestrationManager (Jan 2026)
                if hasattr(self, "job_orchestration") and self.job_orchestration:
                    self.job_orchestration.record_job_started(job_type.value)

                # Dec 31, 2025: Add process monitoring to track completion/failure
                asyncio.create_task(self._monitor_selfplay_process(
                    job_id, proc, output_dir, board_type, num_players, "hybrid_selfplay"
                ))

                return job

            elif job_type == JobType.GUMBEL_SELFPLAY:
                # High-quality Gumbel MCTS selfplay with NN policy for self-improvement training
                # Uses generate_gumbel_selfplay.py with proper MCTS simulation budget
                # Budget tiers: THROUGHPUT(64), BOOTSTRAP(150), QUALITY(800), ULTIMATE(1600), MASTER(3200)
                #
                # Jan 2026: Use adaptive budget based on config Elo instead of hardcoded 150.
                # This ensures mature configs (Elo > 1400) use quality budgets for better training data.
                if simulation_budget is not None:
                    effective_budget = simulation_budget
                else:
                    # Look up config Elo and use adaptive budget
                    config_key = f"{board_type}_{num_players}p"
                    try:
                        config_elo = self.selfplay_scheduler.get_config_elo(config_key) if hasattr(self, 'selfplay_scheduler') else 1200.0
                        effective_budget = get_adaptive_budget_for_elo(config_elo)
                        logger.debug(f"[Gumbel] {config_key}: Elo={config_elo:.0f} -> budget={effective_budget}")
                    except Exception:
                        effective_budget = 150  # Fallback to bootstrap tier

                # Games based on board type and budget
                # Lower budget = can run more games in same time
                num_games = 50 if effective_budget >= 800 else 100  # More games for lower budget
                if board_type == "square19":
                    num_games = 10 if effective_budget >= 800 else 50  # Large board
                elif board_type in ("hex", "hexagonal", "hex8"):
                    num_games = 20 if effective_budget >= 800 else 100

                output_dir = Path(
                    self.ringrift_path,
                    "ai-service",
                    "data",
                    "selfplay",
                    "gumbel",
                    f"{board_type}_{num_players}p",
                    job_id,
                )
                output_dir.mkdir(parents=True, exist_ok=True)

                # Normalize board type for gumbel script
                board_arg = {
                    "hex": "hexagonal",
                    "hex8": "hex8",
                }.get(board_type, board_type)

                # Use venv python if available
                venv_python = Path(self._get_ai_service_path(), "venv", "bin", "python")
                python_exec = str(venv_python) if venv_python.exists() else "python3"

                cmd = [
                    python_exec,
                    self._get_script_path("generate_gumbel_selfplay.py"),
                    "--board", board_arg,
                    "--num-players", str(num_players),
                    "--num-games", str(num_games),
                    "--simulation-budget", str(effective_budget),
                    "--output-dir", str(output_dir),
                    "--db", str(output_dir / "games.db"),
                    "--seed", str(int(time.time() * 1000) % 2**31),
                    "--allow-fresh-weights",  # Allow running even without trained model
                ]

                # Dec 31, 2025: Check if GPU tree is disabled for this node (e.g., vGPU nodes)
                # vGPU instances don't properly accelerate GPU tree MCTS, causing CPU-bound slowdown
                node_config = self._load_distributed_hosts().get("hosts", {}).get(self.node_id, {})
                if not node_config.get("disable_gpu_tree", False):
                    cmd.append("--use-gpu-tree")  # 170x speedup with GPU tensor tree MCTS

                # Start process with GPU environment
                env = os.environ.copy()
                env["PYTHONPATH"] = self._get_ai_service_path()
                env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"
                env["RINGRIFT_JOB_ORIGIN"] = "p2p_orchestrator"

                if cuda_visible_devices is not None and str(cuda_visible_devices).strip():
                    env["CUDA_VISIBLE_DEVICES"] = str(cuda_visible_devices).strip()
                elif "CUDA_VISIBLE_DEVICES" not in env:
                    gpu_count = 0
                    try:
                        out = subprocess.run(
                            ["nvidia-smi", "--query-gpu=name", "--format=csv,noheader"],
                            capture_output=True,
                            text=True,
                            timeout=5,
                        )
                        if out.returncode == 0 and out.stdout.strip():
                            gpu_count = len([line for line in out.stdout.splitlines() if line.strip()])
                    except (subprocess.SubprocessError, subprocess.TimeoutExpired, OSError, KeyError, IndexError, AttributeError):
                        gpu_count = 0

                    if gpu_count > 0:
                        with self.jobs_lock:
                            running_gumbel_jobs = sum(
                                1
                                for j in self.local_jobs.values()
                                if j.job_type == JobType.GUMBEL_SELFPLAY and j.status == "running"
                            )
                        env["CUDA_VISIBLE_DEVICES"] = str(running_gumbel_jobs % gpu_count)
                    else:
                        env["CUDA_VISIBLE_DEVICES"] = "0"

                # SAFEGUARD: Final check before spawning
                can_spawn, spawn_reason = self._can_spawn_process(f"gumbel-selfplay-{board_type}-{num_players}p")
                if not can_spawn:
                    logger.info(f"BLOCKED gumbel selfplay spawn: {spawn_reason}")
                    return None

                log_handle = open(output_dir / "gumbel_run.log", "a")  # noqa: SIM115
                try:
                    proc = subprocess.Popen(
                        cmd,
                        stdout=log_handle,
                        stderr=subprocess.STDOUT,
                        env=env,
                        cwd=self.ringrift_path,
                    )
                    self._record_spawn()
                finally:
                    log_handle.close()

                job = ClusterJob(
                    job_id=job_id,
                    job_type=job_type,
                    node_id=self.node_id,
                    board_type=board_type,
                    num_players=num_players,
                    engine_mode="gumbel-mcts",
                    pid=proc.pid,
                    started_at=time.time(),
                    status="running",
                )

                with self.jobs_lock:
                    self.local_jobs[job_id] = job

                logger.info(f"Started GUMBEL selfplay job {job_id} (PID {proc.pid}, sims={effective_budget})")

                # Track diversity metrics for monitoring (Phase 2B, Dec 2025)
                self.selfplay_scheduler.track_diversity({
                    "board_type": board_type,
                    "num_players": num_players,
                    "engine_mode": "gumbel-mcts",
                })

                self._save_state()

                # Track job start via JobOrchestrationManager (Jan 2026)
                if hasattr(self, "job_orchestration") and self.job_orchestration:
                    self.job_orchestration.record_job_started(job_type.value)

                # Dec 31, 2025: Add process monitoring to track completion/failure
                asyncio.create_task(self._monitor_selfplay_process(
                    job_id, proc, output_dir, board_type, num_players, "gumbel_selfplay"
                ))

                return job

            elif job_type == JobType.DATA_EXPORT:
                # CPU-intensive data export job (NPZ creation)
                # These jobs should be routed to high-CPU nodes (vast nodes preferred)
                if not export_params:
                    logger.info("DATA_EXPORT job requires export_params")
                    return None

                input_path = export_params.get("input_path")
                output_path = export_params.get("output_path")
                encoder_version = export_params.get("encoder_version", "v3")
                max_games = export_params.get("max_games", 5000)
                is_jsonl = export_params.get("is_jsonl", False)

                if not input_path or not output_path:
                    logger.info("DATA_EXPORT requires input_path and output_path")
                    return None

                # Ensure output directory exists
                output_dir = Path(output_path).parent
                output_dir.mkdir(parents=True, exist_ok=True)

                # Use venv python if available
                venv_python = Path(self._get_ai_service_path(), "venv", "bin", "python")
                python_exec = str(venv_python) if venv_python.exists() else "python3"

                if is_jsonl:
                    # Use jsonl_to_npz.py for JSONL input (GPU selfplay data)
                    export_script = self._get_script_path("jsonl_to_npz.py")
                    cmd = [
                        python_exec,
                        export_script,
                        "--input", str(input_path),
                        "--output", str(output_path),
                        "--board-type", board_type,
                        "--num-players", str(num_players),
                        "--gpu-selfplay",
                        "--max-games", str(max_games),
                    ]
                    if encoder_version and encoder_version != "default":
                        cmd.extend(["--encoder-version", encoder_version])
                else:
                    # Use export_replay_dataset.py for DB input
                    export_script = self._get_script_path("export_replay_dataset.py")
                    cmd = [
                        python_exec,
                        export_script,
                        "--db", str(input_path),
                        "--output", str(output_path),
                        "--board-type", board_type,
                        "--num-players", str(num_players),
                        "--max-games", str(max_games),
                        "--require-completed",
                        "--min-moves", "10",
                    ]
                    if encoder_version and encoder_version != "default":
                        cmd.extend(["--encoder-version", encoder_version])

                # Start export process
                env = os.environ.copy()
                env["PYTHONPATH"] = self._get_ai_service_path()
                env["RINGRIFT_SKIP_SHADOW_CONTRACTS"] = "true"

                log_path = output_dir / f"export_{job_id}.log"
                log_handle = open(log_path, "w")  # noqa: SIM115
                try:
                    proc = subprocess.Popen(
                        cmd,
                        stdout=log_handle,
                        stderr=subprocess.STDOUT,
                        env=env,
                        cwd=self._get_ai_service_path(),
                    )
                finally:
                    log_handle.close()

                job = ClusterJob(
                    job_id=job_id,
                    job_type=job_type,
                    node_id=self.node_id,
                    board_type=board_type,
                    num_players=num_players,
                    engine_mode="export",
                    pid=proc.pid,
                    started_at=time.time(),
                    status="running",
                )

                with self.jobs_lock:
                    self.local_jobs[job_id] = job

                logger.info(f"Started DATA_EXPORT job {job_id} (PID {proc.pid}): {input_path} -> {output_path}")
                self._save_state()

                # Track job start via JobOrchestrationManager (Jan 2026)
                if hasattr(self, "job_orchestration") and self.job_orchestration:
                    self.job_orchestration.record_job_started(job_type.value)

                return job

        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to start job: {e}")
        return None

    async def _dispatch_export_job(
        self,
        node: NodeInfo,
        input_path: str,
        output_path: str,
        board_type: str,
        num_players: int,
        encoder_version: str = "v3",
        max_games: int = 5000,
        is_jsonl: bool = False,
    ):
        """Dispatch a CPU-intensive export job to a high-CPU node.

        CPU-intensive jobs like NPZ export should run on vast nodes
        (256-512 CPUs) rather than lambda nodes (64 CPUs) to free
        GPU resources for training/selfplay.
        """
        try:
            job_id = f"export_{board_type}_{num_players}p_{int(time.time())}_{uuid.uuid4().hex[:6]}"

            payload = {
                "job_id": job_id,
                "job_type": JobType.DATA_EXPORT.value,
                "board_type": board_type,
                "num_players": num_players,
                "input_path": input_path,
                "output_path": output_path,
                "encoder_version": encoder_version,
                "max_games": max_games,
                "is_jsonl": is_jsonl,
            }

            # NAT-blocked nodes need relay command
            if getattr(node, "nat_blocked", False):
                cmd_id = await self._enqueue_relay_command_for_peer(node, "start_job", payload)
                if cmd_id:
                    logger.info(f"Enqueued relay export job for {node.node_id}: {job_id}")
                else:
                    logger.info(f"Relay queue full for {node.node_id}; export not dispatched")
                return

            timeout = ClientTimeout(total=30)
            async with get_client_session(timeout) as session:
                last_err: str | None = None
                for url in self._urls_for_peer(node, "/start_job"):
                    try:
                        async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                            if resp.status == 200:
                                result = await resp.json()
                                if result.get("success"):
                                    logger.info(f"Export job dispatched to {node.node_id}: {job_id}")
                                    return
                                last_err = result.get("error", "unknown")
                            else:
                                last_err = f"http_{resp.status}"
                    except Exception as e:  # noqa: BLE001
                        last_err = str(e)

                if last_err:
                    logger.info(f"Export job dispatch failed to {node.node_id}: {last_err}")

        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to dispatch export job to {node.node_id}: {e}")

    async def _request_remote_job(
        self,
        node: NodeInfo,
        job_type: JobType,
        board_type: str = "square8",
        num_players: int = 2,
        engine_mode: str = "hybrid",
    ):
        """Request a remote node to start a job with specific configuration.

        SAFEGUARD: Checks coordination safeguards before requesting remote spawn.
        """
        try:
            # SAFEGUARD: Check safeguards before requesting remote spawn
            if HAS_SAFEGUARDS and _safeguards:
                task_type_str = job_type.value if hasattr(job_type, 'value') else str(job_type)
                allowed, reason = check_before_spawn(task_type_str, node.node_id)
                if not allowed:
                    logger.info(f"SAFEGUARD blocked remote {task_type_str} on {node.node_id}: {reason}")
                    return

            job_id = f"{job_type.value}_{board_type}_{num_players}p_{int(time.time())}_{uuid.uuid4().hex[:6]}"

            # NAT-blocked nodes can't accept inbound /start_job; enqueue a relay command instead.
            if getattr(node, "nat_blocked", False):
                payload = {
                    "job_id": job_id,
                    "job_type": job_type.value,
                    "board_type": board_type,
                    "num_players": num_players,
                    "engine_mode": engine_mode,
                }
                cmd_id = await self._enqueue_relay_command_for_peer(node, "start_job", payload)
                if cmd_id:
                    print(
                        f"[P2P] Enqueued relay job for {node.node_id}: "
                        f"{job_type.value} {board_type} {num_players}p ({job_id})"
                    )
                else:
                    logger.info(f"Relay queue full for {node.node_id}; skipping enqueue")
                return

            timeout = ClientTimeout(total=10)
            async with get_client_session(timeout) as session:
                payload = {
                    "job_id": job_id,
                    "job_type": job_type.value,
                    "board_type": board_type,
                    "num_players": num_players,
                    "engine_mode": engine_mode,
                }
                last_err: str | None = None
                for url in self._urls_for_peer(node, "/start_job"):
                    try:
                        async with session.post(url, json=payload, headers=self._auth_headers()) as resp:
                            if resp.status != 200:
                                last_err = f"http_{resp.status}"
                                continue
                            data = await resp.json()
                            if data.get("success"):
                                logger.info(f"Started remote {board_type} {num_players}p job on {node.node_id}")
                                return
                            last_err = str(data.get("error") or "start_failed")
                    except Exception as e:  # noqa: BLE001
                        last_err = str(e)
                        continue
                if last_err:
                    logger.error(f"Failed to start remote job on {node.node_id}: {last_err}")
        except Exception as e:  # noqa: BLE001
            logger.error(f"Failed to request remote job from {node.node_id}: {e}")

    def _enqueue_relay_command(self, node_id: str, cmd_type: str, payload: dict[str, Any]) -> str | None:
        """Leader-side: enqueue a command for a NAT-blocked node to pull."""
        now = time.time()
        cmd_type = str(cmd_type)
        payload = dict(payload or {})

        with self.relay_lock:
            queue = list(self.relay_command_queue.get(node_id, []))
            queue = [
                cmd for cmd in queue
                if float(cmd.get("expires_at", 0.0) or 0.0) > now
            ]

            if cmd_type == "start_job":
                pending = sum(1 for c in queue if str(c.get("type") or "") == "start_job")
                if pending >= RELAY_MAX_PENDING_START_JOBS:
                    self.relay_command_queue[node_id] = queue
                    return None

                job_id = str(payload.get("job_id") or "")
                if job_id:
                    for c in queue:
                        if str(c.get("payload", {}).get("job_id") or "") == job_id:
                            self.relay_command_queue[node_id] = queue
                            return str(c.get("id") or "")

            cmd_id = uuid.uuid4().hex
            queue.append(
                {
                    "id": cmd_id,
                    "type": cmd_type,
                    "payload": payload,
                    "created_at": now,
                    "expires_at": now + RELAY_COMMAND_TTL_SECONDS,
                }
            )
            self.relay_command_queue[node_id] = queue
            return cmd_id

    async def _enqueue_relay_command_for_peer(
        self,
        peer: NodeInfo,
        cmd_type: str,
        payload: dict[str, Any],
    ) -> str | None:
        """Enqueue a relay command for `peer`, forwarding via its relay hub when needed.

        Default behavior: NAT-blocked nodes poll the leader's `/relay/heartbeat`
        endpoint and the leader stores commands in-memory.

        Some nodes (notably certain containerized GPU providers) may be unable to
        reach the leader over the mesh network (e.g. TUN-less Tailscale) and also
        cannot accept inbound connections. Those nodes will instead send relay
        heartbeats to an internet-reachable hub (e.g. `aws-staging`). When
        `peer.relay_via` points to such a hub, the leader must enqueue the relay
        command on that hub so the node can pull and execute it.
        """
        if not peer or not getattr(peer, "node_id", ""):
            return None

        peer_id = str(getattr(peer, "node_id", "") or "").strip()
        if not peer_id:
            return None

        relay_node_id = str(getattr(peer, "relay_via", "") or "").strip()
        if relay_node_id and relay_node_id != self.node_id:
            with self.peers_lock:
                relay_peer = self.peers.get(relay_node_id)
            if relay_peer:
                timeout = ClientTimeout(total=10)
                async with get_client_session(timeout) as session:
                    last_err: str | None = None
                    for url in self._urls_for_peer(relay_peer, "/relay/enqueue"):
                        try:
                            async with session.post(
                                url,
                                json={
                                    "target_node_id": peer_id,
                                    "type": cmd_type,
                                    "payload": payload or {},
                                },
                                headers=self._auth_headers(),
                            ) as resp:
                                if resp.status != 200:
                                    last_err = f"http_{resp.status}"
                                    continue
                                data = await resp.json()
                                if data.get("success"):
                                    return str(data.get("id") or "")
                                last_err = str(data.get("error") or "enqueue_failed")
                        except Exception as e:  # noqa: BLE001
                            last_err = str(e)
                            continue
                    if last_err:
                        logger.info(f"Relay enqueue via {relay_node_id} failed for {peer_id}: {last_err}")
                        # Dec 30, 2025: Automatic relay failover
                        # If the current relay is unreachable, try to find a new one
                        # January 4, 2026: Pass peer_id for configured relay preferences
                        new_relay = self._select_best_relay(for_peer=peer_id)
                        if new_relay and new_relay != relay_node_id:
                            logger.info(
                                f"[RelayFailover] Switching {peer_id} relay: "
                                f"{relay_node_id} -> {new_relay}"
                            )
                            with self.peers_lock:
                                if peer_id in self.peers:
                                    self.peers[peer_id].relay_via = new_relay
                            # Try enqueue on new relay
                            with self.peers_lock:
                                new_relay_peer = self.peers.get(new_relay)
                            if new_relay_peer:
                                for url in self._urls_for_peer(new_relay_peer, "/relay/enqueue"):
                                    try:
                                        timeout = ClientTimeout(total=10)
                                        async with get_client_session(timeout) as session2:
                                            async with session2.post(
                                                url,
                                                json={
                                                    "target_node_id": peer_id,
                                                    "type": cmd_type,
                                                    "payload": payload or {},
                                                },
                                                headers=self._auth_headers(),
                                            ) as resp2:
                                                if resp2.status == 200:
                                                    data2 = await resp2.json()
                                                    if data2.get("success"):
                                                        return str(data2.get("id") or "")
                                    except Exception:  # noqa: BLE001
                                        continue

        # Fallback: enqueue locally (works when peer polls the leader directly).
        return self._enqueue_relay_command(peer_id, cmd_type, payload)

    async def _discovery_loop(self):
        """Broadcast UDP discovery messages to find peers on local network."""
        # Phase 3.1 Dec 29, 2025: Add max iterations to prevent infinite loop
        # Jan 13, 2026: Fix busy loop - add yield points and run socket ops in thread
        MAX_RECEIVE_ITERATIONS = 100
        YIELD_EVERY_N_PACKETS = 10  # Yield to event loop every N packets

        def _do_udp_discovery() -> list[dict]:
            """Run blocking UDP discovery in thread pool to avoid blocking event loop."""
            discovered = []
            try:
                sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
                sock.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)
                sock.settimeout(1.0)

                # Broadcast our presence
                message = json.dumps({
                    "type": "p2p_discovery",
                    "node_id": self.node_id,
                    "host": self.self_info.host,
                    "port": self.port,
                }).encode()

                with contextlib.suppress(OSError):
                    sock.sendto(message, ("<broadcast>", DISCOVERY_PORT))

                # Listen for responses with iteration limit
                receive_count = 0
                try:
                    while receive_count < MAX_RECEIVE_ITERATIONS:
                        data, _addr = sock.recvfrom(1024)
                        receive_count += 1
                        try:
                            msg = json.loads(data.decode())
                            if msg.get("type") == "p2p_discovery" and msg.get("node_id") != self.node_id:
                                discovered.append(msg)
                        except (json.JSONDecodeError, UnicodeDecodeError):
                            continue
                except TimeoutError:
                    pass

                if receive_count >= MAX_RECEIVE_ITERATIONS:
                    logger.warning(f"[UdpDiscovery] Hit max receive limit ({MAX_RECEIVE_ITERATIONS})")

                sock.close()
            except OSError as e:
                logger.debug(f"[UdpDiscovery] Socket error: {e}")
            return discovered

        while self.running:
            try:
                # Run blocking socket operations in thread pool
                discovered = await asyncio.to_thread(_do_udp_discovery)

                # Process discovered peers (yield periodically to prevent busy loop)
                for i, msg in enumerate(discovered):
                    peer_addr = f"{msg.get('host')}:{msg.get('port')}"
                    if peer_addr not in self.known_peers:
                        self.known_peers.append(peer_addr)
                        logger.info(f"Discovered peer: {msg.get('node_id')} at {peer_addr}")
                    # Yield to event loop every N packets to prevent blocking
                    if (i + 1) % YIELD_EVERY_N_PACKETS == 0:
                        await asyncio.sleep(0)

            except Exception as e:  # noqa: BLE001
                logger.debug(f"[UdpDiscovery] Error: {e}")
                # Brief sleep on error to prevent tight retry loop
                await asyncio.sleep(1.0)
                continue

            await asyncio.sleep(DISCOVERY_INTERVAL)

    def _validate_critical_subsystems(self) -> dict:
        """Validate critical subsystems at startup.

        Returns a status dict with protocol and manager availability.
        Logs clear messages about which protocols are active.

        December 2025: Added to address silent fallback behavior
        where operators couldn't tell if SWIM/Raft was running.
        """
        from app.p2p.constants import (
            SWIM_ENABLED, RAFT_ENABLED, MEMBERSHIP_MODE, CONSENSUS_MODE
        )

        status = {
            "protocols": {
                "membership_mode": MEMBERSHIP_MODE,
                "consensus_mode": CONSENSUS_MODE,
                "swim_enabled": SWIM_ENABLED,
                "raft_enabled": RAFT_ENABLED,
            },
            "managers": {},
            "warnings": [],
            "errors": [],
        }

        # Check SWIM availability
        try:
            from app.p2p.swim_adapter import SWIM_AVAILABLE
            status["protocols"]["swim_available"] = SWIM_AVAILABLE
            if SWIM_ENABLED and not SWIM_AVAILABLE:
                msg = "SWIM_ENABLED=true but swim-p2p not installed. Install: pip install swim-p2p>=1.2.0"
                status["warnings"].append(msg)
                logger.warning(f"[Startup Validation] {msg}")
            elif SWIM_AVAILABLE:
                logger.info(f"[Startup Validation] SWIM protocol available (membership_mode={MEMBERSHIP_MODE})")
        except ImportError:
            status["protocols"]["swim_available"] = False
            if SWIM_ENABLED:
                status["warnings"].append("swim_adapter import failed")

        # Check Raft availability
        try:
            from app.p2p.raft_state import PYSYNCOBJ_AVAILABLE
            status["protocols"]["raft_available"] = PYSYNCOBJ_AVAILABLE
            if RAFT_ENABLED and not PYSYNCOBJ_AVAILABLE:
                msg = "RAFT_ENABLED=true but pysyncobj not installed. Install: pip install pysyncobj>=0.3.14"
                status["warnings"].append(msg)
                logger.warning(f"[Startup Validation] {msg}")
            elif PYSYNCOBJ_AVAILABLE:
                logger.info(f"[Startup Validation] Raft protocol available (consensus_mode={CONSENSUS_MODE})")
        except ImportError:
            status["protocols"]["raft_available"] = False
            if RAFT_ENABLED:
                status["warnings"].append("raft_state import failed")

        # Log active protocol configuration
        logger.info(
            f"[Startup Validation] Protocol config: membership={MEMBERSHIP_MODE}, consensus={CONSENSUS_MODE}"
        )

        # Check critical managers (lazy load check - don't fail, just report)
        manager_checks = [
            ("work_queue", "app.coordination.work_queue", "get_work_queue"),
            ("health_manager", "app.coordination.unified_health_manager", "get_unified_health_manager"),
            ("sync_router", "app.coordination.sync_router", "get_sync_router"),
        ]

        for name, module_path, getter_name in manager_checks:
            try:
                module = importlib.import_module(module_path)
                getter = getattr(module, getter_name, None)
                status["managers"][name] = getter is not None
                if getter:
                    logger.debug(f"[Startup Validation] Manager {name} available")
            except ImportError as e:
                status["managers"][name] = False
                status["warnings"].append(f"{name} import failed: {e}")
                logger.warning(f"[Startup Validation] Manager {name} unavailable: {e}")

        # December 2025: P2P voter connectivity validation
        # Check that at least quorum voters are reachable before starting
        status["voters"] = {
            "configured": len(self.voter_node_ids),
            "quorum": self.voter_quorum_size,
            "reachable": 0,
            "unreachable": [],
        }

        if self.voter_node_ids:
            import socket
            import contextlib

            reachable_count = 0
            for voter_id in self.voter_node_ids:
                # Try to resolve voter IP from config
                try:
                    from app.config.cluster_config import get_cluster_nodes
                    nodes = get_cluster_nodes()
                    node = nodes.get(voter_id)
                    if node:
                        voter_ip = node.best_ip
                        if voter_ip:
                            # Quick TCP connect test to P2P port
                            with contextlib.suppress(Exception):
                                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                                sock.settimeout(2.0)
                                result = sock.connect_ex((voter_ip, self.port))
                                sock.close()
                                if result == 0:
                                    reachable_count += 1
                                    continue
                    status["voters"]["unreachable"].append(voter_id)
                except (socket.error, socket.timeout, OSError, TimeoutError, ConnectionRefusedError):
                    status["voters"]["unreachable"].append(voter_id)

            status["voters"]["reachable"] = reachable_count

            # Log voter connectivity status
            if reachable_count < self.voter_quorum_size:
                msg = (
                    f"Only {reachable_count}/{len(self.voter_node_ids)} voters reachable, "
                    f"need {self.voter_quorum_size} for quorum. Unreachable: {status['voters']['unreachable']}"
                )
                status["warnings"].append(msg)
                logger.warning(f"[Startup Validation] {msg}")
                # January 4, 2026: Emit QUORUM_VALIDATION_FAILED event for monitoring.
                # This enables dashboards and alert systems to track pre-startup quorum issues.
                try:
                    from app.distributed.data_events import DataEventType, get_event_bus
                    get_event_bus().emit(
                        DataEventType.QUORUM_VALIDATION_FAILED,
                        {
                            "node_id": self.node_id,
                            "reachable_voters": reachable_count,
                            "total_voters": len(self.voter_node_ids),
                            "quorum_required": self.voter_quorum_size,
                            "unreachable": status["voters"]["unreachable"],
                            "timestamp": datetime.now(timezone.utc).isoformat(),
                        },
                    )
                except (ImportError, Exception) as e:
                    logger.debug(f"[Startup Validation] Could not emit quorum validation event: {e}")
            else:
                logger.info(
                    f"[Startup Validation] Voter quorum OK: "
                    f"{reachable_count}/{len(self.voter_node_ids)} voters reachable"
                )
        else:
            logger.info("[Startup Validation] No voters configured - quorum checks disabled")

        # Jan 9, 2026: PyTorch CUDA validation - detect CPU-only PyTorch on GPU nodes
        # Root cause of lambda-gh200-10 running at 0% GPU utilization
        try:
            pytorch_status = self._resource_detector.validate_pytorch_cuda()
            status["pytorch"] = pytorch_status

            if pytorch_status.get("warning"):
                status["warnings"].append(pytorch_status["warning"])
                logger.warning(f"[Startup Validation] {pytorch_status['warning']}")

                # Emit event for monitoring and dashboards
                try:
                    from app.distributed.data_events import DataEventType, get_event_bus

                    get_event_bus().emit(
                        DataEventType.PYTORCH_CUDA_MISMATCH,
                        {
                            "node_id": self.node_id,
                            "warning": pytorch_status["warning"],
                            "gpu_detected": pytorch_status.get("gpu_detected", False),
                            "pytorch_cuda_available": pytorch_status.get("pytorch_cuda_available", False),
                            "timestamp": datetime.now(timezone.utc).isoformat(),
                        },
                    )
                except (ImportError, Exception) as e:
                    logger.debug(f"[Startup Validation] Could not emit PyTorch CUDA event: {e}")
            elif pytorch_status.get("pytorch_cuda_available"):
                logger.info(
                    f"[Startup Validation] PyTorch CUDA OK: "
                    f"version={pytorch_status.get('pytorch_cuda_version')}, "
                    f"devices={pytorch_status.get('cuda_device_count')}"
                )
            elif pytorch_status.get("error"):
                logger.debug(f"[Startup Validation] PyTorch not installed: {pytorch_status.get('error')}")
        except Exception as e:
            logger.debug(f"[Startup Validation] PyTorch validation failed: {e}")

        # Summary log
        available_count = sum(1 for v in status["managers"].values() if v)
        total_count = len(status["managers"])
        if status["warnings"]:
            logger.warning(
                f"[Startup Validation] Completed with {len(status['warnings'])} warnings. "
                f"Managers: {available_count}/{total_count} available"
            )
        else:
            logger.info(
                f"[Startup Validation] All checks passed. "
                f"Managers: {available_count}/{total_count} available"
            )

        return status

    def _start_isolated_health_server(self) -> None:
        """Start a lightweight health HTTP server in a separate thread.

        January 2026: This server runs in its own thread with its own event loop,
        guaranteeing that /health endpoints respond even when the main event loop
        is blocked by background tasks.

        The isolated server:
        - Listens on port 8771 (one above the main P2P port)
        - Only serves /health and /ready endpoints
        - Does not access any state that requires the main event loop
        - Responds within 100ms even under heavy load

        This fixes the "zombie P2P" issue where the main HTTP server stops
        responding due to event loop blocking, but the process remains alive.
        """
        orchestrator = self  # Capture self for the inner function
        # Use port + 2 for isolated health server (8772 for P2P on 8770)
        # Port + 1 (8771) is used by daemon_manager's isolated health server
        health_port = self.port + 2

        def _run_health_server_in_thread() -> None:
            """Run the health server in a separate thread with its own event loop."""
            import asyncio as thread_asyncio

            async def handle_health(request: web.Request) -> web.Response:
                """Liveness probe - returns 200 if P2P process is alive.

                This is a lightweight check that doesn't access heavy state.
                """
                uptime = time.time() - getattr(orchestrator, "start_time", time.time())
                return web.json_response({
                    "alive": True,
                    "node_id": orchestrator.node_id,
                    "role": orchestrator.role.value if hasattr(orchestrator.role, 'value') else str(orchestrator.role),
                    "uptime_seconds": uptime,
                    "main_port": orchestrator.port,
                    "isolated_health_server": True,
                    "timestamp": datetime.utcnow().isoformat(),
                })

            async def handle_ready(request: web.Request) -> web.Response:
                """Readiness probe - returns 200 if P2P has started up.

                Checks minimal state without blocking.
                """
                uptime = time.time() - getattr(orchestrator, "start_time", time.time())
                # Consider ready after 30 seconds of uptime
                is_ready = uptime >= 30.0
                return web.json_response({
                    "ready": is_ready,
                    "node_id": orchestrator.node_id,
                    "uptime_seconds": uptime,
                    "startup_complete": is_ready,
                    "timestamp": datetime.utcnow().isoformat(),
                }, status=200 if is_ready else 503)

            async def run_server() -> None:
                """Set up and run the health server."""
                app = web.Application()
                app.router.add_get('/health', handle_health)
                app.router.add_get('/ready', handle_ready)

                runner = web.AppRunner(app)
                await runner.setup()

                try:
                    site = web.TCPSite(runner, '0.0.0.0', health_port, reuse_address=True)
                    await site.start()
                    logger.info(f"Isolated health server started on 0.0.0.0:{health_port}")

                    # Keep the server running forever
                    while True:
                        await thread_asyncio.sleep(3600)
                except OSError as e:
                    if "Address already in use" in str(e):
                        logger.warning(f"Isolated health server port {health_port} in use, skipping")
                    else:
                        logger.error(f"Isolated health server failed: {e}")
                except Exception as e:
                    logger.error(f"Isolated health server error: {e}")

            # Create and run a new event loop in this thread
            loop = thread_asyncio.new_event_loop()
            thread_asyncio.set_event_loop(loop)
            try:
                loop.run_until_complete(run_server())
            except Exception as e:
                logger.error(f"Isolated health server thread failed: {e}")
            finally:
                loop.close()

        # Start the health server in a daemon thread
        health_thread = threading.Thread(
            target=_run_health_server_in_thread,
            name="isolated-health-server",
            daemon=True,
        )
        health_thread.start()
        logger.info(f"Started isolated health server thread (port {health_port})")

    async def restart_http_server(self) -> bool:
        """Restart the HTTP server gracefully without terminating the process.

        January 2026: Added to enable recovery from HTTP server failures without
        requiring full process restart. Called by HttpServerHealthLoop when the
        server becomes unresponsive.

        Returns:
            True if restart succeeded, False otherwise
        """
        async with self._http_restart_lock:
            self._http_restart_count += 1
            attempt = self._http_restart_count
            logger.warning(f"[P2P] HTTP server restart attempt {attempt}")

            try:
                # Stop existing sites
                for site in self._http_sites:
                    try:
                        await site.stop()
                    except Exception as e:
                        logger.debug(f"[P2P] Error stopping site: {e}")
                self._http_sites.clear()

                # Cleanup runner
                if self._http_runner is not None:
                    try:
                        await self._http_runner.cleanup()
                    except Exception as e:
                        logger.debug(f"[P2P] Error cleaning up runner: {e}")

                # Wait briefly for port to be released
                await asyncio.sleep(1.0)

                # Create new runner from existing app
                if self._http_app is None:
                    logger.error("[P2P] Cannot restart: HTTP app not initialized")
                    return False

                self._http_runner = web.AppRunner(self._http_app)
                await self._http_runner.setup()

                # Re-bind ports
                site_v4 = web.TCPSite(
                    self._http_runner, '0.0.0.0', self.port,
                    reuse_address=True, backlog=1024
                )
                await site_v4.start()
                self._http_sites.append(site_v4)
                logger.info(f"[P2P] HTTP server restarted on 0.0.0.0:{self.port}")

                # Try IPv6 as well
                try:
                    site_v6 = web.TCPSite(
                        self._http_runner, '::', self.port,
                        reuse_address=True, backlog=1024
                    )
                    await site_v6.start()
                    self._http_sites.append(site_v6)
                    logger.info(f"[P2P] HTTP server also listening on [::]:{self.port}")
                except OSError:
                    pass  # IPv6 optional

                logger.info(f"[P2P] HTTP server restart {attempt} successful")
                return True

            except Exception as e:
                logger.error(f"[P2P] HTTP server restart {attempt} failed: {e}")
                return False

    async def run(self):
        """Main entry point - start the orchestrator."""
        if not HAS_AIOHTTP:
            logger.error("aiohttp is required. Install with: pip install aiohttp")
            raise RuntimeError("aiohttp is required but not available - install with: pip install aiohttp")

        # Start isolated health server FIRST (January 2026)
        # This ensures /health endpoint is always responsive even if main loop blocks
        self._start_isolated_health_server()

        # Validate critical subsystems before starting (December 2025)
        self._startup_validation = self._validate_critical_subsystems()

        # Set up HTTP server
        @web.middleware
        async def auth_middleware(request: web.Request, handler):
            if self.auth_token and request.method not in ("GET", "HEAD", "OPTIONS") and not self._is_request_authorized(request):
                return web.json_response({"error": "unauthorized"}, status=401)
            return await handler(request)

        # Increase max body size for large file uploads (100MB)
        # Fixes "Request Entity Too Large" for Elo DB and other file uploads
        app = web.Application(
            middlewares=[auth_middleware],
            client_max_size=100 * 1024 * 1024,  # 100 MB
        )
        # Store app for graceful restart (Jan 2026)
        self._http_app = app

        # Register all routes from centralized route registry (December 2025)
        # Replaces 200+ individual route registrations with declarative registry
        _routes_registered = False
        try:
            from scripts.p2p.routes import register_all_routes
            route_count = register_all_routes(app, self)
            logger.info(f"Registered {route_count} HTTP routes from route registry")
            _routes_registered = True
        except ImportError as e:
            logger.warning(f"Route registry not available, using inline routes: {e}")
            _routes_registered = False

        # Register file download routes (December 2025)
        # HTTP-based file sync for nodes with unreliable SSH
        try:
            from scripts.p2p.handlers.file_download import register_file_download_routes
            file_routes = register_file_download_routes(app, self)
            logger.info(f"Registered {file_routes} file download routes for HTTP-based sync")
        except ImportError as e:
            logger.debug(f"File download handler not available: {e}")

        # Register network health routes (December 30, 2025)
        # Cross-verification between P2P mesh and Tailscale connectivity
        try:
            setup_network_health_routes(app, self)
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Network health routes not registered: {e}")

        # Register model inventory routes (January 2026)
        # Used by ClusterModelEnumerator for comprehensive model evaluation
        try:
            model_routes = setup_model_routes(app, self)
            logger.info(f"Registered {model_routes} model inventory routes")
        except Exception as e:  # noqa: BLE001
            logger.debug(f"Model inventory routes not registered: {e}")

        # Skip legacy route registrations if registry succeeded
        # These are kept as fallback and will be removed in a future cleanup
        if not _routes_registered:
            app.router.add_post('/heartbeat', self.handle_heartbeat)
            app.router.add_get('/status', self.handle_status)
            app.router.add_get('/progress', self.handle_progress)  # Jan 16, 2026: Elo progress endpoint
            app.router.add_get('/external_work', self.handle_external_work)
            # Jan 3, 2026: Sprint 10+ P2P hardening - peer health scoring endpoint
            app.router.add_get('/peer-health', self.handle_peer_health)
            # Jan 5, 2026: Session 17.41 - game counts endpoint for P2P seeding
            app.router.add_get('/game_counts', self.handle_game_counts)
            # Jan 6, 2026: Session 17.48 - refresh game counts from peers
            app.router.add_post('/refresh_game_counts', self.handle_refresh_game_counts)
                # Work queue routes (centralized work distribution)
            app.router.add_post('/work/add', self.handle_work_add)
            app.router.add_post('/work/add_batch', self.handle_work_add_batch)
            app.router.add_get('/work/claim', self.handle_work_claim)
            app.router.add_post('/work/start', self.handle_work_start)
            app.router.add_post('/work/complete', self.handle_work_complete)
            app.router.add_post('/work/fail', self.handle_work_fail)
            app.router.add_get('/work/status', self.handle_work_status)
            app.router.add_get('/work/populator', self.handle_populator_status)
            app.router.add_get('/work/node/{node_id}', self.handle_work_for_node)
            app.router.add_post('/work/cancel', self.handle_work_cancel)
            app.router.add_get('/work/history', self.handle_work_history)

            app.router.add_post('/election', self.handle_election)
            app.router.add_post('/election/lease', self.handle_lease_request)
            app.router.add_get('/election/grant', self.handle_voter_grant_status)
            app.router.add_post('/election/reset', self.handle_election_reset)
            app.router.add_post('/election/force_leader', self.handle_election_force_leader)
            # Jan 1, 2026: Phase 3B-C - lease revocation for leadership stability
            app.router.add_post('/election/lease_revoke', self.handle_lease_revoke)
            # December 29, 2025: Allow non-voters to request elections via voters
            app.router.add_post('/election/request', self.handle_election_request)
            # Jan 1, 2026: Probabilistic fallback leadership - provisional claim handler
            app.router.add_post('/provisional-leader/claim', self.handle_provisional_leader_claim)

            # Jan 1, 2026: ULSM - Leadership state change broadcast (ensures peers learn of step-down)
            app.router.add_post('/leader-state-change', self.handle_leader_state_change)

            # Jan 20, 2026: Voter config sync - consensus-safe configuration synchronization
            app.router.add_get('/voter-config', self.handle_voter_config_get)
            app.router.add_post('/voter-config/sync', self.handle_voter_config_sync)
            app.router.add_post('/voter-config/ack', self.handle_voter_config_ack)
            app.router.add_post('/voter-config/propose', self.handle_voter_config_propose)
            app.router.add_get('/voter-config/change-status', self.handle_voter_config_change_status)

            # Serf integration routes (battle-tested SWIM gossip)
            app.router.add_post('/serf/event', self.handle_serf_event)

            # Native SWIM integration (swim-p2p library) - Phase 5 Dec 26, 2025
            app.router.add_get('/swim/status', self.handle_swim_status)
            app.router.add_get('/swim/members', self.handle_swim_members)

            # Raft consensus integration (PySyncObj) - Phase 5 Dec 26, 2025
            app.router.add_get('/raft/status', self.handle_raft_status)
            app.router.add_get('/raft/work', self.handle_raft_work_queue)
            app.router.add_get('/raft/jobs', self.handle_raft_jobs)
            app.router.add_post('/raft/lock/{name}', self.handle_raft_lock)
            app.router.add_delete('/raft/lock/{name}', self.handle_raft_unlock)

            app.router.add_post('/coordinator', self.handle_coordinator)
            app.router.add_post('/start_job', self.handle_start_job)
            app.router.add_post('/stop_job', self.handle_stop_job)
            app.router.add_post('/job/kill', self.handle_job_kill)
            app.router.add_post('/process/kill', self.handle_process_kill)  # Jan 21, 2026: Remote zombie cleanup
            app.router.add_post('/cleanup', self.handle_cleanup)
            app.router.add_post('/restart_stuck_jobs', self.handle_restart_stuck_jobs)
            app.router.add_post('/reduce_selfplay', self.handle_reduce_selfplay)
            app.router.add_post('/selfplay/start', self.handle_selfplay_start)  # GPU selfplay dispatch endpoint
            app.router.add_post('/dispatch_selfplay', self.handle_dispatch_selfplay)  # Coordinator-facing selfplay request endpoint (Dec 29, 2025)
            app.router.add_get('/health', self.handle_health)
            app.router.add_get('/cluster/health', self.handle_cluster_health)
            app.router.add_get('/git/status', self.handle_git_status)
            app.router.add_post('/git/update', self.handle_git_update)

            # Config sync routes (December 30, 2025)
            app.router.add_post('/push_config', self.handle_push_config)

            # Loop management routes (January 1, 2026)
            # For 48h autonomous operation: restart crashed/stopped loops
            app.router.add_post('/loops/restart/{name}', self.handle_loop_restart)
            app.router.add_post('/loops/restart_stopped', self.handle_restart_stopped_loops)
            app.router.add_get('/loops/status', self.handle_loops_status)

            # Parallelism monitoring (Jan 2026, Phase 5)
            app.router.add_get('/status/parallelism', self.handle_parallelism_status)

            # P2P diagnostics (January 2026 - P2P Stability Plan Phase 4)
            app.router.add_get('/p2p/diagnostics', self.handle_p2p_diagnostics)

            # Dynamic host registry routes (for IP auto-updates)
            app.router.add_post('/register', self.handle_register)
            app.router.add_get('/registry/status', self.handle_registry_status)
            app.router.add_post('/registry/update_vast', self.handle_registry_update_vast)
            app.router.add_post('/registry/update_aws', self.handle_registry_update_aws)
            app.router.add_post('/registry/update_tailscale', self.handle_registry_update_tailscale)
            app.router.add_post('/registry/save_yaml', self.handle_registry_save_yaml)

            # Connectivity diagnosis routes (SSH/HTTP fallback)
            app.router.add_get('/connectivity/diagnose/{node_id}', self.handle_connectivity_diagnose)
            app.router.add_get('/connectivity/transport_stats', self.handle_transport_stats)
            app.router.add_post('/connectivity/probe_vast', self.handle_probe_vast_nodes)

            # Dispatch diagnostics (Jan 3, 2026)
            app.router.add_get('/dispatch/stats', self.handle_dispatch_stats)

            # Gauntlet evaluation routes
            app.router.add_post('/gauntlet/execute', self.handle_gauntlet_execute)
            app.router.add_get('/gauntlet/status', self.handle_gauntlet_status)
            app.router.add_post('/gauntlet/quick-eval', self.handle_gauntlet_quick_eval)

            # Relay/Hub routes for NAT-blocked nodes
            app.router.add_post('/relay/heartbeat', self.handle_relay_heartbeat)
            app.router.add_get('/relay/peers', self.handle_relay_peers)
            app.router.add_get('/relay/status', self.handle_relay_status)
            app.router.add_post('/relay/enqueue', self.handle_relay_enqueue)

            # Gossip protocol for decentralized state sharing
            app.router.add_post('/gossip', self.handle_gossip)
            app.router.add_post('/gossip/anti-entropy', self.handle_gossip_anti_entropy)

            # Phase 2: Distributed data manifest routes
            app.router.add_get('/data_manifest', self.handle_data_manifest)
            app.router.add_get('/cluster_data_manifest', self.handle_cluster_data_manifest)
            app.router.add_post('/refresh_manifest', self.handle_refresh_manifest)
            app.router.add_get('/data-inventory', self.handle_data_inventory)  # Dec 30, 2025: Quick game counts
            app.router.add_post('/data/cluster_manifest_broadcast', self.handle_cluster_manifest_broadcast)  # Jan 2026: Leader broadcast
            app.router.add_get('/data/request_manifest', self.handle_request_manifest)  # Jan 16, 2026: On-demand cluster manifest query

            # Model inventory routes (January 2026 - Comprehensive Model Evaluation Pipeline)
            # Used by ClusterModelEnumerator to discover models across the cluster
            app.router.add_get('/models/inventory', self.handle_model_inventory)
            app.router.add_get('/models/evaluation_status', self.handle_evaluation_status)

            # Phase 3: Delivery verification routes (Dec 27, 2025)
            app.router.add_post('/delivery/verify', self.handle_delivery_verify)
            app.router.add_get('/delivery/status/{node_id}', self.handle_delivery_status)

            # Distributed CMA-ES routes
            app.router.add_post('/cmaes/start', self.handle_cmaes_start)
            app.router.add_post('/cmaes/evaluate', self.handle_cmaes_evaluate)
            app.router.add_get('/cmaes/status', self.handle_cmaes_status)
            app.router.add_post('/cmaes/result', self.handle_cmaes_result)

            # Distributed tournament routes
            app.router.add_post('/tournament/start', self.handle_tournament_start)
            app.router.add_post('/tournament/match', self.handle_tournament_match)
            app.router.add_post('/tournament/play_elo_match', self.handle_play_elo_match)
            app.router.add_get('/tournament/status', self.handle_tournament_status)
            app.router.add_post('/tournament/result', self.handle_tournament_result)
            app.router.add_post('/tournament/ssh_start', self.handle_ssh_tournament_start)
            app.router.add_get('/tournament/ssh_status', self.handle_ssh_tournament_status)
            app.router.add_post('/tournament/ssh_cancel', self.handle_ssh_tournament_cancel)

            # Improvement loop routes
            app.router.add_post('/improvement/start', self.handle_improvement_start)
            app.router.add_get('/improvement/status', self.handle_improvement_status)
            app.router.add_post('/improvement/phase_complete', self.handle_improvement_phase_complete)

            # Phase 2: P2P data sync routes
            app.router.add_post('/sync/start', self.handle_sync_start)
            app.router.add_get('/sync/status', self.handle_sync_status)
            app.router.add_post('/sync/pull', self.handle_sync_pull)
            app.router.add_get('/sync/file', self.handle_sync_file)
            app.router.add_post('/sync/job_update', self.handle_sync_job_update)
            app.router.add_post('/sync/training', self.handle_training_sync)  # Training node priority sync
            app.router.add_post('/sync/push', self.handle_sync_push)            # Push data from GPU node (Dec 2025)
            app.router.add_post('/sync/receipt', self.handle_sync_receipt)      # Request sync receipt verification (Dec 2025)
            app.router.add_get('/sync/receipts', self.handle_sync_receipts_status)  # Get sync receipts stats (Dec 2025)
            app.router.add_get('/gpu/rankings', self.handle_gpu_rankings)      # GPU power rankings
            app.router.add_post('/cleanup/files', self.handle_cleanup_files)   # File-specific cleanup
            app.router.add_get('/admin/purge_retired', self.handle_purge_retired_peers)  # Purge retired peers (GET for auth bypass)
            app.router.add_get('/admin/purge_stale', self.handle_purge_stale_peers)      # Purge stale peers by heartbeat age
            app.router.add_get('/admin/deduplicate', self.handle_admin_deduplicate)      # Remove duplicate peer entries (Jan 13, 2026)
            app.router.add_post('/admin/unretire', self.handle_admin_unretire)           # Unretire specific node
            app.router.add_post('/admin/restart', self.handle_admin_restart)             # Force restart orchestrator
            app.router.add_post('/admin/reset_node_jobs', self.handle_admin_reset_node_jobs)  # Reset job counts for zombie nodes
            app.router.add_post('/admin/add_peer', self.handle_admin_add_peer)  # Inject peer for partition healing (Jan 2026)
            app.router.add_get('/admin/clear_nat_blocked', self.handle_admin_clear_nat_blocked)  # Clear NAT-blocked status (Jan 2, 2026)
            app.router.add_post('/admin/ping_work', self.handle_admin_ping_work)  # Frozen leader detection (Jan 2, 2026)

            # Phase 5: Event subscription visibility (December 2025)
            app.router.add_get('/subscriptions', self.handle_subscriptions)              # Show event subscriptions

            # Phase 3: Training pipeline routes
            app.router.add_post('/training/start', self.handle_training_start)
            app.router.add_get('/training/status', self.handle_training_status)
            app.router.add_post('/training/update', self.handle_training_update)
            app.router.add_post('/training/nnue/start', self.handle_nnue_start)
            app.router.add_post('/training/cmaes/start', self.handle_cmaes_start_auto)

            # December 30, 2025: Training trigger RPC endpoints
            app.router.add_post('/training/trigger', self.handle_training_trigger)
            app.router.add_get('/training/trigger-decision/{config_key}', self.handle_training_trigger_decision)
            app.router.add_get('/training/trigger-configs', self.handle_training_trigger_configs)

            # Phase 5: Improvement cycle routes
            app.router.add_get('/improvement_cycles/status', self.handle_improvement_cycles_status)
            app.router.add_get('/improvement_cycles/leaderboard', self.handle_improvement_cycles_leaderboard)
            app.router.add_post('/improvement_cycles/training_complete', self.handle_improvement_training_complete)
            app.router.add_post('/improvement_cycles/evaluation_complete', self.handle_improvement_evaluation_complete)

            # Metrics observability routes
            app.router.add_get('/metrics', self.handle_metrics)
            app.router.add_get('/metrics/prometheus', self.handle_metrics_prometheus)

            # Canonical pipeline routes (for pipeline_orchestrator.py integration)
            app.router.add_post('/pipeline/start', self.handle_pipeline_start)
            app.router.add_get('/pipeline/status', self.handle_pipeline_status)
            app.router.add_post('/pipeline/selfplay_worker', self.handle_pipeline_selfplay_worker)

            # Phase 4: REST API and Dashboard routes
            app.router.add_get('/', self.handle_root)
            app.router.add_get('/api/cluster/status', self.handle_api_cluster_status)
            app.router.add_post('/api/cluster/git/update', self.handle_api_cluster_git_update)
            app.router.add_get('/api/selfplay/stats', self.handle_api_selfplay_stats)
            app.router.add_get('/api/elo/leaderboard', self.handle_api_elo_leaderboard)
            app.router.add_get('/elo/table', self.handle_elo_table)
            app.router.add_get('/elo/history', self.handle_elo_history)

            # Elo Database Sync routes (cluster-wide Elo consistency)
            app.router.add_get('/elo/sync/status', self.handle_elo_sync_status)
            app.router.add_post('/elo/sync/trigger', self.handle_elo_sync_trigger)
            app.router.add_get('/elo/sync/db', self.handle_elo_sync_download)
            app.router.add_post('/elo/sync/upload', self.handle_elo_sync_upload)
            app.router.add_get('/nodes/table', self.handle_nodes_table)
            app.router.add_get('/victory/table', self.handle_victory_table)
            app.router.add_get('/games/analytics', self.handle_games_analytics)
            app.router.add_get('/training/metrics', self.handle_training_metrics)
            app.router.add_get('/holdout/metrics', self.handle_holdout_metrics)
            app.router.add_get('/holdout/table', self.handle_holdout_table)
            app.router.add_get('/mcts/stats', self.handle_mcts_stats)
            app.router.add_get('/mcts/table', self.handle_mcts_table)
            # Feature endpoints
            app.router.add_get('/matchups/matrix', self.handle_matchup_matrix)
            app.router.add_get('/matchups/table', self.handle_matchup_table)
            app.router.add_get('/models/lineage', self.handle_model_lineage)
            app.router.add_get('/models/lineage/table', self.handle_model_lineage_table)
            app.router.add_get('/data/quality', self.handle_data_quality)
            app.router.add_get('/data/quality/table', self.handle_data_quality_table)
            app.router.add_get('/data/quality/issues', self.handle_data_quality_issues)
            app.router.add_get('/training/efficiency', self.handle_training_efficiency)
            app.router.add_get('/training/efficiency/table', self.handle_training_efficiency_table)
            app.router.add_get('/rollback/status', self.handle_rollback_status)
            app.router.add_get('/rollback/candidates', self.handle_rollback_candidates)
            app.router.add_post('/rollback/execute', self.handle_rollback_execute)
            app.router.add_post('/rollback/auto', self.handle_rollback_auto)
            app.router.add_get('/autoscale/metrics', self.handle_autoscale_metrics)
            app.router.add_get('/autoscale/recommendations', self.handle_autoscale_recommendations)
            app.router.add_get('/resource/optimizer', self.handle_resource_optimizer)
            app.router.add_get('/resource/history', self.handle_resource_utilization_history)
            app.router.add_post('/webhook/test', self.handle_webhook_test)
            app.router.add_get('/trends/summary', self.handle_trends_summary)
            app.router.add_get('/trends/history', self.handle_trends_history)
            app.router.add_get('/trends/table', self.handle_trends_table)

            # A/B Testing endpoints
            app.router.add_post('/abtest/create', self.handle_abtest_create)
            app.router.add_post('/abtest/result', self.handle_abtest_result)
            app.router.add_get('/abtest/status', self.handle_abtest_status)
            app.router.add_get('/abtest/list', self.handle_abtest_list)
            app.router.add_post('/abtest/cancel', self.handle_abtest_cancel)
            app.router.add_get('/abtest/table', self.handle_abtest_table)
            app.router.add_post('/abtest/run', self.handle_abtest_run)

            app.router.add_get('/api/training/status', self.handle_api_training_status)
            app.router.add_get('/api/canonical/health', self.handle_api_canonical_health)
            app.router.add_get('/api/canonical/jobs', self.handle_api_canonical_jobs_list)
            app.router.add_get('/api/canonical/jobs/{job_id}', self.handle_api_canonical_job_get)
            app.router.add_get('/api/canonical/jobs/{job_id}/log', self.handle_api_canonical_job_log)
            app.router.add_get('/api/canonical/logs', self.handle_api_canonical_logs_list)
            app.router.add_get('/api/canonical/logs/{log_name}/tail', self.handle_api_canonical_log_tail)
            app.router.add_post('/api/canonical/generate', self.handle_api_canonical_generate)
            app.router.add_post('/api/canonical/jobs/{job_id}/cancel', self.handle_api_canonical_job_cancel)
            app.router.add_get('/api/jobs', self.handle_api_jobs_list)
            app.router.add_post('/api/jobs/submit', self.handle_api_jobs_submit)
            app.router.add_get('/api/jobs/{job_id}', self.handle_api_job_get)
            app.router.add_post('/api/jobs/{job_id}/cancel', self.handle_api_job_cancel)
            app.router.add_get('/dashboard', self.handle_dashboard)
            app.router.add_get('/work_queue', self.handle_work_queue_dashboard)
            app.router.add_get('/work_queue/claim_stats', self.handle_work_queue_claim_stats)

        runner = web.AppRunner(app)
        await runner.setup()
        # Store runner for graceful restart (Jan 2026)
        self._http_runner = runner

        # Verify NFS sync before starting (prevents import errors from stale code)
        try:
            from scripts.verify_nfs_sync import verify_before_startup
            if not verify_before_startup():
                logger.warning("NFS sync verification found mismatches - check logs for details")
        except ImportError:
            logger.debug("NFS sync verification not available")
        except Exception as e:  # noqa: BLE001
            logger.warning(f"NFS sync verification failed: {e}")

        # Wire SyncRouter to event system for real-time sync triggers (December 2025)
        self._wire_sync_router_events()

        # Wire DeadPeerCooldownManager probe function (January 2026)
        # Enables probe-based early recovery from adaptive cooldown
        self._wire_cooldown_manager_probe()

        # Wire connection pool dynamic sizing (January 2026)
        # Scales pool limits based on cluster size to prevent exhaustion
        self._wire_connection_pool_dynamic_sizing()

        # Wire LeadershipStateMachine broadcast callback (ULSM - Jan 2026)
        # This enables the state machine to broadcast step-down to peers
        self._leadership_sm._broadcast_callback = self._broadcast_leader_state_change
        logger.info("ULSM: Leadership state machine broadcast callback wired")

        # Increase backlog to handle burst of connections from many nodes
        # Default is ~128, which can overflow when many vast nodes heartbeat simultaneously
        #
        # Jan 2, 2026: Bind to BOTH IPv4 and IPv6 explicitly
        # Python's asyncio/aiohttp doesn't properly implement dual-stack sockets
        # (IPV6_V6ONLY is not automatically disabled), so we bind to both addresses.
        #
        # Jan 8, 2026: Added retry with exponential backoff for TIME_WAIT state.
        # After a crash, the port may be in TIME_WAIT for up to 60s. Retry instead of failing.

        # Port binding retry configuration (January 2026)
        PORT_BIND_MAX_RETRIES = 5
        PORT_BIND_INITIAL_DELAY = 2.0  # seconds
        PORT_BIND_MAX_DELAY = 30.0  # seconds

        async def _try_bind_port(site: web.TCPSite, host: str, port: int) -> bool:
            """Try to bind port with exponential backoff for TIME_WAIT state."""
            delay = PORT_BIND_INITIAL_DELAY
            for attempt in range(PORT_BIND_MAX_RETRIES):
                try:
                    await site.start()
                    return True
                except OSError as e:
                    errno_val = getattr(e, 'errno', 0)
                    is_addr_in_use = "Address already in use" in str(e) or errno_val == 98

                    if is_addr_in_use and attempt < PORT_BIND_MAX_RETRIES - 1:
                        # Likely TIME_WAIT state - retry with backoff
                        logger.warning(
                            f"Port {port} busy (attempt {attempt + 1}/{PORT_BIND_MAX_RETRIES}), "
                            f"retrying in {delay:.1f}s (likely TIME_WAIT state)..."
                        )
                        await asyncio.sleep(delay)
                        delay = min(delay * 2, PORT_BIND_MAX_DELAY)
                        continue
                    elif is_addr_in_use:
                        # Final attempt failed
                        logger.error(f"Port {port} still in use after {PORT_BIND_MAX_RETRIES} attempts.")
                        logger.error(f"Try: lsof -i :{port} or pkill -f p2p_orchestrator")
                        raise RuntimeError(f"Port {port} bound after retries - cannot start P2P") from e
                    elif "Invalid argument" in str(e):
                        # macOS TCP keepalive socket option issue - don't retry
                        logger.warning(f"TCP socket configuration failed on {host}:{port}: {e}")
                        logger.warning("This may be a macOS TCP keepalive compatibility issue")
                        raise
                    else:
                        # Other errors - don't retry
                        logger.error(f"Failed to bind to {host}:{port}: {e}")
                        raise
            return False  # Should not reach here

        bind_host = self.host
        if self.host == "0.0.0.0":
            # Bind to IPv4 first (always needed)
            site_v4 = web.TCPSite(runner, '0.0.0.0', self.port, reuse_address=True, backlog=1024)
            await _try_bind_port(site_v4, '0.0.0.0', self.port)
            self._http_sites.append(site_v4)  # Store for graceful restart (Jan 2026)
            logger.info(f"HTTP server started on 0.0.0.0:{self.port} (IPv4, backlog=1024)")

            # Also try to bind to IPv6 (optional, for IPv6-only clients)
            try:
                site_v6 = web.TCPSite(runner, '::', self.port, reuse_address=True, backlog=1024)
                await site_v6.start()
                self._http_sites.append(site_v6)  # Store for graceful restart (Jan 2026)
                bind_host = "0.0.0.0 + [::]"
                logger.info(f"HTTP server also listening on [::]:{self.port} (IPv6)")
            except OSError as v6_err:
                # IPv6 binding failed - that's OK, IPv4 is already working
                logger.debug(f"IPv6 binding failed (OK, IPv4 is active): {v6_err}")
                bind_host = "0.0.0.0"
        else:
            # Specific host requested - bind directly with retry
            site = web.TCPSite(runner, self.host, self.port, reuse_address=True, backlog=1024)
            await _try_bind_port(site, self.host, self.port)
            self._http_sites.append(site)  # Store for graceful restart (Jan 2026)
            logger.info(f"HTTP server started on {self.host}:{self.port} (backlog=1024)")

        # Notify systemd that we're ready to serve
        systemd_notify_ready()

        # Jan 5, 2026: Send immediate relay heartbeats for NAT-blocked nodes
        # This ensures relay nodes discover us before the regular heartbeat loop kicks in
        if self._force_relay_mode:
            await self._send_initial_relay_heartbeats()

        # Jan 7, 2026: Send immediate peer announcements for ALL nodes
        # This reduces discovery latency from 15-30s to 2-5s after startup
        await self._send_startup_peer_announcements()

        # Jan 9, 2026: Async fallback for game count seeding from peers
        # Cluster nodes don't have local canonical DBs, so fetch from coordinator
        # This fixes underserved config prioritization on worker nodes
        await self._async_seed_game_counts_from_peers_if_needed()

        # Start background tasks with exception isolation and restart support
        # CRITICAL FIX (Dec 2025): Each task is wrapped to prevent cascade failures.
        # Previously, a single exception in any task would crash all 18+ tasks.
        # Dec 2025 Update: Added factory functions for auto-restart on critical tasks.
        tasks = [
            # Critical heartbeat loop - auto-restart on failure
            self._create_safe_task(
                self._heartbeat_loop(), "heartbeat", factory=self._heartbeat_loop
            ),
            # NOTE: _manifest_collection_loop removed Dec 2025 - now runs via LoopManager (ManifestCollectionLoop)
            # See scripts/p2p/loops/manifest_collection_loop.py for implementation
            # Job management - auto-restart on failure
            self._create_safe_task(
                self._job_management_loop(), "job_management", factory=self._job_management_loop
            ),
            # Discovery - auto-restart on failure
            self._create_safe_task(
                self._discovery_loop(), "discovery", factory=self._discovery_loop
            ),
            # IMPROVED: Dedicated voter heartbeat loop for reliable leader election - auto-restart
            self._create_safe_task(
                self._voter_heartbeat_loop(), "voter_heartbeat", factory=self._voter_heartbeat_loop
            ),
            # Jan 11, 2026: Dead peer reconnection loop (Phase 1 P2P stability fix)
            # Actively probes dead peers with exponential backoff to prevent cluster fragmentation
            self._create_safe_task(
                self._reconnect_dead_peers_loop(), "dead_peer_reconnect", factory=self._reconnect_dead_peers_loop
            ),
            # NOTE: _nat_management_loop removed Dec 2025 - now runs via LoopManager (NATManagementLoop)
            # See scripts/p2p/loops/network_loops.py for implementation
            # SWIM gossip membership for leaderless failure detection (<5s vs 60s+)
            self._create_safe_task(
                self._swim_membership_loop(), "swim_membership", factory=self._swim_membership_loop
            ),
        ]

        # Add git update loop if enabled
        if AUTO_UPDATE_ENABLED:
            tasks.append(self._create_safe_task(
                self._git_update_loop(), "git_update", factory=self._git_update_loop
            ))

        # NOTE: _training_sync_loop removed Dec 2025 - now runs via LoopManager (TrainingSyncLoop)
        # See scripts/p2p/loops/training_sync_loop.py for implementation

        # Add cloud IP refresh loops (best-effort; no-op if not configured).
        if HAS_DYNAMIC_REGISTRY:
            tasks.append(self._create_safe_task(self._vast_ip_update_loop(), "vast_ip_update"))
            tasks.append(self._create_safe_task(self._aws_ip_update_loop(), "aws_ip_update"))
            tasks.append(self._create_safe_task(self._tailscale_ip_update_loop(), "tailscale_ip_update"))

        # NOTE: _tailscale_peer_recovery_loop removed Dec 2025 - now runs via LoopManager (TailscalePeerDiscoveryLoop)
        # See scripts/p2p/loops/network_loops.py for implementation

        # Phase 26: Continuous bootstrap loop - ensures isolated nodes can rejoin
        tasks.append(self._create_safe_task(self._continuous_bootstrap_loop(), "continuous_bootstrap"))

        # Dec 31, 2025: Periodic IP revalidation for late Tailscale availability
        # Fixes nodes advertising private IPs when Tailscale wasn't ready at startup
        tasks.append(self._create_safe_task(
            self._periodic_ip_validation_loop(), "ip_validation", factory=self._periodic_ip_validation_loop
        ))

        # Jan 9, 2026: Periodic game count refresh for underserved config prioritization
        # Keeps scheduler game counts up-to-date as games are generated and consolidated
        tasks.append(self._create_safe_task(
            self._game_count_refresh_loop(), "game_count_refresh", factory=self._game_count_refresh_loop
        ))

        # NOTE: _follower_discovery_loop removed Dec 2025 - now runs via LoopManager (FollowerDiscoveryLoop)
        # See scripts/p2p/loops/discovery_loop.py for implementation

        # NOTE: _data_management_loop removed Dec 2025 - now runs via LoopManager (DataManagementLoop)
        # See scripts/p2p/loops/data_loops.py for implementation

        # NOTE: _model_sync_loop removed Dec 2025 - now runs via LoopManager (ModelSyncLoop)
        # See scripts/p2p/loops/data_loops.py for implementation

        # NOTE: _elo_sync_loop removed Dec 2025 - now runs via LoopManager (EloSyncLoop)

        # NOTE: _worker_pull_loop removed Dec 2025 - now runs via LoopManager (WorkerPullLoop)
        # See scripts/p2p/loops/job_loops.py for implementation

        # NOTE: _work_queue_maintenance_loop removed Dec 2025 - now runs via LoopManager (WorkQueueMaintenanceLoop)
        # See scripts/p2p/loops/job_loops.py for implementation

        # NOTE: _idle_detection_loop removed Dec 2025 - now runs via LoopManager (IdleDetectionLoop)

        # === AUTOMATION LOOPS (2024-12) ===
        # These loops enable hands-free cluster operation

        # NOTE: _auto_scaling_loop removed Dec 2025 - now runs via LoopManager (AutoScalingLoop)

        # NOTE: _predictive_monitoring_loop removed Dec 2025 (~98 LOC)
        # Now runs via LoopManager as PredictiveMonitoringLoop.
        # See scripts/p2p/loops/resilience_loops.py for implementation.

        # NOTE: _self_healing_loop removed Dec 2025 (~71 LOC)
        # Now runs via LoopManager as SelfHealingLoop.
        # See scripts/p2p/loops/resilience_loops.py for implementation.

        # NOTE: _job_reaper_loop removed Dec 2025 - now runs via LoopManager (JobReaperLoop)

        # NOTE: _validation_loop removed Dec 2025 - now runs via LoopManager (ValidationLoop)
        # See scripts/p2p/loops/validation_loop.py for implementation

        # NOTE: _queue_populator_loop removed Dec 2025 - now runs via LoopManager (QueuePopulatorLoop)

        # Store tasks for shutdown handling
        self._background_tasks = tasks

        # Phase 4: Start extracted loops via LoopManager (Dec 2025)
        # These 11 loops now ONLY run via LoopManager (inline versions removed):
        # - EloSyncLoop, IdleDetectionLoop, AutoScalingLoop, JobReaperLoop, QueuePopulatorLoop
        # - WorkQueueMaintenanceLoop, NATManagementLoop, ManifestCollectionLoop, ValidationLoop
        # - DataManagementLoop, ModelSyncLoop
        job_reaper_started = False
        logger.info(f"[LoopManager] Phase 4 startup: EXTRACTED_LOOPS_ENABLED={EXTRACTED_LOOPS_ENABLED}")
        if EXTRACTED_LOOPS_ENABLED and self._register_extracted_loops():
            loop_manager = self._get_loop_manager()
            if loop_manager is not None:
                # Dec 27, 2025: start_all() now returns dict of {loop_name: started_successfully}
                # Check if job_reaper specifically started to avoid duplicate reapers
                startup_results = await loop_manager.start_all()
                job_reaper_started = startup_results.get("job_reaper", False)
                started_count = sum(1 for v in startup_results.values() if v)
                logger.info(
                    f"LoopManager: started {started_count}/{len(startup_results)} loops, "
                    f"job_reaper={'running' if job_reaper_started else 'FAILED'}"
                )

        # Phase 4.1: Inline job reaper fallback (Dec 27, 2025)
        # If JobReaperLoop specifically failed to start, run inline fallback for job cleanup
        # This ensures stuck jobs get cleaned up even if the modular loop system fails
        # Dec 27, 2025: Fixed race condition - now checks job_reaper loop status, not just
        # whether LoopManager.start_all() completed (which could mask loop startup failures)
        if JOB_REAPER_FALLBACK_ENABLED and not job_reaper_started:
            logger.info("[JobReaper] LoopManager not available, starting inline fallback")
            tasks.append(
                self._create_safe_task(
                    self._inline_job_reaper_fallback_loop(),
                    "job_reaper_fallback"
                )
            )

        # Best-effort bootstrap from seed peers before running elections. This
        # helps newly started cloud nodes quickly learn about the full cluster.
        # Jan 15, 2026 (Phase 6 P2P Resilience): Add retry logic with exponential backoff
        bootstrap_success = False
        bootstrap_attempts = 0
        max_bootstrap_attempts = 3
        bootstrap_backoff = [2, 5, 10]  # Exponential backoff in seconds

        while bootstrap_attempts < max_bootstrap_attempts and not bootstrap_success:
            try:
                bootstrap_success = await self._bootstrap_from_known_peers()
                if bootstrap_success:
                    logger.info(
                        f"[Bootstrap] Successfully bootstrapped from peers "
                        f"(attempt {bootstrap_attempts + 1}/{max_bootstrap_attempts})"
                    )
                    break
            except Exception as e:
                logger.warning(f"[Bootstrap] Attempt {bootstrap_attempts + 1} failed: {e}")

            bootstrap_attempts += 1
            if bootstrap_attempts < max_bootstrap_attempts:
                wait_time = bootstrap_backoff[bootstrap_attempts - 1]
                logger.info(f"[Bootstrap] Retrying in {wait_time}s...")
                await asyncio.sleep(wait_time)

        if not bootstrap_success:
            logger.warning(
                f"[Bootstrap] Failed to bootstrap after {max_bootstrap_attempts} attempts"
            )
            # Emit bootstrap failure event for monitoring
            self._safe_emit_event("BOOTSTRAP_FAILED", {
                "node_id": self.node_id,
                "attempts": bootstrap_attempts,
                "seed_count": len(self.known_peers or []),
                "message": "Failed to bootstrap from any seed peer",
            })

        # December 30, 2025: Immediate Tailscale discovery when no --peers provided
        # This fixes the bootstrap problem where nodes started without --peers
        # couldn't join the mesh because continuous_bootstrap_loop has a 30s delay.
        if not self.known_peers:
            logger.info("[Bootstrap] No --peers provided, running immediate Tailscale discovery...")
            with self.peers_lock:
                peers_before = len(self.peers)

            # Try direct Tailscale peer discovery first
            with contextlib.suppress(Exception):
                await self._discover_tailscale_peers()

            with self.peers_lock:
                peers_after = len(self.peers)

            if peers_after > peers_before:
                logger.info(f"[Bootstrap] Tailscale discovery found {peers_after - peers_before} new peer(s)")
                # January 2026: Force reconnect to any peers online in Tailscale but missing from P2P
                # This fixes peer discovery asymmetry where P2P shows 5-7 peers while Tailscale shows 40
                await self._reconnect_missing_tailscale_peers()
            else:
                # Tailscale discovery didn't find peers - try config-based seeds
                logger.info("[Bootstrap] Tailscale discovery found no peers, trying config-based seeds...")
                config_seeds = self._load_bootstrap_seeds_from_config()
                if config_seeds:
                    logger.info(f"[Bootstrap] Loaded {len(config_seeds)} seed(s) from config")
                    self.known_peers = config_seeds
                    with contextlib.suppress(Exception):
                        await self._bootstrap_from_known_peers()

        # December 29, 2025: Extended startup election with retry mechanism
        # If no leader known, start election after allowing time for peer discovery.
        # Previously used 5s which was too short for cluster discovery.
        await asyncio.sleep(15)  # Increased from 5s to allow peer discovery
        if not self.leader_id and not self._maybe_adopt_leader_from_peers():
            # CRITICAL: Check quorum before starting election to prevent quorum bypass
            if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                logger.warning("Skipping startup election: no voter quorum available (will retry)")
            else:
                await self._start_election()

        # December 29, 2025: Add background task to retry election if still no leader
        # This handles cases where initial election fails or quorum wasn't available
        async def _delayed_election_retry():
            """Retry election periodically if no leader after startup."""
            retry_intervals = [30, 60, 120, 300]  # Exponential backoff: 30s, 1m, 2m, 5m
            retry_count = 0

            while self.running and retry_count < len(retry_intervals):
                wait_time = retry_intervals[retry_count]
                await asyncio.sleep(wait_time)

                if not self.running:
                    break

                if self.leader_id:
                    # Leader found, no need to retry
                    logger.info(f"Leader established ({self.leader_id}), stopping election retry task")
                    break

                # Still no leader, try to adopt from peers or start election
                if self._maybe_adopt_leader_from_peers():
                    logger.info(f"Adopted leader from peers: {self.leader_id}")
                    break

                # Check quorum and start election if possible
                if getattr(self, "voter_node_ids", []) and not self._has_voter_quorum():
                    retry_count += 1
                    # Jan 2, 2026: Use _count_alive_voters() to check IP:port matches
                    voters_alive = self._count_alive_voters()
                    logger.warning(
                        f"No voter quorum for election retry {retry_count}/{len(retry_intervals)} "
                        f"(alive={voters_alive}, need={getattr(self, 'voter_quorum_size', 3)})"
                    )
                    continue

                if not getattr(self, "election_in_progress", False):
                    logger.info(f"No leader after {wait_time}s, triggering election retry {retry_count + 1}")
                    await self._start_election()
                    retry_count += 1
                else:
                    logger.debug("Election already in progress, skipping retry")

            if not self.leader_id and self.running:
                logger.warning("Exhausted election retries, operating in leaderless mode")

        tasks.append(
            self._create_safe_task(
                _delayed_election_retry(),
                "delayed_election_retry"
            )
        )

        # Session 17.41: Deferred game counts fetch from peers
        # If local seeding returned empty (no canonical DBs), fetch from coordinator
        async def _deferred_game_counts_fetch():
            """Fetch game counts from coordinator after peer discovery."""
            try:
                await asyncio.sleep(30)  # Wait for peer discovery to complete
                if not self.running:
                    return

                # Check if we already have game counts seeded
                if self.selfplay_scheduler and hasattr(self.selfplay_scheduler, "_p2p_game_counts"):
                    existing_counts = getattr(self.selfplay_scheduler, "_p2p_game_counts", {})
                    if existing_counts:
                        logger.debug(f"[P2P] Already have {len(existing_counts)} game counts, skipping peer fetch")
                        return

                # Fetch from coordinator/peers
                game_counts = await self._fetch_game_counts_from_peers()
                if game_counts and self.selfplay_scheduler:
                    self.selfplay_scheduler.update_p2p_game_counts(game_counts)
                    logger.info(f"[P2P] Deferred fetch: seeded SelfplayScheduler with {len(game_counts)} game counts from peers")
                    for config_key, count in sorted(game_counts.items(), key=lambda x: x[1]):
                        if count < 500:  # Log underserved configs
                            logger.info(f"[P2P] Underserved config (from peers): {config_key} = {count} games")
            except Exception as e:  # noqa: BLE001
                logger.debug(f"[P2P] Deferred game counts fetch failed: {e}")

        tasks.append(
            self._create_safe_task(
                _deferred_game_counts_fetch(),
                "deferred_game_counts_fetch"
            )
        )

        # Session 17.48: Periodic game counts refresh loop
        # The deferred fetch only runs once at startup. This loop ensures game counts
        # are kept fresh on leader nodes that don't have local canonical databases.
        # Without fresh game counts, starvation multipliers can't be applied correctly.
        async def _periodic_game_counts_refresh():
            """Periodically refresh game counts from peers (runs every 5 minutes)."""
            refresh_interval = 300  # 5 minutes
            # Wait for initial deferred fetch to complete
            await asyncio.sleep(60)

            while self.running:
                try:
                    # Only refresh if we don't have local canonical DBs
                    local_counts = await asyncio.to_thread(self._seed_selfplay_scheduler_game_counts_sync)

                    if not local_counts:
                        # No local DBs, fetch from peers
                        peer_counts = await self._fetch_game_counts_from_peers()

                        if peer_counts and self.selfplay_scheduler:
                            self.selfplay_scheduler.update_p2p_game_counts(peer_counts)
                            underserved = sum(1 for c in peer_counts.values() if c < 2000)
                            logger.info(f"[P2P] Periodic refresh: {len(peer_counts)} configs, {underserved} underserved")
                            # Log critically underserved configs
                            for config_key, count in sorted(peer_counts.items(), key=lambda x: x[1]):
                                if count < 500:
                                    logger.warning(f"[P2P] CRITICAL: {config_key} has only {count} games (ULTRA starvation)")

                except Exception as e:  # noqa: BLE001
                    logger.debug(f"[P2P] Periodic game counts refresh failed: {e}")

                await asyncio.sleep(refresh_interval)

        tasks.append(
            self._create_safe_task(
                _periodic_game_counts_refresh(),
                "periodic_game_counts_refresh"
            )
        )

        # January 14, 2026: Unified game counts refresh loop
        # This loop uses UnifiedGameAggregator to get counts from ALL sources:
        # LOCAL, CLUSTER, S3, and OWC external drive on mac-studio.
        # Runs less frequently (10 min) since it's more expensive than peer-only refresh.
        async def _unified_game_counts_refresh():
            """Refresh game counts from all sources including OWC and S3."""
            refresh_interval = 600  # 10 minutes
            # Wait for initial peer-based fetch to complete first
            await asyncio.sleep(120)

            while self.running:
                try:
                    if self.selfplay_scheduler:
                        counts = await self.selfplay_scheduler.refresh_from_unified_aggregator()
                        if counts:
                            total = sum(counts.values())
                            underserved = sum(1 for c in counts.values() if c < 5000)
                            logger.info(
                                f"[P2P] Unified refresh: {total:,} total games across all sources "
                                f"({underserved} configs underserved)"
                            )
                except Exception as e:  # noqa: BLE001
                    logger.debug(f"[P2P] Unified game counts refresh failed: {e}")

                await asyncio.sleep(refresh_interval)

        tasks.append(
            self._create_safe_task(
                _unified_game_counts_refresh(),
                "unified_game_counts_refresh"
            )
        )

        # Run forever
        # December 2025: Added return_exceptions=True to prevent task exceptions from crashing orchestrator
        try:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            # Log any task failures
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"Background task {i} failed: {result}")
        except asyncio.CancelledError:
            pass
        finally:
            self.running = False
            # Stop extracted loops via LoopManager (Dec 2025)
            loop_manager = self._get_loop_manager()
            if loop_manager is not None and loop_manager.is_started:
                try:
                    results = await loop_manager.stop_all(timeout=15.0)
                    stopped = sum(1 for ok in results.values() if ok)
                    logger.info(f"LoopManager: stopped {stopped}/{len(results)} loops")
                except Exception as e:  # noqa: BLE001
                    logger.warning(f"LoopManager: stop failed: {e}")

            # Jan 2026: Shutdown loop executor thread pools (Phase 2)
            try:
                from scripts.p2p.loop_executors import LoopExecutors
                LoopExecutors.shutdown_all(wait=True)
            except ImportError:
                pass  # Module not available
            except Exception as e:  # noqa: BLE001
                logger.warning(f"LoopExecutors shutdown failed: {e}")

            # Jan 2026: Shutdown threaded loop runners (Phase 3)
            try:
                from scripts.p2p.threaded_loop_runner import ThreadedLoopRegistry
                results = await ThreadedLoopRegistry.stop_all(timeout=15.0)
                stopped = sum(1 for ok in results.values() if ok)
                if results:
                    logger.info(f"ThreadedLoopRegistry: stopped {stopped}/{len(results)} runners")
            except ImportError:
                pass  # Module not available
            except Exception as e:  # noqa: BLE001
                logger.warning(f"ThreadedLoopRegistry shutdown failed: {e}")

            try:
                await asyncio.wait_for(runner.cleanup(), timeout=30)
            except asyncio.TimeoutError:
                logger.warning("HTTP server cleanup timed out after 30s")


def _wait_for_tailscale_ip(timeout_seconds: int = 90, interval_seconds: float = 1.0) -> str:
    """Wait for Tailscale IP to become available at startup.

    Jan 12, 2026: Increased timeout from 30s to 90s after observing mac-studio
    consistently advertising local IP (10.0.0.62) instead of Tailscale IP.

    Root cause: When P2P starts before Tailscale CLI is ready, _get_tailscale_ip()
    returns empty and the code falls back to local IP (e.g., 10.0.0.62). This
    persists even after Tailscale becomes available later, causing P2P connectivity
    issues since other nodes can't reach the local IP. On mac-studio specifically,
    Tailscale can take 45-60s to initialize after boot.

    Fix: Retry Tailscale IP detection with exponential backoff for up to 90 seconds
    at startup with faster initial polling (1s intervals). This gives Tailscale
    enough time to initialize even on slow boot scenarios.

    Args:
        timeout_seconds: Maximum time to wait for Tailscale (default 90s)
        interval_seconds: Initial retry interval (doubles with each retry, max 5s)

    Returns:
        Tailscale IP if available within timeout, else empty string
    """
    from scripts.p2p.resource_detector import ResourceDetector

    detector = ResourceDetector()
    start_time = time.time()
    attempt = 0
    current_interval = interval_seconds

    while (time.time() - start_time) < timeout_seconds:
        attempt += 1
        ts_ip = detector.get_tailscale_ip()
        if ts_ip:
            if attempt > 1:
                logger.info(f"[TAILSCALE] IP acquired after {attempt} attempts: {ts_ip}")
            return ts_ip

        elapsed = time.time() - start_time
        remaining = timeout_seconds - elapsed

        if elapsed >= 5 and attempt <= 3:
            logger.warning(f"[TAILSCALE] Still waiting for IP (attempt {attempt}, {elapsed:.1f}s elapsed)")

        if remaining <= 0:
            break

        # Sleep with exponential backoff (max 5s between retries)
        sleep_time = min(current_interval, remaining, 5.0)
        time.sleep(sleep_time)
        current_interval = min(current_interval * 1.5, 5.0)

    logger.warning(f"[TAILSCALE] Timed out waiting for IP after {timeout_seconds}s ({attempt} attempts)")
    return ""


def _auto_detect_node_id() -> str | None:
    """Auto-detect node ID using unified identity resolution.

    Jan 2, 2026: Added to prevent startup failures when --node-id is forgotten.
    Jan 12, 2026: Added /etc/ringrift/node-id file support and IP normalization.
    Jan 13, 2026: Delegated to app.config.node_identity module (P2P Cluster Stability Plan).

    Detection order (from node_identity module):
    0. /etc/ringrift/node-id file (canonical source, written by deployment)
    1. RINGRIFT_NODE_ID environment variable
    2. /etc/default/ringrift-p2p file (legacy compatibility)
    3. Hostname match against distributed_hosts.yaml
    4. Tailscale IP match against distributed_hosts.yaml
    5. Fall back to get_node_id_safe() which uses hostname

    Returns:
        Detected node_id string, or None if detection failed
    """
    try:
        from app.config.node_identity import (
            get_node_identity,
            get_node_id_safe,
            NodeIdentityError,
        )

        # Try strict resolution first
        try:
            identity = get_node_identity()
            logger.info(
                f"[NODE-ID] Resolved node ID via {identity.resolution_method}: "
                f"{identity.canonical_id}"
            )
            return identity.canonical_id
        except NodeIdentityError as e:
            # Strict resolution failed, use safe fallback
            logger.warning(f"[NODE-ID] Strict resolution failed: {e}")
            node_id = get_node_id_safe()
            logger.warning(
                f"[NODE-ID] Using fallback node ID: {node_id} - "
                f"Run 'python scripts/provision_node_id.py --auto-detect' to fix"
            )
            return node_id

    except ImportError as e:
        # Module not available (running standalone or tests)
        logger.debug(f"[NODE-ID] node_identity module not available: {e}")

        # Minimal fallback: check canonical file and env var
        try:
            with open("/etc/ringrift/node-id") as f:
                node_id = f.read().strip()
                if node_id:
                    logger.info(f"[NODE-ID] Using node-id from /etc/ringrift/node-id: {node_id}")
                    return node_id
        except (FileNotFoundError, PermissionError):
            pass

        node_id = os.environ.get("RINGRIFT_NODE_ID")
        if node_id:
            return node_id

        # Fall back to hostname
        import socket
        hostname = socket.gethostname()
        if "." in hostname:
            hostname = hostname.split(".")[0]
        logger.warning(
            f"[NODE-ID] Falling back to hostname '{hostname}' - "
            f"Set RINGRIFT_NODE_ID or run provision_node_id.py"
        )
        return hostname


def _acquire_singleton_lock(
    kill_duplicates: bool = False,
    force_takeover: bool = False,
) -> bool:
    """Acquire singleton lock to prevent duplicate P2P orchestrator instances.

    Uses atomic file locking (fcntl) which is more reliable than PID file checks.
    Automatically handles stale locks from crashed processes.

    Args:
        kill_duplicates: If True, kill any duplicate P2P processes before acquiring
        force_takeover: If True, force-kill any lock holder (even if not P2P).
                        Use when lock is held by a recycled PID.

    Returns:
        True if lock acquired successfully
    """
    global _P2P_LOCK

    lock_dir = Path(__file__).parent.parent / "data" / "coordination"
    lock_dir.mkdir(parents=True, exist_ok=True)

    if kill_duplicates:
        # Find and kill any existing p2p_orchestrator processes
        pattern = r"p2p_orchestrator\.py"
        existing = find_processes_by_pattern(pattern, exclude_self=True)
        if existing:
            logger.info(f"[P2P] Found {len(existing)} duplicate processes, killing...")
            for proc in existing:
                logger.info(f"[P2P] Killing duplicate: PID {proc.pid}")
                if kill_process(proc.pid, wait=True, timeout=5.0):
                    logger.info(f"[P2P] Killed PID {proc.pid}")
                else:
                    logger.warning(f"[P2P] Failed to kill PID {proc.pid}")
            # Wait a moment for locks to release
            time.sleep(0.5)

    # Create lock with auto-cleanup of stale locks (from dead processes)
    _P2P_LOCK = SingletonLock(
        "p2p_orchestrator",
        lock_dir=lock_dir,
        auto_cleanup_stale=True,  # Automatically handle dead process locks
    )

    if not _P2P_LOCK.acquire():
        # Lock acquisition failed - provide detailed diagnostics
        status = _P2P_LOCK.get_lock_status()
        holder_pid = status.get("holder_pid")
        holder_alive = status.get("holder_alive", False)
        holder_command = status.get("holder_command", "")
        is_stale = status.get("is_stale", False)

        if is_stale:
            # This shouldn't happen with auto_cleanup_stale=True, but handle it
            logger.warning(
                f"[P2P] Stale lock detected (dead PID {holder_pid}). "
                f"Attempting force cleanup..."
            )
            if _P2P_LOCK.force_release():
                # Retry acquisition after cleanup
                if _P2P_LOCK.acquire():
                    logger.info(f"[P2P] Acquired lock after stale cleanup (PID {os.getpid()})")
                    return True
            logger.error("[P2P] Failed to clean up stale lock")
            return False

        if holder_pid and holder_alive:
            # Another live process is holding the lock
            is_p2p = _P2P_LOCK.is_holder_expected_process("p2p_orchestrator")
            if is_p2p:
                logger.error(
                    f"[P2P] Another P2P orchestrator is already running (PID {holder_pid}). "
                    f"Use --kill-duplicates to automatically terminate it."
                )
            else:
                # PID reuse - different process now holds the lock file
                # This happens when the old P2P crashed and the PID was reused
                if force_takeover:
                    logger.warning(
                        f"[P2P] Lock held by unexpected process (PID {holder_pid}: {holder_command[:80] if holder_command else 'unknown'}). "
                        f"Force takeover requested - killing holder."
                    )
                    if _P2P_LOCK.force_release(kill_holder=True):
                        if _P2P_LOCK.acquire():
                            logger.info(f"[P2P] Acquired lock after force takeover (PID {os.getpid()})")
                            return True
                    logger.error("[P2P] Force takeover failed")
                else:
                    logger.warning(
                        f"[P2P] Lock held by unexpected process (PID {holder_pid}: {holder_command[:80] if holder_command else 'unknown'}). "
                        f"This may indicate PID reuse after a crash. "
                        f"Use --force-takeover to automatically recover."
                    )
        else:
            logger.error(
                "[P2P] Failed to acquire lock (unknown reason). "
                f"Lock status: {status}"
            )
        return False

    logger.info(f"[P2P] Acquired singleton lock (PID {os.getpid()})")
    return True


def _release_singleton_lock() -> None:
    """Release the singleton lock on shutdown."""
    global _P2P_LOCK
    if _P2P_LOCK:
        _P2P_LOCK.release()
        logger.debug("[P2P] Released singleton lock")
        _P2P_LOCK = None


def _check_and_kill_zombie_p2p(port: int = 8770, timeout: float = 5.0) -> bool:
    """Check for zombie P2P process and kill it if found.

    A zombie P2P process is one that is bound to the port but not responding
    to HTTP requests. This can happen when the process is stuck in a bad state.

    Args:
        port: The P2P HTTP port to check (default 8770)
        timeout: HTTP request timeout in seconds

    Returns:
        True if a zombie was found and killed, False otherwise
    """
    import urllib.request
    import urllib.error

    # Step 1: Check if anything is listening on the port
    try:
        result = subprocess.run(
            ["lsof", "-ti", f":{port}"],
            capture_output=True,
            text=True,
            timeout=5.0,
        )
        if result.returncode != 0 or not result.stdout.strip():
            # Nothing listening on the port
            return False
        pids = [int(p) for p in result.stdout.strip().split("\n") if p.strip()]
        if not pids:
            return False
    except (subprocess.TimeoutExpired, subprocess.SubprocessError, ValueError):
        # lsof failed or timed out, assume no zombie
        return False

    # Step 2: Try to hit the /status endpoint
    try:
        req = urllib.request.Request(
            f"http://127.0.0.1:{port}/status",
            headers={"User-Agent": "zombie-detector"},
        )
        with urllib.request.urlopen(req, timeout=timeout) as resp:
            if resp.status == 200:
                # Process is responding, not a zombie
                return False
    except urllib.error.URLError as e:
        # Connection refused means nothing is really listening (lsof race)
        if "Connection refused" in str(e):
            return False
        # Other errors (timeout, etc.) mean zombie detected
        logger.warning(f"[P2P] Port {port} occupied but unresponsive: {e}")
    except Exception as e:
        # Timeout or other error - this is a zombie
        logger.warning(f"[P2P] Port {port} occupied but unresponsive: {e}")

    # Step 3: Kill the zombie process(es)
    logger.warning(f"[P2P] Detected zombie P2P process on port {port}, killing PIDs: {pids}")
    killed = False
    for pid in pids:
        # Skip ourselves
        if pid == os.getpid():
            continue
        try:
            if kill_process(pid, wait=True, timeout=5.0):
                logger.info(f"[P2P] Killed zombie process PID {pid}")
                killed = True
            else:
                logger.warning(f"[P2P] Failed to kill zombie PID {pid}")
        except Exception as e:
            logger.error(f"[P2P] Error killing zombie PID {pid}: {e}")

    if killed:
        # Give the port time to be released
        time.sleep(0.5)

    return killed


def main():
    # ==========================================================================
    # PRE-FLIGHT VALIDATION (January 2026)
    # ==========================================================================
    # Validate critical dependencies before any complex initialization.
    # This prevents cryptic runtime errors from missing packages.
    deps_ok, dep_errors = _validate_preflight_dependencies()
    if not deps_ok:
        print("[P2P] FATAL: Missing critical dependencies", file=sys.stderr)
        for err in dep_errors:
            print(f"  {err}", file=sys.stderr)
        print("\n[P2P] Fix: pip install aiohttp psutil pyyaml", file=sys.stderr)
        sys.exit(1)

    # Parse lock-related args early (before full argparse)
    import sys
    kill_duplicates = "--kill-duplicates" in sys.argv
    force_takeover = "--force-takeover" in sys.argv
    skip_zombie_check = "--no-zombie-check" in sys.argv

    # ==========================================================================
    # ZOMBIE DETECTION (January 2026)
    # ==========================================================================
    # Check for zombie P2P processes that are bound to the port but unresponsive.
    # This happens when the P2P process gets stuck in a bad state (e.g., 100% CPU).
    if not skip_zombie_check:
        if _check_and_kill_zombie_p2p():
            print("[P2P] Killed zombie P2P process, proceeding with startup")

    # Acquire singleton lock (December 2025: improved atomic locking with stale cleanup)
    if not _acquire_singleton_lock(
        kill_duplicates=kill_duplicates,
        force_takeover=force_takeover,
    ):
        sys.exit(1)

    parser = argparse.ArgumentParser(description="P2P Orchestrator for RingRift cluster")
    parser.add_argument("--node-id", required=False, help="Unique identifier for this node (auto-detects if not provided)")
    parser.add_argument("--host", default="0.0.0.0", help="Host to bind to")
    parser.add_argument("--port", type=int, default=DEFAULT_PORT, help="Port to listen on")
    parser.add_argument(
        "--advertise-host",
        default=None,
        help=f"Host to advertise to peers (or set {ADVERTISE_HOST_ENV})",
    )
    parser.add_argument(
        "--advertise-port",
        type=int,
        default=None,
        help=f"Port to advertise to peers (or set {ADVERTISE_PORT_ENV})",
    )
    parser.add_argument("--peers", help="Comma-separated list of known peers (host[:port] or http(s)://host[:port])")
    parser.add_argument("--relay-peers", help="Comma-separated list of peers to use relay heartbeats with (for NAT-blocked nodes)")
    parser.add_argument("--ringrift-path", help="Path to RingRift installation")
    parser.add_argument("--auth-token", help=f"Shared auth token (or set {AUTH_TOKEN_ENV})")
    parser.add_argument("--require-auth", action="store_true", help="Require auth token to be set")
    parser.add_argument("--storage-type", choices=["disk", "ramdrive", "auto"], default="auto",
                        help="Storage type: 'disk', 'ramdrive' (/dev/shm), or 'auto' (detect based on RAM/disk)")
    parser.add_argument("--sync-to-disk-interval", type=int, default=300,
                        help="When using ramdrive, sync to disk every N seconds (0 = no sync, default: 300)")
    parser.add_argument("--supervised", action="store_true",
                        help="Running under cluster_supervisor.py - disable self-restart logic")
    parser.add_argument("--kill-duplicates", action="store_true",
                        help="Kill any existing P2P orchestrator processes before starting")
    parser.add_argument("--force-takeover", action="store_true",
                        help="Force acquire lock even if held by another process (use when PID was recycled after crash)")
    parser.add_argument("--no-zombie-check", action="store_true",
                        help="Skip automatic zombie P2P detection (zombies are processes bound to port but not responding)")
    parser.add_argument("--training-only", action="store_true",
                        help="Run as training-only node (no selfplay dispatch). Prevents OOM from training + selfplay conflicts.")

    args = parser.parse_args()

    # Jan 2026: Set training-only mode if flag is set
    if args.training_only:
        set_selfplay_disabled_override(disabled=True)
        logger.info("[P2P] Running in training-only mode - selfplay disabled")

    # Jan 2, 2026: Auto-detect node_id if not provided
    if not args.node_id:
        args.node_id = _auto_detect_node_id()
        if not args.node_id:
            logger.error("Could not auto-detect node-id. Please provide --node-id explicitly.")
            sys.exit(1)
        logger.info(f"Auto-detected node-id: {args.node_id}")

    known_peers = []
    if args.peers:
        known_peers = [p.strip() for p in args.peers.split(',')]

    relay_peers = []
    if args.relay_peers:
        relay_peers = [p.strip() for p in args.relay_peers.split(',')]

    # Wrap orchestrator creation and run in try/except to ensure crashes are logged
    orchestrator = None
    try:
        logger.info(f"Initializing P2P orchestrator: node_id={args.node_id}")
        orchestrator = P2POrchestrator(
            node_id=args.node_id,
            host=args.host,
            port=args.port,
            known_peers=known_peers,
            relay_peers=relay_peers,
            ringrift_path=args.ringrift_path,
            advertise_host=args.advertise_host,
            advertise_port=args.advertise_port,
            auth_token=args.auth_token,
            require_auth=args.require_auth,
            storage_type=args.storage_type,
            sync_to_disk_interval=args.sync_to_disk_interval,
        )
        logger.info(f"P2P orchestrator initialized successfully: {args.node_id}")

        # December 28, 2025: Validate event emitters at startup
        # This provides early warning if event system is not properly configured
        if _check_event_emitters():
            logger.info("[P2P] Event emitters available - P2P events will be published")
        else:
            logger.warning(
                "[P2P] Event emitters NOT available - P2P events will be silent. "
                "Ensure app.coordination.event_emitters is importable for full integration."
            )
    except Exception as e:  # noqa: BLE001
        logger.exception(f"Failed to initialize P2P orchestrator: {e}")
        # January 2026: Release lock on initialization failure to prevent
        # stale locks that block future startups
        _release_singleton_lock()
        sys.exit(1)

    # Handle shutdown gracefully - avoid race conditions with async tasks
    # December 2025: Fixed signal handler race condition that caused threading exceptions
    _shutdown_requested = False
    _start_time = time.time()

    def signal_handler(sig, frame):
        nonlocal _shutdown_requested
        import traceback

        uptime = time.time() - _start_time
        sig_name = signal.Signals(sig).name if hasattr(signal, 'Signals') else f"signal {sig}"

        if _shutdown_requested:
            # Force exit on second signal
            logger.warning(f"Forced shutdown (second {sig_name}) after {uptime:.1f}s uptime")
            os._exit(1)
        _shutdown_requested = True

        # Enhanced logging to identify what's sending signals
        logger.warning(f"=== SIGNAL RECEIVED: {sig_name} ===")
        logger.warning(f"PID: {os.getpid()}, Uptime: {uptime:.1f}s, Node: {args.node_id}")
        logger.warning(f"Stack trace at signal:\n{''.join(traceback.format_stack(frame))}")
        logger.info("Shutdown requested, stopping gracefully...")
        if orchestrator:
            orchestrator.running = False
            # Cancel all background tasks for graceful shutdown (Dec 2025)
            if hasattr(orchestrator, '_background_tasks'):
                for task in orchestrator._background_tasks:
                    if not task.done():
                        task.cancel()
            # Schedule ramdrive sync in a thread to avoid blocking signal handler
            # Don't call sys.exit() - let asyncio loop exit cleanly

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    # Run with exception logging
    try:
        logger.info(f"Starting P2P orchestrator main loop: {args.node_id}")
        asyncio.run(orchestrator.run())
    except Exception as e:  # noqa: BLE001
        logger.exception(f"P2P orchestrator crashed: {e}")
        sys.exit(1)
    finally:
        # Ensure ramdrive is synced on exit (moved from signal handler to avoid race)
        if orchestrator:
            try:
                orchestrator.stop_ramdrive_syncer(final_sync=True)
                logger.info("Ramdrive sync completed on shutdown")
            except Exception as e:  # noqa: BLE001
                logger.warning(f"Ramdrive sync on shutdown failed: {e}")
            # December 2025: Close webhook notifier to prevent memory leaks
            try:
                if hasattr(orchestrator, 'notifier') and orchestrator.notifier:
                    orchestrator.notifier.close_sync()
            except (RuntimeError, OSError, AttributeError) as e:
                # Dec 2025: Narrowed from bare Exception; best effort cleanup
                logger.debug(f"Notifier close failed (best effort): {e}")

            # December 2025: Close work queue to persist final stats
            try:
                from app.coordination.work_queue import reset_work_queue
                reset_work_queue()
            except (ImportError, RuntimeError, sqlite3.Error) as e:
                logger.debug(f"Work queue cleanup failed (best effort): {e}")

            # December 2025: Release singleton lock on shutdown
            _release_singleton_lock()


if __name__ == "__main__":
    main()
