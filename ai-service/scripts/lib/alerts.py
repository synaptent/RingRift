"""
Shared Alert Infrastructure

Provides common alert types, severity levels, and alert management
utilities used across monitoring scripts.

Usage:
    from scripts.lib.alerts import (
        AlertSeverity,
        AlertType,
        Alert,
        AlertManager,
        create_alert,
    )

    # Create an alert
    alert = create_alert(
        AlertSeverity.WARNING,
        AlertType.HIGH_DISK_USAGE,
        "Disk usage at 75%",
        details={"disk_percent": 75.0},
    )

    # Use alert manager for tracking and notifications
    manager = AlertManager(name="cluster_monitor")
    manager.add_alert(alert)
    manager.flush()  # Send pending notifications
"""

from __future__ import annotations

import json
import logging
import os
import subprocess
from collections.abc import Callable
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any

logger = logging.getLogger(__name__)


class AlertSeverity(Enum):
    """Severity level for alerts."""
    DEBUG = "debug"
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"

    @property
    def level(self) -> int:
        """Return numeric level for comparison."""
        levels = {
            "debug": 0,
            "info": 1,
            "warning": 2,
            "error": 3,
            "critical": 4,
        }
        return levels[self.value]

    def __lt__(self, other: AlertSeverity) -> bool:
        return self.level < other.level

    def __le__(self, other: AlertSeverity) -> bool:
        return self.level <= other.level

    def __gt__(self, other: AlertSeverity) -> bool:
        return self.level > other.level

    def __ge__(self, other: AlertSeverity) -> bool:
        return self.level >= other.level


class AlertType(Enum):
    """Types of alerts that can be raised."""
    # Resource alerts
    HIGH_DISK_USAGE = "high_disk_usage"
    HIGH_MEMORY_USAGE = "high_memory_usage"
    HIGH_CPU_USAGE = "high_cpu_usage"
    HIGH_GPU_USAGE = "high_gpu_usage"
    LOW_DISK_SPACE = "low_disk_space"
    GPU_IDLE = "gpu_idle"
    GPU_HOT = "gpu_hot"

    # Cluster alerts
    LEADER_UNREACHABLE = "leader_unreachable"
    NODE_UNREACHABLE = "node_unreachable"
    NODE_DEGRADED = "node_degraded"
    ROLE_MISMATCH = "role_mismatch"

    # Training alerts
    TRAINING_FAILED = "training_failed"
    TRAINING_STALLED = "training_stalled"
    TRAINING_SLOW = "training_slow"
    LOSS_SPIKE = "loss_spike"
    GRADIENT_EXPLOSION = "gradient_explosion"

    # Data quality alerts
    DATA_CORRUPTION = "data_corruption"
    DATA_IMBALANCE = "data_imbalance"
    DATA_STALE = "data_stale"
    VALIDATION_FAILED = "validation_failed"
    HIGH_DRAW_RATE = "high_draw_rate"
    GAMES_AT_MOVE_LIMIT = "games_at_move_limit"
    NO_GAMES = "no_games"
    DATABASE_ERROR = "database_error"
    HIGH_MOVES_COUNT = "high_moves_count"

    # Elo/evaluation alerts
    ELO_REGRESSION = "elo_regression"
    ELO_STAGNATION = "elo_stagnation"
    MODEL_DEGRADATION = "model_degradation"

    # General alerts
    THRESHOLD_EXCEEDED = "threshold_exceeded"
    HEALTH_CHECK_FAILED = "health_check_failed"
    CONFIGURATION_ERROR = "configuration_error"
    UNKNOWN = "unknown"


@dataclass
class Alert:
    """An alert generated by a monitoring system."""
    severity: AlertSeverity
    alert_type: AlertType
    message: str
    timestamp: datetime = field(default_factory=datetime.now)
    details: dict[str, Any] = field(default_factory=dict)
    source: str = ""
    acknowledged: bool = False

    def __str__(self) -> str:
        prefix = self.severity.value.upper()
        return f"[{prefix}] {self.message}"

    def __repr__(self) -> str:
        return f"Alert({self.severity.value}, {self.alert_type.value}, {self.message!r})"

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "severity": self.severity.value,
            "alert_type": self.alert_type.value,
            "message": self.message,
            "timestamp": self.timestamp.isoformat(),
            "details": self.details,
            "source": self.source,
            "acknowledged": self.acknowledged,
        }

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> Alert:
        """Create from dictionary."""
        return cls(
            severity=AlertSeverity(data["severity"]),
            alert_type=AlertType(data.get("alert_type", "unknown")),
            message=data["message"],
            timestamp=datetime.fromisoformat(data["timestamp"]) if "timestamp" in data else datetime.now(),
            details=data.get("details", {}),
            source=data.get("source", ""),
            acknowledged=data.get("acknowledged", False),
        )

    def to_json(self) -> str:
        """Convert to JSON string."""
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Alert:
        """Create from JSON string."""
        return cls.from_dict(json.loads(json_str))


def create_alert(
    severity: AlertSeverity,
    alert_type: AlertType,
    message: str,
    details: dict[str, Any] | None = None,
    source: str = "",
) -> Alert:
    """Convenience function to create an alert."""
    return Alert(
        severity=severity,
        alert_type=alert_type,
        message=message,
        details=details or {},
        source=source,
    )


@dataclass
class AlertThresholds:
    """Configurable thresholds for alert generation.

    Disk thresholds aligned with app.config.thresholds (canonical source):
    - 65% warning (5% below DISK_SYNC_TARGET_PERCENT)
    - 70% critical (= DISK_SYNC_TARGET_PERCENT)
    """
    disk_warning_percent: float = 65.0
    disk_critical_percent: float = 70.0  # DISK_SYNC_TARGET_PERCENT
    memory_warning_percent: float = 80.0
    memory_critical_percent: float = 95.0
    cpu_warning_percent: float = 80.0
    cpu_critical_percent: float = 95.0
    gpu_temp_warning_c: int = 80
    gpu_temp_critical_c: int = 90
    gpu_idle_threshold_percent: float = 5.0
    elo_regression_threshold: float = 50.0
    training_stall_minutes: int = 30

    @classmethod
    def from_env(cls) -> AlertThresholds:
        """Create from environment variables."""
        return cls(
            disk_warning_percent=float(os.getenv("RINGRIFT_DISK_WARNING", "65")),
            disk_critical_percent=float(os.getenv("RINGRIFT_DISK_CRITICAL", "70")),  # DISK_SYNC_TARGET_PERCENT
            memory_warning_percent=float(os.getenv("RINGRIFT_MEMORY_WARNING", "80")),
            memory_critical_percent=float(os.getenv("RINGRIFT_MEMORY_CRITICAL", "95")),
            cpu_warning_percent=float(os.getenv("RINGRIFT_CPU_WARNING", "80")),
            cpu_critical_percent=float(os.getenv("RINGRIFT_CPU_CRITICAL", "95")),
        )


# Type alias for alert handlers
AlertHandler = Callable[[Alert], None]


class AlertManager:
    """Manages alert collection, deduplication, and notification."""

    def __init__(
        self,
        name: str,
        thresholds: AlertThresholds | None = None,
        min_severity: AlertSeverity = AlertSeverity.INFO,
        dedup_window_seconds: int = 300,
    ):
        self.name = name
        self.thresholds = thresholds or AlertThresholds()
        self.min_severity = min_severity
        self.dedup_window_seconds = dedup_window_seconds

        self.alerts: list[Alert] = []
        self.alert_history: list[Alert] = []
        self._handlers: list[AlertHandler] = []
        self._last_alert_times: dict[str, datetime] = {}

    def add_handler(self, handler: AlertHandler) -> None:
        """Add an alert handler (e.g., for Slack, email, logging)."""
        self._handlers.append(handler)

    def _should_dedupe(self, alert: Alert) -> bool:
        """Check if this alert should be deduplicated."""
        key = f"{alert.alert_type.value}:{alert.message}"
        last_time = self._last_alert_times.get(key)

        if last_time is None:
            return False

        elapsed = (alert.timestamp - last_time).total_seconds()
        return elapsed < self.dedup_window_seconds

    def add_alert(self, alert: Alert) -> bool:
        """Add an alert. Returns True if alert was added, False if deduplicated."""
        # Filter by minimum severity
        if alert.severity < self.min_severity:
            return False

        # Set source if not set
        if not alert.source:
            alert.source = self.name

        # Check deduplication
        if self._should_dedupe(alert):
            logger.debug(f"Deduplicated alert: {alert}")
            return False

        # Update last alert time
        key = f"{alert.alert_type.value}:{alert.message}"
        self._last_alert_times[key] = alert.timestamp

        # Add to lists
        self.alerts.append(alert)
        self.alert_history.append(alert)

        # Log the alert
        log_level = {
            AlertSeverity.DEBUG: logging.DEBUG,
            AlertSeverity.INFO: logging.INFO,
            AlertSeverity.WARNING: logging.WARNING,
            AlertSeverity.ERROR: logging.ERROR,
            AlertSeverity.CRITICAL: logging.CRITICAL,
        }.get(alert.severity, logging.INFO)

        # Use a safe extra dict that doesn't conflict with logging internals
        safe_extra = {
            "alert_severity": alert.severity.value,
            "alert_type": alert.alert_type.value,
            "alert_message": alert.message,
            "alert_details": alert.details,
            "alert_source": alert.source,
        }
        logger.log(log_level, f"Alert: {alert}", extra=safe_extra)

        return True

    def flush(self) -> list[Alert]:
        """Process pending alerts through handlers and clear."""
        pending = self.alerts.copy()

        for alert in pending:
            for handler in self._handlers:
                try:
                    handler(alert)
                except Exception as e:
                    logger.error(f"Alert handler error: {e}")

        self.alerts.clear()
        return pending

    def get_alerts(
        self,
        severity: AlertSeverity | None = None,
        alert_type: AlertType | None = None,
        since: datetime | None = None,
    ) -> list[Alert]:
        """Get alerts matching criteria."""
        result = self.alert_history.copy()

        if severity is not None:
            result = [a for a in result if a.severity >= severity]

        if alert_type is not None:
            result = [a for a in result if a.alert_type == alert_type]

        if since is not None:
            result = [a for a in result if a.timestamp >= since]

        return result

    def clear_history(self) -> None:
        """Clear alert history."""
        self.alert_history.clear()
        self._last_alert_times.clear()

    def get_summary(self) -> dict[str, Any]:
        """Get summary of alert activity."""
        by_severity = {}
        by_type = {}

        for alert in self.alert_history:
            sev = alert.severity.value
            by_severity[sev] = by_severity.get(sev, 0) + 1

            typ = alert.alert_type.value
            by_type[typ] = by_type.get(typ, 0) + 1

        return {
            "total_alerts": len(self.alert_history),
            "pending_alerts": len(self.alerts),
            "by_severity": by_severity,
            "by_type": by_type,
        }


# Common alert handlers

def log_handler(alert: Alert) -> None:
    """Handler that logs alerts."""
    logger.info(f"Alert: {alert}", extra=alert.to_dict())


def console_handler(alert: Alert) -> None:
    """Handler that prints alerts to console."""
    prefix = {
        AlertSeverity.DEBUG: "DEBUG",
        AlertSeverity.INFO: "INFO",
        AlertSeverity.WARNING: "WARNING",
        AlertSeverity.ERROR: "ERROR",
        AlertSeverity.CRITICAL: "CRITICAL",
    }.get(alert.severity, "INFO")
    print(f"[{prefix}] {alert.message}")


def file_handler(filepath: Path) -> AlertHandler:
    """Create a handler that appends alerts to a file."""
    def handler(alert: Alert) -> None:
        with open(filepath, "a") as f:
            f.write(alert.to_json() + "\n")
    return handler


# Resource checking helpers

def check_disk_alert(
    percent: float,
    thresholds: AlertThresholds,
    path: str = "/",
) -> Alert | None:
    """Check disk usage and return alert if threshold exceeded."""
    if percent >= thresholds.disk_critical_percent:
        return create_alert(
            AlertSeverity.CRITICAL,
            AlertType.HIGH_DISK_USAGE,
            f"Disk usage critical: {percent:.1f}% on {path}",
            details={"disk_percent": percent, "path": path},
        )
    elif percent >= thresholds.disk_warning_percent:
        return create_alert(
            AlertSeverity.WARNING,
            AlertType.HIGH_DISK_USAGE,
            f"Disk usage high: {percent:.1f}% on {path}",
            details={"disk_percent": percent, "path": path},
        )
    return None


def check_memory_alert(
    percent: float,
    thresholds: AlertThresholds,
) -> Alert | None:
    """Check memory usage and return alert if threshold exceeded."""
    if percent >= thresholds.memory_critical_percent:
        return create_alert(
            AlertSeverity.CRITICAL,
            AlertType.HIGH_MEMORY_USAGE,
            f"Memory usage critical: {percent:.1f}%",
            details={"memory_percent": percent},
        )
    elif percent >= thresholds.memory_warning_percent:
        return create_alert(
            AlertSeverity.WARNING,
            AlertType.HIGH_MEMORY_USAGE,
            f"Memory usage high: {percent:.1f}%",
            details={"memory_percent": percent},
        )
    return None


def check_cpu_alert(
    percent: float,
    thresholds: AlertThresholds,
) -> Alert | None:
    """Check CPU usage and return alert if threshold exceeded."""
    if percent >= thresholds.cpu_critical_percent:
        return create_alert(
            AlertSeverity.CRITICAL,
            AlertType.HIGH_CPU_USAGE,
            f"CPU usage critical: {percent:.1f}%",
            details={"cpu_percent": percent},
        )
    elif percent >= thresholds.cpu_warning_percent:
        return create_alert(
            AlertSeverity.WARNING,
            AlertType.HIGH_CPU_USAGE,
            f"CPU usage high: {percent:.1f}%",
            details={"cpu_percent": percent},
        )
    return None


# =============================================================================
# Slack Notification Support
# =============================================================================

# Emoji mapping for alert severity and types
SLACK_EMOJI = {
    # Severity-based emojis
    AlertSeverity.DEBUG: ":mag:",
    AlertSeverity.INFO: ":information_source:",
    AlertSeverity.WARNING: ":warning:",
    AlertSeverity.ERROR: ":x:",
    AlertSeverity.CRITICAL: ":rotating_light:",
    # Special emojis for common scenarios
    "success": ":white_check_mark:",
    "trophy": ":trophy:",
    "rocket": ":rocket:",
    "robot": ":robot_face:",
}


def get_slack_webhook() -> str | None:
    """Get Slack webhook URL from environment or config file.

    Checks in order:
    1. RINGRIFT_SLACK_WEBHOOK environment variable
    2. SLACK_WEBHOOK_URL environment variable
    3. ~/.ringrift_slack_webhook file

    Returns:
        Webhook URL string, or None if not configured
    """
    # Check environment variables
    webhook = os.environ.get("RINGRIFT_SLACK_WEBHOOK")
    if webhook:
        return webhook

    webhook = os.environ.get("SLACK_WEBHOOK_URL")
    if webhook:
        return webhook

    # Check config file
    webhook_file = Path.home() / ".ringrift_slack_webhook"
    if webhook_file.exists():
        try:
            return webhook_file.read_text().strip()
        except (OSError, PermissionError):
            pass

    return None


def send_slack_notification(
    message: str,
    severity: AlertSeverity | str = AlertSeverity.INFO,
    title: str | None = None,
    webhook_url: str | None = None,
    username: str = "RingRift Alert",
    timeout: int = 10,
) -> bool:
    """Send a notification to Slack.

    Args:
        message: The message body to send
        severity: Alert severity (determines emoji) or custom emoji key
        title: Optional bold title for the message
        webhook_url: Override webhook URL (otherwise uses get_slack_webhook)
        username: Slack username to display
        timeout: Request timeout in seconds

    Returns:
        True if sent successfully, False otherwise
    """
    webhook = webhook_url or get_slack_webhook()

    # Determine emoji
    if isinstance(severity, AlertSeverity):
        emoji = SLACK_EMOJI.get(severity, ":robot_face:")
    else:
        emoji = SLACK_EMOJI.get(severity, ":robot_face:")

    # Format message
    if title:
        text = f"{emoji} *{title}*\n{message}"
    else:
        text = f"{emoji} {message}"

    if webhook:
        payload = {
            "text": text,
            "username": username,
        }
        try:
            result = subprocess.run(
                [
                    "curl", "-s", "-X", "POST",
                    "-H", "Content-Type: application/json",
                    "-d", json.dumps(payload),
                    webhook,
                ],
                capture_output=True,
                timeout=timeout,
            )
            if result.returncode == 0:
                logger.debug(f"Slack notification sent: {title or message[:50]}")
                return True
            else:
                logger.warning(f"Slack notification failed: {result.stderr.decode('utf-8', errors='replace')}")
                return False
        except subprocess.TimeoutExpired:
            logger.warning("Slack notification timed out")
            return False
        except Exception as e:
            logger.error(f"Failed to send Slack notification: {e}")
            return False
    else:
        # No webhook configured - log to console instead
        severity_str = severity.value.upper() if isinstance(severity, AlertSeverity) else str(severity).upper()
        print(f"[{severity_str}] {title or ''} {message}")
        return False


def slack_handler(
    webhook_url: str | None = None,
    username: str = "RingRift Alert",
    min_severity: AlertSeverity = AlertSeverity.WARNING,
) -> AlertHandler:
    """Create an AlertManager handler that sends alerts to Slack.

    Args:
        webhook_url: Override webhook URL
        username: Slack username to display
        min_severity: Minimum severity to send to Slack

    Returns:
        Alert handler function for use with AlertManager.add_handler()

    Usage:
        manager = AlertManager(name="my_monitor")
        manager.add_handler(slack_handler())
        manager.add_alert(alert)
        manager.flush()  # Sends to Slack
    """
    def handler(alert: Alert) -> None:
        if alert.severity < min_severity:
            return

        send_slack_notification(
            message=alert.message,
            severity=alert.severity,
            title=f"[{alert.source}] {alert.alert_type.value}",
            webhook_url=webhook_url,
            username=username,
        )

    return handler


def send_simple_alert(
    message: str,
    alert_type: str = "info",
    title: str = "RingRift Alert",
) -> bool:
    """Simple convenience function for sending alerts.

    This is a simplified interface for scripts that don't need
    the full AlertManager infrastructure.

    Args:
        message: The alert message
        alert_type: One of "info", "success", "warning", "error", "trophy", "rocket"
        title: Title for the alert

    Returns:
        True if sent to Slack, False if printed to console

    Usage:
        from scripts.lib.alerts import send_simple_alert
        send_simple_alert("Model promoted!", "success", "Promotion")
    """
    severity_map = {
        "info": AlertSeverity.INFO,
        "success": "success",  # Use special emoji
        "warning": AlertSeverity.WARNING,
        "error": AlertSeverity.ERROR,
        "critical": AlertSeverity.CRITICAL,
        "trophy": "trophy",
        "rocket": "rocket",
    }
    severity = severity_map.get(alert_type, AlertSeverity.INFO)
    return send_slack_notification(message, severity=severity, title=title)


# =============================================================================
# Discord Notification Support
# =============================================================================

# Color mapping for Discord embeds (decimal RGB values)
DISCORD_COLORS = {
    AlertSeverity.DEBUG: 0x808080,    # Gray
    AlertSeverity.INFO: 0x36a64f,     # Green
    AlertSeverity.WARNING: 0xff9800,  # Orange
    AlertSeverity.ERROR: 0xf44336,    # Red
    AlertSeverity.CRITICAL: 0x9c27b0, # Purple
}


def get_discord_webhook() -> str | None:
    """Get Discord webhook URL from environment or config file.

    Checks in order:
    1. RINGRIFT_DISCORD_WEBHOOK environment variable
    2. DISCORD_WEBHOOK_URL environment variable
    3. ~/.ringrift_discord_webhook file

    Returns:
        Webhook URL string, or None if not configured
    """
    webhook = os.environ.get("RINGRIFT_DISCORD_WEBHOOK")
    if webhook:
        return webhook

    webhook = os.environ.get("DISCORD_WEBHOOK_URL")
    if webhook:
        return webhook

    webhook_file = Path.home() / ".ringrift_discord_webhook"
    if webhook_file.exists():
        try:
            return webhook_file.read_text().strip()
        except (OSError, PermissionError):
            pass

    return None


def send_discord_notification(
    message: str,
    severity: AlertSeverity = AlertSeverity.INFO,
    title: str | None = None,
    webhook_url: str | None = None,
    node_id: str = "",
    timeout: int = 10,
) -> bool:
    """Send a notification to Discord.

    Args:
        message: The message body to send
        severity: Alert severity (determines color)
        title: Optional title for the embed
        webhook_url: Override webhook URL (otherwise uses get_discord_webhook)
        node_id: Optional node identifier for footer
        timeout: Request timeout in seconds

    Returns:
        True if sent successfully, False otherwise
    """
    import urllib.error
    import urllib.request

    webhook = webhook_url or get_discord_webhook()

    if not webhook:
        # No webhook configured - log to console
        severity_str = severity.value.upper()
        print(f"[{severity_str}] {title or ''} {message}")
        return False

    # Get color and emoji
    color = DISCORD_COLORS.get(severity, 0x808080)
    emoji = SLACK_EMOJI.get(severity, ":robot_face:")

    # Build embed
    embed = {
        "title": f"{emoji} {title}" if title else f"{emoji} Alert",
        "description": message,
        "color": color,
        "timestamp": datetime.now().isoformat(),
    }

    if node_id:
        embed["footer"] = {"text": f"RingRift AI | {node_id}"}
    else:
        embed["footer"] = {"text": "RingRift AI"}

    payload = {"embeds": [embed]}

    try:
        data = json.dumps(payload).encode("utf-8")
        req = urllib.request.Request(
            webhook,
            data=data,
            headers={"Content-Type": "application/json"},
        )
        with urllib.request.urlopen(req, timeout=timeout) as resp:
            if resp.status in (200, 204):
                logger.debug(f"Discord notification sent: {title or message[:50]}")
                return True
            else:
                logger.warning(f"Discord notification failed: status {resp.status}")
                return False
    except urllib.error.URLError as e:
        logger.warning(f"Discord notification failed: {e}")
        return False
    except Exception as e:
        logger.error(f"Failed to send Discord notification: {e}")
        return False


def discord_handler(
    webhook_url: str | None = None,
    min_severity: AlertSeverity = AlertSeverity.WARNING,
) -> AlertHandler:
    """Create an AlertManager handler that sends alerts to Discord.

    Args:
        webhook_url: Override webhook URL
        min_severity: Minimum severity to send to Discord

    Returns:
        Alert handler function for use with AlertManager.add_handler()

    Usage:
        manager = AlertManager(name="my_monitor")
        manager.add_handler(discord_handler())
        manager.add_alert(alert)
        manager.flush()  # Sends to Discord
    """
    def handler(alert: Alert) -> None:
        if alert.severity < min_severity:
            return

        send_discord_notification(
            message=alert.message,
            severity=alert.severity,
            title=f"[{alert.source}] {alert.alert_type.value}",
            webhook_url=webhook_url,
        )

    return handler


# =============================================================================
# Unified Multi-Channel Alerting
# =============================================================================

def send_alert(
    message: str,
    severity: AlertSeverity = AlertSeverity.INFO,
    title: str | None = None,
    node_id: str = "",
    slack_url: str | None = None,
    discord_url: str | None = None,
) -> bool:
    """Send alert to all configured channels (Slack and/or Discord).

    This is the primary entry point for sending alerts. It automatically
    discovers configured webhooks and sends to all available channels.

    Args:
        message: The alert message
        severity: Alert severity level
        title: Optional title for the alert
        node_id: Optional node identifier
        slack_url: Override Slack webhook URL
        discord_url: Override Discord webhook URL

    Returns:
        True if at least one alert was sent successfully

    Usage:
        from scripts.lib.alerts import send_alert, AlertSeverity
        send_alert("Training complete!", AlertSeverity.INFO, "Training")
    """
    success = False

    # Send to Slack
    slack = slack_url or get_slack_webhook()
    if slack:
        if send_slack_notification(
            message=message,
            severity=severity,
            title=title,
            webhook_url=slack,
        ):
            success = True

    # Send to Discord
    discord = discord_url or get_discord_webhook()
    if discord:
        if send_discord_notification(
            message=message,
            severity=severity,
            title=title,
            webhook_url=discord,
            node_id=node_id,
        ):
            success = True

    return success
