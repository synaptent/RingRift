# RingRift Cluster Configuration
# Single source of truth for all cluster nodes
# Updated: 2025-12-19

cluster:
  name: ringrift-training
  version: '2.0'

# Alert configuration
alerts:
  slack_webhook: '${RINGRIFT_SLACK_WEBHOOK}'
  discord_webhook: '${RINGRIFT_DISCORD_WEBHOOK}'
  email: '${RINGRIFT_ALERT_EMAIL}'
  thresholds:
    gpu_util_low: 10 # Alert if GPU < 10% with active jobs
    gpu_util_critical: 0 # Alert if GPU = 0% for > 5 min with jobs
    disk_usage_warn: 80 # Warn at 80% disk
    disk_usage_critical: 90 # Critical at 90% disk
    memory_warn: 85 # Warn at 85% memory
    offline_timeout_sec: 300 # Alert after 5 min offline

# Node definitions
nodes:
  # === GH200 Nodes (96GB Unified Memory) ===
  lambda-gh200-a:
    host: lambda-gh200-a
    tailscale_ip: '100.123.183.70' # Check tailscale status for current
    ssh_user: ubuntu
    gpu_type: GH200
    gpu_count: 1
    vram_gb: 96
    batch_multiplier: 64
    priority: 1
    roles: [selfplay, training, benchmark]
    status: active

  lambda-gh200-b:
    host: lambda-gh200-b
    ssh_user: ubuntu
    gpu_type: GH200
    gpu_count: 1
    vram_gb: 96
    batch_multiplier: 64
    priority: 1
    roles: [selfplay, training]
    status: needs_ssh_key

  lambda-gh200-c:
    host: lambda-gh200-c
    ssh_user: ubuntu
    gpu_type: GH200
    gpu_count: 1
    vram_gb: 96
    batch_multiplier: 64
    priority: 1
    roles: [selfplay, training]
    status: needs_ssh_key

  lambda-gh200-d:
    host: lambda-gh200-d
    tailscale_ip: '100.88.176.74'
    ssh_user: ubuntu
    gpu_type: GH200
    gpu_count: 1
    vram_gb: 96
    batch_multiplier: 64
    priority: 1
    roles: [selfplay, training, benchmark]
    status: active

  lambda-gh200-e:
    host: lambda-gh200-e
    tailscale_ip: '100.88.176.74'
    ssh_user: ubuntu
    gpu_type: GH200
    gpu_count: 1
    vram_gb: 96
    batch_multiplier: 64
    priority: 1
    roles: [selfplay, training]
    status: active
    notes: 'Recovered from GPU error 2025-12-19'

  lambda-gh200-f:
    host: lambda-gh200-f
    tailscale_ip: '100.104.165.116'
    ssh_user: ubuntu
    gpu_type: GH200
    gpu_count: 1
    vram_gb: 96
    batch_multiplier: 64
    priority: 1
    roles: [selfplay, training, benchmark]
    status: active

  lambda-gh200-g:
    host: lambda-gh200-g
    tailscale_ip: '100.104.126.58'
    ssh_user: ubuntu
    gpu_type: GH200
    gpu_count: 1
    vram_gb: 96
    batch_multiplier: 64
    priority: 1
    roles: [selfplay, training]
    status: active

  lambda-gh200-h:
    host: lambda-gh200-h
    tailscale_ip: '100.65.88.62'
    ssh_user: ubuntu
    gpu_type: GH200
    gpu_count: 1
    vram_gb: 96
    batch_multiplier: 64
    priority: 2
    roles: [selfplay, gauntlet]
    status: active
    notes: 'Often CPU-bound (hex8 benchmark)'

  lambda-gh200-i:
    host: lambda-gh200-i
    tailscale_ip: '100.99.27.56'
    ssh_user: ubuntu
    gpu_type: GH200
    gpu_count: 1
    vram_gb: 96
    batch_multiplier: 64
    priority: 1
    roles: [selfplay, training]
    status: active

  lambda-gh200-k:
    host: lambda-gh200-k
    tailscale_ip: '100.96.142.42'
    ssh_user: ubuntu
    gpu_type: GH200
    gpu_count: 1
    vram_gb: 96
    batch_multiplier: 64
    priority: 2
    roles: [selfplay, gauntlet]
    status: active

  lambda-gh200-l:
    host: lambda-gh200-l
    tailscale_ip: '100.76.145.60'
    ssh_user: ubuntu
    gpu_type: GH200
    gpu_count: 1
    vram_gb: 96
    batch_multiplier: 64
    priority: 2
    roles: [selfplay, gauntlet]
    status: active

  lambda-gh200-m:
    host: ubuntu@100.117.177.83
    tailscale_ip: '100.117.177.83'
    ssh_user: ubuntu
    gpu_type: GH200
    gpu_count: 1
    vram_gb: 96
    batch_multiplier: 64
    priority: 3
    roles: [selfplay]
    status: idle

  lambda-gh200-n:
    host: ubuntu@100.85.106.113
    tailscale_ip: '100.85.106.113'
    ssh_user: ubuntu
    gpu_type: GH200
    gpu_count: 1
    vram_gb: 96
    batch_multiplier: 64
    priority: 3
    roles: [selfplay]
    status: idle

  lambda-gh200-o:
    host: ubuntu@100.97.98.26
    tailscale_ip: '100.97.98.26'
    ssh_user: ubuntu
    gpu_type: GH200
    gpu_count: 1
    vram_gb: 96
    batch_multiplier: 64
    priority: 3
    roles: [selfplay]
    status: idle

  # === H100 Nodes (80GB VRAM) ===
  lambda-h100:
    host: lambda-h100
    ssh_user: ubuntu
    gpu_type: H100
    gpu_count: 1
    vram_gb: 80
    batch_multiplier: 32
    priority: 1
    roles: [selfplay, training, gauntlet]
    status: active

  lambda-2xh100:
    host: lambda-2xh100
    tailscale_ip: '100.97.104.89'
    ssh_user: ubuntu
    gpu_type: H100
    gpu_count: 2
    vram_gb: 160
    batch_multiplier: 32
    priority: 1
    roles: [selfplay, training]
    status: active

  # === A10 Node (24GB VRAM) ===
  lambda-a10:
    host: lambda-a10
    tailscale_ip: '100.91.25.13'
    ssh_user: ubuntu
    gpu_type: A10
    gpu_count: 1
    vram_gb: 24
    batch_multiplier: 8
    priority: 3
    roles: [inference, testing]
    status: active

# Node groups for batch operations
groups:
  primary_training:
    description: 'High-priority nodes for NNUE training'
    nodes: [lambda-gh200-d, lambda-gh200-e, lambda-gh200-f, lambda-gh200-g, lambda-gh200-i]

  selfplay_pool:
    description: 'Nodes for self-play game generation'
    nodes:
      [
        lambda-gh200-d,
        lambda-gh200-e,
        lambda-gh200-f,
        lambda-gh200-g,
        lambda-gh200-h,
        lambda-gh200-i,
        lambda-gh200-k,
        lambda-gh200-l,
        lambda-h100,
        lambda-2xh100,
      ]

  gauntlet_pool:
    description: 'Nodes for running AI gauntlets/benchmarks'
    nodes: [lambda-gh200-h, lambda-gh200-k, lambda-gh200-l, lambda-h100]

  idle_reserve:
    description: 'Idle nodes available for scaling'
    nodes: [lambda-gh200-m, lambda-gh200-n, lambda-gh200-o, lambda-a10]

# Job configuration defaults
job_defaults:
  selfplay:
    games_per_batch: 64
    max_moves: 600 # Square8 theoretical max (600 moves â‰ˆ 120 turns)
    ai_types: [gumbel_mcts, maxn]
    timeout_minutes: 30

  training:
    batch_size: 4096
    learning_rate: 0.001
    checkpoint_interval: 1000
    validation_interval: 500

  gauntlet:
    games_per_matchup: 100
    concurrent_games: 16
    time_control: '10s+0.1s'
