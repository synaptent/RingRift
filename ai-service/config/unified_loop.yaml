# Unified AI Self-Improvement Loop Configuration
# This file configures all aspects of the integrated improvement cycle.
#
# Tuned defaults based on observed cluster performance:
# - Shadow eval at 15min provides rapid feedback with minimal noise
# - Training threshold of 500 games balances quality vs freshness
# - Elo threshold of 25 reduces false positive promotions
# - Curriculum weights capped at 1.5x to avoid over-rotation

version: '1.1'

# Data ingestion from remote hosts
data_ingestion:
  poll_interval_seconds: 60 # Check for new games every 60s
  sync_method: 'incremental' # "incremental" (rsync append) or "full"
  deduplication: true # Deduplicate games by ID across hosts
  min_games_per_sync: 5 # Reduced from 10 - sync smaller batches more frequently
  remote_db_pattern: 'data/games/*.db'
  # Hardening options
  checksum_validation: true # Validate game integrity on sync
  retry_max_attempts: 3 # Max retries with exponential backoff
  retry_base_delay_seconds: 5 # Base delay for exponential backoff
  dead_letter_enabled: true # Enable dead-letter queue for failed syncs

# Automatic training triggers
training:
  trigger_threshold_games: 500 # Reduced from 1000 - more frequent training
  min_interval_seconds: 1200 # 20 min minimum (reduced from 30)
  max_concurrent_jobs: 1 # Only one training job at a time
  prefer_gpu_hosts: true # Schedule training on GPU hosts
  # V3 training scripts (unified pipeline)
  nn_training_script: 'scripts/run_nn_training_baseline.py'
  export_script: 'scripts/export_replay_dataset.py'
  hex_encoder_version: 'v3' # Use HexStateEncoderV3 (16 channels x 4 = 64)
  # Additional options
  warm_start: true # Continue from previous checkpoint if available
  validation_split: 0.1 # Hold out 10% for validation

# Continuous evaluation (shadow + full tournaments)
evaluation:
  # Shadow tournaments: quick evaluation during normal operation
  shadow_interval_seconds: 900 # 15 minutes between shadow evals
  shadow_games_per_config: 15 # Increased from 10 for lower variance

  # Full tournaments: comprehensive evaluation periodically
  full_tournament_interval_seconds: 3600 # 1 hour between full tournaments
  full_tournament_games: 50 # Games per full tournament

  # Baseline models for comparison
  baseline_models:
    - 'random'
    - 'heuristic'
    - 'mcts_100'
    - 'mcts_500'

  # Evaluation quality settings
  min_games_for_elo: 30 # Minimum games before Elo is reliable
  elo_k_factor: 32 # K-factor for Elo updates

# Automatic model promotion
promotion:
  auto_promote: true # Enable automatic promotion
  elo_threshold: 25 # Increased from 20 for more confidence
  min_games: 50 # Minimum games before promotion eligible
  significance_level: 0.05 # Statistical significance requirement
  sync_to_cluster: true # Sync promoted models to all hosts
  # Safety settings
  cooldown_seconds: 1800 # Wait 30min between promotions
  max_promotions_per_day: 10 # Limit promotions to avoid instability
  regression_test: true # Run regression tests before promotion

# Adaptive curriculum (Elo-weighted training)
curriculum:
  adaptive: true # Enable adaptive curriculum
  rebalance_interval_seconds: 3600 # Recompute weights every hour
  max_weight_multiplier: 1.5 # Reduced from 2.0 to avoid over-rotation
  min_weight_multiplier: 0.7 # Increased from 0.5 for better coverage
  # Smoothing to prevent oscillation
  ema_alpha: 0.3 # Exponential moving average for weight updates
  min_games_for_weight: 100 # Min games before adjusting weights

# Host configuration file
hosts_config_path: 'config/remote_hosts.yaml'

# Database paths (relative to ai-service root)
elo_db: 'data/unified_elo.db' # Canonical Elo database for trained models
data_manifest_db: 'data/data_manifest.db'

# Logging
log_dir: 'logs/unified_loop'
verbose: false

# Metrics (Prometheus)
metrics_enabled: true
metrics_port: 9090

# Pipeline orchestration
pipeline:
  parallel_stages: true # Run selfplay concurrent with training
  hot_data_path: true # Use in-memory buffer for recent games
  overlapped_selfplay: true # Start next iteration during training

# Validation settings
validation:
  parallel_workers: 8 # Parallel validation threads
  lightweight_mode: true # Skip full parity for hot path
  full_validation_background: true # Run full validation in background

# Adaptive control
adaptive_control:
  enabled: true
  plateau_threshold: 5 # Stop after N iterations without improvement
  dynamic_games: true # Adjust games based on win rate
  min_games: 50 # Minimum games per evaluation
  max_games: 200 # Maximum games per evaluation

# Regression gate
regression:
  hard_block: true # Block promotion on regression test failure
  test_script: 'scripts/run_regression_tests.py'
  timeout_seconds: 600

# CMA-ES weight propagation
cmaes:
  auto_propagate: true # Auto-inject weights to selfplay
  weights_config_path: 'config/heuristic_weights.json'
  restart_selfplay_on_update: true
  # SAFEGUARD: Limit CMAES workers to prevent process sprawl
  max_parallel_workers: 4 # Max parallel evaluation workers
  evaluation_timeout_seconds: 300 # Kill stuck evaluations
  enabled: false # DISABLED - run manually when needed

# Process safeguards (prevent uncoordinated sprawl)
safeguards:
  # Global process limits per host
  max_python_processes_per_host: 20
  max_selfplay_processes: 2 # Only 2 selfplay at a time
  max_tournament_processes: 1 # Only 1 tournament at a time
  max_training_processes: 1 # Only 1 training at a time

  # Coordination
  single_orchestrator: true # Only ONE unified_ai_loop runs cluster-wide
  orchestrator_host: 'gpu-master' # Master orchestrator host (set to your primary GPU host)
  kill_orphans_on_start: true # Kill orphaned processes on startup

  # Health checks
  process_watchdog: true # Monitor and kill stuck processes
  watchdog_interval_seconds: 60
  max_process_age_hours: 4 # Kill processes older than this

  # Subprocess limits
  max_subprocess_depth: 2 # Prevent infinite subprocess spawning
  subprocess_timeout_seconds: 3600 # 1 hour max for any subprocess

# Board/player configurations to track
configurations:
  - board_type: 'square8'
    num_players: [2, 3, 4]
  - board_type: 'square19'
    num_players: [2, 3, 4]
  - board_type: 'hexagonal'
    num_players: [2, 3, 4]

# External drive sync (Mac Studio only)
external_drive_sync:
  enabled: true
  target_dir: '/Volumes/RingRift-Data/selfplay_repository'
  sync_interval_seconds: 300 # 5 minutes
  sync_models: true
  run_analysis: true
  quarantine_bad_data: true

# Alerting thresholds (for Grafana alerts)
alerting:
  sync_failure_threshold: 5 # Alert after N consecutive sync failures
  training_timeout_hours: 4 # Alert if training exceeds this duration
  elo_drop_threshold: 50 # Alert on significant Elo regression
  games_per_hour_min: 100 # Alert if game collection drops below this

# Cluster orchestration settings (previously hardcoded in cluster_orchestrator.py)
# These are used when running distributed selfplay across multiple hosts.
cluster:
  # Host sync intervals (in iterations, where 1 iteration ~= 5 minutes)
  sync_interval: 6 # Sync data every 6 iterations (30 minutes)
  model_sync_interval: 12 # Sync models every 12 iterations (1 hour)
  model_sync_enabled: true # Enable automatic model syncing

  # Elo calibration
  elo_calibration_interval: 72 # Run Elo tournament every 72 iterations (6 hours)
  elo_calibration_games: 50 # Games per config for calibration

  # Elo-driven curriculum learning
  elo_curriculum_enabled: true # Enable Elo-based opponent selection
  elo_match_window: 200 # Match opponents within this Elo range
  elo_underserved_threshold: 100 # Configs with fewer games are "underserved"

  # Auto-scaling for underutilized hosts
  auto_scale_interval: 12 # Check every 12 iterations (1 hour)
  underutilized_cpu_threshold: 30 # CPU % below this is underutilized
  underutilized_python_jobs: 10 # Fewer jobs than this is underutilized
  scale_up_games_per_host: 50 # Games to start on underutilized host

  # Adaptive game count
  adaptive_games_min: 30
  adaptive_games_max: 150

# SSH execution settings (shared across all orchestrators)
ssh:
  max_retries: 3
  base_delay_seconds: 2.0
  max_delay_seconds: 30.0
  connect_timeout_seconds: 10
  command_timeout_seconds: 3600 # 1 hour max for any command

# Selfplay settings (shared across all selfplay scripts)
selfplay:
  # Game generation
  default_games_per_config: 50
  min_games_for_training: 500
  max_games_per_session: 1000

  # Worker management
  max_concurrent_workers: 4
  worker_timeout_seconds: 7200 # 2 hours max per worker
  checkpoint_interval_games: 100

  # Quality settings
  mcts_simulations: 200
  temperature: 0.5
  noise_fraction: 0.25

# Tournament settings (shared across all tournament scripts)
tournament:
  # Default game counts
  default_games_per_matchup: 20
  shadow_games: 15
  full_tournament_games: 50

  # Time limits
  game_timeout_seconds: 300 # 5 minutes per game
  tournament_timeout_seconds: 7200 # 2 hours max per tournament

  # Elo calculation
  k_factor: 32
  initial_elo: 1500
  min_games_for_rating: 30

  # AI configurations for tournaments
  baseline_models:
    - random
    - heuristic
    - mcts_100
    - mcts_500
