# Unified AI Self-Improvement Loop Configuration
# This file configures all aspects of the integrated improvement cycle.
#
# Tuned defaults based on observed cluster performance:
# - Shadow eval at 15min provides rapid feedback with minimal noise
# - Training threshold of 500 games balances quality vs freshness
# - Elo threshold of 25 reduces false positive promotions
# - Curriculum weights capped at 1.5x to avoid over-rotation

version: '1.1'
execution_backend: 'auto' # auto|local|ssh|p2p|slurm

# Data ingestion from remote hosts
data_ingestion:
  poll_interval_seconds: 30 # Check for new games every 30s (reduced from 60 for faster feedback)
  ephemeral_poll_interval_seconds: 15 # Aggressive sync for RAM disk hosts (Vast.ai)
  sync_method: 'incremental' # "incremental" (rsync append) or "full"
  deduplication: true # Deduplicate games by ID across hosts
  min_games_per_sync: 5 # Reduced from 10 - sync smaller batches more frequently
  remote_db_pattern: 'data/games/*.db'
  # GPU selfplay JSONL sync patterns (in addition to DB files)
  remote_selfplay_patterns:
    - 'data/selfplay/gpu_*/games*.jsonl'
    - 'data/selfplay/p2p_gpu/*/games*.jsonl'
    - 'data/games/gpu_selfplay/*/games*.jsonl'
    # Tournament game data for training
    - 'data/tournaments/*.jsonl'
    - 'data/tournaments/*/*.jsonl'
    - 'data/holdouts/elo_tournaments/*.jsonl'
  # IMPORTANT: Data sync configuration
  # - sync_disabled: Set to true on orchestrator machines with limited disk space
  #   When disabled, this machine won't run unified_data_sync.py (data stays remote)
  # - The unified_ai_loop still reads from data/games/*.db (already consolidated)
  # - For data collection, run unified_data_sync.py on a machine with sufficient storage
  sync_disabled: true # DISABLED on this MacBook (limited disk) - run on mac-studio instead
  # External sync: when true, skip internal data collection and rely on external
  # unified_data_sync.py service (provides P2P fallback, WAL, content dedup)
  # Usage (on a machine with storage): python scripts/unified_data_sync.py --watchdog
  use_external_sync: true # Use external unified_data_sync.py with P2P fallback and WAL
  # Hardening options
  checksum_validation: true # Validate game integrity on sync
  retry_max_attempts: 3 # Max retries with exponential backoff
  retry_base_delay_seconds: 5 # Base delay for exponential backoff
  dead_letter_enabled: true # Enable dead-letter queue for failed syncs
  # Write-ahead log for crash recovery
  wal_enabled: true # Prevent data loss on crash
  wal_db_path: 'data/sync_wal.db' # WAL database path
  # Elo database replication
  elo_replication_enabled: true # Replicate Elo DB to cluster
  elo_replication_interval_seconds: 60 # Replicate every minute

# Automatic training triggers
training:
  trigger_threshold_games: 300 # OPTIMIZED: 300 games for faster feedback loops (was 500)
  min_interval_seconds: 1200 # 20 min minimum (reduced from 30)
  max_concurrent_jobs: 1 # Only one training job at a time
  prefer_gpu_hosts: true # Schedule training on GPU hosts
  # V3 training scripts (unified pipeline)
  nn_training_script: 'scripts/run_nn_training_baseline.py'
  export_script: 'scripts/export_replay_dataset.py'
  hex_encoder_version: 'v4' # Use HexStateEncoderV4 (larger architecture for stronger models)
  # Enhanced training features
  use_knowledge_distill: true # Enable ensemble distillation
  enable_elo_weighting: true # Weight samples by opponent strength
  enable_transfer_learning: true # Enable cross-board transfer
  # Additional options
  warm_start: true # Continue from previous checkpoint if available
  validation_split: 0.1 # Hold out 10% for validation
  # Optimized training settings (Phase 2.5 improvements)
  batch_size: 256 # Higher batch size for better GPU utilization
  sampling_weights: 'victory_type' # Balance across victory types (territory, elimination, etc.)
  warmup_epochs: 5 # LR warmup for training stability
  use_optimized_hyperparams: true # Load board-specific hyperparameters from config/hyperparameters.json

  # Advanced NNUE policy training options (2024-12)
  nnue_policy_script: 'scripts/train_nnue_policy.py'
  nnue_curriculum_script: 'scripts/train_nnue_policy_curriculum.py'
  use_swa: true # Stochastic Weight Averaging for better generalization
  swa_start_fraction: 0.75 # Start SWA at 75% of training
  use_ema: true # Exponential Moving Average for smoother weights
  ema_decay: 0.999 # EMA decay rate
  use_progressive_batch: true # Progressive batch sizing (64 -> 512)
  min_batch_size: 64 # Starting batch size
  max_batch_size: 512 # Maximum batch size
  focal_gamma: 2.0 # Focal loss gamma for hard sample mining
  label_smoothing_warmup: 5 # Warmup epochs for label smoothing
  policy_label_smoothing: 0.05 # Policy label smoothing factor (prevents overconfident predictions)
  use_hex_augmentation: true # D6 symmetry augmentation for hex boards

  # GH200-specific training optimizations (96GB HBM3 + Grace ARM CPU)
  gh200:
    batch_size: 1024 # 4x larger batch for 96GB HBM3
    max_batch_size: 2048 # Progressive can go higher
    gradient_accumulation: 4 # Effective batch size of 4096-8192
    mixed_precision: 'bf16' # BF16 preferred on GH200 (native support)
    num_workers: 16 # More workers for Grace CPU (72 cores)
    pin_memory: true # Pin memory for faster GPU transfer
    prefetch_factor: 4 # Prefetch more batches
    compile_model: true # Use torch.compile for speed
    cudnn_benchmark: true # Optimize cuDNN for fixed input sizes
    memory_efficient_attention: true # Use flash attention if available

  # H100-specific training optimizations (80GB HBM3)
  h100:
    batch_size: 512 # 2x larger batch for 80GB
    max_batch_size: 1024
    gradient_accumulation: 2 # Effective batch size of 1024-2048
    mixed_precision: 'bf16' # BF16 preferred on H100
    num_workers: 8
    pin_memory: true
    compile_model: true
    cudnn_benchmark: true
    memory_efficient_attention: true

  # Default GPU (A10, RTX series)
  default_gpu:
    batch_size: 256
    max_batch_size: 512
    gradient_accumulation: 1
    mixed_precision: 'fp16' # FP16 for older GPUs
    num_workers: 4
    pin_memory: true
    compile_model: false # May have issues on older architectures

# Continuous evaluation (shadow + full tournaments)
evaluation:
  # Shadow tournaments: quick evaluation during normal operation
  # OPTIMIZED: Reduced from 900s to 300s since parallel execution is 3x faster
  shadow_interval_seconds: 300 # 5 minutes between shadow evals
  shadow_games_per_config: 15 # Increased from 10 for lower variance
  # Adaptive interval settings (go faster when cluster is healthy)
  adaptive_interval_enabled: true
  adaptive_interval_min_seconds: 120 # Can go as low as 2 min during fast feedback
  adaptive_interval_max_seconds: 600 # Cap at 10 min during high load

  # Full tournaments: comprehensive evaluation periodically
  full_tournament_interval_seconds: 3600 # 1 hour between full tournaments
  full_tournament_games: 50 # Games per full tournament

  # Baseline models for comparison
  baseline_models:
    - 'random'
    - 'heuristic'
    - 'mcts_100'
    - 'mcts_500'

  # Evaluation quality settings
  min_games_for_elo: 30 # Minimum games before Elo is reliable
  elo_k_factor: 32 # K-factor for Elo updates

# Automatic model promotion
promotion:
  auto_promote: true # Enable automatic promotion
  elo_threshold: 20 # OPTIMIZED: Reduced from 25 for faster exploration with CI safety
  min_games: 40 # OPTIMIZED: Reduced from 50 - Wilson CI provides safety margin
  significance_level: 0.05 # Statistical significance requirement (95% confidence)
  sync_to_cluster: true # Sync promoted models to all hosts
  # Safety settings
  cooldown_seconds: 900 # OPTIMIZED: 15min cooldown (was 30min) for faster iteration
  max_promotions_per_day: 15 # OPTIMIZED: Allow more promotions during fast improvement
  regression_test: true # Run regression tests before promotion

# Adaptive curriculum (Elo-weighted training)
curriculum:
  adaptive: true # Enable adaptive curriculum
  rebalance_interval_seconds: 3600 # Recompute weights every hour
  max_weight_multiplier: 1.5 # Reduced from 2.0 to avoid over-rotation
  min_weight_multiplier: 0.7 # Increased from 0.5 for better coverage
  # Smoothing to prevent oscillation
  ema_alpha: 0.3 # Exponential moving average for weight updates
  min_games_for_weight: 100 # Min games before adjusting weights

# Host configuration file
hosts_config_path: 'config/distributed_hosts.yaml'

# Database paths (relative to ai-service root)
elo_db: 'data/unified_elo.db' # Canonical Elo database for trained models
data_manifest_db: 'data/data_manifest.db'

# Logging
log_dir: 'logs/unified_loop'
verbose: false

# Metrics (Prometheus)
metrics_enabled: true
metrics_port: 9090 # Prometheus server port (app exposes on 9091)

# Safety thresholds (prevent bad models from being promoted)
safety:
  overfit_threshold: 0.15 # Max gap between train/val loss
  min_memory_gb: 64 # Minimum RAM required to run unified loop
  max_consecutive_failures: 3 # Stop after N consecutive failures
  parity_failure_rate_max: 0.10 # Block training if parity failures exceed this
  data_quality_score_min: 0.70 # Minimum data quality to proceed

# Pipeline orchestration
pipeline:
  parallel_stages: true # Run selfplay concurrent with training
  hot_data_path: true # Use in-memory buffer for recent games
  overlapped_selfplay: true # Start next iteration during training

# Validation settings
validation:
  parallel_workers: 8 # Parallel validation threads
  lightweight_mode: true # Skip full parity for hot path
  full_validation_background: true # Run full validation in background

# Adaptive control
adaptive_control:
  enabled: true
  plateau_threshold: 5 # Stop after N iterations without improvement
  dynamic_games: true # Adjust games based on win rate
  min_games: 50 # Minimum games per evaluation
  max_games: 200 # Maximum games per evaluation

# Plateau detection and automatic hyperparameter search
plateau_detection:
  elo_plateau_threshold: 15.0 # Elo gain below this triggers plateau detection
  elo_plateau_lookback: 5 # Number of evaluations to look back
  win_rate_degradation_threshold: 0.40 # Win rate below this triggers retraining
  plateau_count_for_cmaes: 2 # Trigger CMA-ES after this many consecutive plateaus
  plateau_count_for_nas: 4 # Trigger NAS after this many consecutive plateaus

# Population-based training (PBT)
pbt:
  enabled: false # Run manually when needed
  check_interval_seconds: 1800 # Check PBT status every 30 min

# Neural architecture search (NAS)
nas:
  enabled: false # Run manually when needed
  check_interval_seconds: 3600 # Check NAS status every hour

# Prioritized experience replay buffer
replay_buffer:
  priority_alpha: 0.6 # Priority exponent
  importance_beta: 0.4 # Importance sampling exponent
  capacity: 100000 # Max experiences in buffer
  rebuild_interval_seconds: 7200 # Rebuild buffer every 2 hours

# Regression gate
regression:
  hard_block: true # Block promotion on regression test failure
  test_script: 'scripts/run_regression_tests.py'
  timeout_seconds: 600

# CMA-ES weight propagation
cmaes:
  auto_propagate: true # Auto-inject weights to selfplay
  weights_config_path: 'config/heuristic_weights.json'
  restart_selfplay_on_update: true
  # SAFEGUARD: Limit CMAES workers to prevent process sprawl
  max_parallel_workers: 4 # Max parallel evaluation workers
  evaluation_timeout_seconds: 300 # Kill stuck evaluations
  enabled: false # DISABLED - run manually when needed

# Process safeguards (prevent uncoordinated sprawl)
safeguards:
  # Global process limits per host
  max_python_processes_per_host: 64 # Increased for multi-config selfplay
  max_selfplay_processes: 50 # 5 processes x 9 configs + buffer
  max_tournament_processes: 1 # Only 1 tournament at a time
  max_training_processes: 1 # Only 1 training at a time

  # Coordination
  single_orchestrator: true # Only ONE unified_ai_loop runs cluster-wide
  orchestrator_host: 'gpu-master' # Master orchestrator host (set to your primary GPU host)
  kill_orphans_on_start: true # Kill orphaned processes on startup

  # Health checks
  process_watchdog: true # Monitor and kill stuck processes
  watchdog_interval_seconds: 60
  max_process_age_hours: 4 # Kill processes older than this

  # Subprocess limits
  max_subprocess_depth: 2 # Prevent infinite subprocess spawning
  subprocess_timeout_seconds: 3600 # 1 hour max for any subprocess

# Board/player configurations to track
# Each configuration has explicit Elo targets for NN strengthening to 2000+ Elo
configurations:
  - board_type: 'square8'
    num_players: [2, 3, 4]
  - board_type: 'square19'
    num_players: [2, 3, 4]
  - board_type: 'hexagonal'
    num_players: [2, 3, 4]
  - board_type: 'hex8'
    num_players: [2, 3, 4]

# Per-configuration Elo targets for NN strengthening (December 2025)
# Target: Exceed 2000 Elo for all 12 board/player combinations
# Current baseline Elo from production models as of 2025-12-21
elo_targets:
  square8_2p:
    target_elo: 2000
    baseline_elo: 1562 # distilled_sq8_2p_v6
    min_games_for_training: 1000
    status: 'NN_COMPETITIVE'
  square8_3p:
    target_elo: 2000
    baseline_elo: 1450
    min_games_for_training: 200
    status: 'NN_WEAK'
  square8_4p:
    target_elo: 2000
    baseline_elo: 1400
    min_games_for_training: 200
    status: 'NN_WEAK'
  square19_2p:
    target_elo: 2000
    baseline_elo: 1482 # sq19_2p_nn_baseline
    min_games_for_training: 1000
    status: 'NN_WEAK'
  square19_3p:
    target_elo: 2000
    baseline_elo: 1400
    min_games_for_training: 200
    status: 'NOT_OPTIMIZED'
  square19_4p:
    target_elo: 2000
    baseline_elo: 1400
    min_games_for_training: 200
    status: 'NOT_OPTIMIZED'
  hexagonal_2p:
    target_elo: 2000
    baseline_elo: 1500 # hex_2p_nn_baseline
    min_games_for_training: 1000
    status: 'UNBLOCKED' # parity fix 7f43c368
  hexagonal_3p:
    target_elo: 2000
    baseline_elo: 1400
    min_games_for_training: 200
    status: 'NOT_OPTIMIZED'
  hexagonal_4p:
    target_elo: 2000
    baseline_elo: 1400
    min_games_for_training: 200
    status: 'NOT_OPTIMIZED'
  hex8_2p:
    target_elo: 2000
    baseline_elo: 1450
    min_games_for_training: 1000
    status: 'NOT_OPTIMIZED'
  hex8_3p:
    target_elo: 2000
    baseline_elo: 1400
    min_games_for_training: 200
    status: 'NOT_OPTIMIZED'
  hex8_4p:
    target_elo: 2000
    baseline_elo: 1400
    min_games_for_training: 200
    status: 'NOT_OPTIMIZED'

# External drive sync (Mac Studio only)
external_drive_sync:
  enabled: true
  target_dir: '/Volumes/RingRift-Data/selfplay_repository'
  sync_interval_seconds: 300 # 5 minutes
  sync_models: true
  run_analysis: true
  quarantine_bad_data: true

# Alerting thresholds (for Grafana alerts)
alerting:
  sync_failure_threshold: 5 # Alert after N consecutive sync failures
  training_timeout_hours: 4 # Alert if training exceeds this duration
  elo_drop_threshold: 50 # Alert on significant Elo regression
  games_per_hour_min: 100 # Alert if game collection drops below this

# Cluster orchestration settings (previously hardcoded in cluster_orchestrator.py)
# These are used when running distributed selfplay across multiple hosts.
cluster:
  # Target performance
  target_selfplay_games_per_hour: 1000 # Target selfplay rate across cluster

  # Health monitoring
  health_check_interval_seconds: 60 # Seconds between cluster health checks
  sync_interval_seconds: 300 # Seconds between data sync with cluster

  # Host sync intervals (in iterations, where 1 iteration ~= 5 minutes)
  sync_interval: 6 # Sync data every 6 iterations (30 minutes)
  model_sync_interval: 12 # Sync models every 12 iterations (1 hour)
  model_sync_enabled: true # Enable automatic model syncing

  # Elo calibration
  elo_calibration_interval: 72 # Run Elo tournament every 72 iterations (6 hours)
  elo_calibration_games: 50 # Games per config for calibration

  # Elo-driven curriculum learning
  elo_curriculum_enabled: true # Enable Elo-based opponent selection
  elo_match_window: 200 # Match opponents within this Elo range
  elo_underserved_threshold: 100 # Configs with fewer games are "underserved"

  # Auto-scaling for underutilized hosts
  auto_scale_interval: 12 # Check every 12 iterations (1 hour)
  underutilized_cpu_threshold: 60 # CPU % below this is underutilized (aligned with resource_targets.cpu_min)
  underutilized_python_jobs: 10 # Fewer jobs than this is underutilized
  scale_up_games_per_host: 50 # Games to start on underutilized host

  # Adaptive game count
  adaptive_games_min: 30
  adaptive_games_max: 150

# Resource utilization targets (60-80% optimal range for sustained throughput)
# These targets are used by ResourceTargetManager for cluster-wide coordination
resource_targets:
  # CPU utilization targets (%)
  cpu_min: 60 # Below this, scale up jobs
  cpu_target: 70 # Ideal operating point
  cpu_max: 80 # Above this, throttle new jobs
  cpu_critical: 90 # Emergency throttle threshold

  # GPU utilization targets (%)
  gpu_min: 60 # Below this, prioritize GPU work
  gpu_target: 75 # Ideal operating point (slightly higher than CPU)
  gpu_max: 85 # Above this, defer non-critical GPU work
  gpu_critical: 95 # Emergency threshold

  # Memory utilization targets (%)
  memory_min: 40 # Below this, can spawn more workers
  memory_target: 60 # Ideal operating point
  memory_max: 75 # Above this, stop spawning
  memory_critical: 85 # Emergency threshold

  # Job scaling parameters
  jobs_per_core: 0.5 # Base jobs per CPU core
  max_jobs_per_node: 48 # Hard cap per node
  scale_up_increment: 4 # Add jobs in increments
  scale_down_increment: 2 # Remove jobs in increments
  hysteresis_band: 5.0 # % band to prevent oscillation

  # PID controller tuning for utilization targeting
  # These parameters control how aggressively the system adjusts workload
  # to maintain target utilization (60-80% range)
  pid:
    # Proportional gain: How strongly to react to current error
    # Higher = faster response but more oscillation
    # Range: 0.1 - 1.0, Default: 0.3
    kp: 0.3

    # Integral gain: Eliminates steady-state error over time
    # Higher = faster correction of persistent offsets but risk of windup
    # Range: 0.01 - 0.2, Default: 0.05
    ki: 0.05

    # Derivative gain: Dampens oscillation by predicting future error
    # Higher = more damping but amplifies noise
    # Range: 0.0 - 0.3, Default: 0.1
    kd: 0.1

    # Anti-windup integral clamp (prevents integral term runaway)
    integral_clamp: 100.0

    # Minimum time between PID updates (seconds)
    # Lower values = more responsive but noisier
    min_update_interval: 30.0

    # Smoothing factor for output (0 = no smoothing, 1 = full smoothing)
    # Helps reduce sudden job count changes
    output_smoothing: 0.3

    # Enable gain scheduling (adjust gains based on error magnitude)
    gain_scheduling: true
    # When error > threshold, multiply gains by this factor for faster response
    large_error_threshold: 15.0 # % deviation from target
    large_error_gain_multiplier: 1.5 # Increase gains for large errors
    # When error < threshold, reduce gains for stability
    small_error_threshold: 5.0 # % deviation from target
    small_error_gain_multiplier: 0.7 # Reduce gains for small errors

  # Per-tier overrides (HIGH_END, MID_TIER, LOW_TIER, CPU_ONLY)
  tier_overrides:
    HIGH_END:
      cpu_target: 75
      gpu_target: 80
      max_jobs_per_node: 64
    MID_TIER:
      cpu_target: 70
      gpu_target: 75
      max_jobs_per_node: 32
    LOW_TIER:
      cpu_target: 65
      max_jobs_per_node: 16
    CPU_ONLY:
      gpu_min: 0
      gpu_target: 0
      gpu_max: 0
      max_jobs_per_node: 24

# SSH execution settings (shared across all orchestrators)
ssh:
  max_retries: 3
  base_delay_seconds: 2.0
  max_delay_seconds: 30.0
  connect_timeout_seconds: 10
  command_timeout_seconds: 3600 # 1 hour max for any command

# Slurm execution settings (optional HPC backend)
slurm:
  enabled: false
  partition_training: 'gpu-train'
  partition_selfplay: 'gpu-selfplay'
  partition_tournament: 'cpu-eval'
  account: null
  qos: null
  default_time_training: '08:00:00'
  default_time_selfplay: '02:00:00'
  default_time_tournament: '02:00:00'
  gpus_training: 1
  cpus_training: 16
  mem_training: '64G'
  gpus_selfplay: 0
  cpus_selfplay: 8
  mem_selfplay: '16G'
  gpus_tournament: 0
  cpus_tournament: 8
  mem_tournament: '16G'
  job_dir: 'data/slurm/jobs'
  log_dir: 'data/slurm/logs'
  shared_root: null # e.g. "/shared/ringrift"
  repo_subdir: 'ai-service'
  venv_activate: null # e.g. "/shared/ringrift/ai-service/venv/bin/activate"
  setup_commands: []
  extra_sbatch_args: []
  poll_interval_seconds: 20

# Selfplay settings (shared across all selfplay scripts)
selfplay:
  # Game generation
  default_games_per_config: 50
  min_games_for_training: 300 # OPTIMIZED: Aligned with training.trigger_threshold_games
  max_games_per_session: 1000

  # Worker management
  max_concurrent_workers: 4
  worker_timeout_seconds: 7200 # 2 hours max per worker
  checkpoint_interval_games: 100

  # Quality settings
  mcts_simulations: 200
  temperature: 0.7 # Increased from 0.5 for more diverse positions
  noise_fraction: 0.35 # Increased from 0.25 for more exploration

  # Diversity settings for breaking plateaus
  use_diverse_openings: true # Random opening moves for variety
  diverse_opening_moves: 4 # First 4 moves randomized
  use_challenging_positions: true # Focus on tactically complex positions
  min_game_length: 20 # Skip very short games

  # Board type weights (even distribution with hex8)
  # Used for balanced training data across all board types
  board_type_weights:
    square8: 0.25
    square19: 0.25
    hexagonal: 0.25
    hex8: 0.25

  # AI type weights (GPU-accelerated Gumbel MCTS is now fast and primary)
  # Goal: 70%+ Gumbel MCTS for soft policy targets with GPU acceleration (177x speedup)
  ai_type_weights:
    gumbel_mcts: 0.70 # 70% - GPU-accelerated, highest quality search (soft targets)
    policy_only: 0.15 # 15% - Fast NN-only (volume)
    mcts: 0.05 # 5% - Traditional MCTS
    descent: 0.04 # 4% - Gradient search
    heuristic: 0.03 # 3% - Baseline comparison
    nnue_guided: 0.02 # 2% - NNUE evaluation

  # Large board configuration (sq19=361 cells, hexagonal=469 cells)
  # These boards require specialized engine selection for quality selfplay
  large_board_config:
    # Boards classified as "large" - use Gumbel MCTS exclusively
    large_boards:
      - square19
      - hexagonal
      - full_hex
      - fullhex

    # Engine override for large boards (Gumbel MCTS is best for quality+speed)
    engine_override: gumbel_mcts

    # Simulation budget tiers for Gumbel MCTS
    # THROUGHPUT (64) for fast generation, QUALITY (800) for tournaments
    simulation_budget: 64 # THROUGHPUT tier: 5-10 games/sec

    # AI type weights override for large boards (100% Gumbel)
    ai_type_weights:
      gumbel_mcts: 1.00 # 100% Gumbel for best quality on large boards

    # Lower training threshold for large boards (fewer games needed initially)
    min_games_for_training: 200

    # Prioritize data-starved large board configs
    priority: HIGH

  # Board-specific priority overrides (0=CRITICAL, 1=HIGH, 2=MEDIUM, 3=LOW)
  # Used by orchestrators to prioritize selfplay/training for data-starved configs
  # Updated: Dec 2025 based on model coverage assessment
  board_priority_overrides:
    # CRITICAL: No models, critically low data (10-112 games)
    hexagonal_2p: 0
    hexagonal_3p: 0
    hexagonal_4p: 0
    # HIGH: No models, low data (16-126 games)
    square19_3p: 1
    square19_4p: 1
    # MEDIUM: Has model, needs more data
    square8_3p: 2
    square19_2p: 2
    # LOW: Has model, adequate data
    square8_2p: 3
    square8_4p: 3
    hex8_2p: 3
    hex8_3p: 3
    hex8_4p: 3

  # Minimum games required before training can start (per config)
  # Aligned with board_priority_overrides
  min_games_by_config:
    hexagonal_2p: 200
    hexagonal_3p: 200
    hexagonal_4p: 200
    square19_2p: 200
    square19_3p: 200
    square19_4p: 200
    hex8_2p: 300
    hex8_3p: 300
    hex8_4p: 300
    square8_2p: 500
    square8_3p: 300
    square8_4p: 500

  # GPU MCTS selfplay settings (app/training/gpu_mcts_selfplay.py)
  # Produces high-quality soft policy targets from visit distributions
  gpu_mcts:
    enabled: true # Use GPU MCTS for selfplay on CUDA nodes
    simulation_budget: 64 # MCTS simulations per move
    num_sampled_actions: 16 # Gumbel top-K actions
    batch_size: 32 # Games per batch
    max_moves_per_game: 300 # Safety limit
    encoder_version: 'v3' # Feature encoder version
    export_format: 'npz' # Direct NPZ export for training
    output_dir: 'data/training/gpu_mcts' # Output directory

# Tournament settings (shared across all tournament scripts)
tournament:
  # Default game counts
  default_games_per_matchup: 20
  shadow_games: 15
  full_tournament_games: 50

  # Time limits
  game_timeout_seconds: 300 # 5 minutes per game
  tournament_timeout_seconds: 7200 # 2 hours max per tournament

  # Elo calculation
  k_factor: 32
  initial_elo: 1500
  min_games_for_rating: 30

  # AI configurations for tournaments
  baseline_models:
    - random
    - heuristic
    - mcts_100
    - mcts_500

# P2P Distributed Cluster Configuration
# Enables decentralized coordination without central orchestrator
p2p:
  # Master switch for P2P cluster features
  enabled: true # P2P distributed coordination ENABLED

  # P2P Orchestrator settings
  p2p_base_url: 'http://localhost:8770' # URL of local/leader P2P orchestrator
  auth_token: '' # Optional auth token for P2P API

  # Model synchronization across cluster
  model_sync_enabled: true # Sync trained models to all nodes
  model_sync_on_promotion: true # Auto-sync when model is promoted

  # Selfplay coordination
  target_selfplay_games_per_hour: 1000 # Target games/hour across cluster
  auto_scale_selfplay: true # Auto-scale selfplay based on utilization

  # Distributed tournament support
  use_distributed_tournament: true # Run tournaments across multiple nodes
  tournament_nodes_per_eval: 3 # Number of nodes to use per evaluation

  # Health monitoring
  health_check_interval: 30 # Seconds between health checks
  unhealthy_threshold: 3 # Failures before marking node unhealthy

  # Sync intervals
  sync_interval_seconds: 300 # Data sync interval (5 min)

  # Gossip sync integration (P2P data replication)
  gossip_sync_enabled: true # Gossip-based data replication ENABLED
  gossip_port: 8771 # Port for gossip protocol

# P2P Orchestrator Daemon settings (for p2p_orchestrator.py)
p2p_daemon:
  # Discovery
  discovery_method: 'peers' # "broadcast" or "peers"
  known_peers: [] # List of known peer URLs for bootstrap

  # Leader election
  leader_election_timeout: 10 # Seconds to wait for election
  heartbeat_interval: 30 # Seconds between heartbeats

  # Job scheduling
  max_parallel_selfplay: 50 # Allow 5 processes per config (9 configs)
  max_parallel_training: 1 # Max training jobs per node (usually 1)
  prefer_gpu_for_training: true # Schedule training on GPU nodes

  # Multi-config selfplay settings
  selfplay_per_config: 5 # Target processes per board/player config
  ensure_all_configs: true # Ensure all 9 configs have selfplay running

  # Resource thresholds
  cpu_high_threshold: 80 # Throttle above this CPU %
  memory_high_threshold: 80 # Throttle above this memory %
  disk_high_threshold: 90 # Alert above this disk %

  # Auto-restart settings
  auto_restart_failed_jobs: true
  max_job_restarts: 3
  restart_backoff_seconds: 60

# Model pruning - Automated model count management
# When model count exceeds threshold, runs distributed gauntlet evaluation
# and keeps only top quartile performers (archives the rest)
model_pruning:
  enabled: true # Enable automatic model pruning
  threshold: 100 # Trigger pruning when model count exceeds this
  check_interval_seconds: 3600 # Check model count every hour
  top_quartile_keep: 0.25 # Keep top 25% of models
  games_per_baseline: 10 # Games per baseline for evaluation
  parallel_workers: 50 # Parallel evaluation workers
  archive_models: true # Archive pruned models instead of deleting
  # Execution settings
  prefer_high_cpu_hosts: true # Schedule on high-CPU hosts (vast.ai)
  evaluation_timeout_seconds: 7200 # 2 hour timeout for full evaluation
  dry_run: false # If true, log what would be pruned but don't act
  use_elo_based_culling: true # Use fast ELO-based culling (uses existing ratings)

# ============================================================================
# AUTOMATION CONFIGURATION (2024-12)
# These sections enable hands-free cluster operation
# ============================================================================

# Auto-scaling - Provision/deprovision Vast.ai instances based on queue depth
auto_scaling:
  enabled: true
  queue_depth_scale_up: 10 # Scale up if > N pending work items
  queue_depth_scale_down: 2 # Scale down if < N pending items
  gpu_idle_minutes: 15 # Consider node idle after N minutes < 10% util
  min_instances: 2 # Always keep at least N instances
  max_instances: 20 # Cap for cost control
  max_hourly_cost: 2.00 # Budget cap in $/hour
  scale_cooldown_seconds: 600 # 10 min between scale operations
  max_scale_up_per_cycle: 3 # Max instances to add per cycle
  max_scale_down_per_cycle: 2 # Max instances to remove per cycle
  predictive_scaling: true # Enable demand prediction
  prediction_hours_ahead: 2 # Hours ahead to predict

# Queue Populator - Maintain minimum work queue depth until Elo targets met
queue_populator:
  enabled: true
  min_queue_depth: 50 # Always maintain at least 50 work items
  target_elo: 2000.0 # Target Elo for all configurations
  check_interval_seconds: 60 # Check queue depth every minute

  # Work distribution (must sum to 1.0)
  selfplay_ratio: 0.50 # 50% selfplay for data generation
  training_ratio: 0.25 # 25% training for model improvement
  tournament_ratio: 0.25 # 25% tournament for faster Elo feedback (increased from 15%)

  # Board types and player counts to train
  board_types:
    - square8
    - square19
    - hex8
    - hexagonal
  player_counts: [2, 3, 4] # 2p, 3p, 4p for each board

  # Selfplay settings
  selfplay_games_per_item: 50 # Games per selfplay work item
  selfplay_priority: 50 # Lower priority (background)

  # Training settings
  training_priority: 100 # Highest priority
  min_games_for_training: 300 # Min games before training trigger

  # Tournament settings
  tournament_games: 50 # Games per tournament
  tournament_priority: 80 # Medium-high priority

# Self-healing - Auto-recover from failures without manual intervention
self_healing:
  enabled: true
  stuck_job_timeout_multiplier: 1.5 # Kill jobs at N * expected timeout
  max_recovery_attempts_per_node: 3 # Max attempts before escalation
  max_recovery_attempts_per_job: 2 # Max retry per job
  recovery_attempt_cooldown: 300 # 5 min between attempts per node
  consecutive_failures_for_escalation: 3 # Escalate after N failures
  escalation_cooldown: 3600 # 1 hour between escalations
  graduated_rollback_response: true # Use graduated response vs hard limit

# Proactive monitoring - Alert before problems occur
proactive_monitoring:
  enabled: true
  disk_prediction_hours: 4 # Alert N hours before disk full
  disk_critical_threshold: 90.0 # Critical at 90%
  memory_prediction_hours: 2 # Alert N hours before OOM
  memory_critical_threshold: 95.0 # Critical at 95%
  elo_trend_window_hours: 6 # Window for Elo trend analysis
  elo_degradation_threshold: -5.0 # Alert if losing > 5 Elo/hour
  queue_backlog_threshold: 50 # Alert if > 50 pending items
  queue_growth_rate_threshold: 10.0 # Alert if growing > 10/hour
  training_stall_hours: 6 # Alert if no training in N hours
  alert_throttle_minutes: 30 # Min time between same alert
  max_alerts_per_hour: 20 # Rate limit alerts

# Tier calibration - Automated A/B testing for tier thresholds
tier_calibration:
  auto_calibrate: true # Enable automatic calibration
  min_games_for_significance: 200 # Games needed for statistical significance
  significance_threshold: 0.95 # 95% confidence required
  max_concurrent_calibrations: 2 # Max parallel calibration tests
  min_elo_threshold: 800 # Minimum tier threshold
  max_elo_threshold: 2400 # Maximum tier threshold
  max_threshold_change_percent: 10.0 # Max 10% change per calibration
  calibration_timeout_hours: 72 # 3 days max per test
  min_time_between_calibrations_hours: 24 # Min gap between calibrations
  trigger_on_promotion_failure_rate: 0.3 # Trigger if > 30% fail gate
  trigger_on_win_rate_deviation: 0.15 # Trigger if win rate off by > 15%

# Model validation - Auto-validate new models against baselines
model_validation:
  enabled: true
  validation_interval_seconds: 300 # Check for unvalidated models every 5 min
  games_per_baseline: 50 # Games per baseline matchup
  baselines:
    - mcts_500
    - predecessor_model
  validation_timeout_seconds: 7200 # 2 hour timeout per validation
  auto_promote_on_pass: true # Auto-promote if validation passes
  min_elo_improvement: 25 # Required Elo improvement to pass

# ============================================================================
# STORAGE CONFIGURATION
# Provider-specific storage paths for data persistence
# ============================================================================

storage:
  # Default storage (relative paths, used when no provider-specific config)
  default:
    selfplay_games: 'data/selfplay'
    model_checkpoints: 'models/checkpoints'
    training_data: 'data/training'
    elo_database: 'data/unified_elo.db'
    sync_staging: 'data/sync_staging'

  # Lambda Labs - Shared 14PB NFS filesystem
  # All GH200/H100/A10 nodes mount this at /lambda/nfs
  lambda:
    # Shared NFS base path (mounted on all Lambda nodes)
    nfs_base: '/lambda/nfs/RingRift'

    # Selfplay game storage (write directly to NFS - no sync needed)
    selfplay_games: '/lambda/nfs/RingRift/selfplay'

    # Model checkpoints (shared for zero-copy model sync)
    model_checkpoints: '/lambda/nfs/RingRift/models'

    # Training data buffer (shared for distributed training)
    training_data: '/lambda/nfs/RingRift/training_data'

    # Elo database (single source of truth on NFS)
    elo_database: '/lambda/nfs/RingRift/elo/unified_elo.db'

    # Sync staging (not needed when using NFS, but keep for fallback)
    sync_staging: '/lambda/nfs/RingRift/sync_staging'

    # Node-local scratch (fast SSD for temp files)
    local_scratch: '/tmp/ringrift'

    # Use NFS for inter-node data sharing (no rsync needed)
    use_nfs_for_sync: true

    # Skip rsync for nodes with NFS access
    skip_rsync_to_nfs_nodes: true

  # Vast.ai - Ephemeral instances with local storage only
  vast:
    # No NFS - use ephemeral local storage
    selfplay_games: '/workspace/data/selfplay'
    model_checkpoints: '/workspace/models'
    training_data: '/workspace/data/training'
    elo_database: '/workspace/data/unified_elo.db'
    sync_staging: '/workspace/data/sync_staging'

    # RAM disk for ultra-fast temp storage (if available)
    local_scratch: '/dev/shm/ringrift'

    # Vast needs rsync for data persistence
    use_nfs_for_sync: false
    skip_rsync_to_nfs_nodes: false

  # Mac machines - Local storage only
  mac:
    selfplay_games: 'data/selfplay'
    model_checkpoints: 'models/checkpoints'
    training_data: 'data/training'
    elo_database: 'data/unified_elo.db'
    sync_staging: 'data/sync_staging'
    local_scratch: '/tmp/ringrift'
    use_nfs_for_sync: false

# Provider detection rules (used to auto-select storage config)
provider_detection:
  # Lambda: Check for NFS mount
  lambda:
    check_path: '/lambda/nfs'
    hostname_patterns:
      - 'lambda-*'
      - '*.lambdalabs.cloud'

  # Vast: Check for /workspace directory
  vast:
    check_path: '/workspace'
    hostname_patterns:
      - 'vast-*'
      - 'C.*' # Vast container hostnames

  # Mac: Check for /Volumes and Darwin
  mac:
    check_path: '/Volumes'
    os_type: 'Darwin'
