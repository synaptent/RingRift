# Unified AI Self-Improvement Loop Configuration
# This file configures all aspects of the integrated improvement cycle.
#
# Tuned defaults based on observed cluster performance:
# - Shadow eval at 15min provides rapid feedback with minimal noise
# - Training threshold of 500 games balances quality vs freshness
# - Elo threshold of 25 reduces false positive promotions
# - Curriculum weights capped at 1.5x to avoid over-rotation

version: '1.1'

# Data ingestion from remote hosts
data_ingestion:
  poll_interval_seconds: 30 # Check for new games every 30s (reduced from 60 for faster feedback)
  ephemeral_poll_interval_seconds: 15 # Aggressive sync for RAM disk hosts (Vast.ai)
  sync_method: 'incremental' # "incremental" (rsync append) or "full"
  deduplication: true # Deduplicate games by ID across hosts
  min_games_per_sync: 5 # Reduced from 10 - sync smaller batches more frequently
  remote_db_pattern: 'data/games/*.db'
  # IMPORTANT: Data sync configuration
  # - sync_disabled: Set to true on orchestrator machines with limited disk space
  #   When disabled, this machine won't run unified_data_sync.py (data stays remote)
  # - The unified_ai_loop still reads from data/games/*.db (already consolidated)
  # - For data collection, run unified_data_sync.py on a machine with sufficient storage
  sync_disabled: true # DISABLED on this MacBook (limited disk) - run on mac-studio instead
  # External sync: when true, skip internal data collection and rely on external
  # unified_data_sync.py service (provides P2P fallback, WAL, content dedup)
  # Usage (on a machine with storage): python scripts/unified_data_sync.py --watchdog
  use_external_sync: true # Use external unified_data_sync.py with P2P fallback and WAL
  # Hardening options
  checksum_validation: true # Validate game integrity on sync
  retry_max_attempts: 3 # Max retries with exponential backoff
  retry_base_delay_seconds: 5 # Base delay for exponential backoff
  dead_letter_enabled: true # Enable dead-letter queue for failed syncs
  # Write-ahead log for crash recovery
  wal_enabled: true # Prevent data loss on crash
  wal_db_path: 'data/sync_wal.db' # WAL database path
  # Elo database replication
  elo_replication_enabled: true # Replicate Elo DB to cluster
  elo_replication_interval_seconds: 60 # Replicate every minute

# Automatic training triggers
training:
  trigger_threshold_games: 300 # OPTIMIZED: 300 games for faster feedback loops (was 500)
  min_interval_seconds: 1200 # 20 min minimum (reduced from 30)
  max_concurrent_jobs: 1 # Only one training job at a time
  prefer_gpu_hosts: true # Schedule training on GPU hosts
  # V3 training scripts (unified pipeline)
  nn_training_script: 'scripts/run_nn_training_baseline.py'
  export_script: 'scripts/export_replay_dataset.py'
  hex_encoder_version: 'v3' # Use HexStateEncoderV3 (16 channels x 4 = 64)
  # Additional options
  warm_start: true # Continue from previous checkpoint if available
  validation_split: 0.1 # Hold out 10% for validation
  # Optimized training settings (Phase 2.5 improvements)
  batch_size: 256 # Higher batch size for better GPU utilization
  sampling_weights: 'victory_type' # Balance across victory types (territory, elimination, etc.)
  warmup_epochs: 5 # LR warmup for training stability
  use_optimized_hyperparams: true # Load board-specific hyperparameters from config/hyperparameters.json

  # Advanced NNUE policy training options (2024-12)
  nnue_policy_script: 'scripts/train_nnue_policy.py'
  nnue_curriculum_script: 'scripts/train_nnue_policy_curriculum.py'
  use_swa: true # Stochastic Weight Averaging for better generalization
  swa_start_fraction: 0.75 # Start SWA at 75% of training
  use_ema: true # Exponential Moving Average for smoother weights
  ema_decay: 0.999 # EMA decay rate
  use_progressive_batch: true # Progressive batch sizing (64 -> 512)
  min_batch_size: 64 # Starting batch size
  max_batch_size: 512 # Maximum batch size
  focal_gamma: 2.0 # Focal loss gamma for hard sample mining
  label_smoothing_warmup: 5 # Warmup epochs for label smoothing
  policy_label_smoothing: 0.05 # Policy label smoothing factor (prevents overconfident predictions)
  use_hex_augmentation: true # D6 symmetry augmentation for hex boards

# Continuous evaluation (shadow + full tournaments)
evaluation:
  # Shadow tournaments: quick evaluation during normal operation
  # OPTIMIZED: Reduced from 900s to 300s since parallel execution is 3x faster
  shadow_interval_seconds: 300 # 5 minutes between shadow evals
  shadow_games_per_config: 15 # Increased from 10 for lower variance
  # Adaptive interval settings (go faster when cluster is healthy)
  adaptive_interval_enabled: true
  adaptive_interval_min_seconds: 120 # Can go as low as 2 min during fast feedback
  adaptive_interval_max_seconds: 600 # Cap at 10 min during high load

  # Full tournaments: comprehensive evaluation periodically
  full_tournament_interval_seconds: 3600 # 1 hour between full tournaments
  full_tournament_games: 50 # Games per full tournament

  # Baseline models for comparison
  baseline_models:
    - 'random'
    - 'heuristic'
    - 'mcts_100'
    - 'mcts_500'

  # Evaluation quality settings
  min_games_for_elo: 30 # Minimum games before Elo is reliable
  elo_k_factor: 32 # K-factor for Elo updates

# Automatic model promotion
promotion:
  auto_promote: true # Enable automatic promotion
  elo_threshold: 20 # OPTIMIZED: Reduced from 25 for faster exploration with CI safety
  min_games: 40 # OPTIMIZED: Reduced from 50 - Wilson CI provides safety margin
  significance_level: 0.05 # Statistical significance requirement (95% confidence)
  sync_to_cluster: true # Sync promoted models to all hosts
  # Safety settings
  cooldown_seconds: 900 # OPTIMIZED: 15min cooldown (was 30min) for faster iteration
  max_promotions_per_day: 15 # OPTIMIZED: Allow more promotions during fast improvement
  regression_test: true # Run regression tests before promotion

# Adaptive curriculum (Elo-weighted training)
curriculum:
  adaptive: true # Enable adaptive curriculum
  rebalance_interval_seconds: 3600 # Recompute weights every hour
  max_weight_multiplier: 1.5 # Reduced from 2.0 to avoid over-rotation
  min_weight_multiplier: 0.7 # Increased from 0.5 for better coverage
  # Smoothing to prevent oscillation
  ema_alpha: 0.3 # Exponential moving average for weight updates
  min_games_for_weight: 100 # Min games before adjusting weights

# Host configuration file
hosts_config_path: 'config/remote_hosts.yaml'

# Database paths (relative to ai-service root)
elo_db: 'data/unified_elo.db' # Canonical Elo database for trained models
data_manifest_db: 'data/data_manifest.db'

# Logging
log_dir: 'logs/unified_loop'
verbose: false

# Metrics (Prometheus)
metrics_enabled: true
metrics_port: 9090 # Prometheus server port (app exposes on 9091)

# Safety thresholds (prevent bad models from being promoted)
safety:
  overfit_threshold: 0.15 # Max gap between train/val loss
  min_memory_gb: 64 # Minimum RAM required to run unified loop
  max_consecutive_failures: 3 # Stop after N consecutive failures
  parity_failure_rate_max: 0.10 # Block training if parity failures exceed this
  data_quality_score_min: 0.70 # Minimum data quality to proceed

# Pipeline orchestration
pipeline:
  parallel_stages: true # Run selfplay concurrent with training
  hot_data_path: true # Use in-memory buffer for recent games
  overlapped_selfplay: true # Start next iteration during training

# Validation settings
validation:
  parallel_workers: 8 # Parallel validation threads
  lightweight_mode: true # Skip full parity for hot path
  full_validation_background: true # Run full validation in background

# Adaptive control
adaptive_control:
  enabled: true
  plateau_threshold: 5 # Stop after N iterations without improvement
  dynamic_games: true # Adjust games based on win rate
  min_games: 50 # Minimum games per evaluation
  max_games: 200 # Maximum games per evaluation

# Plateau detection and automatic hyperparameter search
plateau_detection:
  elo_plateau_threshold: 15.0 # Elo gain below this triggers plateau detection
  elo_plateau_lookback: 5 # Number of evaluations to look back
  win_rate_degradation_threshold: 0.40 # Win rate below this triggers retraining
  plateau_count_for_cmaes: 2 # Trigger CMA-ES after this many consecutive plateaus
  plateau_count_for_nas: 4 # Trigger NAS after this many consecutive plateaus

# Population-based training (PBT)
pbt:
  enabled: false # Run manually when needed
  check_interval_seconds: 1800 # Check PBT status every 30 min

# Neural architecture search (NAS)
nas:
  enabled: false # Run manually when needed
  check_interval_seconds: 3600 # Check NAS status every hour

# Prioritized experience replay buffer
replay_buffer:
  priority_alpha: 0.6 # Priority exponent
  importance_beta: 0.4 # Importance sampling exponent
  capacity: 100000 # Max experiences in buffer
  rebuild_interval_seconds: 7200 # Rebuild buffer every 2 hours

# Regression gate
regression:
  hard_block: true # Block promotion on regression test failure
  test_script: 'scripts/run_regression_tests.py'
  timeout_seconds: 600

# CMA-ES weight propagation
cmaes:
  auto_propagate: true # Auto-inject weights to selfplay
  weights_config_path: 'config/heuristic_weights.json'
  restart_selfplay_on_update: true
  # SAFEGUARD: Limit CMAES workers to prevent process sprawl
  max_parallel_workers: 4 # Max parallel evaluation workers
  evaluation_timeout_seconds: 300 # Kill stuck evaluations
  enabled: false # DISABLED - run manually when needed

# Process safeguards (prevent uncoordinated sprawl)
safeguards:
  # Global process limits per host
  max_python_processes_per_host: 64 # Increased for multi-config selfplay
  max_selfplay_processes: 50 # 5 processes x 9 configs + buffer
  max_tournament_processes: 1 # Only 1 tournament at a time
  max_training_processes: 1 # Only 1 training at a time

  # Coordination
  single_orchestrator: true # Only ONE unified_ai_loop runs cluster-wide
  orchestrator_host: 'gpu-master' # Master orchestrator host (set to your primary GPU host)
  kill_orphans_on_start: true # Kill orphaned processes on startup

  # Health checks
  process_watchdog: true # Monitor and kill stuck processes
  watchdog_interval_seconds: 60
  max_process_age_hours: 4 # Kill processes older than this

  # Subprocess limits
  max_subprocess_depth: 2 # Prevent infinite subprocess spawning
  subprocess_timeout_seconds: 3600 # 1 hour max for any subprocess

# Board/player configurations to track
configurations:
  - board_type: 'square8'
    num_players: [2, 3, 4]
  - board_type: 'square19'
    num_players: [2, 3, 4]
  - board_type: 'hexagonal'
    num_players: [2, 3, 4]
  - board_type: 'hex8'
    num_players: [2, 3, 4]

# External drive sync (Mac Studio only)
external_drive_sync:
  enabled: true
  target_dir: '/Volumes/RingRift-Data/selfplay_repository'
  sync_interval_seconds: 300 # 5 minutes
  sync_models: true
  run_analysis: true
  quarantine_bad_data: true

# Alerting thresholds (for Grafana alerts)
alerting:
  sync_failure_threshold: 5 # Alert after N consecutive sync failures
  training_timeout_hours: 4 # Alert if training exceeds this duration
  elo_drop_threshold: 50 # Alert on significant Elo regression
  games_per_hour_min: 100 # Alert if game collection drops below this

# Cluster orchestration settings (previously hardcoded in cluster_orchestrator.py)
# These are used when running distributed selfplay across multiple hosts.
cluster:
  # Target performance
  target_selfplay_games_per_hour: 1000 # Target selfplay rate across cluster

  # Health monitoring
  health_check_interval_seconds: 60 # Seconds between cluster health checks
  sync_interval_seconds: 300 # Seconds between data sync with cluster

  # Host sync intervals (in iterations, where 1 iteration ~= 5 minutes)
  sync_interval: 6 # Sync data every 6 iterations (30 minutes)
  model_sync_interval: 12 # Sync models every 12 iterations (1 hour)
  model_sync_enabled: true # Enable automatic model syncing

  # Elo calibration
  elo_calibration_interval: 72 # Run Elo tournament every 72 iterations (6 hours)
  elo_calibration_games: 50 # Games per config for calibration

  # Elo-driven curriculum learning
  elo_curriculum_enabled: true # Enable Elo-based opponent selection
  elo_match_window: 200 # Match opponents within this Elo range
  elo_underserved_threshold: 100 # Configs with fewer games are "underserved"

  # Auto-scaling for underutilized hosts
  auto_scale_interval: 12 # Check every 12 iterations (1 hour)
  underutilized_cpu_threshold: 60 # CPU % below this is underutilized (aligned with resource_targets.cpu_min)
  underutilized_python_jobs: 10 # Fewer jobs than this is underutilized
  scale_up_games_per_host: 50 # Games to start on underutilized host

  # Adaptive game count
  adaptive_games_min: 30
  adaptive_games_max: 150

# Resource utilization targets (60-80% optimal range for sustained throughput)
# These targets are used by ResourceTargetManager for cluster-wide coordination
resource_targets:
  # CPU utilization targets (%)
  cpu_min: 60 # Below this, scale up jobs
  cpu_target: 70 # Ideal operating point
  cpu_max: 80 # Above this, throttle new jobs
  cpu_critical: 90 # Emergency throttle threshold

  # GPU utilization targets (%)
  gpu_min: 60 # Below this, prioritize GPU work
  gpu_target: 75 # Ideal operating point (slightly higher than CPU)
  gpu_max: 85 # Above this, defer non-critical GPU work
  gpu_critical: 95 # Emergency threshold

  # Memory utilization targets (%)
  memory_min: 40 # Below this, can spawn more workers
  memory_target: 60 # Ideal operating point
  memory_max: 75 # Above this, stop spawning
  memory_critical: 85 # Emergency threshold

  # Job scaling parameters
  jobs_per_core: 0.5 # Base jobs per CPU core
  max_jobs_per_node: 48 # Hard cap per node
  scale_up_increment: 4 # Add jobs in increments
  scale_down_increment: 2 # Remove jobs in increments
  hysteresis_band: 5.0 # % band to prevent oscillation

  # PID controller tuning for utilization targeting
  # These parameters control how aggressively the system adjusts workload
  # to maintain target utilization (60-80% range)
  pid:
    # Proportional gain: How strongly to react to current error
    # Higher = faster response but more oscillation
    # Range: 0.1 - 1.0, Default: 0.3
    kp: 0.3

    # Integral gain: Eliminates steady-state error over time
    # Higher = faster correction of persistent offsets but risk of windup
    # Range: 0.01 - 0.2, Default: 0.05
    ki: 0.05

    # Derivative gain: Dampens oscillation by predicting future error
    # Higher = more damping but amplifies noise
    # Range: 0.0 - 0.3, Default: 0.1
    kd: 0.1

    # Anti-windup integral clamp (prevents integral term runaway)
    integral_clamp: 100.0

    # Minimum time between PID updates (seconds)
    # Lower values = more responsive but noisier
    min_update_interval: 30.0

    # Smoothing factor for output (0 = no smoothing, 1 = full smoothing)
    # Helps reduce sudden job count changes
    output_smoothing: 0.3

    # Enable gain scheduling (adjust gains based on error magnitude)
    gain_scheduling: true
    # When error > threshold, multiply gains by this factor for faster response
    large_error_threshold: 15.0 # % deviation from target
    large_error_gain_multiplier: 1.5 # Increase gains for large errors
    # When error < threshold, reduce gains for stability
    small_error_threshold: 5.0 # % deviation from target
    small_error_gain_multiplier: 0.7 # Reduce gains for small errors

  # Per-tier overrides (HIGH_END, MID_TIER, LOW_TIER, CPU_ONLY)
  tier_overrides:
    HIGH_END:
      cpu_target: 75
      gpu_target: 80
      max_jobs_per_node: 64
    MID_TIER:
      cpu_target: 70
      gpu_target: 75
      max_jobs_per_node: 32
    LOW_TIER:
      cpu_target: 65
      max_jobs_per_node: 16
    CPU_ONLY:
      gpu_min: 0
      gpu_target: 0
      gpu_max: 0
      max_jobs_per_node: 24

# SSH execution settings (shared across all orchestrators)
ssh:
  max_retries: 3
  base_delay_seconds: 2.0
  max_delay_seconds: 30.0
  connect_timeout_seconds: 10
  command_timeout_seconds: 3600 # 1 hour max for any command

# Selfplay settings (shared across all selfplay scripts)
selfplay:
  # Game generation
  default_games_per_config: 50
  min_games_for_training: 300 # OPTIMIZED: Aligned with training.trigger_threshold_games
  max_games_per_session: 1000

  # Worker management
  max_concurrent_workers: 4
  worker_timeout_seconds: 7200 # 2 hours max per worker
  checkpoint_interval_games: 100

  # Quality settings
  mcts_simulations: 200
  temperature: 0.5
  noise_fraction: 0.25

  # Board type weights (even distribution with hex8)
  # Used for balanced training data across all board types
  board_type_weights:
    square8: 0.25
    square19: 0.25
    hexagonal: 0.25
    hex8: 0.25

  # AI type weights (prioritize NN-guided for higher quality games)
  # Goal: 70%+ NN-guided games for better training signal with soft policy targets
  ai_type_weights:
    gumbel_mcts: 0.40 # 40% - Highest quality search with visit distribution (soft targets)
    policy_only: 0.30 # 30% - Fast NN-only (volume)
    nnue_guided: 0.10 # 10% - NNUE evaluation
    mcts: 0.08 # 8% - Traditional MCTS
    descent: 0.05 # 5% - Gradient search
    heuristic: 0.04 # 4% - Baseline comparison
    minimax: 0.02 # 2% - Paranoid search
    random: 0.01 # 1% - Weak baseline (minimal)

# Tournament settings (shared across all tournament scripts)
tournament:
  # Default game counts
  default_games_per_matchup: 20
  shadow_games: 15
  full_tournament_games: 50

  # Time limits
  game_timeout_seconds: 300 # 5 minutes per game
  tournament_timeout_seconds: 7200 # 2 hours max per tournament

  # Elo calculation
  k_factor: 32
  initial_elo: 1500
  min_games_for_rating: 30

  # AI configurations for tournaments
  baseline_models:
    - random
    - heuristic
    - mcts_100
    - mcts_500

# P2P Distributed Cluster Configuration
# Enables decentralized coordination without central orchestrator
p2p:
  # Master switch for P2P cluster features
  enabled: true # P2P distributed coordination ENABLED

  # P2P Orchestrator settings
  p2p_base_url: 'http://localhost:8770' # URL of local/leader P2P orchestrator
  auth_token: '' # Optional auth token for P2P API

  # Model synchronization across cluster
  model_sync_enabled: true # Sync trained models to all nodes
  model_sync_on_promotion: true # Auto-sync when model is promoted

  # Selfplay coordination
  target_selfplay_games_per_hour: 1000 # Target games/hour across cluster
  auto_scale_selfplay: true # Auto-scale selfplay based on utilization

  # Distributed tournament support
  use_distributed_tournament: true # Run tournaments across multiple nodes
  tournament_nodes_per_eval: 3 # Number of nodes to use per evaluation

  # Health monitoring
  health_check_interval: 30 # Seconds between health checks
  unhealthy_threshold: 3 # Failures before marking node unhealthy

  # Sync intervals
  sync_interval_seconds: 300 # Data sync interval (5 min)

  # Gossip sync integration (P2P data replication)
  gossip_sync_enabled: true # Gossip-based data replication ENABLED
  gossip_port: 8771 # Port for gossip protocol

# P2P Orchestrator Daemon settings (for p2p_orchestrator.py)
p2p_daemon:
  # Discovery
  discovery_method: 'peers' # "broadcast" or "peers"
  known_peers: [] # List of known peer URLs for bootstrap

  # Leader election
  leader_election_timeout: 10 # Seconds to wait for election
  heartbeat_interval: 30 # Seconds between heartbeats

  # Job scheduling
  max_parallel_selfplay: 50 # Allow 5 processes per config (9 configs)
  max_parallel_training: 1 # Max training jobs per node (usually 1)
  prefer_gpu_for_training: true # Schedule training on GPU nodes

  # Multi-config selfplay settings
  selfplay_per_config: 5 # Target processes per board/player config
  ensure_all_configs: true # Ensure all 9 configs have selfplay running

  # Resource thresholds
  cpu_high_threshold: 80 # Throttle above this CPU %
  memory_high_threshold: 80 # Throttle above this memory %
  disk_high_threshold: 90 # Alert above this disk %

  # Auto-restart settings
  auto_restart_failed_jobs: true
  max_job_restarts: 3
  restart_backoff_seconds: 60

# Model pruning - Automated model count management
# When model count exceeds threshold, runs distributed gauntlet evaluation
# and keeps only top quartile performers (archives the rest)
model_pruning:
  enabled: true # Enable automatic model pruning
  threshold: 100 # Trigger pruning when model count exceeds this
  check_interval_seconds: 3600 # Check model count every hour
  top_quartile_keep: 0.25 # Keep top 25% of models
  games_per_baseline: 10 # Games per baseline for evaluation
  parallel_workers: 50 # Parallel evaluation workers
  archive_models: true # Archive pruned models instead of deleting
  # Execution settings
  prefer_high_cpu_hosts: true # Schedule on high-CPU hosts (vast.ai)
  evaluation_timeout_seconds: 7200 # 2 hour timeout for full evaluation
  dry_run: false # If true, log what would be pruned but don't act
  use_elo_based_culling: true # Use fast ELO-based culling (uses existing ratings)
