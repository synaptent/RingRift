"""Gumbel MCTS AI implementation for RingRift.

This module implements Gumbel AlphaZero-style MCTS with Sequential Halving
for more sample-efficient search compared to standard MCTS.

Key innovations from the Gumbel AlphaZero paper:
1. **Gumbel-Top-K sampling**: Sample k actions without replacement at the root
   using Gumbel noise added to policy logits. This focuses computation on the
   most promising actions while maintaining exploration.

2. **Sequential Halving**: Divide the simulation budget across log2(k) phases,
   progressively halving the number of candidate actions. This is more
   efficient than uniform allocation across all actions.

3. **Completed Q-values**: Use a principled estimate of action values that
   accounts for visit count asymmetry between actions.

GPU Acceleration (default enabled):
- Batches all neural network evaluations across simulation phases for 5-50x speedup
- Instead of evaluating one state per simulation, collects all states and runs batch NN inference
- Automatic fallback to sequential evaluation if no GPU available
- Control via RINGRIFT_GPU_GUMBEL_DISABLE=1 environment variable
- Shadow validation available via RINGRIFT_GPU_GUMBEL_SHADOW_VALIDATE=1

References:
- Danihelka et al. "Policy improvement by planning with Gumbel" (2022)
- https://arxiv.org/abs/2104.06303
"""

from __future__ import annotations

import logging
import math
import os
from dataclasses import dataclass, field
from typing import TYPE_CHECKING, Any

import numpy as np

from ..models import AIConfig, BoardType, GameState, Move
from ..rules.mutable_state import MoveUndo, MutableGameState
from .base import BaseAI
from .heuristic_ai import HeuristicAI
from .neural_net import INVALID_MOVE_INDEX, NeuralNetAI

if TYPE_CHECKING:
    import torch

logger = logging.getLogger(__name__)

# Environment variable controls for GPU acceleration
_GPU_GUMBEL_DISABLE = os.environ.get("RINGRIFT_GPU_GUMBEL_DISABLE", "").lower() in (
    "1", "true", "yes", "on"
)
_GPU_GUMBEL_SHADOW_VALIDATE = os.environ.get("RINGRIFT_GPU_GUMBEL_SHADOW_VALIDATE", "").lower() in (
    "1", "true", "yes", "on"
)

# Normalization scale for heuristic scores to [-1, 1]
# HeuristicAI returns scores in range [-100000, 100000] for terminal states
# and typically [-1000, 1000] for non-terminal positions
_HEURISTIC_NORMALIZATION_SCALE = 1000.0


@dataclass
class GumbelAction:
    """Represents an action with its Gumbel-perturbed value and statistics."""

    move: Move
    policy_logit: float  # Raw log-probability from NN
    gumbel_noise: float  # Gumbel(0,1) noise sample
    perturbed_value: float  # logit + gumbel (for initial ranking)
    visit_count: int = 0
    total_value: float = 0.0  # Sum of values from simulations

    @property
    def mean_value(self) -> float:
        """Mean value from simulations (Q-value)."""
        if self.visit_count == 0:
            return 0.0
        return self.total_value / self.visit_count

    def completed_q(self, max_visits: int, c_visit: float = 50.0) -> float:
        """Compute the completed Q-value for action selection.

        This accounts for visit count asymmetry by mixing the empirical
        Q-value with a prior-based completion.

        Args:
            max_visits: Maximum visit count among all actions.
            c_visit: Mixing coefficient for visit completion.

        Returns:
            Completed Q-value estimate.
        """
        if self.visit_count == 0:
            # Use prior value (normalized policy logit as proxy)
            return self.policy_logit / 10.0  # Scale to reasonable range

        # Mix empirical Q with prior based on visit ratio
        self.visit_count / (max_visits + 1e-8)
        mix = c_visit / (c_visit + max_visits)
        return (1 - mix) * self.mean_value + mix * (self.policy_logit / 10.0)


@dataclass
class GumbelNode:
    """Lightweight node for Gumbel MCTS tree traversal."""

    move: Move | None = None
    parent: GumbelNode | None = None
    children: dict[str, GumbelNode] = field(default_factory=dict)
    visit_count: int = 0
    total_value: float = 0.0
    prior: float = 0.0
    to_move_is_root: bool = True

    @property
    def mean_value(self) -> float:
        """Mean value (Q-value) for this node."""
        if self.visit_count == 0:
            return 0.0
        return self.total_value / self.visit_count


class GumbelMCTSAI(BaseAI):
    """Gumbel MCTS AI with Sequential Halving for sample-efficient search.

    This AI combines neural network policy/value evaluation with Gumbel-based
    action sampling and Sequential Halving for budget allocation.

    Compared to standard MCTS:
    - More sample-efficient (better use of limited simulations)
    - Focuses computation on promising actions
    - Better theoretical guarantees for action selection

    GPU Acceleration:
        When GPU is available, batches all neural network evaluations within
        each Sequential Halving phase for 5-50x speedup. The GPU path provides:

        - **Rules parity**: Game states are generated using the canonical rules
          engine (MutableGameState.from_immutable, make_move) - identical to CPU.

        - **Evaluation parity**: The same neural network evaluate_batch() is
          called with batched states vs sequential states. No approximations.

        - **Move selection parity**: Shadow validation (5% sample) confirms
          batch and sequential paths produce identical NN outputs.

        Control via environment variables:
        - RINGRIFT_GPU_GUMBEL_DISABLE=1: Force sequential (CPU) evaluation
        - RINGRIFT_GPU_GUMBEL_SHADOW_VALIDATE=1: Enable parity validation

    Attributes:
        neural_net: Neural network for policy/value evaluation.
        num_sampled_actions: Number of actions for Gumbel-Top-K (m).
        simulation_budget: Total simulation budget (n).
        c_puct: Exploration constant for tree traversal.
    """

    def __init__(
        self,
        player_number: int,
        config: AIConfig,
        board_type: BoardType = BoardType.SQUARE8,
    ) -> None:
        """Initialize Gumbel MCTS AI.

        Args:
            player_number: The player number this AI controls.
            config: AI configuration including nn_model_id and gumbel parameters.
            board_type: Board type for move encoding.

        Raises:
            RuntimeError: If neural network cannot be loaded.
        """
        super().__init__(player_number, config)

        self.board_type = board_type

        # Gumbel MCTS parameters
        self.num_sampled_actions = config.gumbel_num_sampled_actions or 16
        self.simulation_budget = config.gumbel_simulation_budget or 150
        self.c_puct = 1.5  # Exploration constant for tree policy

        # Store search results for training data extraction
        self._last_search_actions: list[GumbelAction] | None = None

        # GPU acceleration state (lazy initialized)
        self._gpu_batch_enabled: bool = not _GPU_GUMBEL_DISABLE
        self._gpu_available: bool | None = None  # None = not yet checked
        self._gpu_device: torch.device | None = None

        # Shadow validation for GPU batch vs sequential parity checking
        self._shadow_validate: bool = _GPU_GUMBEL_SHADOW_VALIDATE
        self._shadow_divergence_count: int = 0
        self._shadow_total_checks: int = 0

        # Batch size for GPU evaluation (tune based on GPU memory)
        self._gpu_batch_size: int = getattr(config, 'gpu_batch_size', 128)

        # Hybrid NN + Heuristic evaluation configuration (RR-CANON-HYBRID-001)
        self._heuristic_blend_alpha: float | None = getattr(
            config, 'heuristic_blend_alpha', None
        )
        self._heuristic_fallback_enabled: bool = getattr(
            config, 'heuristic_fallback_enabled', True
        )
        # Lazy-initialized heuristic evaluator (only created when needed)
        self._heuristic_ai: HeuristicAI | None = None

        # Load neural network (required for Gumbel MCTS)
        self.neural_net: NeuralNetAI | None = None
        try:
            self.neural_net = NeuralNetAI(player_number, config)
            logger.info(
                f"GumbelMCTSAI(player={player_number}): loaded neural network "
                f"(model={config.nn_model_id}, m={self.num_sampled_actions}, "
                f"budget={self.simulation_budget}, gpu_batch={self._gpu_batch_enabled})"
            )
        except Exception as e:
            if not config.allow_fresh_weights:
                raise RuntimeError(
                    f"GumbelMCTSAI requires a neural network but failed to load: {e}"
                ) from e
            logger.warning(f"GumbelMCTSAI: failed to load neural net ({e})")
            self.neural_net = None

    def _ensure_heuristic_evaluator(self) -> HeuristicAI | None:
        """Lazily initialize heuristic evaluator if needed.

        Returns:
            HeuristicAI instance if hybrid evaluation or fallback is enabled,
            None otherwise.
        """
        if self._heuristic_ai is not None:
            return self._heuristic_ai

        # Only create if blending or fallback is enabled
        if self._heuristic_blend_alpha is not None or self._heuristic_fallback_enabled:
            try:
                # Use light mode for faster heuristic evaluation in hybrid mode
                from copy import copy
                heuristic_config = copy(self.config)
                heuristic_config.heuristic_eval_mode = "light"
                self._heuristic_ai = HeuristicAI(self.player_number, heuristic_config)
                logger.debug(
                    f"GumbelMCTSAI: initialized heuristic evaluator for hybrid eval "
                    f"(blend_alpha={self._heuristic_blend_alpha})"
                )
            except Exception as e:
                logger.warning(f"GumbelMCTSAI: failed to create heuristic evaluator: {e}")
                return None

        return self._heuristic_ai

    def _normalize_heuristic_score(self, raw_score: float) -> float:
        """Normalize heuristic score to [-1, 1] range using tanh.

        Args:
            raw_score: Raw heuristic score (typically in range [-1000, 1000],
                      or Â±100000 for terminal states).

        Returns:
            Normalized score in [-1, 1] range.
        """
        return math.tanh(raw_score / _HEURISTIC_NORMALIZATION_SCALE)

    def _evaluate_leaf_hybrid(
        self,
        mstate: MutableGameState,
        is_opponent_perspective: bool,
    ) -> float:
        """Evaluate a leaf node using hybrid NN + heuristic blending.

        When heuristic_blend_alpha is set, combines NN value with normalized
        heuristic score. Otherwise uses pure NN with optional heuristic fallback.

        Args:
            mstate: Mutable game state at the leaf node.
            is_opponent_perspective: True if state's current player is opponent.

        Returns:
            Value estimate in [-1, 1] from the root player's perspective.
        """
        sim_state = mstate.to_immutable()

        # Try NN evaluation first
        nn_value: float | None = None
        if self.neural_net is not None:
            try:
                values, _ = self.neural_net.evaluate_batch([sim_state])
                if values:
                    nn_value = float(values[0])
            except Exception as e:
                logger.debug(f"GumbelMCTSAI: NN evaluation failed: {e}")

        # Compute heuristic value if needed for blending or fallback
        heuristic_value: float | None = None
        need_heuristic = (
            self._heuristic_blend_alpha is not None or
            (nn_value is None and self._heuristic_fallback_enabled)
        )

        if need_heuristic:
            heuristic_ai = self._ensure_heuristic_evaluator()
            if heuristic_ai is not None:
                try:
                    # Evaluate from the current player's perspective, then flip if needed.
                    heuristic_ai.player_number = sim_state.current_player
                    raw_score = heuristic_ai.evaluate_position(sim_state)
                    heuristic_value = self._normalize_heuristic_score(raw_score)
                except Exception as e:
                    logger.debug(f"GumbelMCTSAI: heuristic evaluation failed: {e}")

        # Compute final value
        if nn_value is not None and heuristic_value is not None:
            # Blend NN and heuristic
            alpha = (
                self._heuristic_blend_alpha
                if self._heuristic_blend_alpha is not None
                else 1.0
            )
            value = alpha * nn_value + (1.0 - alpha) * heuristic_value
        elif nn_value is not None:
            # Pure NN
            value = nn_value
        elif heuristic_value is not None:
            # Pure heuristic fallback
            value = heuristic_value
        else:
            # No evaluation available
            value = 0.0

        # Flip for opponent perspective
        if is_opponent_perspective:
            value = -value

        return value

    def select_move(self, game_state: GameState) -> Move | None:
        """Select best move using Gumbel MCTS with Sequential Halving.

        Args:
            game_state: Current game state.

        Returns:
            Selected move or None if no valid moves.
        """
        valid_moves = self.get_valid_moves(game_state)

        if not valid_moves:
            return None

        if len(valid_moves) == 1:
            self.move_count += 1
            return valid_moves[0]

        # Check for swap decision first
        swap_move = self.maybe_select_swap_move(game_state, valid_moves)
        if swap_move is not None:
            self.move_count += 1
            return swap_move

        # Run Gumbel MCTS
        best_move = self._gumbel_mcts_search(game_state, valid_moves)

        self.move_count += 1
        return best_move

    def _gumbel_mcts_search(
        self,
        game_state: GameState,
        valid_moves: list[Move],
    ) -> Move:
        """Run Gumbel MCTS search with Sequential Halving.

        Args:
            game_state: Current game state.
            valid_moves: List of valid moves.

        Returns:
            Best move according to search.
        """
        # Get policy logits from neural network
        policy_logits = self._get_policy_logits(game_state, valid_moves)

        # Step 1: Gumbel-Top-K sampling
        actions = self._gumbel_top_k_sample(valid_moves, policy_logits)

        if len(actions) == 1:
            # Single action - store it with full visit count
            actions[0].visit_count = 1
            self._last_search_actions = actions
            return actions[0].move

        # Step 2: Sequential Halving
        best_action = self._sequential_halving(game_state, actions)

        # Store all actions with their visit counts for training data extraction
        self._last_search_actions = actions

        return best_action.move

    def _get_policy_logits(
        self,
        game_state: GameState,
        valid_moves: list[Move],
    ) -> np.ndarray:
        """Get policy logits for valid moves from neural network.

        Args:
            game_state: Current game state.
            valid_moves: List of valid moves.

        Returns:
            Array of log-probabilities for each valid move.
        """
        if self.neural_net is None:
            # Uniform logits when no neural net
            return np.zeros(len(valid_moves))

        try:
            _, policy = self.neural_net.evaluate_batch([game_state])

            if policy.size == 0:
                return np.zeros(len(valid_moves))

            policy_vec = policy[0]

            logits = []
            for move in valid_moves:
                idx = self.neural_net.encode_move(move, game_state.board)
                if idx != INVALID_MOVE_INDEX and 0 <= idx < len(policy_vec):
                    # Convert probability to logit (log-odds)
                    prob = max(float(policy_vec[idx]), 1e-10)
                    logit = np.log(prob)
                else:
                    logit = -10.0  # Low logit for unrecognized moves

                logits.append(logit)

            return np.array(logits, dtype=np.float32)

        except Exception as e:
            logger.warning(f"GumbelMCTSAI: policy evaluation failed ({e})")
            return np.zeros(len(valid_moves))

    def _gumbel_top_k_sample(
        self,
        valid_moves: list[Move],
        policy_logits: np.ndarray,
    ) -> list[GumbelAction]:
        """Sample top-k actions using Gumbel-Top-K.

        This samples k actions without replacement by adding Gumbel noise
        to the policy logits and taking the top k.

        Args:
            valid_moves: List of valid moves.
            policy_logits: Log-probabilities for each move.

        Returns:
            List of GumbelAction objects for the top-k actions.
        """
        k = min(self.num_sampled_actions, len(valid_moves))

        # Generate Gumbel(0,1) noise using inverse CDF method
        # Gumbel(0,1) = -log(-log(U)) where U ~ Uniform(0,1)
        seed = int(self.rng.randrange(0, 2**32 - 1))
        np_rng = np.random.default_rng(seed)
        uniform = np_rng.uniform(1e-10, 1.0 - 1e-10, size=len(valid_moves))
        gumbel_noise = -np.log(-np.log(uniform))

        # Perturb logits with Gumbel noise
        perturbed = policy_logits + gumbel_noise

        # Select top-k indices
        top_k_indices = np.argsort(perturbed)[-k:][::-1]

        # Create GumbelAction objects
        actions = []
        for idx in top_k_indices:
            action = GumbelAction(
                move=valid_moves[idx],
                policy_logit=float(policy_logits[idx]),
                gumbel_noise=float(gumbel_noise[idx]),
                perturbed_value=float(perturbed[idx]),
            )
            actions.append(action)

        return actions

    # =========================================================================
    # GPU Batch Evaluation Methods
    # =========================================================================

    def _ensure_gpu_available(self) -> bool:
        """Lazily check if GPU is available for batch evaluation.

        Returns:
            True if GPU is available and neural network supports batch evaluation.
        """
        if self._gpu_available is not None:
            return self._gpu_available

        if not self._gpu_batch_enabled:
            self._gpu_available = False
            logger.debug("GumbelMCTSAI: GPU batching disabled via environment variable")
            return False

        if self.neural_net is None:
            self._gpu_available = False
            return False

        try:
            from .gpu_batch import get_device

            self._gpu_device = get_device(prefer_gpu=True)
            self._gpu_available = self._gpu_device.type in ('cuda', 'mps')

            if self._gpu_available:
                logger.info(
                    f"GumbelMCTSAI: GPU batch evaluation enabled on {self._gpu_device} "
                    f"(batch_size={self._gpu_batch_size})"
                )
            else:
                logger.debug(
                    f"GumbelMCTSAI: No GPU available (device={self._gpu_device.type}), "
                    "using sequential evaluation"
                )
        except Exception as e:
            logger.warning(f"GumbelMCTSAI: GPU check failed, using sequential: {e}")
            self._gpu_available = False

        return self._gpu_available

    def _simulate_actions_batched(
        self,
        game_state: GameState,
        actions: list[GumbelAction],
        sims_per_action: int,
    ) -> None:
        """Simulate all actions in a batch for GPU efficiency.

        Instead of evaluating one state at a time, this method:
        1. Collects all (action, simulation) state pairs
        2. Batch evaluates all non-terminal states via neural network
        3. Aggregates results back to each action

        This provides significant speedup on GPU by amortizing kernel launch
        overhead and maximizing memory bandwidth utilization.

        Args:
            game_state: Current game state.
            actions: List of actions to simulate.
            sims_per_action: Number of simulations per action.
        """
        if self.neural_net is None:
            # Fallback to sequential if no neural network
            for action in actions:
                value_sum = self._simulate_action(game_state, action.move, sims_per_action)
                action.visit_count += sims_per_action
                action.total_value += value_sum
            return

        # Collect all states that need neural network evaluation
        # Format: (action_idx, sim_state, needs_flip)
        evaluation_requests: list[tuple[int, GameState, bool]] = []
        terminal_values: dict[int, list[float]] = {i: [] for i in range(len(actions))}

        for action_idx, action in enumerate(actions):
            mstate = MutableGameState.from_immutable(game_state)
            undo = mstate.make_move(action.move)
            sim_state = mstate.to_immutable()

            for _ in range(sims_per_action):
                if sim_state.game_status == "completed":
                    # Terminal state - compute value directly
                    winner = sim_state.winner
                    if winner == self.player_number:
                        value = 1.0
                    elif winner is None:
                        value = 0.0  # Draw
                    else:
                        value = -1.0
                    terminal_values[action_idx].append(value)
                else:
                    # Non-terminal - add to batch
                    needs_flip = sim_state.current_player != self.player_number
                    evaluation_requests.append((action_idx, sim_state, needs_flip))

            mstate.unmake_move(undo)

        # Add terminal values to actions
        for action_idx, values in terminal_values.items():
            if values:
                actions[action_idx].visit_count += len(values)
                actions[action_idx].total_value += sum(values)

        # Batch evaluate all non-terminal states
        if evaluation_requests:
            states = [req[1] for req in evaluation_requests]

            # Split into chunks if batch is too large
            batch_size = self._gpu_batch_size
            all_values: list[float] = []

            for i in range(0, len(states), batch_size):
                batch_states = states[i:i + batch_size]
                try:
                    values, _ = self.neural_net.evaluate_batch(batch_states)
                    all_values.extend(values if values else [0.0] * len(batch_states))
                except Exception as e:
                    logger.warning(f"GumbelMCTSAI: Batch evaluation failed: {e}")
                    all_values.extend([0.0] * len(batch_states))

            # Shadow validation if enabled
            if self._shadow_validate and len(evaluation_requests) > 0:
                self._shadow_validate_batch(evaluation_requests, all_values)

            # Aggregate values back to actions
            for i, (action_idx, _, needs_flip) in enumerate(evaluation_requests):
                value = all_values[i] if i < len(all_values) else 0.0
                if needs_flip:
                    value = -value
                actions[action_idx].visit_count += 1
                actions[action_idx].total_value += value

    def _shadow_validate_batch(
        self,
        requests: list[tuple[int, GameState, bool]],
        batch_values: list[float],
    ) -> None:
        """Validate batch NN results against sequential evaluation (5% sample).

        This validates that batching neural network calls produces identical results
        to sequential calls. This catches:
        - Numerical precision differences between batch and sequential inference
        - Any bugs in batch aggregation logic

        Note: Rules parity is maintained by construction since both paths use
        the canonical rules engine (MutableGameState.from_immutable, make_move).
        This validation focuses on NN evaluation consistency.

        Args:
            requests: List of (action_idx, game_state, needs_flip) tuples
            batch_values: Values returned from batched NN evaluation
        """
        import random

        if self.neural_net is None:
            return

        sample_rate = 0.05  # Check 5% of batch
        for i, (action_idx, state, needs_flip) in enumerate(requests):
            if random.random() > sample_rate:
                continue

            self._shadow_total_checks += 1

            # Get batch result
            batch_value = batch_values[i] if i < len(batch_values) else 0.0

            # Compute sequential result using identical code path
            try:
                seq_values, _ = self.neural_net.evaluate_batch([state])
                seq_value = seq_values[0] if seq_values else 0.0
            except Exception as e:
                logger.debug(f"GumbelMCTSAI: Shadow validation NN call failed: {e}")
                continue

            # Check for divergence using both absolute and relative thresholds
            abs_diff = abs(batch_value - seq_value)

            # For values near zero, use absolute threshold
            # For larger values, use relative threshold
            is_divergent = False
            divergence_str = ""
            if abs(seq_value) < 0.01:
                # Near-zero values: use absolute threshold of 0.001
                if abs_diff > 0.001:
                    is_divergent = True
                    divergence_str = f"abs_diff={abs_diff:.6f}"
            else:
                # Larger values: use 1% relative threshold
                rel_diff = abs_diff / abs(seq_value)
                if rel_diff > 0.01:
                    is_divergent = True
                    divergence_str = f"rel_diff={rel_diff:.1%}"

            if is_divergent:
                self._shadow_divergence_count += 1
                logger.warning(
                    f"GumbelMCTSAI: Batch/sequential NN divergence ({divergence_str}): "
                    f"batch={batch_value:.6f}, seq={seq_value:.6f}, "
                    f"action_idx={action_idx}, needs_flip={needs_flip}"
                )

                # Log detailed state info for debugging (first 3 divergences only)
                if self._shadow_divergence_count <= 3:
                    logger.warning(
                        f"  State: player={state.current_player}, "
                        f"status={state.game_status}, move_count={state.move_count}"
                    )

        # Periodic stats logging
        if self._shadow_total_checks > 0 and self._shadow_total_checks % 100 == 0:
            rate = self._shadow_divergence_count / self._shadow_total_checks
            logger.info(
                f"GumbelMCTSAI: Shadow validation stats: "
                f"{self._shadow_divergence_count}/{self._shadow_total_checks} divergences ({rate:.2%})"
            )

    def get_shadow_validation_stats(self) -> dict[str, Any]:
        """Get shadow validation statistics.

        Returns:
            Dictionary with:
            - total_checks: Number of validation checks performed
            - divergences: Number of divergences detected
            - divergence_rate: Percentage of checks that diverged
            - shadow_enabled: Whether shadow validation is enabled
        """
        rate = 0.0
        if self._shadow_total_checks > 0:
            rate = self._shadow_divergence_count / self._shadow_total_checks

        return {
            "total_checks": self._shadow_total_checks,
            "divergences": self._shadow_divergence_count,
            "divergence_rate": rate,
            "shadow_enabled": self._shadow_validate,
        }

    def validate_move_parity(
        self,
        game_state: GameState,
        valid_moves: list[Move],
    ) -> tuple[bool, str | None]:
        """Validate that GPU batch and CPU sequential paths select the same move.

        This is the ultimate parity check - it runs the full search with both
        batch (GPU) and sequential (CPU) evaluation and compares the selected moves.

        Note: This is expensive and should only be used for debugging/validation,
        not during normal gameplay or training.

        Args:
            game_state: Current game state.
            valid_moves: List of valid moves.

        Returns:
            Tuple of (moves_match: bool, difference_info: Optional[str])
        """
        if len(valid_moves) <= 1:
            return True, None

        # Get policy logits (same for both paths)
        policy_logits = self._get_policy_logits(game_state, valid_moves)

        # Sample actions (use same seed for both paths)
        import copy
        rng_state = copy.deepcopy(self.rng.getstate()) if hasattr(self.rng, 'getstate') else None

        # Run with GPU batch path
        original_gpu_available = self._gpu_available
        self._gpu_available = True
        actions_gpu = self._gumbel_top_k_sample(valid_moves, policy_logits)
        if len(actions_gpu) > 1:
            # Reset action stats for fresh comparison
            for a in actions_gpu:
                a.visit_count = 0
                a.total_value = 0.0
            best_gpu = self._sequential_halving(game_state, actions_gpu)
        else:
            best_gpu = actions_gpu[0] if actions_gpu else None

        # Restore RNG and run with CPU sequential path
        if rng_state is not None:
            self.rng.setstate(rng_state)

        self._gpu_available = False
        actions_cpu = self._gumbel_top_k_sample(valid_moves, policy_logits)
        if len(actions_cpu) > 1:
            for a in actions_cpu:
                a.visit_count = 0
                a.total_value = 0.0
            best_cpu = self._sequential_halving(game_state, actions_cpu)
        else:
            best_cpu = actions_cpu[0] if actions_cpu else None

        # Restore original GPU state
        self._gpu_available = original_gpu_available

        # Compare moves
        if best_gpu is None and best_cpu is None:
            return True, None

        if best_gpu is None or best_cpu is None:
            return False, f"GPU={best_gpu}, CPU={best_cpu}"

        if str(best_gpu.move) == str(best_cpu.move):
            return True, None

        return False, (
            f"GPU selected {best_gpu.move} (Q={best_gpu.mean_value:.4f}, visits={best_gpu.visit_count}), "
            f"CPU selected {best_cpu.move} (Q={best_cpu.mean_value:.4f}, visits={best_cpu.visit_count})"
        )

    def _sequential_halving(
        self,
        game_state: GameState,
        actions: list[GumbelAction],
    ) -> GumbelAction:
        """Run Sequential Halving to find the best action.

        Progressively halves the number of candidate actions, allocating
        simulation budget evenly across phases.

        GPU Acceleration:
            When GPU is available, uses batch evaluation to evaluate all
            simulations for all actions in each phase with a single neural
            network forward pass, providing 5-50x speedup.

        Args:
            game_state: Current game state.
            actions: List of candidate actions from Gumbel-Top-K.

        Returns:
            Best action after Sequential Halving.
        """
        m = len(actions)
        if m == 1:
            return actions[0]

        # Check GPU availability (lazy initialization)
        self._ensure_gpu_available()  # Side effect: sets self._gpu_available

        # Number of phases = ceil(log2(m))
        num_phases = int(np.ceil(np.log2(m)))
        budget_per_phase = self.simulation_budget // max(num_phases, 1)

        remaining = list(actions)

        for _phase in range(num_phases):
            if len(remaining) == 1:
                break

            # Allocate budget evenly across remaining actions
            sims_per_action = max(1, budget_per_phase // len(remaining))

            # Always use tree-based search for proper MCTS exploration
            # The batched version only does depth-1 evaluation without tree search
            for action in remaining:
                value_sum = self._simulate_action(
                    game_state, action.move, sims_per_action
                )
                action.visit_count += sims_per_action
                action.total_value += value_sum

            # Sort by completed Q-value and keep top half
            max_visits = max(a.visit_count for a in remaining)
            remaining.sort(key=lambda a: a.completed_q(max_visits), reverse=True)
            remaining = remaining[: max(1, len(remaining) // 2)]

        return remaining[0]

    def _simulate_action(
        self,
        game_state: GameState,
        action: Move,
        num_sims: int,
    ) -> float:
        """Simulate an action and return cumulative value.

        Runs MCTS-style tree simulations from the state after taking the action.
        Uses proper tree search with selection, expansion, evaluation, and backup.

        Args:
            game_state: Current game state.
            action: Action to simulate.
            num_sims: Number of simulations to run.

        Returns:
            Sum of values from all simulations.
        """
        # Create mutable state for simulation
        mstate = MutableGameState.from_immutable(game_state)

        # Apply the action
        undo = mstate.make_move(action)

        # Check if terminal after action
        if mstate.is_game_over():
            winner = mstate.winner
            if winner == self.player_number:
                value = 1.0
            elif winner is None:
                value = 0.0
            else:
                value = -1.0
            mstate.unmake_move(undo)
            return value * num_sims

        # Create root node for tree search from this position
        root = GumbelNode(
            move=action,
            parent=None,
            prior=1.0,
            to_move_is_root=(mstate.current_player == self.player_number),
        )

        total_value = 0.0

        # Run tree simulations
        for _ in range(num_sims):
            # Clone state for each simulation (tree simulation modifies state)
            sim_mstate = MutableGameState.from_immutable(mstate.to_immutable())
            value = self._run_tree_simulation(sim_mstate, root, max_depth=10)
            total_value += value

        # Unmake the action
        mstate.unmake_move(undo)

        return total_value

    def _run_tree_simulation(
        self,
        mstate: MutableGameState,
        root: GumbelNode,
        max_depth: int = 10,
    ) -> float:
        """Run a single tree simulation from the current state.

        Uses PUCT-style selection for internal nodes and neural network
        evaluation at leaves.

        Args:
            mstate: Mutable game state (will be modified in-place).
            root: Root node of the search tree.
            max_depth: Maximum depth to search.

        Returns:
            Value estimate from the simulation.
        """
        path: list[tuple[GumbelNode, MoveUndo]] = []
        node = root
        depth = 0

        # Selection phase - traverse tree until leaf
        while depth < max_depth and not mstate.is_game_over():
            valid_moves = self.rules_engine.get_valid_moves(
                mstate.to_immutable(), mstate.current_player
            )

            if not valid_moves:
                break

            # Check if node is fully expanded
            unexpanded = [m for m in valid_moves if str(m) not in node.children]

            if unexpanded:
                # Expansion - add new child
                move = self.rng.choice(unexpanded)
                undo = mstate.make_move(move)

                child = GumbelNode(
                    move=move,
                    parent=node,
                    prior=1.0 / len(valid_moves),
                    to_move_is_root=(mstate.current_player == self.player_number),
                )
                node.children[str(move)] = child
                path.append((child, undo))
                node = child
                break
            else:
                # Selection - pick best child by PUCT
                best_move = self._select_puct_move(node, valid_moves)
                undo = mstate.make_move(best_move)

                child = node.children.get(str(best_move))
                if child is None:
                    # Shouldn't happen but handle gracefully
                    break

                path.append((child, undo))
                node = child
                depth += 1

        # Evaluation phase
        if mstate.is_game_over():
            # Terminal state evaluation (exact values)
            winner = mstate.winner
            if winner == self.player_number:
                value = 1.0
            elif winner is None:
                value = 0.0
            else:
                value = -1.0
        else:
            # Non-terminal: use hybrid NN + heuristic evaluation
            is_opponent = mstate.current_player != self.player_number
            value = self._evaluate_leaf_hybrid(mstate, is_opponent)

        # Backpropagation - update all nodes in path
        current_value = value
        for child_node, undo in reversed(path):
            child_node.visit_count += 1
            child_node.total_value += current_value

            # Flip value for opponent nodes
            if child_node.parent and (
                child_node.to_move_is_root
                != getattr(child_node.parent, "to_move_is_root", True)
            ):
                current_value = -current_value

            # Unmake move
            mstate.unmake_move(undo)

        return value

    def _select_puct_move(
        self,
        node: GumbelNode,
        valid_moves: list[Move],
    ) -> Move:
        """Select move using PUCT formula.

        Args:
            node: Current node with children.
            valid_moves: Valid moves from current position.

        Returns:
            Best move according to PUCT.
        """
        total_visits = node.visit_count + 1

        def puct_score(move: Move) -> float:
            child = node.children.get(str(move))
            if child is None:
                return float("inf")  # Prefer unexplored

            q = child.mean_value
            # Flip Q for opponent nodes
            if not child.to_move_is_root:
                q = -q

            u = self.c_puct * child.prior * math.sqrt(total_visits) / (
                1 + child.visit_count
            )
            return q + u

        return max(valid_moves, key=puct_score)

    def evaluate_position(self, game_state: GameState) -> float:
        """Evaluate position using hybrid NN + heuristic blending.

        Args:
            game_state: Current game state.

        Returns:
            Value estimate (blended NN + heuristic if configured, or pure NN/heuristic).
        """
        # Terminal state check
        if game_state.game_status == "completed":
            if game_state.winner == self.player_number:
                return 1.0
            elif game_state.winner is not None:
                return -1.0
            return 0.0

        # Try NN evaluation first
        nn_value: float | None = None
        if self.neural_net is not None:
            try:
                values, _ = self.neural_net.evaluate_batch([game_state])
                nn_value = values[0] if values else None
            except Exception:
                pass

        # Compute heuristic value if needed
        heuristic_value: float | None = None
        need_heuristic = (
            self._heuristic_blend_alpha is not None or
            (nn_value is None and self._heuristic_fallback_enabled)
        )

        if need_heuristic:
            heuristic_ai = self._ensure_heuristic_evaluator()
            if heuristic_ai is not None:
                try:
                    heuristic_ai.player_number = self.player_number
                    raw_score = heuristic_ai.evaluate_position(game_state)
                    heuristic_value = self._normalize_heuristic_score(raw_score)
                except Exception:
                    pass

        # Compute final value
        if nn_value is not None and heuristic_value is not None:
            alpha = (
                self._heuristic_blend_alpha
                if self._heuristic_blend_alpha is not None
                else 1.0
            )
            return alpha * nn_value + (1.0 - alpha) * heuristic_value
        elif nn_value is not None:
            return nn_value
        elif heuristic_value is not None:
            return heuristic_value
        return 0.0

    def reset_for_new_game(self, *, rng_seed: int | None = None) -> None:
        """Reset state for a new game.

        Args:
            rng_seed: Optional new RNG seed.
        """
        super().reset_for_new_game(rng_seed=rng_seed)
        # No persistent tree to reset in Gumbel MCTS (tree is built fresh each move)

    def get_visit_distribution(self) -> tuple[list[Move], list[float]]:
        """Extract normalized visit count distribution from the last search.

        Returns a tuple of (moves, visit_probabilities) representing the
        Gumbel MCTS policy based on visit counts from Sequential Halving.
        This can be used as soft policy targets during training.

        Returns:
            Tuple of (list of moves, list of visit probabilities) where
            probabilities sum to 1.0. Returns ([], []) if no search has
            been performed.
        """
        if self._last_search_actions is None or not self._last_search_actions:
            return [], []

        # Filter actions with non-zero visits
        visited_actions = [a for a in self._last_search_actions if a.visit_count > 0]
        if not visited_actions:
            return [], []

        total_visits = sum(a.visit_count for a in visited_actions)
        if total_visits == 0:
            return [], []

        moves: list[Move] = []
        probs: list[float] = []

        for action in visited_actions:
            moves.append(action.move)
            probs.append(action.visit_count / total_visits)

        return moves, probs

    def __repr__(self) -> str:
        """String representation."""
        gpu_status = "enabled" if self._gpu_batch_enabled else "disabled"
        if self._gpu_available is not None:
            gpu_status = f"active:{self._gpu_device.type}" if self._gpu_available else "unavailable"
        return (
            f"GumbelMCTSAI(player={self.player_number}, "
            f"m={self.num_sampled_actions}, "
            f"budget={self.simulation_budget}, "
            f"model={self.config.nn_model_id}, "
            f"gpu={gpu_status})"
        )
