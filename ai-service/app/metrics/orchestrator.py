"""Orchestrator-specific Prometheus metrics.

This module provides metrics for tracking the AI self-improvement pipeline,
including selfplay, training, evaluation, and model promotion stages.

All orchestrator scripts should use these metrics instead of defining
their own to ensure consistent monitoring across the pipeline.

Usage:
    from app.metrics.orchestrator import (
        record_selfplay_batch,
        record_training_run,
        record_evaluation,
        record_model_promotion,
    )

    # After running selfplay
    record_selfplay_batch(
        board_type="square8",
        num_players=2,
        games=100,
        duration_seconds=60.5,
        errors=0,
    )

    # After training
    record_training_run(
        board_type="square8",
        num_players=2,
        duration_seconds=3600,
        final_loss=0.25,
        final_accuracy=0.85,
        samples=10000,
    )
"""

from __future__ import annotations

import logging
import time
from collections.abc import Generator
from contextlib import contextmanager
from typing import Final

from prometheus_client import Counter, Gauge, Histogram

logger = logging.getLogger(__name__)

from app.metrics.constants import (
    DURATION_BUCKETS_SECONDS,
    LATENCY_BUCKETS_SECONDS,
    SHORT_DURATION_BUCKETS,
)

# =============================================================================
# Safe Metric Registration (December 2025: Consolidated)
# =============================================================================
# Use the centralized registry to avoid duplicate metric registration.
from app.metrics.registry import safe_metric as _safe_metric

# =============================================================================
# Selfplay Metrics
# =============================================================================

SELFPLAY_GAMES_TOTAL: Final[Counter] = _safe_metric(Counter,
    "ringrift_selfplay_games_total",
    "Total selfplay games generated by orchestrators.",
    labelnames=("board_type", "num_players", "orchestrator"),
)

SELFPLAY_GAMES_PER_SECOND: Final[Gauge] = _safe_metric(Gauge,
    "ringrift_selfplay_games_per_second",
    "Current selfplay throughput in games per second.",
    labelnames=("board_type", "num_players"),
)

SELFPLAY_BATCH_DURATION: Final[Histogram] = _safe_metric(Histogram,
    "ringrift_selfplay_batch_duration_seconds",
    "Duration of selfplay batches in seconds.",
    labelnames=("board_type", "num_players"),
    buckets=DURATION_BUCKETS_SECONDS,
)

SELFPLAY_ERRORS_TOTAL: Final[Counter] = _safe_metric(Counter,
    "ringrift_selfplay_errors_total",
    "Total errors during selfplay execution.",
    labelnames=("board_type", "num_players", "error_type"),
)

# Wired via update_selfplay_queue_size() - called from unified_ai_loop.py
SELFPLAY_QUEUE_SIZE: Final[Gauge] = _safe_metric(Gauge,
    "ringrift_selfplay_queue_size",
    "Current size of the selfplay job queue.",
    labelnames=("orchestrator",),
)

# =============================================================================
# Training Metrics
# =============================================================================

TRAINING_RUNS_TOTAL: Final[Counter] = _safe_metric(Counter,
    "ringrift_orchestrator_training_runs_total",
    "Total training runs completed.",
    labelnames=("board_type", "num_players", "model_type"),
)

TRAINING_RUN_DURATION: Final[Histogram] = _safe_metric(Histogram,
    "ringrift_training_run_duration_seconds",
    "Duration of training runs in seconds.",
    labelnames=("board_type", "num_players"),
    buckets=(300, 600, 1200, 1800, 3600, 7200, 14400),
)

TRAINING_LOSS: Final[Gauge] = _safe_metric(Gauge,
    "ringrift_orchestrator_training_loss",
    "Current/final training loss.",
    labelnames=("board_type", "num_players", "loss_type"),
)

TRAINING_ACCURACY: Final[Gauge] = _safe_metric(Gauge,
    "ringrift_training_accuracy",
    "Current/final training accuracy.",
    labelnames=("board_type", "num_players", "metric_type"),
)

TRAINING_SAMPLES_PROCESSED: Final[Counter] = _safe_metric(Counter,
    "ringrift_training_samples_processed_total",
    "Total training samples processed.",
    labelnames=("board_type", "num_players"),
)

TRAINING_EPOCHS_TOTAL: Final[Counter] = _safe_metric(Counter,
    "ringrift_orchestrator_training_epochs_total",
    "Total training epochs completed.",
    labelnames=("board_type", "num_players"),
)

# =============================================================================
# Evaluation Metrics
# =============================================================================

EVALUATION_GAMES_TOTAL: Final[Counter] = _safe_metric(Counter,
    "ringrift_evaluation_games_total",
    "Total evaluation games played.",
    labelnames=("board_type", "num_players", "eval_type"),
)

EVALUATION_ELO_DELTA: Final[Gauge] = _safe_metric(Gauge,
    "ringrift_evaluation_elo_delta",
    "Elo delta from evaluation (candidate - baseline).",
    labelnames=("board_type", "num_players", "candidate_model"),
)

EVALUATION_WIN_RATE: Final[Gauge] = _safe_metric(Gauge,
    "ringrift_evaluation_win_rate",
    "Win rate from evaluation.",
    labelnames=("board_type", "num_players", "candidate_model", "opponent"),
)

EVALUATION_DURATION: Final[Histogram] = _safe_metric(Histogram,
    "ringrift_evaluation_duration_seconds",
    "Duration of evaluation tournaments.",
    labelnames=("board_type", "num_players", "eval_type"),
    buckets=(60, 300, 600, 1200, 1800, 3600),
)

# =============================================================================
# Model Promotion Metrics
# =============================================================================

MODEL_PROMOTIONS_TOTAL: Final[Counter] = _safe_metric(Counter,
    "ringrift_model_promotions_total",
    "Total model promotions.",
    labelnames=("board_type", "num_players", "promotion_type"),
)

MODEL_PROMOTION_ELO_GAIN: Final[Histogram] = _safe_metric(Histogram,
    "ringrift_model_promotion_elo_gain",
    "Elo gain from model promotion.",
    labelnames=("board_type", "num_players"),
    buckets=(5, 10, 20, 30, 50, 75, 100, 150, 200),
)

MODEL_PROMOTION_REJECTIONS: Final[Counter] = _safe_metric(Counter,
    "ringrift_model_promotion_rejections_total",
    "Total model promotion rejections.",
    labelnames=("board_type", "num_players", "rejection_reason"),
)

CURRENT_MODEL_ELO: Final[Gauge] = _safe_metric(Gauge,
    "ringrift_current_model_elo",
    "Elo rating of the current production model.",
    labelnames=("board_type", "num_players"),
)

# =============================================================================
# Pipeline Metrics
# =============================================================================

PIPELINE_STAGE_DURATION: Final[Histogram] = _safe_metric(Histogram,
    "ringrift_pipeline_stage_duration_seconds",
    "Duration of pipeline stages.",
    labelnames=("stage",),
    buckets=(10, 30, 60, 300, 600, 1200, 1800, 3600, 7200),
)

# Wired via record_pipeline_iteration() - called from unified_ai_loop.py
PIPELINE_ITERATIONS_TOTAL: Final[Counter] = _safe_metric(Counter,
    "ringrift_pipeline_iterations_total",
    "Total pipeline iterations completed.",
    labelnames=("orchestrator",),
)

PIPELINE_ERRORS_TOTAL: Final[Counter] = _safe_metric(Counter,
    "ringrift_pipeline_errors_total",
    "Total pipeline errors by stage.",
    labelnames=("stage", "error_type"),
)

PIPELINE_STATE: Final[Gauge] = _safe_metric(Gauge,
    "ringrift_pipeline_state",
    "Current pipeline state (0=idle, 1=selfplay, 2=training, 3=eval, 4=promotion).",
    labelnames=("orchestrator",),
)

# =============================================================================
# Data/Model Sync Metrics
# =============================================================================

DATA_SYNC_DURATION: Final[Histogram] = _safe_metric(Histogram,
    "ringrift_data_sync_duration_seconds",
    "Duration of data sync operations.",
    labelnames=("source", "destination"),
    buckets=SHORT_DURATION_BUCKETS,
)

DATA_SYNC_GAMES: Final[Counter] = _safe_metric(Counter,
    "ringrift_data_sync_games_total",
    "Total games synced between hosts.",
    labelnames=("source", "destination"),
)

DATA_SYNC_ERRORS: Final[Counter] = _safe_metric(Counter,
    "ringrift_data_sync_errors_total",
    "Total data sync errors.",
    labelnames=("source", "destination", "error_type"),
)

MODEL_SYNC_DURATION: Final[Histogram] = _safe_metric(Histogram,
    "ringrift_model_sync_duration_seconds",
    "Duration of model sync operations.",
    labelnames=("model_type",),
    buckets=(5, 10, 30, 60, 120, 300),
)

MODEL_SYNC_TOTAL: Final[Counter] = _safe_metric(Counter,
    "ringrift_model_sync_total",
    "Total model sync operations.",
    labelnames=("model_type", "direction"),
)

# =============================================================================
# Enhanced Sync Coordinator Metrics
# =============================================================================

SYNC_COORDINATOR_OPS: Final[Counter] = _safe_metric(Counter,
    "ringrift_sync_coordinator_operations_total",
    "Total sync operations by category and transport.",
    labelnames=("category", "transport"),
)

SYNC_COORDINATOR_BYTES: Final[Counter] = _safe_metric(Counter,
    "ringrift_sync_coordinator_bytes_total",
    "Total bytes transferred by sync coordinator.",
    labelnames=("category", "transport", "direction"),
)

SYNC_COORDINATOR_FILES: Final[Counter] = _safe_metric(Counter,
    "ringrift_sync_coordinator_files_total",
    "Total files synced by category.",
    labelnames=("category", "transport"),
)

SYNC_COORDINATOR_DURATION: Final[Histogram] = _safe_metric(Histogram,
    "ringrift_sync_coordinator_duration_seconds",
    "Duration of sync coordinator operations.",
    labelnames=("category", "transport"),
    buckets=(1, 5, 10, 30, 60, 120, 300, 600, 1800),
)

SYNC_COORDINATOR_ERRORS: Final[Counter] = _safe_metric(Counter,
    "ringrift_sync_coordinator_errors_total",
    "Total errors during sync operations.",
    labelnames=("category", "transport", "error_type"),
)

DATA_SERVER_STATUS: Final[Gauge] = _safe_metric(Gauge,
    "ringrift_data_server_running",
    "Whether the aria2 data server is running (1=running, 0=stopped).",
    labelnames=("port",),
)

SYNC_SOURCES_DISCOVERED: Final[Gauge] = _safe_metric(Gauge,
    "ringrift_sync_sources_discovered",
    "Number of aria2 data sources discovered in cluster.",
    labelnames=(),
)

SYNC_NFS_SKIP: Final[Counter] = _safe_metric(Counter,
    "ringrift_sync_nfs_skip_total",
    "Sync operations skipped due to shared NFS storage.",
    labelnames=("category",),
)

# =============================================================================
# Training Data Quality Metrics
# =============================================================================

TRAINING_DATA_QUALITY_SCORE: Final[Gauge] = _safe_metric(Gauge,
    "ringrift_training_data_quality_score",
    "Quality score of training data (avg/min/max).",
    labelnames=("board_type", "num_players", "stat_type"),
)

TRAINING_DATA_QUALITY_HISTOGRAM: Final[Histogram] = _safe_metric(Histogram,
    "ringrift_training_data_quality_distribution",
    "Distribution of quality scores in training data.",
    labelnames=("board_type", "num_players"),
    buckets=(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0),
)

TRAINING_DATA_HIGH_QUALITY_COUNT: Final[Gauge] = _safe_metric(Gauge,
    "ringrift_training_data_high_quality_count",
    "Number of high-quality games (quality > 0.7) available for training.",
    labelnames=("board_type", "num_players"),
)

TRAINING_DATA_ELO: Final[Gauge] = _safe_metric(Gauge,
    "ringrift_training_data_avg_elo",
    "Average player Elo in training data (avg/min/max).",
    labelnames=("board_type", "num_players", "stat_type"),
)

TRAINING_DATA_DECISIVE_RATIO: Final[Gauge] = _safe_metric(Gauge,
    "ringrift_training_data_decisive_ratio",
    "Ratio of decisive games in training data.",
    labelnames=("board_type", "num_players"),
)

TRAINING_DATA_GAMES_TOTAL: Final[Gauge] = _safe_metric(Gauge,
    "ringrift_training_data_games_total",
    "Total games available for training with quality scores.",
    labelnames=("board_type", "num_players"),
)

QUALITY_BRIDGE_STATUS: Final[Gauge] = _safe_metric(Gauge,
    "ringrift_quality_bridge_status",
    "Quality bridge status metrics.",
    labelnames=("metric_type",),
)

QUALITY_SYNC_STATS: Final[Counter] = _safe_metric(Counter,
    "ringrift_quality_sync_high_quality_games_total",
    "High-quality games synced with priority.",
    labelnames=("transport",),
)

# =============================================================================
# Helper Functions
# =============================================================================


def record_selfplay_batch(
    board_type: str,
    num_players: int,
    games: int,
    duration_seconds: float,
    errors: int = 0,
    orchestrator: str = "unified_ai_loop",
) -> None:
    """Record metrics for a completed selfplay batch.

    Args:
        board_type: Board type (square8, square19, hexagonal)
        num_players: Number of players
        games: Number of games completed
        duration_seconds: Batch duration in seconds
        errors: Number of errors during batch
        orchestrator: Orchestrator identifier
    """
    np_str = str(num_players)

    SELFPLAY_GAMES_TOTAL.labels(board_type, np_str, orchestrator).inc(games)
    SELFPLAY_BATCH_DURATION.labels(board_type, np_str).observe(duration_seconds)

    if duration_seconds > 0:
        rate = games / duration_seconds
        SELFPLAY_GAMES_PER_SECOND.labels(board_type, np_str).set(rate)

    if errors > 0:
        SELFPLAY_ERRORS_TOTAL.labels(board_type, np_str, "batch_error").inc(errors)


def record_training_run(
    board_type: str,
    num_players: int,
    duration_seconds: float,
    final_loss: float,
    final_accuracy: float | None = None,
    samples: int = 0,
    epochs: int = 0,
    model_type: str = "nnue",
) -> None:
    """Record metrics for a completed training run.

    Args:
        board_type: Board type
        num_players: Number of players
        duration_seconds: Training duration in seconds
        final_loss: Final training loss
        final_accuracy: Final accuracy (optional)
        samples: Number of samples processed
        epochs: Number of epochs completed
        model_type: Type of model trained
    """
    np_str = str(num_players)

    TRAINING_RUNS_TOTAL.labels(board_type, np_str, model_type).inc()
    TRAINING_RUN_DURATION.labels(board_type, np_str).observe(duration_seconds)
    TRAINING_LOSS.labels(board_type, np_str, "final").set(final_loss)

    if final_accuracy is not None:
        TRAINING_ACCURACY.labels(board_type, np_str, "final").set(final_accuracy)

    if samples > 0:
        TRAINING_SAMPLES_PROCESSED.labels(board_type, np_str).inc(samples)

    if epochs > 0:
        TRAINING_EPOCHS_TOTAL.labels(board_type, np_str).inc(epochs)


def record_evaluation(
    board_type: str,
    num_players: int,
    games: int,
    elo_delta: float,
    win_rate: float | None = None,
    duration_seconds: float = 0,
    candidate_model: str = "candidate",
    opponent: str = "baseline",
    eval_type: str = "shadow",
) -> None:
    """Record metrics for an evaluation tournament.

    Args:
        board_type: Board type
        num_players: Number of players
        games: Number of games played
        elo_delta: Elo difference (candidate - baseline)
        win_rate: Win rate for candidate
        duration_seconds: Evaluation duration
        candidate_model: Identifier for candidate model
        opponent: Opponent identifier
        eval_type: Type of evaluation (shadow, full)
    """
    np_str = str(num_players)

    EVALUATION_GAMES_TOTAL.labels(board_type, np_str, eval_type).inc(games)
    EVALUATION_ELO_DELTA.labels(board_type, np_str, candidate_model).set(elo_delta)

    if win_rate is not None:
        EVALUATION_WIN_RATE.labels(board_type, np_str, candidate_model, opponent).set(win_rate)

    if duration_seconds > 0:
        EVALUATION_DURATION.labels(board_type, np_str, eval_type).observe(duration_seconds)


def record_model_promotion(
    board_type: str,
    num_players: int,
    elo_gain: float,
    new_elo: float,
    promotion_type: str = "automatic",
) -> None:
    """Record metrics for a model promotion.

    Args:
        board_type: Board type
        num_players: Number of players
        elo_gain: Elo improvement from promotion
        new_elo: New model's Elo rating
        promotion_type: Type of promotion (automatic, manual)
    """
    np_str = str(num_players)

    MODEL_PROMOTIONS_TOTAL.labels(board_type, np_str, promotion_type).inc()
    MODEL_PROMOTION_ELO_GAIN.labels(board_type, np_str).observe(elo_gain)
    CURRENT_MODEL_ELO.labels(board_type, np_str).set(new_elo)


def record_promotion_rejection(
    board_type: str,
    num_players: int,
    reason: str,
) -> None:
    """Record a model promotion rejection.

    Args:
        board_type: Board type
        num_players: Number of players
        reason: Rejection reason
    """
    np_str = str(num_players)
    MODEL_PROMOTION_REJECTIONS.labels(board_type, np_str, reason).inc()


def record_pipeline_stage(
    stage: str,
    duration_seconds: float,
    success: bool = True,
    error_type: str | None = None,
) -> None:
    """Record metrics for a pipeline stage.

    Args:
        stage: Stage name (selfplay, training, evaluation, promotion)
        duration_seconds: Stage duration
        success: Whether the stage succeeded
        error_type: Error type if failed
    """
    PIPELINE_STAGE_DURATION.labels(stage).observe(duration_seconds)

    if not success and error_type:
        PIPELINE_ERRORS_TOTAL.labels(stage, error_type).inc()


def record_data_sync(
    source: str,
    destination: str,
    games: int,
    duration_seconds: float,
    success: bool = True,
    error_type: str | None = None,
) -> None:
    """Record metrics for a data sync operation.

    Args:
        source: Source host identifier
        destination: Destination host identifier
        games: Number of games synced
        duration_seconds: Sync duration
        success: Whether sync succeeded
        error_type: Error type if failed
    """
    DATA_SYNC_DURATION.labels(source, destination).observe(duration_seconds)
    DATA_SYNC_GAMES.labels(source, destination).inc(games)

    if not success and error_type:
        DATA_SYNC_ERRORS.labels(source, destination, error_type).inc()


def record_model_sync(
    model_type: str,
    duration_seconds: float,
    direction: str = "push",
) -> None:
    """Record metrics for a model sync operation.

    Args:
        model_type: Type of model (nnue, policy, value)
        duration_seconds: Sync duration
        direction: Sync direction (push, pull)
    """
    MODEL_SYNC_DURATION.labels(model_type).observe(duration_seconds)
    MODEL_SYNC_TOTAL.labels(model_type, direction).inc()


def record_sync_coordinator_op(
    category: str,
    transport: str,
    files_synced: int,
    bytes_transferred: int,
    duration_seconds: float,
    success: bool = True,
    error_type: str | None = None,
) -> None:
    """Record metrics for a SyncCoordinator operation.

    Args:
        category: Sync category (games, training, models)
        transport: Transport used (aria2, ssh, p2p, gossip)
        files_synced: Number of files synced
        bytes_transferred: Total bytes transferred
        duration_seconds: Operation duration
        success: Whether operation succeeded
        error_type: Error type if failed
    """
    SYNC_COORDINATOR_OPS.labels(category, transport).inc()
    SYNC_COORDINATOR_FILES.labels(category, transport).inc(files_synced)
    SYNC_COORDINATOR_BYTES.labels(category, transport, "download").inc(bytes_transferred)
    SYNC_COORDINATOR_DURATION.labels(category, transport).observe(duration_seconds)

    if not success and error_type:
        SYNC_COORDINATOR_ERRORS.labels(category, transport, error_type).inc()


def update_data_server_status(port: int, running: bool) -> None:
    """Update data server status gauge.

    Args:
        port: Server port
        running: Whether server is running
    """
    DATA_SERVER_STATUS.labels(str(port)).set(1 if running else 0)


def update_sync_sources_count(count: int) -> None:
    """Update the count of discovered sync sources.

    Args:
        count: Number of sources discovered
    """
    SYNC_SOURCES_DISCOVERED.set(count)


def record_nfs_skip(category: str) -> None:
    """Record that a sync was skipped due to shared NFS storage.

    Args:
        category: Sync category that was skipped
    """
    SYNC_NFS_SKIP.labels(category).inc()


@contextmanager
def time_pipeline_stage(stage: str) -> Generator[None]:
    """Context manager to time a pipeline stage.

    Usage:
        with time_pipeline_stage("selfplay"):
            run_selfplay()
    """
    start = time.time()
    success = True
    error_type = None

    try:
        yield
    except Exception as e:
        success = False
        error_type = type(e).__name__
        raise
    finally:
        duration = time.time() - start
        record_pipeline_stage(stage, duration, success, error_type)


def set_pipeline_state(orchestrator: str, state: int) -> None:
    """Set the current pipeline state.

    Args:
        orchestrator: Orchestrator identifier
        state: State code (0=idle, 1=selfplay, 2=training, 3=eval, 4=promotion)
    """
    PIPELINE_STATE.labels(orchestrator).set(state)


# Pipeline state constants
PIPELINE_IDLE = 0
PIPELINE_SELFPLAY = 1
PIPELINE_TRAINING = 2
PIPELINE_EVALUATION = 3
PIPELINE_PROMOTION = 4


# =============================================================================
# Training Data Quality Helper Functions
# =============================================================================


def record_training_data_quality(
    board_type: str,
    num_players: int,
    avg_quality: float,
    min_quality: float = 0.0,
    max_quality: float = 1.0,
    high_quality_count: int = 0,
    total_games: int = 0,
    avg_elo: float = 1500.0,
    min_elo: float = 1200.0,
    max_elo: float = 2400.0,
    decisive_ratio: float = 0.5,
    quality_scores: list | None = None,
) -> None:
    """Record comprehensive training data quality metrics.

    Args:
        board_type: Board type (square8, square19, hexagonal)
        num_players: Number of players
        avg_quality: Average quality score of training data
        min_quality: Minimum quality score
        max_quality: Maximum quality score
        high_quality_count: Number of games with quality > 0.7
        total_games: Total games available for training
        avg_elo: Average player Elo in training data
        min_elo: Minimum Elo in training data
        max_elo: Maximum Elo in training data
        decisive_ratio: Ratio of decisive (non-draw) games
        quality_scores: Optional list of quality scores for histogram
    """
    np_str = str(num_players)

    # Quality score gauges
    TRAINING_DATA_QUALITY_SCORE.labels(board_type, np_str, "avg").set(avg_quality)
    TRAINING_DATA_QUALITY_SCORE.labels(board_type, np_str, "min").set(min_quality)
    TRAINING_DATA_QUALITY_SCORE.labels(board_type, np_str, "max").set(max_quality)

    # High quality count and total
    TRAINING_DATA_HIGH_QUALITY_COUNT.labels(board_type, np_str).set(high_quality_count)
    TRAINING_DATA_GAMES_TOTAL.labels(board_type, np_str).set(total_games)

    # Elo gauges
    TRAINING_DATA_ELO.labels(board_type, np_str, "avg").set(avg_elo)
    TRAINING_DATA_ELO.labels(board_type, np_str, "min").set(min_elo)
    TRAINING_DATA_ELO.labels(board_type, np_str, "max").set(max_elo)

    # Decisive ratio
    TRAINING_DATA_DECISIVE_RATIO.labels(board_type, np_str).set(decisive_ratio)

    # Histogram of quality scores (if provided)
    if quality_scores:
        for score in quality_scores:
            TRAINING_DATA_QUALITY_HISTOGRAM.labels(board_type, np_str).observe(score)


def update_quality_bridge_status(
    quality_lookup_size: int,
    elo_lookup_size: int,
    refresh_age_seconds: float,
    avg_quality: float = 0.0,
) -> None:
    """Update quality bridge status metrics.

    Args:
        quality_lookup_size: Number of games in quality lookup
        elo_lookup_size: Number of games in Elo lookup
        refresh_age_seconds: Seconds since last refresh
        avg_quality: Average quality score in lookup
    """
    QUALITY_BRIDGE_STATUS.labels("quality_lookup_size").set(quality_lookup_size)
    QUALITY_BRIDGE_STATUS.labels("elo_lookup_size").set(elo_lookup_size)
    QUALITY_BRIDGE_STATUS.labels("refresh_age_seconds").set(refresh_age_seconds)
    QUALITY_BRIDGE_STATUS.labels("avg_quality").set(avg_quality)


def record_high_quality_sync(
    games_synced: int,
    transport: str = "aria2",
) -> None:
    """Record high-quality games synced with priority.

    Args:
        games_synced: Number of high-quality games synced
        transport: Transport used for sync
    """
    QUALITY_SYNC_STATS.labels(transport).inc(games_synced)


def collect_quality_metrics_from_bridge() -> bool:
    """Collect and update quality metrics from the QualityBridge.

    This function should be called periodically to update Prometheus metrics
    from the quality bridge's current state.

    Returns:
        True if metrics were collected successfully
    """
    try:
        from app.training.quality_bridge import get_quality_bridge

        bridge = get_quality_bridge()
        status = bridge.get_status()

        update_quality_bridge_status(
            quality_lookup_size=status.get("quality_lookup_size", 0),
            elo_lookup_size=status.get("elo_lookup_size", 0),
            refresh_age_seconds=status.get("last_refresh_age_seconds", 0),
            avg_quality=status.get("avg_quality_score", 0),
        )

        return True
    except (ImportError, ModuleNotFoundError, AttributeError, KeyError, TypeError):
        logger.exception("Failed to collect quality metrics from bridge")
        return False


def collect_quality_metrics_from_manifest(
    board_type: str = "all",
    num_players: int = 2,
) -> bool:
    """Collect and update quality metrics from the DataManifest.

    Args:
        board_type: Board type to collect metrics for
        num_players: Number of players

    Returns:
        True if metrics were collected successfully
    """
    try:
        from pathlib import Path

        from app.distributed.unified_manifest import DataManifest

        # Try to load manifest
        manifest_paths = [
            Path(__file__).parent.parent.parent / "data" / "data_manifest.db",
            Path.home() / "ringrift" / "ai-service" / "data" / "data_manifest.db",
        ]

        manifest = None
        for path in manifest_paths:
            if path.exists():
                manifest = DataManifest(path)
                break

        if not manifest:
            return False

        # Get quality distribution
        dist = manifest.get_quality_distribution(
            board_type=board_type if board_type != "all" else None,
            num_players=num_players if board_type != "all" else None,
        )

        record_training_data_quality(
            board_type=board_type,
            num_players=num_players,
            avg_quality=dist.get("avg_quality_score", 0.0),
            min_quality=dist.get("min_quality_score", 0.0),
            max_quality=dist.get("max_quality_score", 0.0),
            high_quality_count=0,  # Would need additional query
            total_games=dist.get("total_games", 0),
            avg_elo=dist.get("avg_player_elo", 1500.0),
            min_elo=1200.0,  # Default
            max_elo=2400.0,  # Default
            decisive_ratio=dist.get("decisive_rate", 0.5),
        )

        return True
    except (ImportError, ModuleNotFoundError, OSError, AttributeError, KeyError, TypeError):
        logger.exception("Failed to collect quality metrics from manifest")
        return False


# =============================================================================
# Queue and Iteration Tracking (December 2025)
# =============================================================================

def update_selfplay_queue_size(
    queue_size: int,
    orchestrator: str = "unified_ai_loop",
) -> None:
    """Update the selfplay job queue size metric.

    This wires the TODO at line 81 - tracks pending selfplay jobs.

    Args:
        queue_size: Current number of pending selfplay jobs/configs
        orchestrator: Orchestrator identifier
    """
    SELFPLAY_QUEUE_SIZE.labels(orchestrator).set(queue_size)


def record_pipeline_iteration(
    orchestrator: str = "unified_ai_loop",
) -> None:
    """Record completion of a pipeline iteration.

    This wires the TODO at line 198 - tracks main training loop iterations.

    Args:
        orchestrator: Orchestrator identifier
    """
    PIPELINE_ITERATIONS_TOTAL.labels(orchestrator).inc()


def get_selfplay_queue_size(orchestrator: str = "unified_ai_loop") -> float:
    """Get current selfplay queue size from metric.

    Args:
        orchestrator: Orchestrator identifier

    Returns:
        Current queue size value
    """
    try:
        return SELFPLAY_QUEUE_SIZE.labels(orchestrator)._value.get()
    except (AttributeError, KeyError, ValueError):
        logger.debug("Failed to get selfplay queue size metric", exc_info=True)
        return 0.0


def get_pipeline_iterations(orchestrator: str = "unified_ai_loop") -> float:
    """Get total pipeline iterations from metric.

    Args:
        orchestrator: Orchestrator identifier

    Returns:
        Total iteration count
    """
    try:
        return PIPELINE_ITERATIONS_TOTAL.labels(orchestrator)._value.get()
    except (AttributeError, KeyError, ValueError):
        logger.debug("Failed to get pipeline iterations metric", exc_info=True)
        return 0.0
