"""Orchestrator-specific Prometheus metrics.

This module provides metrics for tracking the AI self-improvement pipeline,
including selfplay, training, evaluation, and model promotion stages.

All orchestrator scripts should use these metrics instead of defining
their own to ensure consistent monitoring across the pipeline.

Usage:
    from app.metrics.orchestrator import (
        record_selfplay_batch,
        record_training_run,
        record_evaluation,
        record_model_promotion,
    )

    # After running selfplay
    record_selfplay_batch(
        board_type="square8",
        num_players=2,
        games=100,
        duration_seconds=60.5,
        errors=0,
    )

    # After training
    record_training_run(
        board_type="square8",
        num_players=2,
        duration_seconds=3600,
        final_loss=0.25,
        final_accuracy=0.85,
        samples=10000,
    )
"""

from __future__ import annotations

import time
from contextlib import contextmanager
from typing import Final, Generator, Optional

from prometheus_client import Counter, Gauge, Histogram, Summary, REGISTRY

# =============================================================================
# Safe Metric Registration
# =============================================================================
# Avoid "Duplicated timeseries" errors when metrics are registered multiple
# times (e.g., during test collection or when importing from multiple paths).


def _safe_metric(metric_class, name: str, doc: str, **kwargs):
    """Create metric or return existing one to avoid duplicate registration."""
    if name in REGISTRY._names_to_collectors:
        return REGISTRY._names_to_collectors[name]
    return metric_class(name, doc, **kwargs)


# =============================================================================
# Selfplay Metrics
# =============================================================================

SELFPLAY_GAMES_TOTAL: Final[Counter] = _safe_metric(Counter,
    "ringrift_selfplay_games_total",
    "Total selfplay games generated by orchestrators.",
    labelnames=("board_type", "num_players", "orchestrator"),
)

SELFPLAY_GAMES_PER_SECOND: Final[Gauge] = _safe_metric(Gauge,
    "ringrift_selfplay_games_per_second",
    "Current selfplay throughput in games per second.",
    labelnames=("board_type", "num_players"),
)

SELFPLAY_BATCH_DURATION: Final[Histogram] = _safe_metric(Histogram,
    "ringrift_selfplay_batch_duration_seconds",
    "Duration of selfplay batches in seconds.",
    labelnames=("board_type", "num_players"),
    buckets=(10, 30, 60, 120, 300, 600, 1200, 1800, 3600),
)

SELFPLAY_ERRORS_TOTAL: Final[Counter] = _safe_metric(Counter,
    "ringrift_selfplay_errors_total",
    "Total errors during selfplay execution.",
    labelnames=("board_type", "num_players", "error_type"),
)

SELFPLAY_QUEUE_SIZE: Final[Gauge] = _safe_metric(Gauge,
    "ringrift_selfplay_queue_size",
    "Current size of the selfplay job queue.",
    labelnames=("orchestrator",),
)

# =============================================================================
# Training Metrics
# =============================================================================

TRAINING_RUNS_TOTAL: Final[Counter] = _safe_metric(Counter,
    "ringrift_orchestrator_training_runs_total",
    "Total training runs completed.",
    labelnames=("board_type", "num_players", "model_type"),
)

TRAINING_RUN_DURATION: Final[Histogram] = _safe_metric(Histogram,
    "ringrift_training_run_duration_seconds",
    "Duration of training runs in seconds.",
    labelnames=("board_type", "num_players"),
    buckets=(300, 600, 1200, 1800, 3600, 7200, 14400),
)

TRAINING_LOSS: Final[Gauge] = _safe_metric(Gauge,
    "ringrift_orchestrator_training_loss",
    "Current/final training loss.",
    labelnames=("board_type", "num_players", "loss_type"),
)

TRAINING_ACCURACY: Final[Gauge] = _safe_metric(Gauge,
    "ringrift_training_accuracy",
    "Current/final training accuracy.",
    labelnames=("board_type", "num_players", "metric_type"),
)

TRAINING_SAMPLES_PROCESSED: Final[Counter] = _safe_metric(Counter,
    "ringrift_training_samples_processed_total",
    "Total training samples processed.",
    labelnames=("board_type", "num_players"),
)

TRAINING_EPOCHS_TOTAL: Final[Counter] = _safe_metric(Counter,
    "ringrift_orchestrator_training_epochs_total",
    "Total training epochs completed.",
    labelnames=("board_type", "num_players"),
)

# =============================================================================
# Evaluation Metrics
# =============================================================================

EVALUATION_GAMES_TOTAL: Final[Counter] = _safe_metric(Counter,
    "ringrift_evaluation_games_total",
    "Total evaluation games played.",
    labelnames=("board_type", "num_players", "eval_type"),
)

EVALUATION_ELO_DELTA: Final[Gauge] = _safe_metric(Gauge,
    "ringrift_evaluation_elo_delta",
    "Elo delta from evaluation (candidate - baseline).",
    labelnames=("board_type", "num_players", "candidate_model"),
)

EVALUATION_WIN_RATE: Final[Gauge] = _safe_metric(Gauge,
    "ringrift_evaluation_win_rate",
    "Win rate from evaluation.",
    labelnames=("board_type", "num_players", "candidate_model", "opponent"),
)

EVALUATION_DURATION: Final[Histogram] = _safe_metric(Histogram,
    "ringrift_evaluation_duration_seconds",
    "Duration of evaluation tournaments.",
    labelnames=("board_type", "num_players", "eval_type"),
    buckets=(60, 300, 600, 1200, 1800, 3600),
)

# =============================================================================
# Model Promotion Metrics
# =============================================================================

MODEL_PROMOTIONS_TOTAL: Final[Counter] = _safe_metric(Counter,
    "ringrift_model_promotions_total",
    "Total model promotions.",
    labelnames=("board_type", "num_players", "promotion_type"),
)

MODEL_PROMOTION_ELO_GAIN: Final[Histogram] = _safe_metric(Histogram,
    "ringrift_model_promotion_elo_gain",
    "Elo gain from model promotion.",
    labelnames=("board_type", "num_players"),
    buckets=(5, 10, 20, 30, 50, 75, 100, 150, 200),
)

MODEL_PROMOTION_REJECTIONS: Final[Counter] = _safe_metric(Counter,
    "ringrift_model_promotion_rejections_total",
    "Total model promotion rejections.",
    labelnames=("board_type", "num_players", "rejection_reason"),
)

CURRENT_MODEL_ELO: Final[Gauge] = _safe_metric(Gauge,
    "ringrift_current_model_elo",
    "Elo rating of the current production model.",
    labelnames=("board_type", "num_players"),
)

# =============================================================================
# Pipeline Metrics
# =============================================================================

PIPELINE_STAGE_DURATION: Final[Histogram] = _safe_metric(Histogram,
    "ringrift_pipeline_stage_duration_seconds",
    "Duration of pipeline stages.",
    labelnames=("stage",),
    buckets=(10, 30, 60, 300, 600, 1200, 1800, 3600, 7200),
)

PIPELINE_ITERATIONS_TOTAL: Final[Counter] = _safe_metric(Counter,
    "ringrift_pipeline_iterations_total",
    "Total pipeline iterations completed.",
    labelnames=("orchestrator",),
)

PIPELINE_ERRORS_TOTAL: Final[Counter] = _safe_metric(Counter,
    "ringrift_pipeline_errors_total",
    "Total pipeline errors by stage.",
    labelnames=("stage", "error_type"),
)

PIPELINE_STATE: Final[Gauge] = _safe_metric(Gauge,
    "ringrift_pipeline_state",
    "Current pipeline state (0=idle, 1=selfplay, 2=training, 3=eval, 4=promotion).",
    labelnames=("orchestrator",),
)

# =============================================================================
# Data/Model Sync Metrics
# =============================================================================

DATA_SYNC_DURATION: Final[Histogram] = _safe_metric(Histogram,
    "ringrift_data_sync_duration_seconds",
    "Duration of data sync operations.",
    labelnames=("source", "destination"),
    buckets=(5, 10, 30, 60, 120, 300, 600),
)

DATA_SYNC_GAMES: Final[Counter] = _safe_metric(Counter,
    "ringrift_data_sync_games_total",
    "Total games synced between hosts.",
    labelnames=("source", "destination"),
)

DATA_SYNC_ERRORS: Final[Counter] = _safe_metric(Counter,
    "ringrift_data_sync_errors_total",
    "Total data sync errors.",
    labelnames=("source", "destination", "error_type"),
)

MODEL_SYNC_DURATION: Final[Histogram] = _safe_metric(Histogram,
    "ringrift_model_sync_duration_seconds",
    "Duration of model sync operations.",
    labelnames=("model_type",),
    buckets=(5, 10, 30, 60, 120, 300),
)

MODEL_SYNC_TOTAL: Final[Counter] = _safe_metric(Counter,
    "ringrift_model_sync_total",
    "Total model sync operations.",
    labelnames=("model_type", "direction"),
)

# =============================================================================
# Helper Functions
# =============================================================================


def record_selfplay_batch(
    board_type: str,
    num_players: int,
    games: int,
    duration_seconds: float,
    errors: int = 0,
    orchestrator: str = "unified_ai_loop",
) -> None:
    """Record metrics for a completed selfplay batch.

    Args:
        board_type: Board type (square8, square19, hexagonal)
        num_players: Number of players
        games: Number of games completed
        duration_seconds: Batch duration in seconds
        errors: Number of errors during batch
        orchestrator: Orchestrator identifier
    """
    np_str = str(num_players)

    SELFPLAY_GAMES_TOTAL.labels(board_type, np_str, orchestrator).inc(games)
    SELFPLAY_BATCH_DURATION.labels(board_type, np_str).observe(duration_seconds)

    if duration_seconds > 0:
        rate = games / duration_seconds
        SELFPLAY_GAMES_PER_SECOND.labels(board_type, np_str).set(rate)

    if errors > 0:
        SELFPLAY_ERRORS_TOTAL.labels(board_type, np_str, "batch_error").inc(errors)


def record_training_run(
    board_type: str,
    num_players: int,
    duration_seconds: float,
    final_loss: float,
    final_accuracy: Optional[float] = None,
    samples: int = 0,
    epochs: int = 0,
    model_type: str = "nnue",
) -> None:
    """Record metrics for a completed training run.

    Args:
        board_type: Board type
        num_players: Number of players
        duration_seconds: Training duration in seconds
        final_loss: Final training loss
        final_accuracy: Final accuracy (optional)
        samples: Number of samples processed
        epochs: Number of epochs completed
        model_type: Type of model trained
    """
    np_str = str(num_players)

    TRAINING_RUNS_TOTAL.labels(board_type, np_str, model_type).inc()
    TRAINING_RUN_DURATION.labels(board_type, np_str).observe(duration_seconds)
    TRAINING_LOSS.labels(board_type, np_str, "final").set(final_loss)

    if final_accuracy is not None:
        TRAINING_ACCURACY.labels(board_type, np_str, "final").set(final_accuracy)

    if samples > 0:
        TRAINING_SAMPLES_PROCESSED.labels(board_type, np_str).inc(samples)

    if epochs > 0:
        TRAINING_EPOCHS_TOTAL.labels(board_type, np_str).inc(epochs)


def record_evaluation(
    board_type: str,
    num_players: int,
    games: int,
    elo_delta: float,
    win_rate: Optional[float] = None,
    duration_seconds: float = 0,
    candidate_model: str = "candidate",
    opponent: str = "baseline",
    eval_type: str = "shadow",
) -> None:
    """Record metrics for an evaluation tournament.

    Args:
        board_type: Board type
        num_players: Number of players
        games: Number of games played
        elo_delta: Elo difference (candidate - baseline)
        win_rate: Win rate for candidate
        duration_seconds: Evaluation duration
        candidate_model: Identifier for candidate model
        opponent: Opponent identifier
        eval_type: Type of evaluation (shadow, full)
    """
    np_str = str(num_players)

    EVALUATION_GAMES_TOTAL.labels(board_type, np_str, eval_type).inc(games)
    EVALUATION_ELO_DELTA.labels(board_type, np_str, candidate_model).set(elo_delta)

    if win_rate is not None:
        EVALUATION_WIN_RATE.labels(board_type, np_str, candidate_model, opponent).set(win_rate)

    if duration_seconds > 0:
        EVALUATION_DURATION.labels(board_type, np_str, eval_type).observe(duration_seconds)


def record_model_promotion(
    board_type: str,
    num_players: int,
    elo_gain: float,
    new_elo: float,
    promotion_type: str = "automatic",
) -> None:
    """Record metrics for a model promotion.

    Args:
        board_type: Board type
        num_players: Number of players
        elo_gain: Elo improvement from promotion
        new_elo: New model's Elo rating
        promotion_type: Type of promotion (automatic, manual)
    """
    np_str = str(num_players)

    MODEL_PROMOTIONS_TOTAL.labels(board_type, np_str, promotion_type).inc()
    MODEL_PROMOTION_ELO_GAIN.labels(board_type, np_str).observe(elo_gain)
    CURRENT_MODEL_ELO.labels(board_type, np_str).set(new_elo)


def record_promotion_rejection(
    board_type: str,
    num_players: int,
    reason: str,
) -> None:
    """Record a model promotion rejection.

    Args:
        board_type: Board type
        num_players: Number of players
        reason: Rejection reason
    """
    np_str = str(num_players)
    MODEL_PROMOTION_REJECTIONS.labels(board_type, np_str, reason).inc()


def record_pipeline_stage(
    stage: str,
    duration_seconds: float,
    success: bool = True,
    error_type: Optional[str] = None,
) -> None:
    """Record metrics for a pipeline stage.

    Args:
        stage: Stage name (selfplay, training, evaluation, promotion)
        duration_seconds: Stage duration
        success: Whether the stage succeeded
        error_type: Error type if failed
    """
    PIPELINE_STAGE_DURATION.labels(stage).observe(duration_seconds)

    if not success and error_type:
        PIPELINE_ERRORS_TOTAL.labels(stage, error_type).inc()


def record_data_sync(
    source: str,
    destination: str,
    games: int,
    duration_seconds: float,
    success: bool = True,
    error_type: Optional[str] = None,
) -> None:
    """Record metrics for a data sync operation.

    Args:
        source: Source host identifier
        destination: Destination host identifier
        games: Number of games synced
        duration_seconds: Sync duration
        success: Whether sync succeeded
        error_type: Error type if failed
    """
    DATA_SYNC_DURATION.labels(source, destination).observe(duration_seconds)
    DATA_SYNC_GAMES.labels(source, destination).inc(games)

    if not success and error_type:
        DATA_SYNC_ERRORS.labels(source, destination, error_type).inc()


def record_model_sync(
    model_type: str,
    duration_seconds: float,
    direction: str = "push",
) -> None:
    """Record metrics for a model sync operation.

    Args:
        model_type: Type of model (nnue, policy, value)
        duration_seconds: Sync duration
        direction: Sync direction (push, pull)
    """
    MODEL_SYNC_DURATION.labels(model_type).observe(duration_seconds)
    MODEL_SYNC_TOTAL.labels(model_type, direction).inc()


@contextmanager
def time_pipeline_stage(stage: str) -> Generator[None, None, None]:
    """Context manager to time a pipeline stage.

    Usage:
        with time_pipeline_stage("selfplay"):
            run_selfplay()
    """
    start = time.time()
    success = True
    error_type = None

    try:
        yield
    except Exception as e:
        success = False
        error_type = type(e).__name__
        raise
    finally:
        duration = time.time() - start
        record_pipeline_stage(stage, duration, success, error_type)


def set_pipeline_state(orchestrator: str, state: int) -> None:
    """Set the current pipeline state.

    Args:
        orchestrator: Orchestrator identifier
        state: State code (0=idle, 1=selfplay, 2=training, 3=eval, 4=promotion)
    """
    PIPELINE_STATE.labels(orchestrator).set(state)


# Pipeline state constants
PIPELINE_IDLE = 0
PIPELINE_SELFPLAY = 1
PIPELINE_TRAINING = 2
PIPELINE_EVALUATION = 3
PIPELINE_PROMOTION = 4
