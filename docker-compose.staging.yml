# =============================================================================
# RingRift Staging Environment - Production Topology
# =============================================================================
#
# This docker-compose file configures a staging environment that mirrors
# production topology for meaningful load testing and validation.
#
# Target Scale (from PROJECT_GOALS.md):
# - 100 concurrent games
# - 300 concurrent players
#
# SLOs to validate:
# - <500ms p95 latency
# - <1% error rate
# - >99.9% uptime
#
# Usage:
#   docker-compose -f docker-compose.staging.yml up -d
#
# Or with explicit .env file:
#   docker-compose -f docker-compose.staging.yml --env-file .env.staging up -d
#
# =============================================================================

version: '3.8'

services:
  # ===========================================================================
  # Node.js API Server (scaled for production)
  # ===========================================================================
  app:
    build:
      context: .
      dockerfile: Dockerfile
    environment:
      - NODE_ENV=production
      - PORT=3000
      - SOCKET_PORT=3001
      - DATABASE_URL=${DATABASE_URL:-postgresql://ringrift:${DB_PASSWORD:-stagingpass}@postgres:5432/ringrift}
      - REDIS_URL=${REDIS_URL:-redis://redis:6379}
      - AI_SERVICE_URL=${AI_SERVICE_URL:-http://ai-service:8001}
      - JWT_SECRET=${JWT_SECRET}
      - JWT_REFRESH_SECRET=${JWT_REFRESH_SECRET}
      - JWT_EXPIRES_IN=${JWT_EXPIRES_IN:-15m}
      - JWT_REFRESH_EXPIRES_IN=${JWT_REFRESH_EXPIRES_IN:-7d}
      - CORS_ORIGIN=${CORS_ORIGIN:-http://localhost:3000}
      - ALLOWED_ORIGINS=${ALLOWED_ORIGINS:-http://localhost:3000}
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - LOG_FORMAT=json
      # Orchestrator configuration (production posture)
      - RINGRIFT_RULES_MODE=ts
      - RINGRIFT_APP_TOPOLOGY=single
      - ORCHESTRATOR_ADAPTER_ENABLED=true
      - ORCHESTRATOR_SHADOW_MODE_ENABLED=false
      - ORCHESTRATOR_CIRCUIT_BREAKER_ENABLED=true
      - ORCHESTRATOR_ERROR_THRESHOLD_PERCENT=5
      - ORCHESTRATOR_ERROR_WINDOW_SECONDS=300
      - ORCHESTRATOR_LATENCY_THRESHOLD_MS=500
      # Rate limits (production-realistic with headroom for load testing)
      - RATE_LIMIT_API_POINTS=${RATE_LIMIT_API_POINTS:-10000}
      - RATE_LIMIT_API_AUTH_POINTS=${RATE_LIMIT_API_AUTH_POINTS:-10000}
      - RATE_LIMIT_AUTH_POINTS=${RATE_LIMIT_AUTH_POINTS:-10000}
      - RATE_LIMIT_AUTH_LOGIN_POINTS=${RATE_LIMIT_AUTH_LOGIN_POINTS:-10000}
      - RATE_LIMIT_AUTH_REGISTER_POINTS=${RATE_LIMIT_AUTH_REGISTER_POINTS:-10000}
      - RATE_LIMIT_GAME_POINTS=${RATE_LIMIT_GAME_POINTS:-10000}
      - RATE_LIMIT_GAME_CREATE_USER_POINTS=${RATE_LIMIT_GAME_CREATE_USER_POINTS:-1000}
      - RATE_LIMIT_GAME_CREATE_IP_POINTS=${RATE_LIMIT_GAME_CREATE_IP_POINTS:-1000}
      # Load testing capacity targets
      - MAX_CONCURRENT_GAMES=${MAX_CONCURRENT_GAMES:-150}
      - MAX_CONCURRENT_CONNECTIONS=${MAX_CONCURRENT_CONNECTIONS:-400}
    command: >
      sh -c "npx prisma migrate deploy && node dist/server/server/index.js"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      ai-service:
        condition: service_healthy
    ports:
      - '3000:3000'
      - '3001:3001'
    volumes:
      - ./uploads:/app/uploads
      - ./logs:/app/logs
    restart: unless-stopped
    deploy:
      # Simulating production with 2 replicas
      # Note: For true multi-instance, need load balancer with sticky sessions
      replicas: 1 # Use 1 for staging; multi-instance requires external LB
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test:
        [
          'CMD',
          'node',
          '-e',
          "require('http').get('http://localhost:3000/health', (res) => { process.exit(res.statusCode === 200 ? 0 : 1) })",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - staging-network
    labels:
      - 'com.ringrift.environment=staging'
      - 'com.ringrift.component=app'

  # ===========================================================================
  # Nginx Reverse Proxy
  # ===========================================================================
  nginx:
    image: nginx:alpine
    ports:
      - '80:80'
      - '443:443'
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/ssl:ro
    depends_on:
      - app
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M
    healthcheck:
      test: ['CMD', 'wget', '-q', '--spider', 'http://localhost/health']
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - staging-network
    labels:
      - 'com.ringrift.environment=staging'
      - 'com.ringrift.component=nginx'

  # ===========================================================================
  # PostgreSQL with production-like configuration
  # ===========================================================================
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-ringrift}
      POSTGRES_USER: ${POSTGRES_USER:-ringrift}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-${DB_PASSWORD:-stagingpass}}
      # PostgreSQL tuning for production-like behavior
      POSTGRES_INITDB_ARGS: '--encoding=UTF8 --lc-collate=en_US.utf8 --lc-ctype=en_US.utf8'
    volumes:
      - postgres_staging_data:/var/lib/postgresql/data
      - ./backups:/backups
    ports:
      - '5432:5432'
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
    # PostgreSQL performance tuning for staging load testing
    command: >
      postgres
        -c max_connections=200
        -c shared_buffers=512MB
        -c effective_cache_size=1536MB
        -c maintenance_work_mem=128MB
        -c work_mem=16MB
        -c wal_buffers=16MB
        -c checkpoint_completion_target=0.9
        -c random_page_cost=1.1
        -c effective_io_concurrency=200
        -c min_wal_size=1GB
        -c max_wal_size=4GB
        -c log_min_duration_statement=500
        -c log_statement=none
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U ${POSTGRES_USER:-ringrift} -d ${POSTGRES_DB:-ringrift}']
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - staging-network
    labels:
      - 'com.ringrift.environment=staging'
      - 'com.ringrift.component=database'

  # ===========================================================================
  # Redis for session/cache with production config
  # ===========================================================================
  redis:
    image: redis:7-alpine
    command: >
      redis-server
        --maxmemory 256mb
        --maxmemory-policy allkeys-lru
        --appendonly yes
        --appendfsync everysec
        --save 900 1
        --save 300 10
        --save 60 10000
        --tcp-keepalive 300
        --timeout 0
    volumes:
      - redis_staging_data:/data
    ports:
      - '6379:6379'
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
    healthcheck:
      test: ['CMD', 'redis-cli', 'ping']
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - staging-network
    labels:
      - 'com.ringrift.environment=staging'
      - 'com.ringrift.component=cache'

  # ===========================================================================
  # Python AI Service (scaled for production)
  # ===========================================================================
  ai-service:
    build:
      context: ./ai-service
      dockerfile: Dockerfile
    environment:
      - PYTHON_ENV=production
      - LOG_LEVEL=${AI_LOG_LEVEL:-INFO}
      - AI_SERVICE_PORT=8001
      # AI performance configuration
      - AI_MAX_CONCURRENT_REQUESTS=${AI_MAX_CONCURRENT_REQUESTS:-50}
      - AI_REQUEST_TIMEOUT=${AI_REQUEST_TIMEOUT:-5000}
    ports:
      - '8001:8001'
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test:
        [
          'CMD',
          'python',
          '-c',
          "import os, httpx; port = os.getenv('AI_SERVICE_PORT', '8001'); resp = httpx.get(f'http://localhost:{port}/health', timeout=2.0); raise SystemExit(0 if resp.status_code == 200 else 1)",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    networks:
      - staging-network
    labels:
      - 'com.ringrift.environment=staging'
      - 'com.ringrift.component=ai-service'

  # ===========================================================================
  # Prometheus for metrics collection
  # ===========================================================================
  prometheus:
    image: prom/prometheus:v2.47.0
    ports:
      - '9090:9090'
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/prometheus/alerts.yml:/etc/prometheus/alerts.yml:ro
      - prometheus_staging_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=15d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    depends_on:
      - app
      - ai-service
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 256M
    healthcheck:
      test: ['CMD', 'wget', '-q', '--spider', 'http://localhost:9090/-/healthy']
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s
    networks:
      - staging-network
    labels:
      - 'com.ringrift.environment=staging'
      - 'com.ringrift.component=monitoring'

  # ===========================================================================
  # Alertmanager for alert routing
  # ===========================================================================
  alertmanager:
    image: prom/alertmanager:v0.26.0
    ports:
      - '9093:9093'
    volumes:
      - ./monitoring/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager_staging_data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.listen-address=:9093'
      - '--cluster.listen-address='
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 128M
        reservations:
          cpus: '0.1'
          memory: 64M
    healthcheck:
      test: ['CMD', 'wget', '-q', '--spider', 'http://localhost:9093/-/healthy']
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - staging-network
    labels:
      - 'com.ringrift.environment=staging'
      - 'com.ringrift.component=monitoring'

  # ===========================================================================
  # Grafana for visualization
  # ===========================================================================
  grafana:
    image: grafana/grafana:10.1.0
    ports:
      - '3002:3000'
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=${GRAFANA_ROOT_URL:-http://localhost:3002}
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource
      # Staging-specific settings
      - GF_DASHBOARDS_MIN_REFRESH_INTERVAL=5s
      - GF_ALERTING_ENABLED=true
    volumes:
      - grafana_staging_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
      - ./monitoring/grafana/provisioning/dashboards.yml:/etc/grafana/provisioning/dashboards/dashboards.yml:ro
      - ./monitoring/grafana/provisioning/datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml:ro
    depends_on:
      - prometheus
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
    healthcheck:
      test: ['CMD-SHELL', 'wget -q --spider http://localhost:3000/api/health || exit 1']
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    networks:
      - staging-network
    labels:
      - 'com.ringrift.environment=staging'
      - 'com.ringrift.component=monitoring'

# =============================================================================
# Volumes
# =============================================================================
volumes:
  postgres_staging_data:
    driver: local
  redis_staging_data:
    driver: local
  prometheus_staging_data:
    driver: local
  alertmanager_staging_data:
    driver: local
  grafana_staging_data:
    driver: local

# =============================================================================
# Networks
# =============================================================================
networks:
  staging-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16
